Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-06-02 17:23:42.700013: step 0, loss = 4.67 (16.3 examples/sec; 7.872 sec/batch)
2017-06-02 17:23:43.888070: step 10, loss = 4.62 (2471.3 examples/sec; 0.052 sec/batch)
2017-06-02 17:23:44.985888: step 20, loss = 4.49 (2224.4 examples/sec; 0.058 sec/batch)
2017-06-02 17:23:46.037420: step 30, loss = 4.33 (2543.9 examples/sec; 0.050 sec/batch)
2017-06-02 17:23:47.069964: step 40, loss = 4.48 (2833.6 examples/sec; 0.045 sec/batch)
2017-06-02 17:23:48.073033: step 50, loss = 4.21 (2325.6 examples/sec; 0.055 sec/batch)
2017-06-02 17:23:49.114901: step 60, loss = 4.23 (2659.1 examples/sec; 0.048 sec/batch)
2017-06-02 17:23:50.167381: step 70, loss = 4.19 (2606.8 examples/sec; 0.049 sec/batch)
2017-06-02 17:23:51.224285: step 80, loss = 4.07 (2336.5 examples/sec; 0.055 sec/batch)
2017-06-02 17:23:52.229884: step 90, loss = 4.03 (2536.0 examples/sec; 0.050 sec/batch)
2017-06-02 17:23:53.245077: step 100, loss = 4.04 (2486.0 examples/sec; 0.051 sec/batch)
2017-06-02 17:23:54.345816: step 110, loss = 4.01 (2674.9 examples/sec; 0.048 sec/batch)
2017-06-02 17:23:55.324449: step 120, loss = 4.11 (2498.6 examples/sec; 0.051 sec/batch)
2017-06-02 17:23:56.282774: step 130, loss = 4.05 (2357.4 examples/sec; 0.054 sec/batch)
2017-06-02 17:23:57.649959: step 140, loss = 3.92 (1976.4 examples/sec; 0.065 sec/batch)
2017-06-02 17:23:58.891082: step 150, loss = 3.83 (1945.6 examples/sec; 0.066 sec/batch)
2017-06-02 17:24:00.125823: step 160, loss = 3.80 (2104.7 examples/sec; 0.061 sec/batch)
2017-06-02 17:24:01.085908: step 170, loss = 3.93 (2437.6 examples/sec; 0.053 sec/batch)
2017-06-02 17:24:02.211449: step 180, loss = 3.76 (1991.6 examples/sec; 0.064 sec/batch)
2017-06-02 17:24:03.496777: step 190, loss = 3.97 (1805.5 examples/sec; 0.071 sec/batch)
2017-06-02 17:24:04.843862: step 200, loss = 3.74 (1661.7 examples/sec; 0.077 sec/batch)
2017-06-02 17:24:06.229578: step 210, loss = 3.60 (2603.0 examples/sec; 0.049 sec/batch)
2017-06-02 17:24:07.254663: step 220, loss = 3.62 (1914.4 examples/sec; 0.067 sec/batch)
2017-06-02 17:24:08.223558: step 230, loss = 3.62 (2656.6 examples/sec; 0.048 sec/batch)
2017-06-02 17:24:09.254171: step 240, loss = 3.60 (2720.5 examples/sec; 0.047 sec/batch)
2017-06-02 17:24:10.267643: step 250, loss = 3.66 (3267.6 examples/sec; 0.039 sec/batch)
2017-06-02 17:24:11.267115: step 260, loss = 3.45 (2538.6 examples/sec; 0.050 sec/batch)
2017-06-02 17:24:12.272908: step 270, loss = 3.95 (2695.0 examples/sec; 0.047 sec/batch)
2017-06-02 17:24:13.303739: step 280, loss = 3.65 (2441.9 examples/sec; 0.052 sec/batch)
2017-06-02 17:24:14.303705: step 290, loss = 3.62 (2126.8 examples/sec; 0.060 sec/batch)
2017-06-02 17:24:15.284044: step 300, loss = 3.45 (2581.3 examples/sec; 0.050 sec/batch)
2017-06-02 17:24:16.382928: step 310, loss = 3.50 (2720.0 examples/sec; 0.047 sec/batch)
2017-06-02 17:24:17.374478: step 320, loss = 3.52 (2761.7 examples/sec; 0.046 sec/batch)
2017-06-02 17:24:18.382016: step 330, loss = 3.43 (2752.8 examples/sec; 0.046 sec/batch)
2017-06-02 17:24:19.352659: step 340, loss = 3.39 (2196.3 examples/sec; 0.058 sec/batch)
2017-06-02 17:24:20.342473: step 350, loss = 3.48 (2777.7 examples/sec; 0.046 sec/batch)
2017-06-02 17:24:21.365343: step 360, loss = 3.40 (2999.4 examples/sec; 0.043 sec/batch)
2017-06-02 17:24:22.353424: step 370, loss = 3.24 (2618.1 examples/sec; 0.049 sec/batch)
2017-06-02 17:24:23.350751: step 380, loss = 3.42 (1935.1 examples/sec; 0.066 sec/batch)
2017-06-02 17:24:24.330445: step 390, loss = 3.30 (2701.0 examples/sec; 0.047 sec/batch)
2017-06-02 17:24:25.380356: step 400, loss = 3.29 (2808.1 examples/sec; 0.046 sec/batch)
2017-06-02 17:24:26.485006: step 410, loss = 3.37 (2781.3 examples/sec; 0.046 sec/batch)
2017-06-02 17:24:27.472592: step 420, loss = 3.26 (2349.0 examples/sec; 0.054 sec/batch)
2017-06-02 17:24:28.465039: step 430, loss = 3.26 (2949.7 examples/sec; 0.043 sec/batch)
2017-06-02 17:24:29.460132: step 440, loss = 3.18 (2870.8 examples/sec; 0.045 sec/batch)
2017-06-02 17:24:30.412019: step 450, loss = 3.28 (2647.3 examples/sec; 0.048 sec/batch)
2017-06-02 17:24:31.426794: step 460, loss = 3.03 (2155.2 examples/sec; 0.059 sec/batch)
2017-06-02 17:24:32.441194: step 470, loss = 3.10 (2737.3 examples/sec; 0.047 sec/batch)
2017-06-02 17:24:33.389831: step 480, loss = 3.15 (2795.9 examples/sec; 0.046 sec/batch)
2017-06-02 17:24:34.417113: step 490, loss = 3.18 (2636.5 examples/sec; 0.049 sec/batch)
2017-06-02 17:24:35.425061: step 500, loss = 3.21 (2822.1 examples/sec; 0.045 sec/batch)
2017-06-02 17:24:36.541631: step 510, loss = 3.39 (2413.1 examples/sec; 0.053 sec/batch)
2017-06-02 17:24:37.487724: step 520, loss = 3.01 (2675.5 examples/sec; 0.048 sec/batch)
2017-06-02 17:24:38.482027: step 530, loss = 3.10 (2758.2 examples/sec; 0.046 sec/batch)
2017-06-02 17:24:39.474162: step 540, loss = 3.06 (2603.5 examples/sec; 0.049 sec/batch)
2017-06-02 17:24:40.452389: step 550, loss = 3.04 (1967.8 examples/sec; 0.065 sec/batch)
2017-06-02 17:24:41.407294: step 560, loss = 3.00 (2739.6 examples/sec; 0.047 sec/batch)
2017-06-02 17:24:42.403561: step 570, loss = 3.07 (2718.6 examples/sec; 0.047 sec/batch)
2017-06-02 17:24:43.383940: step 580, loss = 3.06 (2145.6 examples/sec; 0.060 sec/batch)
2017-06-02 17:24:44.356943: step 590, loss = 2.88 (2770.6 examples/sec; 0.046 sec/batch)
2017-06-02 17:24:45.357722: step 600, loss = 3.03 (2732.5 examples/sec; 0.047 sec/batch)
2017-06-02 17:24:46.477585: step 610, loss = 2.87 (2854.3 examples/sec; 0.045 sec/batch)
2017-06-02 17:24:47.475182: step 620, loss = 2.94 (2393.8 examples/sec; 0.053 sec/batch)
2017-06-02 17:24:48.456255: step 630, loss = 2.92 (2977.1 examples/sec; 0.043 sec/batch)
2017-06-02 17:24:49.480422: step 640, loss = 3.27 (2852.8 examples/sec; 0.045 sec/batch)
2017-06-02 17:24:50.464594: step 650, loss = 2.76 (2318.8 examples/sec; 0.055 sec/batch)
2017-06-02 17:24:51.418520: step 660, loss = 2.87 (2978.8 examples/sec; 0.043 sec/batch)
2017-06-02 17:24:52.426007: step 670, loss = 2.82 (2970.9 examples/sec; 0.043 sec/batch)
2017-06-02 17:24:53.460090: step 680, loss = 2.65 (2808.1 examples/sec; 0.046 sec/batch)
2017-06-02 17:24:54.390332: step 690, loss = 2.66 (2718.6 examples/sec; 0.047 sec/batch)
2017-06-02 17:24:55.405771: step 700, loss = 2.87 (2449.4 examples/sec; 0.052 sec/batch)
2017-06-02 17:24:56.498029: step 710, loss = 2.76 (2536.4 examples/sec; 0.050 sec/batch)
2017-06-02 17:24:57.423557: step 720, loss = 2.77 (2881.7 examples/sec; 0.044 sec/batch)
2017-06-02 17:24:58.405893: step 730, loss = 2.80 (2777.4 examples/sec; 0.046 sec/batch)
2017-06-02 17:24:59.384533: step 740, loss = 2.70 (2829.8 examples/sec; 0.045 sec/batch)
2017-06-02 17:25:00.330911: step 750, loss = 2.55 (2714.6 examples/sec; 0.047 sec/batch)
2017-06-02 17:25:01.357011: step 760, loss = 2.70 (2448.5 examples/sec; 0.052 sec/batch)
2017-06-02 17:25:02.359889: step 770, loss = 2.81 (2661.3 examples/sec; 0.048 sec/batch)
2017-06-02 17:25:03.376174: step 780, loss = 2.55 (2431.0 examples/sec; 0.053 sec/batch)
2017-06-02 17:25:04.348222: step 790, loss = 2.49 (2926.6 examples/sec; 0.044 sec/batch)
2017-06-02 17:25:05.360562: step 800, loss = 2.56 (3580.3 examples/sec; 0.036 sec/batch)
2017-06-02 17:25:06.461108: step 810, loss = 2.78 (2665.6 examples/sec; 0.048 sec/batch)
2017-06-02 17:25:07.416064: step 820, loss = 2.76 (2764.9 examples/sec; 0.046 sec/batch)
2017-06-02 17:25:08.432955: step 830, loss = 2.42 (2753.9 examples/sec; 0.046 sec/batch)
2017-06-02 17:25:09.400691: step 840, loss = 2.61 (2713.2 examples/sec; 0.047 sec/batch)
2017-06-02 17:25:10.396353: step 850, loss = 2.52 (2430.1 examples/sec; 0.053 sec/batch)
2017-06-02 17:25:11.394551: step 860, loss = 2.56 (2750.2 examples/sec; 0.047 sec/batch)
2017-06-02 17:25:12.417906: step 870, loss = 2.74 (2688.9 examples/sec; 0.048 sec/batch)
2017-06-02 17:25:13.384153: step 880, loss = 2.50 (2096.3 examples/sec; 0.061 sec/batch)
2017-06-02 17:25:14.337545: step 890, loss = 2.46 (3037.1 examples/sec; 0.042 sec/batch)
2017-06-02 17:25:15.336426: step 900, loss = 2.43 (2720.6 examples/sec; 0.047 sec/batch)
2017-06-02 17:25:16.416967: step 910, loss = 2.41 (2840.4 examples/sec; 0.045 sec/batch)
2017-06-02 17:25:17.391903: step 920, loss = 2.70 (2620.3 examples/sec; 0.049 sec/batch)
2017-06-02 17:25:18.366582: step 930, loss = 2.29 (2700.4 examples/sec; 0.047 sec/batch)
2017-06-02 17:25:19.340035: step 940, loss = 2.29 (2750.6 examples/sec; 0.047 sec/batch)
2017-06-02 17:25:20.348227: step 950, loss = 2.40 (2595.2 examples/sec; 0.049 sec/batch)
2017-06-02 17:25:21.287042: step 960, loss = 2.56 (2617.0 examples/sec; 0.049 sec/batch)
2017-06-02 17:25:22.335291: step 970, loss = 2.48 (2445.1 examples/sec; 0.052 sec/batch)
2017-06-02 17:25:23.294771: step 980, loss = 2.49 (2832.3 examples/sec; 0.045 sec/batch)
2017-06-02 17:25:24.262014: step 990, loss = 2.48 (2673.4 examples/sec; 0.048 sec/batch)
2017-06-02 17:25:25.246058: step 1000, loss = 2.33 (2851.0 examples/sec; 0.045 sec/batch)
2017-06-02 17:25:26.476660: step 1010, loss = 2.33 (2721.0 examples/sec; 0.047 sec/batch)
2017-06-02 17:25:27.456030: step 1020, loss = 2.53 (2354.7 examples/sec; 0.054 sec/batch)
2017-06-02 17:25:28.520775: step 1030, loss = 2.32 (2752.5 examples/sec; 0.047 sec/batch)
2017-06-02 17:25:29.493359: step 1040, loss = 2.41 (2489.0 examples/sec; 0.051 sec/batch)
2017-06-02 17:25:30.465275: step 1050, loss = 2.44 (2623.9 examples/sec; 0.049 sec/batch)
2017-06-02 17:25:31.467243: step 1060, loss = 2.28 (2817.1 examples/sec; 0.045 sec/batch)
2017-06-02 17:25:32.453920: step 1070, loss = 2.34 (2201.0 examples/sec; 0.058 sec/batch)
2017-06-02 17:25:33.449250: step 1080, loss = 2.27 (2423.8 examples/sec; 0.053 sec/batch)
2017-06-02 17:25:34.423256: step 1090, loss = 2.20 (2624.9 examples/sec; 0.049 sec/batch)
2017-06-02 17:25:35.453767: step 1100, loss = 2.45 (2709.5 examples/sec; 0.047 sec/batch)
2017-06-02 17:25:36.535696: step 1110, loss = 2.22 (3211.8 examples/sec; 0.040 sec/batch)
2017-06-02 17:25:37.533759: step 1120, loss = 2.06 (3027.3 examples/sec; 0.042 sec/batch)
2017-06-02 17:25:38.506963: step 1130, loss = 2.33 (2806.9 examples/sec; 0.046 sec/batch)
2017-06-02 17:25:39.538223: step 1140, loss = 2.21 (2635.1 examples/sec; 0.049 sec/batch)
2017-06-02 17:25:40.495795: step 1150, loss = 2.09 (2651.8 examples/sec; 0.048 sec/batch)
2017-06-02 17:25:41.500761: step 1160, loss = 2.18 (2817.8 examples/sec; 0.045 sec/batch)
2017-06-02 17:25:42.541011: step 1170, loss = 2.21 (2202.8 examples/sec; 0.058 sec/batch)
2017-06-02 17:25:43.536490: step 1180, loss = 2.29 (2792.7 examples/sec; 0.046 sec/batch)
2017-06-02 17:25:44.557579: step 1190, loss = 2.05 (2264.7 examples/sec; 0.057 sec/batch)
2017-06-02 17:25:45.533681: step 1200, loss = 2.06 (2758.5 examples/sec; 0.046 sec/batch)
2017-06-02 17:25:46.631395: step 1210, loss = 2.10 (1930.6 examples/sec; 0.066 sec/batch)
2017-06-02 17:25:47.603023: step 1220, loss = 2.08 (2520.4 examples/sec; 0.051 sec/batch)
2017-06-02 17:25:48.600661: step 1230, loss = 2.12 (2495.7 examples/sec; 0.051 sec/batch)
2017-06-02 17:25:49.644680: step 1240, loss = 2.10 (2367.8 examples/sec; 0.054 sec/batch)
2017-06-02 17:25:50.644804: step 1250, loss = 2.27 (2543.4 examples/sec; 0.050 sec/batch)
2017-06-02 17:25:51.618518: step 1260, loss = 2.21 (2806.9 examples/sec; 0.046 sec/batch)
2017-06-02 17:25:52.610772: step 1270, loss = 2.12 (1994.5 examples/sec; 0.064 sec/batch)
2017-06-02 17:25:53.541640: step 1280, loss = 1.93 (2840.4 examples/sec; 0.045 sec/batch)
2017-06-02 17:25:54.540502: step 1290, loss = 2.00 (2158.1 examples/sec; 0.059 sec/batch)
2017-06-02 17:25:55.544636: step 1300, loss = 1.90 (2175.2 examples/sec; 0.059 sec/batch)
2017-06-02 17:25:56.616033: step 1310, loss = 2.14 (2873.9 examples/sec; 0.045 sec/batch)
2017-06-02 17:25:57.600593: step 1320, loss = 2.17 (2636.1 examples/sec; 0.049 sec/batch)
2017-06-02 17:25:58.589140: step 1330, loss = 1.96 (2624.8 examples/sec; 0.049 sec/batch)
2017-06-02 17:25:59.565224: step 1340, loss = 2.04 (1985.2 examples/sec; 0.064 sec/batch)
2017-06-02 17:26:00.500884: step 1350, loss = 2.03 (2847.5 examples/sec; 0.045 sec/batch)
2017-06-02 17:26:01.480459: step 1360, loss = 2.15 (2518.6 examples/sec; 0.051 sec/batch)
2017-06-02 17:26:02.473379: step 1370, loss = 1.91 (2648.2 examples/sec; 0.048 sec/batch)
2017-06-02 17:26:03.411964: step 1380, loss = 1.93 (2595.1 examples/sec; 0.049 sec/batch)
2017-06-02 17:26:04.391048: step 1390, loss = 2.08 (2770.7 examples/sec; 0.046 sec/batch)
2017-06-02 17:26:05.368830: step 1400, loss = 1.83 (2746.8 examples/sec; 0.047 sec/batch)
2017-06-02 17:26:06.457934: step 1410, loss = 1.90 (2620.6 examples/sec; 0.049 sec/batch)
2017-06-02 17:26:07.429116: step 1420, loss = 2.01 (2367.5 examples/sec; 0.054 sec/batch)
2017-06-02 17:26:08.400764: step 1430, loss = 1.79 (2786.9 examples/sec; 0.046 sec/batch)
2017-06-02 17:26:09.397142: step 1440, loss = 1.83 (2497.9 examples/sec; 0.051 sec/batch)
2017-06-02 17:26:10.410262: step 1450, loss = 1.97 (2654.1 examples/sec; 0.048 sec/batch)
2017-06-02 17:26:11.366449: step 1460, loss = 2.00 (2534.6 examples/sec; 0.051 sec/batch)
2017-06-02 17:26:12.409297: step 1470, loss = 2.20 (2477.5 examples/sec; 0.052 sec/batch)
2017-06-02 17:26:13.385354: step 1480, loss = 1.87 (2781.8 examples/sec; 0.046 sec/batch)
2017-06-02 17:26:14.368426: step 1490, loss = 1.93 (2597.0 examples/sec; 0.049 sec/batch)
2017-06-02 17:26:15.302069: step 1500, loss = 1.79 (2910.1 examples/sec; 0.044 sec/batch)
2017-06-02 17:26:16.369673: step 1510, loss = 2.13 (2656.8 examples/sec; 0.048 sec/batch)
2017-06-02 17:26:17.328754: step 1520, loss = 2.11 (2942.9 examples/sec; 0.043 sec/batch)
2017-06-02 17:26:18.255463: step 1530, loss = 1.90 (2486.2 examples/sec; 0.051 sec/batch)
2017-06-02 17:26:19.227002: step 1540, loss = 1.88 (2863.7 examples/sec; 0.045 sec/batch)
2017-06-02 17:26:20.214337: step 1550, loss = 1.90 (2872.9 examples/sec; 0.045 sec/batch)
2017-06-02 17:26:21.128011: step 1560, loss = 1.77 (2839.5 examples/sec; 0.045 sec/batch)
2017-06-02 17:26:22.102259: step 1570, loss = 1.82 (2720.1 examples/sec; 0.047 sec/batch)
2017-06-02 17:26:23.087399: step 1580, loss = 1.94 (2359.2 examples/sec; 0.054 sec/batch)
2017-06-02 17:26:24.056756: step 1590, loss = 1.55 (2296.5 examples/sec; 0.056 sec/batch)
2017-06-02 17:26:25.085037: step 1600, loss = 1.94 (2290.1 examples/sec; 0.056 sec/batch)
2017-06-02 17:26:26.203221: step 1610, loss = 1.88 (2568.3 examples/sec; 0.050 sec/batch)
2017-06-02 17:26:27.197627: step 1620, loss = 2.03 (2759.3 examples/sec; 0.046 sec/batch)
2017-06-02 17:26:28.168470: step 1630, loss = 1.79 (2863.9 examples/sec; 0.045 sec/batch)
2017-06-02 17:26:29.170030: step 1640, loss = 1.73 (2387.2 examples/sec; 0.054 sec/batch)
2017-06-02 17:26:30.152471: step 1650, loss = 1.65 (2639.2 examples/sec; 0.049 sec/batch)
2017-06-02 17:26:31.103315: step 1660, loss = 1.79 (2825.0 examples/sec; 0.045 sec/batch)
2017-06-02 17:26:32.137943: step 1670, loss = 1.73 (2899.4 examples/sec; 0.044 sec/batch)
2017-06-02 17:26:33.102885: step 1680, loss = 1.93 (2775.3 examples/sec; 0.046 sec/batch)
2017-06-02 17:26:34.041972: step 1690, loss = 1.85 (2481.8 examples/sec; 0.052 sec/batch)
2017-06-02 17:26:35.051751: step 1700, loss = 1.77 (2604.1 examples/sec; 0.049 sec/batch)
2017-06-02 17:26:36.136962: step 1710, loss = 1.87 (2508.9 examples/sec; 0.051 sec/batch)
2017-06-02 17:26:37.105495: step 1720, loss = 1.71 (3005.0 examples/sec; 0.043 sec/batch)
2017-06-02 17:26:38.056350: step 1730, loss = 1.87 (2815.7 examples/sec; 0.045 sec/batch)
2017-06-02 17:26:39.004069: step 1740, loss = 1.81 (2867.5 examples/sec; 0.045 sec/batch)
2017-06-02 17:26:39.976407: step 1750, loss = 1.67 (2799.3 examples/sec; 0.046 sec/batch)
2017-06-02 17:26:40.910876: step 1760, loss = 1.73 (2839.1 examples/sec; 0.045 sec/batch)
2017-06-02 17:26:41.937566: step 1770, loss = 1.76 (2575.8 examples/sec; 0.050 sec/batch)
2017-06-02 17:26:42.904721: step 1780, loss = 1.76 (2719.8 examples/sec; 0.047 sec/batch)
2017-06-02 17:26:43.868944: step 1790, loss = 1.67 (3010.0 examples/sec; 0.043 sec/batch)
2017-06-02 17:26:44.772064: step 1800, loss = 1.75 (2664.5 examples/sec; 0.048 sec/batch)
2017-06-02 17:26:45.822100: step 1810, loss = 1.62 (2863.6 examples/sec; 0.045 sec/batch)
2017-06-02 17:26:46.784941: step 1820, loss = 1.71 (2663.6 examples/sec; 0.048 sec/batch)
2017-06-02 17:26:47.790938: step 1830, loss = 1.75 (2773.5 examples/sec; 0.046 sec/batch)
2017-06-02 17:26:48.764356: step 1840, loss = 1.89 (2807.8 examples/sec; 0.046 sec/batch)
2017-06-02 17:26:49.743822: step 1850, loss = 1.68 (2898.1 examples/sec; 0.044 sec/batch)
2017-06-02 17:26:50.710705: step 1860, loss = 1.82 (2813.2 examples/sec; 0.046 sec/batch)
2017-06-02 17:26:51.715002: step 1870, loss = 1.84 (2315.5 examples/sec; 0.055 sec/batch)
2017-06-02 17:26:52.641318: step 1880, loss = 1.63 (2477.8 examples/sec; 0.052 sec/batch)
2017-06-02 17:26:53.680068: step 1890, loss = 1.64 (2623.8 examples/sec; 0.049 sec/batch)
2017-06-02 17:26:54.676574: step 1900, loss = 1.51 (2798.0 examples/sec; 0.046 sec/batch)
2017-06-02 17:26:55.787060: step 1910, loss = 1.75 (2803.1 examples/sec; 0.046 sec/batch)
2017-06-02 17:26:56.768865: step 1920, loss = 1.58 (2854.1 examples/sec; 0.045 sec/batch)
2017-06-02 17:26:57.753748: step 1930, loss = 1.58 (2608.1 examples/sec; 0.049 sec/batch)
2017-06-02 17:26:58.713973: step 1940, loss = 1.48 (2591.6 examples/sec; 0.049 sec/batch)
2017-06-02 17:26:59.673569: step 1950, loss = 1.50 (2052.2 examples/sec; 0.062 sec/batch)
2017-06-02 17:27:00.642242: step 1960, loss = 1.70 (2589.4 examples/sec; 0.049 sec/batch)
2017-06-02 17:27:01.611470: step 1970, loss = 1.41 (2512.3 examples/sec; 0.051 sec/batch)
2017-06-02 17:27:02.600898: step 1980, loss = 1.59 (2717.7 examples/sec; 0.047 sec/batch)
2017-06-02 17:27:03.562186: step 1990, loss = 1.43 (2813.3 examples/sec; 0.045 sec/batch)
2017-06-02 17:27:04.552791: step 2000, loss = 1.48 (2753.9 examples/sec; 0.046 sec/batch)
2017-06-02 17:27:05.698305: step 2010, loss = 1.54 (2824.4 examples/sec; 0.045 sec/batch)
2017-06-02 17:27:06.644775: step 2020, loss = 1.78 (2361.4 examples/sec; 0.054 sec/batch)
2017-06-02 17:27:07.693266: step 2030, loss = 1.44 (2430.0 examples/sec; 0.053 sec/batch)
2017-06-02 17:27:08.689222: step 2040, loss = 1.53 (2549.5 examples/sec; 0.050 sec/batch)
2017-06-02 17:27:09.658239: step 2050, loss = 1.69 (2753.3 examples/sec; 0.046 sec/batch)
2017-06-02 17:27:10.610389: step 2060, loss = 1.52 (2559.3 examples/sec; 0.050 sec/batch)
2017-06-02 17:27:11.636855: step 2070, loss = 1.40 (2690.7 examples/sec; 0.048 sec/batch)
2017-06-02 17:27:12.614657: step 2080, loss = 1.48 (2769.0 examples/sec; 0.046 sec/batch)
2017-06-02 17:27:13.655176: step 2090, loss = 1.64 (2632.2 examples/sec; 0.049 sec/batch)
2017-06-02 17:27:14.634683: step 2100, loss = 1.57 (2223.6 examples/sec; 0.058 sec/batch)
2017-06-02 17:27:15.673199: step 2110, loss = 1.59 (2497.2 examples/sec; 0.051 sec/batch)
2017-06-02 17:27:16.669661: step 2120, loss = 1.63 (2418.6 examples/sec; 0.053 sec/batch)
2017-06-02 17:27:17.655485: step 2130, loss = 1.64 (2766.7 examples/sec; 0.046 sec/batch)
2017-06-02 17:27:18.709289: step 2140, loss = 1.48 (2327.0 examples/sec; 0.055 sec/batch)
2017-06-02 17:27:19.714044: step 2150, loss = 1.60 (2492.5 examples/sec; 0.051 sec/batch)
2017-06-02 17:27:20.783568: step 2160, loss = 1.64 (2731.3 examples/sec; 0.047 sec/batch)
2017-06-02 17:27:21.707894: step 2170, loss = 1.49 (2751.8 examples/sec; 0.047 sec/batch)
2017-06-02 17:27:22.682251: step 2180, loss = 1.48 (2850.6 examples/sec; 0.045 sec/batch)
2017-06-02 17:27:23.687553: step 2190, loss = 1.55 (2563.1 examples/sec; 0.050 sec/batch)
2017-06-02 17:27:24.645292: step 2200, loss = 1.45 (2792.4 examples/sec; 0.046 sec/batch)
2017-06-02 17:27:25.756563: step 2210, loss = 1.56 (2560.6 examples/sec; 0.050 sec/batch)
2017-06-02 17:27:26.755457: step 2220, loss = 1.45 (2470.5 examples/sec; 0.052 sec/batch)
2017-06-02 17:27:27.732160: step 2230, loss = 1.41 (2020.7 examples/sec; 0.063 sec/batch)
2017-06-02 17:27:28.730789: step 2240, loss = 1.48 (2738.3 examples/sec; 0.047 sec/batch)
2017-06-02 17:27:29.775105: step 2250, loss = 1.48 (2511.0 examples/sec; 0.051 sec/batch)
2017-06-02 17:27:30.852875: step 2260, loss = 1.51 (2472.8 examples/sec; 0.052 sec/batch)
2017-06-02 17:27:31.826871: step 2270, loss = 1.35 (2623.7 examples/sec; 0.049 sec/batch)
2017-06-02 17:27:32.836093: step 2280, loss = 1.39 (2462.5 examples/sec; 0.052 sec/batch)
2017-06-02 17:27:33.835559: step 2290, loss = 1.53 (2540.3 examples/sec; 0.050 sec/batch)
2017-06-02 17:27:34.851755: step 2300, loss = 1.36 (2286.6 examples/sec; 0.056 sec/batch)
2017-06-02 17:27:35.928428: step 2310, loss = 1.30 (2275.0 examples/sec; 0.056 sec/batch)
2017-06-02 17:27:36.930618: step 2320, loss = 1.45 (2889.3 examples/sec; 0.044 sec/batch)
2017-06-02 17:27:37.928761: step 2330, loss = 1.42 (2739.3 examples/sec; 0.047 sec/batch)
2017-06-02 17:27:38.942034: step 2340, loss = 1.34 (2585.7 examples/sec; 0.050 sec/batch)
2017-06-02 17:27:40.048767: step 2350, loss = 1.39 (2484.7 examples/sec; 0.052 sec/batch)
2017-06-02 17:27:41.062132: step 2360, loss = 1.56 (2538.2 examples/sec; 0.050 sec/batch)
2017-06-02 17:27:42.046472: step 2370, loss = 1.43 (2781.3 examples/sec; 0.046 sec/batch)
2017-06-02 17:27:43.055631: step 2380, loss = 1.41 (2724.6 examples/sec; 0.047 sec/batch)
2017-06-02 17:27:44.103340: step 2390, loss = 1.37 (2968.7 examples/sec; 0.043 sec/batch)
2017-06-02 17:27:45.074915: step 2400, loss = 1.48 (2174.1 examples/sec; 0.059 sec/batch)
2017-06-02 17:27:46.188808: step 2410, loss = 1.49 (2212.1 examples/sec; 0.058 sec/batch)
2017-06-02 17:27:47.193429: step 2420, loss = 1.25 (2757.9 examples/sec; 0.046 sec/batch)
2017-06-02 17:27:48.239265: step 2430, loss = 1.44 (2375.0 examples/sec; 0.054 sec/batch)
2017-06-02 17:27:49.237511: step 2440, loss = 1.39 (2198.1 examples/sec; 0.058 sec/batch)
2017-06-02 17:27:50.218635: step 2450, loss = 1.33 (2835.2 examples/sec; 0.045 sec/batch)
2017-06-02 17:27:51.227610: step 2460, loss = 1.04 (2646.3 examples/sec; 0.048 sec/batch)
2017-06-02 17:27:52.227297: step 2470, loss = 1.42 (2846.7 examples/sec; 0.045 sec/batch)
2017-06-02 17:27:53.167023: step 2480, loss = 1.42 (2604.8 examples/sec; 0.049 sec/batch)
2017-06-02 17:27:54.228395: step 2490, loss = 1.27 (2843.8 examples/sec; 0.045 sec/batch)
2017-06-02 17:27:55.229176: step 2500, loss = 1.46 (2713.3 examples/sec; 0.047 sec/batch)
2017-06-02 17:27:56.320787: step 2510, loss = 1.22 (2458.7 examples/sec; 0.052 sec/batch)
2017-06-02 17:27:57.265033: step 2520, loss = 1.43 (2901.7 examples/sec; 0.044 sec/batch)
2017-06-02 17:27:58.233736: step 2530, loss = 1.32 (2656.1 examples/sec; 0.048 sec/batch)
2017-06-02 17:27:59.198079: step 2540, loss = 1.37 (2874.9 examples/sec; 0.045 sec/batch)
2017-06-02 17:28:00.155100: step 2550, loss = 1.24 (2345.0 examples/sec; 0.055 sec/batch)
2017-06-02 17:28:01.158153: step 2560, loss = 1.19 (2564.6 examples/sec; 0.050 sec/batch)
2017-06-02 17:28:02.180613: step 2570, loss = 1.26 (2840.8 examples/sec; 0.045 sec/batch)
2017-06-02 17:28:03.180835: step 2580, loss = 1.22 (2782.7 examples/sec; 0.046 sec/batch)
2017-06-02 17:28:04.174110: step 2590, loss = 1.33 (2644.8 examples/sec; 0.048 sec/batch)
2017-06-02 17:28:05.174692: step 2600, loss = 1.17 (2854.4 examples/sec; 0.045 sec/batch)
2017-06-02 17:28:06.274445: step 2610, loss = 1.27 (2533.7 examples/sec; 0.051 sec/batch)
2017-06-02 17:28:07.279450: step 2620, loss = 1.35 (2898.1 examples/sec; 0.044 sec/batch)
2017-06-02 17:28:08.223595: step 2630, loss = 1.58 (2589.8 examples/sec; 0.049 sec/batch)
2017-06-02 17:28:09.200773: step 2640, loss = 1.32 (2396.2 examples/sec; 0.053 sec/batch)
2017-06-02 17:28:10.233427: step 2650, loss = 1.24 (2543.8 examples/sec; 0.050 sec/batch)
2017-06-02 17:28:11.188092: step 2660, loss = 1.32 (2789.3 examples/sec; 0.046 sec/batch)
2017-06-02 17:28:12.191549: step 2670, loss = 1.36 (2780.6 examples/sec; 0.046 sec/batch)
2017-06-02 17:28:13.216718: step 2680, loss = 1.22 (2530.4 examples/sec; 0.051 sec/batch)
2017-06-02 17:28:14.200054: step 2690, loss = 1.36 (2731.5 examples/sec; 0.047 sec/batch)
2017-06-02 17:28:15.167116: step 2700, loss = 1.22 (2207.8 examples/sec; 0.058 sec/batch)
2017-06-02 17:28:16.187025: step 2710, loss = 1.24 (2793.2 examples/sec; 0.046 sec/batch)
2017-06-02 17:28:17.171831: step 2720, loss = 1.31 (2778.1 examples/sec; 0.046 sec/batch)
2017-06-02 17:28:18.164509: step 2730, loss = 1.10 (2808.2 examples/sec; 0.046 sec/batch)
2017-06-02 17:28:19.112169: step 2740, loss = 1.12 (2767.3 examples/sec; 0.046 sec/batch)
2017-06-02 17:28:20.103902: step 2750, loss = 1.27 (2500.5 examples/sec; 0.051 sec/batch)
2017-06-02 17:28:21.124895: step 2760, loss = 1.30 (2418.7 examples/sec; 0.053 sec/batch)
2017-06-02 17:28:22.106484: step 2770, loss = 1.27 (2250.2 examples/sec; 0.057 sec/batch)
2017-06-02 17:28:23.077156: step 2780, loss = 1.27 (2529.3 examples/sec; 0.051 sec/batch)
2017-06-02 17:28:24.069554: step 2790, loss = 1.41 (2525.6 examples/sec; 0.051 sec/batch)
2017-06-02 17:28:25.073633: step 2800, loss = 1.23 (1987.4 examples/sec; 0.064 sec/batch)
2017-06-02 17:28:26.135799: step 2810, loss = 1.21 (2575.7 examples/sec; 0.050 sec/batch)
2017-06-02 17:28:27.140961: step 2820, loss = 1.50 (2627.0 examples/sec; 0.049 sec/batch)
2017-06-02 17:28:28.170367: step 2830, loss = 1.13 (2318.0 examples/sec; 0.055 sec/batch)
2017-06-02 17:28:29.116396: step 2840, loss = 1.17 (2888.4 examples/sec; 0.044 sec/batch)
2017-06-02 17:28:30.117179: step 2850, loss = 1.19 (2453.7 examples/sec; 0.052 sec/batch)
2017-06-02 17:28:31.109690: step 2860, loss = 1.30 (2742.6 examples/sec; 0.047 sec/batch)
2017-06-02 17:28:32.088654: step 2870, loss = 1.09 (2550.4 examples/sec; 0.050 sec/batch)
2017-06-02 17:28:33.059254: step 2880, loss = 1.26 (2305.6 examples/sec; 0.056 sec/batch)
2017-06-02 17:28:34.066571: step 2890, loss = 1.12 (2828.5 examples/sec; 0.045 sec/batch)
2017-06-02 17:28:35.044246: step 2900, loss = 1.31 (2548.6 examples/sec; 0.050 sec/batch)
2017-06-02 17:28:36.110540: step 2910, loss = 1.24 (2763.8 examples/sec; 0.046 sec/batch)
2017-06-02 17:28:37.091182: step 2920, loss = 1.30 (2927.4 examples/sec; 0.044 sec/batch)
2017-06-02 17:28:38.099775: step 2930, loss = 1.22 (2733.8 examples/sec; 0.047 sec/batch)
2017-06-02 17:28:39.046424: step 2940, loss = 1.11 (2251.3 examples/sec; 0.057 sec/batch)
2017-06-02 17:28:40.057251: step 2950, loss = 1.15 (2740.0 examples/sec; 0.047 sec/batch)
2017-06-02 17:28:41.059041: step 2960, loss = 1.27 (2307.4 examples/sec; 0.055 sec/batch)
2017-06-02 17:28:42.051502: step 2970, loss = 1.26 (2647.6 examples/sec; 0.048 sec/batch)
2017-06-02 17:28:42.982688: step 2980, loss = 1.23 (2709.4 examples/sec; 0.047 sec/batch)
2017-06-02 17:28:43.945654: step 2990, loss = 1.14 (2752.1 examples/sec; 0.047 sec/batch)
2017-06-02 17:28:44.980062: step 3000, loss = 1.35 (2630.7 examples/sec; 0.049 sec/batch)
2017-06-02 17:28:46.147789: step 3010, loss = 1.21 (2500.0 examples/sec; 0.051 sec/batch)
2017-06-02 17:28:47.090938: step 3020, loss = 1.18 (2400.6 examples/sec; 0.053 sec/batch)
2017-06-02 17:28:48.124806: step 3030, loss = 1.08 (2699.5 examples/sec; 0.047 sec/batch)
2017-06-02 17:28:49.104602: step 3040, loss = 1.02 (2375.6 examples/sec; 0.054 sec/batch)
2017-06-02 17:28:50.160204: step 3050, loss = 1.31 (2753.4 examples/sec; 0.046 sec/batch)
2017-06-02 17:28:51.113534: step 3060, loss = 1.16 (2773.2 examples/sec; 0.046 sec/batch)
2017-06-02 17:28:52.056371: step 3070, loss = 1.26 (2624.3 examples/sec; 0.049 sec/batch)
2017-06-02 17:28:53.047213: step 3080, loss = 1.39 (2811.1 examples/sec; 0.046 sec/batch)
2017-06-02 17:28:54.084768: step 3090, loss = 1.05 (2681.4 examples/sec; 0.048 sec/batch)
2017-06-02 17:28:55.063033: step 3100, loss = 1.07 (2245.9 examples/sec; 0.057 sec/batch)
2017-06-02 17:28:56.125262: step 3110, loss = 1.23 (2838.7 examples/sec; 0.045 sec/batch)
2017-06-02 17:28:57.134228: step 3120, loss = 1.04 (2584.0 examples/sec; 0.050 sec/batch)
2017-06-02 17:28:58.120937: step 3130, loss = 1.21 (2635.6 examples/sec; 0.049 sec/batch)
2017-06-02 17:28:59.105863: step 3140, loss = 1.10 (2613.2 examples/sec; 0.049 sec/batch)
2017-06-02 17:29:00.056797: step 3150, loss = 1.11 (2915.2 examples/sec; 0.044 sec/batch)
2017-06-02 17:29:00.974751: step 3160, loss = 1.14 (2777.4 examples/sec; 0.046 sec/batch)
2017-06-02 17:29:01.922302: step 3170, loss = 1.18 (2850.5 examples/sec; 0.045 sec/batch)
2017-06-02 17:29:02.833854: step 3180, loss = 1.19 (2861.1 examples/sec; 0.045 sec/batch)
2017-06-02 17:29:03.749167: step 3190, loss = 1.11 (2793.4 examples/sec; 0.046 sec/batch)
2017-06-02 17:29:04.816743: step 3200, loss = 1.17 (2086.9 examples/sec; 0.061 sec/batch)
2017-06-02 17:29:06.256490: step 3210, loss = 1.13 (1978.5 examples/sec; 0.065 sec/batch)
2017-06-02 17:29:07.556299: step 3220, loss = 1.20 (1967.0 examples/sec; 0.065 sec/batch)
2017-06-02 17:29:08.539423: step 3230, loss = 1.08 (2651.9 examples/sec; 0.048 sec/batch)
2017-06-02 17:29:09.755358: step 3240, loss = 1.11 (1919.9 examples/sec; 0.067 sec/batch)
2017-06-02 17:29:11.074302: step 3250, loss = 1.38 (1828.7 examples/sec; 0.070 sec/batch)
2017-06-02 17:29:12.365614: step 3260, loss = 0.92 (1886.0 examples/sec; 0.068 sec/batch)
2017-06-02 17:29:13.436677: step 3270, loss = 1.11 (2542.6 examples/sec; 0.050 sec/batch)
2017-06-02 17:29:14.424779: step 3280, loss = 1.18 (2451.8 examples/sec; 0.052 sec/batch)
2017-06-02 17:29:15.404216: step 3290, loss = 1.23 (2931.5 examples/sec; 0.044 sec/batch)
2017-06-02 17:29:16.364677: step 3300, loss = 1.29 (2659.6 examples/sec; 0.048 sec/batch)
2017-06-02 17:29:17.437262: step 3310, loss = 1.20 (2256.1 examples/sec; 0.057 sec/batch)
2017-06-02 17:29:18.375579: step 3320, loss = 0.92 (2804.2 examples/sec; 0.046 sec/batch)
2017-06-02 17:29:19.361739: step 3330, loss = 1.18 (2413.6 examples/sec; 0.053 sec/batch)
2017-06-02 17:29:20.344364: step 3340, loss = 0.96 (2728.1 examples/sec; 0.047 sec/batch)
2017-06-02 17:29:21.291479: step 3350, loss = 1.15 (2213.5 examples/sec; 0.058 sec/batch)
2017-06-02 17:29:22.264056: step 3360, loss = 1.03 (2850.6 examples/sec; 0.045 sec/batch)
2017-06-02 17:29:23.280848: step 3370, loss = 0.97 (2396.8 examples/sec; 0.053 sec/batch)
2017-06-02 17:29:24.251507: step 3380, loss = 1.29 (2696.7 examples/sec; 0.047 sec/batch)
2017-06-02 17:29:25.222368: step 3390, loss = 1.24 (2823.4 examples/sec; 0.045 sec/batch)
2017-06-02 17:29:26.147947: step 3400, loss = 0.96 (2880.1 examples/sec; 0.044 sec/batch)
2017-06-02 17:29:27.221024: step 3410, loss = 1.28 (2586.0 examples/sec; 0.049 sec/batch)
2017-06-02 17:29:28.220209: step 3420, loss = 1.03 (2577.8 examples/sec; 0.050 sec/batch)
2017-06-02 17:29:29.185746: step 3430, loss = 1.06 (2929.3 examples/sec; 0.044 sec/batch)
2017-06-02 17:29:30.171426: step 3440, loss = 1.41 (2896.4 examples/sec; 0.044 sec/batch)
2017-06-02 17:29:31.141690: step 3450, loss = 0.99 (2783.0 examples/sec; 0.046 sec/batch)
2017-06-02 17:29:32.107401: step 3460, loss = 1.18 (2596.9 examples/sec; 0.049 sec/batch)
2017-06-02 17:29:33.088895: step 3470, loss = 1.21 (2720.2 examples/sec; 0.047 sec/batch)
2017-06-02 17:29:34.048977: step 3480, loss = 1.09 (2590.2 examples/sec; 0.049 sec/batch)
2017-06-02 17:29:34.998076: step 3490, loss = 0.98 (2529.6 examples/sec; 0.051 sec/batch)
2017-06-02 17:29:35.965172: step 3500, loss = 1.17 (2814.3 examples/sec; 0.045 sec/batch)
2017-06-02 17:29:37.055365: step 3510, loss = 1.02 (2995.9 examples/sec; 0.043 sec/batch)
2017-06-02 17:29:38.019738: step 3520, loss = 1.28 (2803.2 examples/sec; 0.046 sec/batch)
2017-06-02 17:29:39.024485: step 3530, loss = 0.94 (2507.5 examples/sec; 0.051 sec/batch)
2017-06-02 17:29:40.066541: step 3540, loss = 1.18 (1930.6 examples/sec; 0.066 sec/batch)
2017-06-02 17:29:41.055545: step 3550, loss = 1.13 (2333.1 examples/sec; 0.055 sec/batch)
2017-06-02 17:29:42.074637: step 3560, loss = 1.35 (2564.1 examples/sec; 0.050 sec/batch)
2017-06-02 17:29:43.093815: step 3570, loss = 1.33 (2468.0 examples/sec; 0.052 sec/batch)
2017-06-02 17:29:44.084267: step 3580, loss = 0.93 (2593.2 examples/sec; 0.049 sec/batch)
2017-06-02 17:29:45.037272: step 3590, loss = 1.08 (2311.7 examples/sec; 0.055 sec/batch)
2017-06-02 17:29:46.003927: step 3600, loss = 1.14 (2820.8 examples/sec; 0.045 sec/batch)
2017-06-02 17:29:47.086209: step 3610, loss = 1.03 (2945.7 examples/sec; 0.043 sec/batch)
2017-06-02 17:29:48.072462: step 3620, loss = 1.06 (2835.2 examples/sec; 0.045 sec/batch)
2017-06-02 17:29:49.102743: step 3630, loss = 1.03 (2576.1 examples/sec; 0.050 sec/batch)
2017-06-02 17:29:50.082244: step 3640, loss = 1.00 (1815.0 examples/sec; 0.071 sec/batch)
2017-06-02 17:29:51.020893: step 3650, loss = 1.21 (2686.7 examples/sec; 0.048 sec/batch)
2017-06-02 17:29:51.979673: step 3660, loss = 1.11 (2837.1 examples/sec; 0.045 sec/batch)
2017-06-02 17:29:52.982168: step 3670, loss = 0.88 (2644.5 examples/sec; 0.048 sec/batch)
2017-06-02 17:29:53.915326: step 3680, loss = 1.07 (2716.1 examples/sec; 0.047 sec/batch)
2017-06-02 17:29:54.946213: step 3690, loss = 1.09 (2930.1 examples/sec; 0.044 sec/batch)
2017-06-02 17:29:55.949178: step 3700, loss = 0.92 (2892.3 examples/sec; 0.044 sec/batch)
2017-06-02 17:29:57.001487: step 3710, loss = 1.13 (2889.1 examples/sec; 0.044 sec/batch)
2017-06-02 17:29:57.985353: step 3720, loss = 0.99 (2996.2 examples/sec; 0.043 sec/batch)
2017-06-02 17:29:58.954087: step 3730, loss = 1.09 (2396.8 examples/sec; 0.053 sec/batch)
2017-06-02 17:29:59.893206: step 3740, loss = 1.04 (2651.0 examples/sec; 0.048 sec/batch)
2017-06-02 17:30:00.877021: step 3750, loss = 0.90 (2548.7 examples/sec; 0.050 sec/batch)
2017-06-02 17:30:01.853715: step 3760, loss = 1.03 (2730.4 examples/sec; 0.047 sec/batch)
2017-06-02 17:30:02.849632: step 3770, loss = 1.04 (2624.4 examples/sec; 0.049 sec/batch)
2017-06-02 17:30:03.828989: step 3780, loss = 1.22 (2407.4 examples/sec; 0.053 sec/batch)
2017-06-02 17:30:04.833965: step 3790, loss = 1.13 (2291.7 examples/sec; 0.056 sec/batch)
2017-06-02 17:30:05.802743: step 3800, loss = 0.97 (2865.1 examples/sec; 0.045 sec/batch)
2017-06-02 17:30:06.873128: step 3810, loss = 1.02 (2734.8 examples/sec; 0.047 sec/batch)
2017-06-02 17:30:07.851335: step 3820, loss = 1.01 (2530.4 examples/sec; 0.051 sec/batch)
2017-06-02 17:30:08.754566: step 3830, loss = 1.00 (2867.0 examples/sec; 0.045 sec/batch)
2017-06-02 17:30:09.720694: step 3840, loss = 1.01 (2743.8 examples/sec; 0.047 sec/batch)
2017-06-02 17:30:10.684816: step 3850, loss = 0.83 (2872.8 examples/sec; 0.045 sec/batch)
2017-06-02 17:30:11.669233: step 3860, loss = 1.17 (2813.1 examples/sec; 0.046 sec/batch)
2017-06-02 17:30:12.618932: step 3870, loss = 0.96 (2353.2 examples/sec; 0.054 sec/batch)
2017-06-02 17:30:13.544930: step 3880, loss = 1.01 (2789.9 examples/sec; 0.046 sec/batch)
2017-06-02 17:30:14.575116: step 3890, loss = 1.04 (2417.8 examples/sec; 0.053 sec/batch)
2017-06-02 17:30:15.566654: step 3900, loss = 0.91 (2591.3 examples/sec; 0.049 sec/batch)
2017-06-02 17:30:16.657755: step 3910, loss = 1.06 (2747.0 examples/sec; 0.047 sec/batch)
2017-06-02 17:30:17.637791: step 3920, loss = 0.93 (2682.5 examples/sec; 0.048 sec/batch)
2017-06-02 17:30:18.597822: step 3930, loss = 0.95 (2040.3 examples/sec; 0.063 sec/batch)
2017-06-02 17:30:19.524490: step 3940, loss = 1.04 (2898.9 examples/sec; 0.044 sec/batch)
2017-06-02 17:30:20.577026: step 3950, loss = 0.88 (2869.3 examples/sec; 0.045 sec/batch)
2017-06-02 17:30:21.566691: step 3960, loss = 0.99 (2790.1 examples/sec; 0.046 sec/batch)
2017-06-02 17:30:22.556309: step 3970, loss = 1.01 (2826.3 examples/sec; 0.045 sec/batch)
2017-06-02 17:30:23.576981: step 3980, loss = 1.13 (2394.1 examples/sec; 0.053 sec/batch)
2017-06-02 17:30:24.552440: step 3990, loss = 0.91 (2683.7 examples/sec; 0.048 sec/batch)
2017-06-02 17:30:25.569368: step 4000, loss = 1.14 (2740.2 examples/sec; 0.047 sec/batch)
2017-06-02 17:30:26.705579: step 4010, loss = 1.01 (2736.5 examples/sec; 0.047 sec/batch)
2017-06-02 17:30:27.707031: step 4020, loss = 1.18 (2584.4 examples/sec; 0.050 sec/batch)
2017-06-02 17:30:28.685655: step 4030, loss = 1.05 (2354.9 examples/sec; 0.054 sec/batch)
2017-06-02 17:30:29.608077: step 4040, loss = 0.94 (2824.7 examples/sec; 0.045 sec/batch)
2017-06-02 17:30:30.577647: step 4050, loss = 1.14 (2735.4 examples/sec; 0.047 sec/batch)
2017-06-02 17:30:31.552124: step 4060, loss = 1.10 (2538.9 examples/sec; 0.050 sec/batch)
2017-06-02 17:30:32.595147: step 4070, loss = 1.02 (2780.7 examples/sec; 0.046 sec/batch)
2017-06-02 17:30:33.563657: step 4080, loss = 0.97 (2771.0 examples/sec; 0.046 sec/batch)
2017-06-02 17:30:34.539546: step 4090, loss = 1.00 (1970.7 examples/sec; 0.065 sec/batch)
2017-06-02 17:30:35.524931: step 4100, loss = 0.97 (2716.4 examples/sec; 0.047 sec/batch)
2017-06-02 17:30:36.656718: step 4110, loss = 0.91 (2792.6 examples/sec; 0.046 sec/batch)
2017-06-02 17:30:37.664907: step 4120, loss = 0.95 (2577.8 examples/sec; 0.050 sec/batch)
2017-06-02 17:30:38.715261: step 4130, loss = 0.87 (2390.3 examples/sec; 0.054 sec/batch)
2017-06-02 17:30:39.709614: step 4140, loss = 1.08 (2800.8 examples/sec; 0.046 sec/batch)
2017-06-02 17:30:40.666914: step 4150, loss = 0.97 (2692.8 examples/sec; 0.048 sec/batch)
2017-06-02 17:30:41.587791: step 4160, loss = 1.07 (2879.4 examples/sec; 0.044 sec/batch)
2017-06-02 17:30:42.595617: step 4170, loss = 0.92 (2740.3 examples/sec; 0.047 sec/batch)
2017-06-02 17:30:43.651915: step 4180, loss = 1.08 (2430.9 examples/sec; 0.053 sec/batch)
2017-06-02 17:30:44.699241: step 4190, loss = 0.94 (2140.3 examples/sec; 0.060 sec/batch)
2017-06-02 17:30:45.666370: step 4200, loss = 0.94 (2143.8 examples/sec; 0.060 sec/batch)
2017-06-02 17:30:46.791066: step 4210, loss = 1.13 (2824.0 examples/sec; 0.045 sec/batch)
2017-06-02 17:30:47.782913: step 4220, loss = 1.01 (2231.9 examples/sec; 0.057 sec/batch)
2017-06-02 17:30:48.755141: step 4230, loss = 0.93 (2250.2 examples/sec; 0.057 sec/batch)
2017-06-02 17:30:49.758340: step 4240, loss = 0.99 (2437.7 examples/sec; 0.053 sec/batch)
2017-06-02 17:30:50.750617: step 4250, loss = 1.10 (2740.5 examples/sec; 0.047 sec/batch)
2017-06-02 17:30:51.774221: step 4260, loss = 1.00 (2629.9 examples/sec; 0.049 sec/batch)
2017-06-02 17:30:52.770405: step 4270, loss = 0.84 (2182.4 examples/sec; 0.059 sec/batch)
2017-06-02 17:30:53.744442: step 4280, loss = 0.93 (2787.1 examples/sec; 0.046 sec/batch)
2017-06-02 17:30:54.730085: step 4290, loss = 0.90 (2692.6 examples/sec; 0.048 sec/batch)
2017-06-02 17:30:55.747295: step 4300, loss = 0.87 (2815.5 examples/sec; 0.045 sec/batch)
2017-06-02 17:30:56.836920: step 4310, loss = 1.09 (2615.1 examples/sec; 0.049 sec/batch)
2017-06-02 17:30:57.836003: step 4320, loss = 0.89 (2659.8 examples/sec; 0.048 sec/batch)
2017-06-02 17:30:58.931466: step 4330, loss = 0.88 (2734.7 examples/sec; 0.047 sec/batch)
2017-06-02 17:30:59.895018: step 4340, loss = 0.99 (2133.3 examples/sec; 0.060 sec/batch)
2017-06-02 17:31:00.896179: step 4350, loss = 0.92 (2777.5 examples/sec; 0.046 sec/batch)
2017-06-02 17:31:01.914775: step 4360, loss = 0.95 (2331.6 examples/sec; 0.055 sec/batch)
2017-06-02 17:31:02.999160: step 4370, loss = 0.80 (2620.9 examples/sec; 0.049 sec/batch)
2017-06-02 17:31:04.045618: step 4380, loss = 0.82 (2210.3 examples/sec; 0.058 sec/batch)
2017-06-02 17:31:05.228156: step 4390, loss = 0.95 (2708.0 examples/sec; 0.047 sec/batch)
2017-06-02 17:31:06.359692: step 4400, loss = 0.90 (2326.4 examples/sec; 0.055 sec/batch)
2017-06-02 17:31:07.450789: step 4410, loss = 0.92 (2865.6 examples/sec; 0.045 sec/batch)
2017-06-02 17:31:08.536259: step 4420, loss = 1.06 (2486.9 examples/sec; 0.051 sec/batch)
2017-06-02 17:31:09.587369: step 4430, loss = 0.99 (2465.5 examples/sec; 0.052 sec/batch)
2017-06-02 17:31:10.605464: step 4440, loss = 0.90 (2317.4 examples/sec; 0.055 sec/batch)
2017-06-02 17:31:11.548367: step 4450, loss = 0.93 (2605.0 examples/sec; 0.049 sec/batch)
2017-06-02 17:31:12.682833: step 4460, loss = 0.89 (2383.3 examples/sec; 0.054 sec/batch)
2017-06-02 17:31:13.786654: step 4470, loss = 0.93 (2266.4 examples/sec; 0.056 sec/batch)
2017-06-02 17:31:14.867411: step 4480, loss = 0.96 (2231.7 examples/sec; 0.057 sec/batch)
2017-06-02 17:31:15.947029: step 4490, loss = 0.91 (2673.0 examples/sec; 0.048 sec/batch)
2017-06-02 17:31:17.012836: step 4500, loss = 0.84 (1864.6 examples/sec; 0.069 sec/batch)
2017-06-02 17:31:18.178303: step 4510, loss = 1.00 (1753.6 examples/sec; 0.073 sec/batch)
2017-06-02 17:31:19.292830: step 4520, loss = 1.07 (2351.9 examples/sec; 0.054 sec/batch)
2017-06-02 17:31:20.361695: step 4530, loss = 0.87 (2872.3 examples/sec; 0.045 sec/batch)
2017-06-02 17:31:21.427165: step 4540, loss = 0.82 (1747.9 examples/sec; 0.073 sec/batch)
2017-06-02 17:31:22.481934: step 4550, loss = 0.94 (2173.3 examples/sec; 0.059 sec/batch)
2017-06-02 17:31:23.577840: step 4560, loss = 0.87 (2286.6 examples/sec; 0.056 sec/batch)
2017-06-02 17:31:24.610807: step 4570, loss = 1.08 (2493.0 examples/sec; 0.051 sec/batch)
2017-06-02 17:31:25.659150: step 4580, loss = 0.90 (2468.5 examples/sec; 0.052 sec/batch)
2017-06-02 17:31:26.739014: step 4590, loss = 0.85 (1960.1 examples/sec; 0.065 sec/batch)
2017-06-02 17:31:27.811348: step 4600, loss = 0.94 (1899.6 examples/sec; 0.067 sec/batch)
2017-06-02 17:31:28.909709: step 4610, loss = 0.73 (2040.0 examples/sec; 0.063 sec/batch)
2017-06-02 17:31:29.971138: step 4620, loss = 0.99 (2871.8 examples/sec; 0.045 sec/batch)
2017-06-02 17:31:30.961408: step 4630, loss = 0.91 (2412.4 examples/sec; 0.053 sec/batch)
2017-06-02 17:31:32.062108: step 4640, loss = 0.97 (2217.2 examples/sec; 0.058 sec/batch)
2017-06-02 17:31:33.182812: step 4650, loss = 0.87 (2226.3 examples/sec; 0.057 sec/batch)
2017-06-02 17:31:34.192530: step 4660, loss = 0.90 (2034.4 examples/sec; 0.063 sec/batch)
2017-06-02 17:31:35.264117: step 4670, loss = 0.93 (2429.8 examples/sec; 0.053 sec/batch)
2017-06-02 17:31:36.326916: step 4680, loss = 1.13 (2465.5 examples/sec; 0.052 sec/batch)
2017-06-02 17:31:37.367466: step 4690, loss = 0.83 (2597.7 examples/sec; 0.049 sec/batch)
2017-06-02 17:31:38.458946: step 4700, loss = 1.08 (1893.5 examples/sec; 0.068 sec/batch)
2017-06-02 17:31:39.649694: step 4710, loss = 0.95 (2332.9 examples/sec; 0.055 sec/batch)
2017-06-02 17:31:40.689094: step 4720, loss = 1.02 (2592.4 examples/sec; 0.049 sec/batch)
2017-06-02 17:31:41.730662: step 4730, loss = 0.99 (2735.5 examples/sec; 0.047 sec/batch)
2017-06-02 17:31:42.817781: step 4740, loss = 0.95 (2682.7 examples/sec; 0.048 sec/batch)
2017-06-02 17:31:43.908674: step 4750, loss = 0.90 (2268.9 examples/sec; 0.056 sec/batch)
2017-06-02 17:31:45.005994: step 4760, loss = 0.87 (2345.7 examples/sec; 0.055 sec/batch)
2017-06-02 17:31:46.046605: step 4770, loss = 1.00 (2315.2 examples/sec; 0.055 sec/batch)
2017-06-02 17:31:47.081380: step 4780, loss = 0.85 (2701.6 examples/sec; 0.047 sec/batch)
2017-06-02 17:31:48.069306: step 4790, loss = 0.93 (2699.3 examples/sec; 0.047 sec/batch)
2017-06-02 17:31:49.186847: step 4800, loss = 0.84 (2555.7 examples/sec; 0.050 sec/batch)
2017-06-02 17:31:50.351773: step 4810, loss = 0.97 (2517.4 examples/sec; 0.051 sec/batch)
2017-06-02 17:31:51.424492: step 4820, loss = 0.88 (2298.5 examples/sec; 0.056 sec/batch)
2017-06-02 17:31:52.519946: step 4830, loss = 0.89 (2506.4 examples/sec; 0.051 sec/batch)
2017-06-02 17:31:53.568545: step 4840, loss = 1.07 (2572.8 examples/sec; 0.050 sec/batch)
2017-06-02 17:31:54.599802: step 4850, loss = 1.01 (2437.0 examples/sec; 0.053 sec/batch)
2017-06-02 17:31:55.640630: step 4860, loss = 1.07 (2109.8 examples/sec; 0.061 sec/batch)
2017-06-02 17:31:56.722846: step 4870, loss = 0.92 (2720.8 examples/sec; 0.047 sec/batch)
2017-06-02 17:31:57.779121: step 4880, loss = 0.85 (2139.0 examples/sec; 0.060 sec/batch)
2017-06-02 17:31:58.820741: step 4890, loss = 0.98 (2438.0 examples/sec; 0.053 sec/batch)
2017-06-02 17:31:59.848138: step 4900, loss = 1.18 (2563.8 examples/sec; 0.050 sec/batch)
2017-06-02 17:32:01.072920: step 4910, loss = 0.81 (2454.6 examples/sec; 0.052 sec/batch)
2017-06-02 17:32:02.097312: step 4920, loss = 0.94 (2220.1 examples/sec; 0.058 sec/batch)
2017-06-02 17:32:03.158963: step 4930, loss = 0.84 (2526.1 examples/sec; 0.051 sec/batch)
2017-06-02 17:32:04.281082: step 4940, loss = 0.82 (2516.2 examples/sec; 0.051 sec/batch)
2017-06-02 17:32:05.341926: step 4950, loss = 1.00 (2628.0 examples/sec; 0.049 sec/batch)
2017-06-02 17:32:06.443953: step 4960, loss = 0.98 (1781.8 examples/sec; 0.072 sec/batch)
2017-06-02 17:32:07.381803: step 4970, loss = 0.93 (2663.9 examples/sec; 0.048 sec/batch)
2017-06-02 17:32:08.411683: step 4980, loss = 0.74 (3035.4 examples/sec; 0.042 sec/batch)
2017-06-02 17:32:09.441945: step 4990, loss = 0.92 (2827.6 examples/sec; 0.045 sec/batch)
2017-06-02 17:32:10.516447: step 5000, loss = 0.95 (1817.3 examples/sec; 0.070 sec/batch)
2017-06-02 17:32:11.752002: step 5010, loss = 0.73 (1973.4 examples/sec; 0.065 sec/batch)
2017-06-02 17:32:12.802506: step 5020, loss = 0.87 (2455.9 examples/sec; 0.052 sec/batch)
2017-06-02 17:32:13.909804: step 5030, loss = 0.98 (2430.0 examples/sec; 0.053 sec/batch)
2017-06-02 17:32:14.959182: step 5040, loss = 0.92 (2635.2 examples/sec; 0.049 sec/batch)
2017-06-02 17:32:16.009237: step 5050, loss = 0.98 (2186.0 examples/sec; 0.059 sec/batch)
2017-06-02 17:32:17.038022: step 5060, loss = 0.85 (2614.2 examples/sec; 0.049 sec/batch)
2017-06-02 17:32:18.047638: step 5070, loss = 0.97 (2201.7 examples/sec; 0.058 sec/batch)
2017-06-02 17:32:19.077041: step 5080, loss = 0.99 (2063.2 examples/sec; 0.062 sec/batch)
2017-06-02 17:32:20.124714: step 5090, loss = 0.82 (2431.2 examples/sec; 0.053 sec/batch)
2017-06-02 17:32:21.235459: step 5100, loss = 0.85 (1952.5 examples/sec; 0.066 sec/batch)
2017-06-02 17:32:22.286127: step 5110, loss = 0.99 (2764.2 examples/sec; 0.046 sec/batch)
2017-06-02 17:32:23.424050: step 5120, loss = 0.94 (2162.6 examples/sec; 0.059 sec/batch)
2017-06-02 17:32:24.493067: step 5130, loss = 0.93 (2229.5 examples/sec; 0.057 sec/batch)
2017-06-02 17:32:25.506824: step 5140, loss = 0.79 (2443.9 examples/sec; 0.052 sec/batch)
2017-06-02 17:32:26.570951: step 5150, loss = 0.86 (2519.9 examples/sec; 0.051 sec/batch)
2017-06-02 17:32:27.660316: step 5160, loss = 1.08 (2448.2 examples/sec; 0.052 sec/batch)
2017-06-02 17:32:28.704912: step 5170, loss = 0.74 (2404.8 examples/sec; 0.053 sec/batch)
2017-06-02 17:32:29.766430: step 5180, loss = 0.85 (2344.2 examples/sec; 0.055 sec/batch)
2017-06-02 17:32:30.752587: step 5190, loss = 0.87 (2364.4 examples/sec; 0.054 sec/batch)
2017-06-02 17:32:31.901777: step 5200, loss = 0.84 (2233.2 examples/sec; 0.057 sec/batch)
2017-06-02 17:32:33.060848: step 5210, loss = 0.93 (2379.1 examples/sec; 0.054 sec/batch)
2017-06-02 17:32:34.146042: step 5220, loss = 0.69 (2311.2 examples/sec; 0.055 sec/batch)
2017-06-02 17:32:35.177180: step 5230, loss = 1.03 (2375.6 examples/sec; 0.054 sec/batch)
2017-06-02 17:32:36.292231: step 5240, loss = 0.89 (1998.0 examples/sec; 0.064 sec/batch)
2017-06-02 17:32:37.258197: step 5250, loss = 0.88 (2669.6 examples/sec; 0.048 sec/batch)
2017-06-02 17:32:38.352584: step 5260, loss = 0.97 (2555.9 examples/sec; 0.050 sec/batch)
2017-06-02 17:32:39.401560: step 5270, loss = 0.93 (2606.0 examples/sec; 0.049 sec/batch)
2017-06-02 17:32:40.453717: step 5280, loss = 1.01 (2307.8 examples/sec; 0.055 sec/batch)
2017-06-02 17:32:41.494514: step 5290, loss = 0.80 (2177.3 examples/sec; 0.059 sec/batch)
2017-06-02 17:32:42.570072: step 5300, loss = 0.84 (2821.7 examples/sec; 0.045 sec/batch)
2017-06-02 17:32:43.705788: step 5310, loss = 0.87 (2548.7 examples/sec; 0.050 sec/batch)
2017-06-02 17:32:44.753453: step 5320, loss = 0.75 (1760.8 examples/sec; 0.073 sec/batch)
2017-06-02 17:32:45.757124: step 5330, loss = 0.84 (2523.6 examples/sec; 0.051 sec/batch)
2017-06-02 17:32:46.768947: step 5340, loss = 1.03 (2674.3 examples/sec; 0.048 sec/batch)
2017-06-02 17:32:47.807488: step 5350, loss = 0.84 (1895.5 examples/sec; 0.068 sec/batch)
2017-06-02 17:32:48.831331: step 5360, loss = 0.93 (2270.1 examples/sec; 0.056 sec/batch)
2017-06-02 17:32:49.923052: step 5370, loss = 0.87 (2305.4 examples/sec; 0.056 sec/batch)
2017-06-02 17:32:51.028916: step 5380, loss = 0.89 (2377.0 examples/sec; 0.054 sec/batch)
2017-06-02 17:32:52.102871: step 5390, loss = 0.76 (2345.6 examples/sec; 0.055 sec/batch)
2017-06-02 17:32:53.184608: step 5400, loss = 0.87 (2512.7 examples/sec; 0.051 sec/batch)
2017-06-02 17:32:54.401305: step 5410, loss = 0.90 (2385.2 examples/sec; 0.054 sec/batch)
2017-06-02 17:32:55.452857: step 5420, loss = 1.03 (2687.9 examples/sec; 0.048 sec/batch)
2017-06-02 17:32:56.487859: step 5430, loss = 0.94 (2085.0 examples/sec; 0.061 sec/batch)
2017-06-02 17:32:57.585091: step 5440, loss = 1.03 (2328.3 examples/sec; 0.055 sec/batch)
2017-06-02 17:32:58.676105: step 5450, loss = 0.94 (2623.8 examples/sec; 0.049 sec/batch)
2017-06-02 17:32:59.681838: step 5460, loss = 0.86 (2816.7 examples/sec; 0.045 sec/batch)
2017-06-02 17:33:00.695579: step 5470, loss = 0.96 (2021.8 examples/sec; 0.063 sec/batch)
2017-06-02 17:33:01.673802: step 5480, loss = 0.86 (2579.9 examples/sec; 0.050 sec/batch)
2017-06-02 17:33:02.682881: step 5490, loss = 1.01 (2610.4 examples/sec; 0.049 sec/batch)
2017-06-02 17:33:03.703951: step 5500, loss = 0.99 (2636.6 examples/sec; 0.049 sec/batch)
2017-06-02 17:33:04.818405: step 5510, loss = 0.92 (2747.9 examples/sec; 0.047 sec/batch)
2017-06-02 17:33:05.857435: step 5520, loss = 1.00 (2746.3 examples/sec; 0.047 sec/batch)
2017-06-02 17:33:06.817401: step 5530, loss = 0.86 (2842.3 examples/sec; 0.045 sec/batch)
2017-06-02 17:33:07.845388: step 5540, loss = 0.87 (2093.2 examples/sec; 0.061 sec/batch)
2017-06-02 17:33:08.864605: step 5550, loss = 0.95 (2727.2 examples/sec; 0.047 sec/batch)
2017-06-02 17:33:09.885987: step 5560, loss = 0.96 (2839.9 examples/sec; 0.045 sec/batch)
2017-06-02 17:33:11.001395: step 5570, loss = 1.07 (2033.2 examples/sec; 0.063 sec/batch)
2017-06-02 17:33:11.958161: step 5580, loss = 0.98 (2593.1 examples/sec; 0.049 sec/batch)
2017-06-02 17:33:13.061010: step 5590, loss = 0.91 (2428.7 examples/sec; 0.053 sec/batch)
2017-06-02 17:33:14.153507: step 5600, loss = 1.00 (2734.8 examples/sec; 0.047 sec/batch)
2017-06-02 17:33:15.266871: step 5610, loss = 0.88 (2054.2 examples/sec; 0.062 sec/batch)
2017-06-02 17:33:16.254979: step 5620, loss = 0.82 (2843.2 examples/sec; 0.045 sec/batch)
2017-06-02 17:33:17.242363: step 5630, loss = 1.01 (2839.9 examples/sec; 0.045 sec/batch)
2017-06-02 17:33:18.248679: step 5640, loss = 0.91 (2903.5 examples/sec; 0.044 sec/batch)
2017-06-02 17:33:19.373525: step 5650, loss = 0.90 (2313.5 examples/sec; 0.055 sec/batch)
2017-06-02 17:33:20.422151: step 5660, loss = 0.94 (2261.2 examples/sec; 0.057 sec/batch)
2017-06-02 17:33:21.455624: step 5670, loss = 0.83 (2454.7 examples/sec; 0.052 sec/batch)
2017-06-02 17:33:22.570824: step 5680, loss = 0.98 (2330.4 examples/sec; 0.055 sec/batch)
2017-06-02 17:33:23.623906: step 5690, loss = 0.76 (2091.9 examples/sec; 0.061 sec/batch)
2017-06-02 17:33:24.707487: step 5700, loss = 0.81 (2882.5 examples/sec; 0.044 sec/batch)
2017-06-02 17:33:25.848179: step 5710, loss = 0.84 (2510.1 examples/sec; 0.051 sec/batch)
2017-06-02 17:33:26.875762: step 5720, loss = 0.80 (2548.0 examples/sec; 0.050 sec/batch)
2017-06-02 17:33:27.950122: step 5730, loss = 1.01 (2406.6 examples/sec; 0.053 sec/batch)
2017-06-02 17:33:29.076625: step 5740, loss = 0.90 (2388.3 examples/sec; 0.054 sec/batch)
2017-06-02 17:33:30.145169: step 5750, loss = 0.86 (2418.0 examples/sec; 0.053 sec/batch)
2017-06-02 17:33:31.175218: step 5760, loss = 0.85 (2765.8 examples/sec; 0.046 sec/batch)
2017-06-02 17:33:32.219429: step 5770, loss = 1.03 (2094.3 examples/sec; 0.061 sec/batch)
2017-06-02 17:33:33.290121: step 5780, loss = 0.89 (2412.5 examples/sec; 0.053 sec/batch)
2017-06-02 17:33:34.386561: step 5790, loss = 0.97 (2305.1 examples/sec; 0.056 sec/batch)
2017-06-02 17:33:35.467886: step 5800, loss = 0.81 (2711.4 examples/sec; 0.047 sec/batch)
2017-06-02 17:33:36.666439: step 5810, loss = 0.90 (1714.1 examples/sec; 0.075 sec/batch)
2017-06-02 17:33:37.687562: step 5820, loss = 0.91 (2824.2 examples/sec; 0.045 sec/batch)
2017-06-02 17:33:38.715339: step 5830, loss = 1.04 (2555.3 examples/sec; 0.050 sec/batch)
2017-06-02 17:33:39.749785: step 5840, loss = 0.81 (2299.9 examples/sec; 0.056 sec/batch)
2017-06-02 17:33:40.768358: step 5850, loss = 0.88 (2343.5 examples/sec; 0.055 sec/batch)
2017-06-02 17:33:41.906867: step 5860, loss = 0.80 (2407.4 examples/sec; 0.053 sec/batch)
2017-06-02 17:33:42.977091: step 5870, loss = 0.87 (1785.6 examples/sec; 0.072 sec/batch)
2017-06-02 17:33:44.050149: step 5880, loss = 0.95 (2397.5 examples/sec; 0.053 sec/batch)
2017-06-02 17:33:45.098213: step 5890, loss = 0.69 (2098.6 examples/sec; 0.061 sec/batch)
2017-06-02 17:33:46.120417: step 5900, loss = 0.91 (2774.4 examples/sec; 0.046 sec/batch)
2017-06-02 17:33:47.302612: step 5910, loss = 0.84 (2433.3 examples/sec; 0.053 sec/batch)
2017-06-02 17:33:48.357921: step 5920, loss = 0.84 (2563.7 examples/sec; 0.050 sec/batch)
2017-06-02 17:33:49.354777: step 5930, loss = 0.85 (2916.5 examples/sec; 0.044 sec/batch)
2017-06-02 17:33:50.353289: step 5940, loss = 0.80 (2722.0 examples/sec; 0.047 sec/batch)
2017-06-02 17:33:51.386980: step 5950, loss = 0.79 (2812.6 examples/sec; 0.046 sec/batch)
2017-06-02 17:33:52.462030: step 5960, loss = 0.91 (2483.3 examples/sec; 0.052 sec/batch)
2017-06-02 17:33:53.511620: step 5970, loss = 1.01 (2783.3 examples/sec; 0.046 sec/batch)
2017-06-02 17:33:54.588395: step 5980, loss = 0.83 (2386.4 examples/sec; 0.054 sec/batch)
2017-06-02 17:33:55.644528: step 5990, loss = 0.92 (2795.6 examples/sec; 0.046 sec/batch)
2017-06-02 17:33:56.617892: step 6000, loss = 0.86 (2720.9 examples/sec; 0.047 sec/batch)
2017-06-02 17:33:57.818242: step 6010, loss = 0.91 (2549.8 examples/sec; 0.050 sec/batch)
2017-06-02 17:33:58.832126: step 6020, loss = 1.17 (3010.3 examples/sec; 0.043 sec/batch)
2017-06-02 17:33:59.922062: step 6030, loss = 0.68 (2062.0 examples/sec; 0.062 sec/batch)
2017-06-02 17:34:00.921944: step 6040, loss = 0.85 (2298.8 examples/sec; 0.056 sec/batch)
2017-06-02 17:34:02.057714: step 6050, loss = 0.83 (2300.2 examples/sec; 0.056 sec/batch)
2017-06-02 17:34:03.216618: step 6060, loss = 0.79 (2396.4 examples/sec; 0.053 sec/batch)
2017-06-02 17:34:04.265869: step 6070, loss = 0.72 (2368.2 examples/sec; 0.054 sec/batch)
2017-06-02 17:34:05.341785: step 6080, loss = 0.75 (2423.2 examples/sec; 0.053 sec/batch)
2017-06-02 17:34:06.440635: step 6090, loss = 0.76 (2252.0 examples/sec; 0.057 sec/batch)
2017-06-02 17:34:07.468230: step 6100, loss = 0.99 (2253.0 examples/sec; 0.057 sec/batch)
2017-06-02 17:34:08.606487: step 6110, loss = 0.88 (2440.8 examples/sec; 0.052 sec/batch)
2017-06-02 17:34:09.649235: step 6120, loss = 0.85 (2783.5 examples/sec; 0.046 sec/batch)
2017-06-02 17:34:10.783068: step 6130, loss = 0.82 (2448.1 examples/sec; 0.052 sec/batch)
2017-06-02 17:34:11.803850: step 6140, loss = 0.82 (2462.8 examples/sec; 0.052 sec/batch)
2017-06-02 17:34:12.829557: step 6150, loss = 1.01 (2479.4 examples/sec; 0.052 sec/batch)
2017-06-02 17:34:13.883242: step 6160, loss = 0.77 (2979.3 examples/sec; 0.043 sec/batch)
2017-06-02 17:34:14.960967: step 6170, loss = 0.82 (2666.7 examples/sec; 0.048 sec/batch)
2017-06-02 17:34:16.024523: step 6180, loss = 0.86 (2016.7 examples/sec; 0.063 sec/batch)
2017-06-02 17:34:17.058614: step 6190, loss = 0.86 (2510.4 examples/sec; 0.051 sec/batch)
2017-06-02 17:34:18.137595: step 6200, loss = 0.82 (2526.6 examples/sec; 0.051 sec/batch)
2017-06-02 17:34:19.380594: step 6210, loss = 0.94 (2489.5 examples/sec; 0.051 sec/batch)
2017-06-02 17:34:20.393494: step 6220, loss = 0.84 (2940.8 examples/sec; 0.044 sec/batch)
2017-06-02 17:34:21.314979: step 6230, loss = 0.77 (2753.5 examples/sec; 0.046 sec/batch)
2017-06-02 17:34:22.311984: step 6240, loss = 0.81 (2601.6 examples/sec; 0.049 sec/batch)
2017-06-02 17:34:23.254375: step 6250, loss = 0.85 (2752.7 examples/sec; 0.047 sec/batch)
2017-06-02 17:34:24.168243: step 6260, loss = 0.81 (2766.6 examples/sec; 0.046 sec/batch)
2017-06-02 17:34:25.186604: step 6270, loss = 0.92 (2062.6 examples/sec; 0.062 sec/batch)
2017-06-02 17:34:26.388426: step 6280, loss = 0.82 (2115.7 examples/sec; 0.061 sec/batch)
2017-06-02 17:34:27.657285: step 6290, loss = 0.87 (1981.0 examples/sec; 0.065 sec/batch)
2017-06-02 17:34:28.703745: step 6300, loss = 0.88 (2837.4 examples/sec; 0.045 sec/batch)
2017-06-02 17:34:29.880717: step 6310, loss = 0.96 (1812.2 examples/sec; 0.071 sec/batch)
2017-06-02 17:34:31.184347: step 6320, loss = 0.86 (1970.9 examples/sec; 0.065 sec/batch)
2017-06-02 17:34:32.501697: step 6330, loss = 0.93 (1910.2 examples/sec; 0.067 sec/batch)
2017-06-02 17:34:33.509658: step 6340, loss = 0.94 (2513.9 examples/sec; 0.051 sec/batch)
2017-06-02 17:34:34.471478: step 6350, loss = 0.85 (2755.7 examples/sec; 0.046 sec/batch)
2017-06-02 17:34:35.455507: step 6360, loss = 0.82 (2435.9 examples/sec; 0.053 sec/batch)
2017-06-02 17:34:36.428987: step 6370, loss = 0.77 (2543.9 examples/sec; 0.050 sec/batch)
2017-06-02 17:34:37.398018: step 6380, loss = 0.84 (2587.5 examples/sec; 0.049 sec/batch)
2017-06-02 17:34:38.358347: step 6390, loss = 0.91 (2511.9 examples/sec; 0.051 sec/batch)
2017-06-02 17:34:39.363742: step 6400, loss = 0.83 (2393.7 examples/sec; 0.053 sec/batch)
2017-06-02 17:34:40.395009: step 6410, loss = 0.78 (2832.3 examples/sec; 0.045 sec/batch)
2017-06-02 17:34:41.405368: step 6420, loss = 0.80 (2660.3 examples/sec; 0.048 sec/batch)
2017-06-02 17:34:42.407085: step 6430, loss = 0.93 (2952.4 examples/sec; 0.043 sec/batch)
2017-06-02 17:34:43.407901: step 6440, loss = 0.95 (2781.3 examples/sec; 0.046 sec/batch)
2017-06-02 17:34:44.398788: step 6450, loss = 0.81 (2363.7 examples/sec; 0.054 sec/batch)
2017-06-02 17:34:45.387906: step 6460, loss = 0.85 (2647.0 examples/sec; 0.048 sec/batch)
2017-06-02 17:34:46.374081: step 6470, loss = 0.82 (2435.6 examples/sec; 0.053 sec/batch)
2017-06-02 17:34:47.342284: step 6480, loss = 0.88 (2430.4 examples/sec; 0.053 sec/batch)
2017-06-02 17:34:48.307205: step 6490, loss = 0.81 (2566.3 examples/sec; 0.050 sec/batch)
2017-06-02 17:34:49.296640: step 6500, loss = 0.80 (2823.3 examples/sec; 0.045 sec/batch)
2017-06-02 17:34:50.378580: step 6510, loss = 0.80 (2668.3 examples/sec; 0.048 sec/batch)
2017-06-02 17:34:51.359385: step 6520, loss = 1.04 (2646.4 examples/sec; 0.048 sec/batch)
2017-06-02 17:34:52.303586: step 6530, loss = 0.85 (2753.9 examples/sec; 0.046 sec/batch)
2017-06-02 17:34:53.306516: step 6540, loss = 0.79 (2799.7 examples/sec; 0.046 sec/batch)
2017-06-02 17:34:54.275909: step 6550, loss = 0.99 (2820.9 examples/sec; 0.045 sec/batch)
2017-06-02 17:34:55.250726: step 6560, loss = 0.86 (2805.4 examples/sec; 0.046 sec/batch)
2017-06-02 17:34:56.220015: step 6570, loss = 0.78 (2497.7 examples/sec; 0.051 sec/batch)
2017-06-02 17:34:57.238112: step 6580, loss = 0.80 (2922.0 examples/sec; 0.044 sec/batch)
2017-06-02 17:34:58.217382: step 6590, loss = 0.93 (2728.8 examples/sec; 0.047 sec/batch)
2017-06-02 17:34:59.212609: step 6600, loss = 0.80 (2728.5 examples/sec; 0.047 sec/batch)
2017-06-02 17:35:00.301750: step 6610, loss = 0.71 (2294.9 examples/sec; 0.056 sec/batch)
2017-06-02 17:35:01.274111: step 6620, loss = 0.98 (2401.5 examples/sec; 0.053 sec/batch)
2017-06-02 17:35:02.228927: step 6630, loss = 0.79 (2712.1 examples/sec; 0.047 sec/batch)
2017-06-02 17:35:03.181098: step 6640, loss = 0.90 (2366.7 examples/sec; 0.054 sec/batch)
2017-06-02 17:35:04.149168: step 6650, loss = 0.95 (2942.6 examples/sec; 0.043 sec/batch)
2017-06-02 17:35:05.182970: step 6660, loss = 0.83 (2532.4 examples/sec; 0.051 sec/batch)
2017-06-02 17:35:06.219028: step 6670, loss = 0.87 (2831.2 examples/sec; 0.045 sec/batch)
2017-06-02 17:35:07.202478: step 6680, loss = 0.81 (2701.2 examples/sec; 0.047 sec/batch)
2017-06-02 17:35:08.167135: step 6690, loss = 0.78 (2819.7 examples/sec; 0.045 sec/batch)
2017-06-02 17:35:09.165229: step 6700, loss = 0.89 (2696.9 examples/sec; 0.047 sec/batch)
2017-06-02 17:35:10.233298: step 6710, loss = 0.86 (2577.2 examples/sec; 0.050 sec/batch)
2017-06-02 17:35:11.169598: step 6720, loss = 0.79 (2599.0 examples/sec; 0.049 sec/batch)
2017-06-02 17:35:12.132987: step 6730, loss = 0.86 (2719.4 examples/sec; 0.047 sec/batch)
2017-06-02 17:35:13.135063: step 6740, loss = 0.97 (2599.1 examples/sec; 0.049 sec/batch)
2017-06-02 17:35:14.142197: step 6750, loss = 1.16 (2030.9 examples/sec; 0.063 sec/batch)
2017-06-02 17:35:15.120783: step 6760, loss = 0.78 (2788.9 examples/sec; 0.046 sec/batch)
2017-06-02 17:35:16.115314: step 6770, loss = 0.73 (2649.0 examples/sec; 0.048 sec/batch)
2017-06-02 17:35:17.099804: step 6780, loss = 0.97 (2720.6 examples/sec; 0.047 sec/batch)
2017-06-02 17:35:18.040167: step 6790, loss = 0.92 (2432.7 examples/sec; 0.053 sec/batch)
2017-06-02 17:35:18.985206: step 6800, loss = 0.82 (2701.1 examples/sec; 0.047 sec/batch)
2017-06-02 17:35:20.081239: step 6810, loss = 0.75 (2590.3 examples/sec; 0.049 sec/batch)
2017-06-02 17:35:21.132763: step 6820, loss = 0.90 (2565.0 examples/sec; 0.050 sec/batch)
2017-06-02 17:35:22.105258: step 6830, loss = 0.68 (2427.3 examples/sec; 0.053 sec/batch)
2017-06-02 17:35:23.154610: step 6840, loss = 0.81 (2890.3 examples/sec; 0.044 sec/batch)
2017-06-02 17:35:24.145435: step 6850, loss = 0.72 (2808.9 examples/sec; 0.046 sec/batch)
2017-06-02 17:35:25.148467: step 6860, loss = 0.77 (2338.9 examples/sec; 0.055 sec/batch)
2017-06-02 17:35:26.096994: step 6870, loss = 0.74 (2661.3 examples/sec; 0.048 sec/batch)
2017-06-02 17:35:27.053086: step 6880, loss = 0.72 (2665.8 examples/sec; 0.048 sec/batch)
2017-06-02 17:35:28.024558: step 6890, loss = 0.78 (2759.2 examples/sec; 0.046 sec/batch)
2017-06-02 17:35:28.992021: step 6900, loss = 0.70 (1928.5 examples/sec; 0.066 sec/batch)
2017-06-02 17:35:30.023135: step 6910, loss = 0.67 (2985.0 examples/sec; 0.043 sec/batch)
2017-06-02 17:35:31.007639: step 6920, loss = 0.75 (2833.1 examples/sec; 0.045 sec/batch)
2017-06-02 17:35:31.974812: step 6930, loss = 0.84 (2157.0 examples/sec; 0.059 sec/batch)
2017-06-02 17:35:32.900964: step 6940, loss = 0.71 (2686.3 examples/sec; 0.048 sec/batch)
2017-06-02 17:35:33.882889: step 6950, loss = 0.99 (2690.2 examples/sec; 0.048 sec/batch)
2017-06-02 17:35:34.889017: step 6960, loss = 0.79 (2642.5 examples/sec; 0.048 sec/batch)
2017-06-02 17:35:35.879807: step 6970, loss = 0.88 (2938.4 examples/sec; 0.044 sec/batch)
2017-06-02 17:35:36.785962: step 6980, loss = 0.80 (2883.7 examples/sec; 0.044 sec/batch)
2017-06-02 17:35:37.762519: step 6990, loss = 0.79 (2768.7 examples/sec; 0.046 sec/batch)
2017-06-02 17:35:38.697203: step 7000, loss = 0.86 (2725.0 examples/sec; 0.047 sec/batch)
2017-06-02 17:35:39.832456: step 7010, loss = 0.77 (2226.7 examples/sec; 0.057 sec/batch)
2017-06-02 17:35:40.784742: step 7020, loss = 0.77 (2725.4 examples/sec; 0.047 sec/batch)
2017-06-02 17:35:41.764496: step 7030, loss = 0.82 (2736.8 examples/sec; 0.047 sec/batch)
2017-06-02 17:35:42.725060: step 7040, loss = 0.90 (2701.3 examples/sec; 0.047 sec/batch)
2017-06-02 17:35:43.687827: step 7050, loss = 0.89 (2823.1 examples/sec; 0.045 sec/batch)
2017-06-02 17:35:44.661312: step 7060, loss = 0.74 (2824.8 examples/sec; 0.045 sec/batch)
2017-06-02 17:35:45.674491: step 7070, loss = 0.69 (2366.4 examples/sec; 0.054 sec/batch)
2017-06-02 17:35:46.655007: step 7080, loss = 0.72 (2897.0 examples/sec; 0.044 sec/batch)
2017-06-02 17:35:47.645944: step 7090, loss = 0.93 (2792.0 examples/sec; 0.046 sec/batch)
2017-06-02 17:35:48.621975: step 7100, loss = 0.85 (2863.1 examples/sec; 0.045 sec/batch)
2017-06-02 17:35:49.719532: step 7110, loss = 0.82 (2808.4 examples/sec; 0.046 sec/batch)
2017-06-02 17:35:50.723362: step 7120, loss = 0.82 (2349.2 examples/sec; 0.054 sec/batch)
2017-06-02 17:35:51.675981: step 7130, loss = 0.71 (2856.7 examples/sec; 0.045 sec/batch)
2017-06-02 17:35:52.647719: step 7140, loss = 0.86 (2582.4 examples/sec; 0.050 sec/batch)
2017-06-02 17:35:53.693064: step 7150, loss = 0.81 (1871.4 examples/sec; 0.068 sec/batch)
2017-06-02 17:35:54.801599: step 7160, loss = 0.92 (2639.1 examples/sec; 0.049 sec/batch)
2017-06-02 17:35:55.783869: step 7170, loss = 0.86 (2398.5 examples/sec; 0.053 sec/batch)
2017-06-02 17:35:56.772199: step 7180, loss = 0.79 (2529.5 examples/sec; 0.051 sec/batch)
2017-06-02 17:35:57.737969: step 7190, loss = 0.76 (2715.3 examples/sec; 0.047 sec/batch)
2017-06-02 17:35:58.762683: step 7200, loss = 0.85 (2472.1 examples/sec; 0.052 sec/batch)
2017-06-02 17:35:59.915521: step 7210, loss = 0.76 (2676.1 examples/sec; 0.048 sec/batch)
2017-06-02 17:36:00.984924: step 7220, loss = 0.71 (2350.3 examples/sec; 0.054 sec/batch)
2017-06-02 17:36:02.066149: step 7230, loss = 0.81 (2193.7 examples/sec; 0.058 sec/batch)
2017-06-02 17:36:03.120948: step 7240, loss = 0.71 (2822.0 examples/sec; 0.045 sec/batch)
2017-06-02 17:36:04.120043: step 7250, loss = 0.96 (2376.9 examples/sec; 0.054 sec/batch)
2017-06-02 17:36:05.188742: step 7260, loss = 0.76 (2494.1 examples/sec; 0.051 sec/batch)
2017-06-02 17:36:06.290668: step 7270, loss = 0.77 (2295.9 examples/sec; 0.056 sec/batch)
2017-06-02 17:36:07.398048: step 7280, loss = 0.69 (2501.7 examples/sec; 0.051 sec/batch)
2017-06-02 17:36:08.428302: step 7290, loss = 0.83 (2234.3 examples/sec; 0.057 sec/batch)
2017-06-02 17:36:09.506278: step 7300, loss = 0.93 (2217.9 examples/sec; 0.058 sec/batch)
2017-06-02 17:36:10.657478: step 7310, loss = 0.96 (2771.3 examples/sec; 0.046 sec/batch)
2017-06-02 17:36:11.722817: step 7320, loss = 0.77 (2780.4 examples/sec; 0.046 sec/batch)
2017-06-02 17:36:12.713369: step 7330, loss = 0.92 (2412.8 examples/sec; 0.053 sec/batch)
2017-06-02 17:36:13.764069: step 7340, loss = 0.91 (2147.5 examples/sec; 0.060 sec/batch)
2017-06-02 17:36:14.848451: step 7350, loss = 0.95 (2591.0 examples/sec; 0.049 sec/batch)
2017-06-02 17:36:15.878451: step 7360, loss = 0.68 (2268.9 examples/sec; 0.056 sec/batch)
2017-06-02 17:36:17.002852: step 7370, loss = 0.85 (2455.9 examples/sec; 0.052 sec/batch)
2017-06-02 17:36:18.095134: step 7380, loss = 0.76 (2380.3 examples/sec; 0.054 sec/batch)
2017-06-02 17:36:19.194464: step 7390, loss = 0.71 (2593.3 examples/sec; 0.049 sec/batch)
2017-06-02 17:36:20.198398: step 7400, loss = 0.87 (2489.9 examples/sec; 0.051 sec/batch)
2017-06-02 17:36:21.441107: step 7410, loss = 0.89 (2765.5 examples/sec; 0.046 sec/batch)
2017-06-02 17:36:22.498498: step 7420, loss = 0.76 (2428.5 examples/sec; 0.053 sec/batch)
2017-06-02 17:36:23.600893: step 7430, loss = 0.98 (1922.6 examples/sec; 0.067 sec/batch)
2017-06-02 17:36:24.652433: step 7440, loss = 0.84 (2811.7 examples/sec; 0.046 sec/batch)
2017-06-02 17:36:25.768574: step 7450, loss = 0.81 (2295.6 examples/sec; 0.056 sec/batch)
2017-06-02 17:36:26.848213: step 7460, loss = 0.80 (2419.7 examples/sec; 0.053 sec/batch)
2017-06-02 17:36:27.897675: step 7470, loss = 0.90 (2181.3 examples/sec; 0.059 sec/batch)
2017-06-02 17:36:28.997970: step 7480, loss = 0.77 (2435.2 examples/sec; 0.053 sec/batch)
2017-06-02 17:36:30.056792: step 7490, loss = 0.85 (2479.9 examples/sec; 0.052 sec/batch)
2017-06-02 17:36:31.138330: step 7500, loss = 0.79 (2457.8 examples/sec; 0.052 sec/batch)
2017-06-02 17:36:32.310360: step 7510, loss = 0.94 (2307.1 examples/sec; 0.055 sec/batch)
2017-06-02 17:36:33.356975: step 7520, loss = 0.73 (2804.9 examples/sec; 0.046 sec/batch)
2017-06-02 17:36:34.393024: step 7530, loss = 0.77 (2324.3 examples/sec; 0.055 sec/batch)
2017-06-02 17:36:35.454403: step 7540, loss = 0.73 (2482.3 examples/sec; 0.052 sec/batch)
2017-06-02 17:36:36.487200: step 7550, loss = 0.62 (2336.5 examples/sec; 0.055 sec/batch)
2017-06-02 17:36:37.565055: step 7560, loss = 0.82 (1924.2 examples/sec; 0.067 sec/batch)
2017-06-02 17:36:38.618193: step 7570, loss = 0.81 (2199.2 examples/sec; 0.058 sec/batch)
2017-06-02 17:36:39.672608: step 7580, loss = 0.78 (2236.0 examples/sec; 0.057 sec/batch)
2017-06-02 17:36:40.722810: step 7590, loss = 0.79 (2302.6 examples/sec; 0.056 sec/batch)
2017-06-02 17:36:41.805614: step 7600, loss = 0.89 (2462.2 examples/sec; 0.052 sec/batch)
2017-06-02 17:36:42.914327: step 7610, loss = 0.68 (2774.2 examples/sec; 0.046 sec/batch)
2017-06-02 17:36:43.939602: step 7620, loss = 0.88 (2645.1 examples/sec; 0.048 sec/batch)
2017-06-02 17:36:45.000872: step 7630, loss = 0.80 (2467.5 examples/sec; 0.052 sec/batch)
2017-06-02 17:36:46.058480: step 7640, loss = 0.86 (2511.2 examples/sec; 0.051 sec/batch)
2017-06-02 17:36:47.131408: step 7650, loss = 0.79 (2446.0 examples/sec; 0.052 sec/batch)
2017-06-02 17:36:48.205748: step 7660, loss = 0.79 (2102.8 examples/sec; 0.061 sec/batch)
2017-06-02 17:36:49.354832: step 7670, loss = 0.99 (1991.5 examples/sec; 0.064 sec/batch)
2017-06-02 17:36:50.405402: step 7680, loss = 0.74 (2493.4 examples/sec; 0.051 sec/batch)
2017-06-02 17:36:51.529896: step 7690, loss = 0.85 (2272.8 examples/sec; 0.056 sec/batch)
2017-06-02 17:36:52.595178: step 7700, loss = 0.71 (2855.7 examples/sec; 0.045 sec/batch)
2017-06-02 17:36:53.784817: step 7710, loss = 0.71 (2379.0 examples/sec; 0.054 sec/batch)
2017-06-02 17:36:54.824348: step 7720, loss = 0.87 (2198.0 examples/sec; 0.058 sec/batch)
2017-06-02 17:36:55.861568: step 7730, loss = 0.78 (2471.4 examples/sec; 0.052 sec/batch)
2017-06-02 17:36:57.009506: step 7740, loss = 1.06 (2286.0 examples/sec; 0.056 sec/batch)
2017-06-02 17:36:58.105050: step 7750, loss = 0.74 (2487.0 examples/sec; 0.051 sec/batch)
2017-06-02 17:36:59.178913: step 7760, loss = 0.80 (2480.4 examples/sec; 0.052 sec/batch)
2017-06-02 17:37:00.201942: step 7770, loss = 0.88 (3228.7 examples/sec; 0.040 sec/batch)
2017-06-02 17:37:01.298220: step 7780, loss = 0.79 (2253.9 examples/sec; 0.057 sec/batch)
2017-06-02 17:37:02.427495: step 7790, loss = 0.90 (1957.7 examples/sec; 0.065 sec/batch)
2017-06-02 17:37:03.465146: step 7800, loss = 0.72 (2858.2 examples/sec; 0.045 sec/batch)
2017-06-02 17:37:04.607638: step 7810, loss = 0.72 (2337.8 examples/sec; 0.055 sec/batch)
2017-06-02 17:37:05.651652: step 7820, loss = 0.84 (2636.3 examples/sec; 0.049 sec/batch)
2017-06-02 17:37:06.729346: step 7830, loss = 0.73 (1951.0 examples/sec; 0.066 sec/batch)
2017-06-02 17:37:07.791226: step 7840, loss = 0.77 (2462.5 examples/sec; 0.052 sec/batch)
2017-06-02 17:37:08.898442: step 7850, loss = 0.76 (2782.0 examples/sec; 0.046 sec/batch)
2017-06-02 17:37:09.969518: step 7860, loss = 0.76 (2434.8 examples/sec; 0.053 sec/batch)
2017-06-02 17:37:11.071556: step 7870, loss = 1.01 (1799.6 examples/sec; 0.071 sec/batch)
2017-06-02 17:37:12.209305: step 7880, loss = 0.78 (2247.5 examples/sec; 0.057 sec/batch)
2017-06-02 17:37:13.253063: step 7890, loss = 0.82 (2374.6 examples/sec; 0.054 sec/batch)
2017-06-02 17:37:14.348721: step 7900, loss = 0.85 (1682.0 examples/sec; 0.076 sec/batch)
2017-06-02 17:37:15.474990: step 7910, loss = 0.80 (2454.5 examples/sec; 0.052 sec/batch)
2017-06-02 17:37:16.569313: step 7920, loss = 0.77 (2302.2 examples/sec; 0.056 sec/batch)
2017-06-02 17:37:17.588757: step 7930, loss = 0.82 (2334.3 examples/sec; 0.055 sec/batch)
2017-06-02 17:37:18.692167: step 7940, loss = 0.81 (2306.2 examples/sec; 0.056 sec/batch)
2017-06-02 17:37:19.723818: step 7950, loss = 0.79 (2475.6 examples/sec; 0.052 sec/batch)
2017-06-02 17:37:20.734446: step 7960, loss = 1.01 (2561.6 examples/sec; 0.050 sec/batch)
2017-06-02 17:37:21.829048: step 7970, loss = 0.75 (2853.2 examples/sec; 0.045 sec/batch)
2017-06-02 17:37:22.878693: step 7980, loss = 0.82 (2749.2 examples/sec; 0.047 sec/batch)
2017-06-02 17:37:23.997506: step 7990, loss = 0.79 (2342.8 examples/sec; 0.055 sec/batch)
2017-06-02 17:37:25.106278: step 8000, loss = 0.84 (2444.6 examples/sec; 0.052 sec/batch)
2017-06-02 17:37:26.535783: step 8010, loss = 0.89 (2364.4 examples/sec; 0.054 sec/batch)
2017-06-02 17:37:27.610984: step 8020, loss = 0.83 (3031.7 examples/sec; 0.042 sec/batch)
2017-06-02 17:37:28.709820: step 8030, loss = 1.15 (2539.8 examples/sec; 0.050 sec/batch)
2017-06-02 17:37:29.819269: step 8040, loss = 0.85 (2147.6 examples/sec; 0.060 sec/batch)
2017-06-02 17:37:30.884811: step 8050, loss = 0.76 (2254.2 examples/sec; 0.057 sec/batch)
2017-06-02 17:37:32.031136: step 8060, loss = 0.94 (2042.8 examples/sec; 0.063 sec/batch)
2017-06-02 17:37:33.086687: step 8070, loss = 0.77 (2426.4 examples/sec; 0.053 sec/batch)
2017-06-02 17:37:34.127082: step 8080, loss = 0.77 (2489.2 examples/sec; 0.051 sec/batch)
2017-06-02 17:37:35.202960: step 8090, loss = 0.83 (2432.8 examples/sec; 0.053 sec/batch)
2017-06-02 17:37:36.264403: step 8100, loss = 0.80 (2445.7 examples/sec; 0.052 sec/batch)
2017-06-02 17:37:37.436640: step 8110, loss = 0.71 (2184.9 examples/sec; 0.059 sec/batch)
2017-06-02 17:37:38.457544: step 8120, loss = 0.80 (2614.6 examples/sec; 0.049 sec/batch)
2017-06-02 17:37:39.559105: step 8130, loss = 0.77 (2385.3 examples/sec; 0.054 sec/batch)
2017-06-02 17:37:40.656083: step 8140, loss = 0.89 (1737.7 examples/sec; 0.074 sec/batch)
2017-06-02 17:37:41.756292: step 8150, loss = 0.77 (2009.3 examples/sec; 0.064 sec/batch)
2017-06-02 17:37:42.818615: step 8160, loss = 0.82 (2349.8 examples/sec; 0.054 sec/batch)
2017-06-02 17:37:43.920441: step 8170, loss = 0.92 (2389.5 examples/sec; 0.054 sec/batch)
2017-06-02 17:37:44.947017: step 8180, loss = 0.70 (2362.5 examples/sec; 0.054 sec/batch)
2017-06-02 17:37:46.008774: step 8190, loss = 0.83 (2397.2 examples/sec; 0.053 sec/batch)
2017-06-02 17:37:47.057061: step 8200, loss = 0.84 (2428.5 examples/sec; 0.053 sec/batch)
2017-06-02 17:37:48.195904: step 8210, loss = 0.71 (2569.5 examples/sec; 0.050 sec/batch)
2017-06-02 17:37:49.328373: step 8220, loss = 0.81 (2443.3 examples/sec; 0.052 sec/batch)
2017-06-02 17:37:50.442874: step 8230, loss = 0.76 (2119.3 examples/sec; 0.060 sec/batch)
2017-06-02 17:37:51.529954: step 8240, loss = 0.82 (2421.2 examples/sec; 0.053 sec/batch)
2017-06-02 17:37:52.629149: step 8250, loss = 0.80 (2706.3 examples/sec; 0.047 sec/batch)
2017-06-02 17:37:53.684957: step 8260, loss = 0.68 (2319.3 examples/sec; 0.055 sec/batch)
2017-06-02 17:37:54.735973: step 8270, loss = 0.77 (2861.7 examples/sec; 0.045 sec/batch)
2017-06-02 17:37:55.834212: step 8280, loss = 0.80 (2298.4 examples/sec; 0.056 sec/batch)
2017-06-02 17:37:56.904270: step 8290, loss = 0.84 (2250.9 examples/sec; 0.057 sec/batch)
2017-06-02 17:37:57.971924: step 8300, loss = 0.90 (2285.2 examples/sec; 0.056 sec/batch)
2017-06-02 17:37:59.195207: step 8310, loss = 0.70 (2675.4 examples/sec; 0.048 sec/batch)
2017-06-02 17:38:00.273379: step 8320, loss = 0.80 (2311.9 examples/sec; 0.055 sec/batch)
2017-06-02 17:38:01.359159: step 8330, loss = 1.03 (2479.1 examples/sec; 0.052 sec/batch)
2017-06-02 17:38:02.444106: step 8340, loss = 0.89 (2374.5 examples/sec; 0.054 sec/batch)
2017-06-02 17:38:03.498923: step 8350, loss = 0.87 (2240.1 examples/sec; 0.057 sec/batch)
2017-06-02 17:38:04.575457: step 8360, loss = 0.91 (2339.0 examples/sec; 0.055 sec/batch)
2017-06-02 17:38:05.686348: step 8370, loss = 0.78 (2327.0 examples/sec; 0.055 sec/batch)
2017-06-02 17:38:06.750402: step 8380, loss = 0.80 (2429.2 examples/sec; 0.053 sec/batch)
2017-06-02 17:38:07.797537: step 8390, loss = 0.71 (2329.6 examples/sec; 0.055 sec/batch)
2017-06-02 17:38:08.909809: step 8400, loss = 0.84 (1907.9 examples/sec; 0.067 sec/batch)
2017-06-02 17:38:10.087657: step 8410, loss = 0.81 (2407.6 examples/sec; 0.053 sec/batch)
2017-06-02 17:38:11.146867: step 8420, loss = 0.75 (2599.5 examples/sec; 0.049 sec/batch)
2017-06-02 17:38:12.186794: step 8430, loss = 0.75 (2349.1 examples/sec; 0.054 sec/batch)
2017-06-02 17:38:13.303016: step 8440, loss = 0.81 (2220.8 examples/sec; 0.058 sec/batch)
2017-06-02 17:38:14.375725: step 8450, loss = 0.77 (2112.9 examples/sec; 0.061 sec/batch)
2017-06-02 17:38:15.514963: step 8460, loss = 0.72 (2123.2 examples/sec; 0.060 sec/batch)
2017-06-02 17:38:16.608429: step 8470, loss = 1.21 (2726.8 examples/sec; 0.047 sec/batch)
2017-06-02 17:38:17.760503: step 8480, loss = 0.65 (2507.5 examples/sec; 0.051 sec/batch)
2017-06-02 17:38:18.824100: step 8490, loss = 0.79 (2313.2 examples/sec; 0.055 sec/batch)
2017-06-02 17:38:19.888186: step 8500, loss = 0.79 (2452.7 examples/sec; 0.052 sec/batch)
2017-06-02 17:38:21.104567: step 8510, loss = 0.82 (2674.4 examples/sec; 0.048 sec/batch)
2017-06-02 17:38:22.180594: step 8520, loss = 0.78 (1878.6 examples/sec; 0.068 sec/batch)
2017-06-02 17:38:23.286392: step 8530, loss = 0.76 (2553.3 examples/sec; 0.050 sec/batch)
2017-06-02 17:38:24.399039: step 8540, loss = 0.79 (2393.4 examples/sec; 0.053 sec/batch)
2017-06-02 17:38:25.459136: step 8550, loss = 1.01 (2353.7 examples/sec; 0.054 sec/batch)
2017-06-02 17:38:26.509122: step 8560, loss = 0.72 (2338.3 examples/sec; 0.055 sec/batch)
2017-06-02 17:38:27.618706: step 8570, loss = 0.72 (2051.8 examples/sec; 0.062 sec/batch)
2017-06-02 17:38:28.766049: step 8580, loss = 0.64 (2423.8 examples/sec; 0.053 sec/batch)
2017-06-02 17:38:29.889354: step 8590, loss = 0.71 (2467.3 examples/sec; 0.052 sec/batch)
2017-06-02 17:38:30.975042: step 8600, loss = 0.84 (1858.9 examples/sec; 0.069 sec/batch)
2017-06-02 17:38:32.143688: step 8610, loss = 1.20 (2177.0 examples/sec; 0.059 sec/batch)
2017-06-02 17:38:33.237436: step 8620, loss = 0.69 (2397.1 examples/sec; 0.053 sec/batch)
2017-06-02 17:38:34.279663: step 8630, loss = 0.94 (2631.5 examples/sec; 0.049 sec/batch)
2017-06-02 17:38:35.350357: step 8640, loss = 0.77 (2503.4 examples/sec; 0.051 sec/batch)
2017-06-02 17:38:36.455186: step 8650, loss = 0.81 (2213.9 examples/sec; 0.058 sec/batch)
2017-06-02 17:38:37.548866: step 8660, loss = 0.96 (2108.9 examples/sec; 0.061 sec/batch)
2017-06-02 17:38:39.668077: step 8670, loss = 0.73 (10286.9 examples/sec; 0.012 sec/batch)
2017-06-02 17:38:40.611147: step 8680, loss = 0.69 (2667.0 examples/sec; 0.048 sec/batch)
2017-06-02 17:38:41.708547: step 8690, loss = 0.71 (2938.2 examples/sec; 0.044 sec/batch)
2017-06-02 17:38:42.772226: step 8700, loss = 0.82 (2223.1 examples/sec; 0.058 sec/batch)
2017-06-02 17:38:43.997727: step 8710, loss = 0.78 (2270.4 examples/sec; 0.056 sec/batch)
2017-06-02 17:38:45.085602: step 8720, loss = 0.79 (2146.7 examples/sec; 0.060 sec/batch)
2017-06-02 17:38:46.137931: step 8730, loss = 0.90 (2143.5 examples/sec; 0.060 sec/batch)
2017-06-02 17:38:47.214418: step 8740, loss = 0.57 (2550.8 examples/sec; 0.050 sec/batch)
2017-06-02 17:38:48.325880: step 8750, loss = 0.71 (2463.3 examples/sec; 0.052 sec/batch)
2017-06-02 17:38:49.413034: step 8760, loss = 0.84 (2599.7 examples/sec; 0.049 sec/batch)
2017-06-02 17:38:50.461069: step 8770, loss = 0.69 (2550.2 examples/sec; 0.050 sec/batch)
2017-06-02 17:38:51.535723: step 8780, loss = 0.72 (2431.7 examples/sec; 0.053 sec/batch)
2017-06-02 17:38:52.657360: step 8790, loss = 0.75 (2477.9 examples/sec; 0.052 sec/batch)
2017-06-02 17:38:53.746517: step 8800, loss = 0.72 (2290.7 examples/sec; 0.056 sec/batch)
2017-06-02 17:38:54.902424: step 8810, loss = 0.87 (2483.9 examples/sec; 0.052 sec/batch)
2017-06-02 17:38:55.958196: step 8820, loss = 0.74 (2259.1 examples/sec; 0.057 sec/batch)
2017-06-02 17:38:57.045554: step 8830, loss = 0.71 (2245.1 examples/sec; 0.057 sec/batch)
2017-06-02 17:38:58.158887: step 8840, loss = 0.72 (2466.5 examples/sec; 0.052 sec/batch)
2017-06-02 17:38:59.304845: step 8850, loss = 0.77 (2563.2 examples/sec; 0.050 sec/batch)
2017-06-02 17:39:00.389270: step 8860, loss = 0.78 (2255.4 examples/sec; 0.057 sec/batch)
2017-06-02 17:39:01.436142: step 8870, loss = 0.80 (2315.0 examples/sec; 0.055 sec/batch)
2017-06-02 17:39:02.523521: step 8880, loss = 0.90 (2038.1 examples/sec; 0.063 sec/batch)
2017-06-02 17:39:03.615672: step 8890, loss = 0.74 (2771.1 examples/sec; 0.046 sec/batch)
2017-06-02 17:39:04.632679: step 8900, loss = 0.87 (2638.8 examples/sec; 0.049 sec/batch)
2017-06-02 17:39:05.858326: step 8910, loss = 0.63 (2859.6 examples/sec; 0.045 sec/batch)
2017-06-02 17:39:06.945228: step 8920, loss = 0.66 (2421.5 examples/sec; 0.053 sec/batch)
2017-06-02 17:39:08.026258: step 8930, loss = 0.78 (2433.1 examples/sec; 0.053 sec/batch)
2017-06-02 17:39:09.083424: step 8940, loss = 0.74 (2586.8 examples/sec; 0.049 sec/batch)
2017-06-02 17:39:10.171031: step 8950, loss = 0.79 (2331.2 examples/sec; 0.055 sec/batch)
2017-06-02 17:39:11.210413: step 8960, loss = 0.81 (2549.8 examples/sec; 0.050 sec/batch)
2017-06-02 17:39:12.344800: step 8970, loss = 0.72 (2353.4 examples/sec; 0.054 sec/batch)
2017-06-02 17:39:13.439430: step 8980, loss = 0.67 (2751.8 examples/sec; 0.047 sec/batch)
2017-06-02 17:39:14.577907: step 8990, loss = 0.66 (2046.5 examples/sec; 0.063 sec/batch)
2017-06-02 17:39:15.674013: step 9000, loss = 0.73 (2898.4 examples/sec; 0.044 sec/batch)
2017-06-02 17:39:17.384570: step 9010, loss = 0.86 (2814.1 examples/sec; 0.045 sec/batch)
2017-06-02 17:39:18.467670: step 9020, loss = 0.87 (2134.5 examples/sec; 0.060 sec/batch)
2017-06-02 17:39:19.504427: step 9030, loss = 0.86 (2538.4 examples/sec; 0.050 sec/batch)
2017-06-02 17:39:20.585754: step 9040, loss = 0.71 (2357.2 examples/sec; 0.054 sec/batch)
2017-06-02 17:39:21.688217: step 9050, loss = 0.73 (2768.2 examples/sec; 0.046 sec/batch)
2017-06-02 17:39:22.782445: step 9060, loss = 0.79 (2233.8 examples/sec; 0.057 sec/batch)
2017-06-02 17:39:23.838778: step 9070, loss = 0.82 (2774.7 examples/sec; 0.046 sec/batch)
2017-06-02 17:39:24.894777: step 9080, loss = 0.79 (2932.4 examples/sec; 0.044 sec/batch)
2017-06-02 17:39:25.970411: step 9090, loss = 0.85 (2253.5 examples/sec; 0.057 sec/batch)
2017-06-02 17:39:27.025480: step 9100, loss = 0.75 (2581.1 examples/sec; 0.050 sec/batch)
2017-06-02 17:39:28.191013: step 9110, loss = 0.75 (2686.4 examples/sec; 0.048 sec/batch)
2017-06-02 17:39:29.276990: step 9120, loss = 0.75 (2243.4 examples/sec; 0.057 sec/batch)
2017-06-02 17:39:30.341391: step 9130, loss = 0.73 (2689.1 examples/sec; 0.048 sec/batch)
2017-06-02 17:39:31.432593: step 9140, loss = 0.85 (2896.2 examples/sec; 0.044 sec/batch)
2017-06-02 17:39:32.473262: step 9150, loss = 0.71 (2747.5 examples/sec; 0.047 sec/batch)
2017-06-02 17:39:33.567991: step 9160, loss = 0.87 (2068.8 examples/sec; 0.062 sec/batch)
2017-06-02 17:39:34.658044: step 9170, loss = 0.82 (2297.0 examples/sec; 0.056 sec/batch)
2017-06-02 17:39:35.738824: step 9180, loss = 0.76 (2465.4 examples/sec; 0.052 sec/batch)
2017-06-02 17:39:36.839680: step 9190, loss = 0.66 (2674.7 examples/sec; 0.048 sec/batch)
2017-06-02 17:39:37.951775: step 9200, loss = 0.69 (1976.8 examples/sec; 0.065 sec/batch)
2017-06-02 17:39:39.152169: step 9210, loss = 0.70 (2229.1 examples/sec; 0.057 sec/batch)
2017-06-02 17:39:40.253360: step 9220, loss = 0.72 (1986.8 examples/sec; 0.064 sec/batch)
2017-06-02 17:39:41.327492: step 9230, loss = 0.84 (2880.1 examples/sec; 0.044 sec/batch)
2017-06-02 17:39:42.413801: step 9240, loss = 0.78 (2476.7 examples/sec; 0.052 sec/batch)
2017-06-02 17:39:43.480184: step 9250, loss = 0.74 (2446.1 examples/sec; 0.052 sec/batch)
2017-06-02 17:39:44.545987: step 9260, loss = 0.91 (1882.2 examples/sec; 0.068 sec/batch)
2017-06-02 17:39:45.629306: step 9270, loss = 0.85 (2410.8 examples/sec; 0.053 sec/batch)
2017-06-02 17:39:46.734161: step 9280, loss = 0.95 (2580.8 examples/sec; 0.050 sec/batch)
2017-06-02 17:39:47.878250: step 9290, loss = 0.81 (1997.7 examples/sec; 0.064 sec/batch)
2017-06-02 17:39:48.964667: step 9300, loss = 0.70 (2438.8 examples/sec; 0.052 sec/batch)
2017-06-02 17:39:50.125060: step 9310, loss = 0.73 (2767.6 examples/sec; 0.046 sec/batch)
2017-06-02 17:39:51.225140: step 9320, loss = 0.74 (2383.1 examples/sec; 0.054 sec/batch)
2017-06-02 17:39:52.324151: step 9330, loss = 0.77 (2483.7 examples/sec; 0.052 sec/batch)
2017-06-02 17:39:53.402875: step 9340, loss = 0.78 (2270.0 examples/sec; 0.056 sec/batch)
2017-06-02 17:39:54.466357: step 9350, loss = 0.69 (2521.3 examples/sec; 0.051 sec/batch)
2017-06-02 17:39:55.748020: step 9360, loss = 0.73 (2211.4 examples/sec; 0.058 sec/batch)
2017-06-02 17:39:56.871679: step 9370, loss = 0.71 (2403.9 examples/sec; 0.053 sec/batch)
2017-06-02 17:39:57.913341: step 9380, loss = 0.83 (2317.0 examples/sec; 0.055 sec/batch)
2017-06-02 17:39:59.047740: step 9390, loss = 0.72 (2432.4 examples/sec; 0.053 sec/batch)
2017-06-02 17:40:00.185246: step 9400, loss = 0.87 (2502.0 examples/sec; 0.051 sec/batch)
2017-06-02 17:40:01.403570: step 9410, loss = 0.76 (2552.2 examples/sec; 0.050 sec/batch)
2017-06-02 17:40:02.508528: step 9420, loss = 0.78 (2364.5 examples/sec; 0.054 sec/batch)
2017-06-02 17:40:03.616297: step 9430, loss = 0.71 (2048.9 examples/sec; 0.062 sec/batch)
2017-06-02 17:40:04.756557: step 9440, loss = 0.63 (2222.1 examples/sec; 0.058 sec/batch)
2017-06-02 17:40:05.801719: step 9450, loss = 0.60 (2299.5 examples/sec; 0.056 sec/batch)
2017-06-02 17:40:06.876622: step 9460, loss = 0.77 (2516.8 examples/sec; 0.051 sec/batch)
2017-06-02 17:40:07.966419: step 9470, loss = 0.83 (2163.5 examples/sec; 0.059 sec/batch)
2017-06-02 17:40:09.075010: step 9480, loss = 0.85 (2473.6 examples/sec; 0.052 sec/batch)
2017-06-02 17:40:10.178437: step 9490, loss = 0.76 (2433.3 examples/sec; 0.053 sec/batch)
2017-06-02 17:40:11.234857: step 9500, loss = 0.84 (1948.9 examples/sec; 0.066 sec/batch)
2017-06-02 17:40:12.402181: step 9510, loss = 0.95 (2414.1 examples/sec; 0.053 sec/batch)
2017-06-02 17:40:13.483297: step 9520, loss = 0.81 (2315.2 examples/sec; 0.055 sec/batch)
2017-06-02 17:40:14.656904: step 9530, loss = 0.68 (2055.7 examples/sec; 0.062 sec/batch)
2017-06-02 17:40:15.748959: step 9540, loss = 0.84 (2135.1 examples/sec; 0.060 sec/batch)
2017-06-02 17:40:16.862492: step 9550, loss = 0.67 (2342.5 examples/sec; 0.055 sec/batch)
2017-06-02 17:40:18.090443: step 9560, loss = 0.97 (1884.2 examples/sec; 0.068 sec/batch)
2017-06-02 17:40:19.175096: step 9570, loss = 0.86 (2457.8 examples/sec; 0.052 sec/batch)
2017-06-02 17:40:20.294897: step 9580, loss = 0.80 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 17:40:21.423040: step 9590, loss = 0.82 (2378.0 examples/sec; 0.054 sec/batch)
2017-06-02 17:40:22.534195: step 9600, loss = 0.85 (2415.6 examples/sec; 0.053 sec/batch)
2017-06-02 17:40:23.721296: step 9610, loss = 0.72 (2362.1 examples/sec; 0.054 sec/batch)
2017-06-02 17:40:24.793182: step 9620, loss = 0.83 (2285.7 examples/sec; 0.056 sec/batch)
2017-06-02 17:40:25.879045: step 9630, loss = 0.72 (2144.8 examples/sec; 0.060 sec/batch)
2017-06-02 17:40:26.954588: step 9640, loss = 0.74 (2331.5 examples/sec; 0.055 sec/batch)
2017-06-02 17:40:28.089433: step 9650, loss = 0.88 (2485.8 examples/sec; 0.051 sec/batch)
2017-06-02 17:40:29.255539: step 9660, loss = 0.71 (2482.4 examples/sec; 0.052 sec/batch)
2017-06-02 17:40:30.352446: step 9670, loss = 0.73 (2215.5 examples/sec; 0.058 sec/batch)
2017-06-02 17:40:31.501484: step 9680, loss = 0.89 (2128.1 examples/sec; 0.060 sec/batch)
2017-06-02 17:40:32.636593: step 9690, loss = 0.76 (2513.4 examples/sec; 0.051 sec/batch)
2017-06-02 17:40:33.719294: step 9700, loss = 0.80 (2294.2 examples/sec; 0.056 sec/batch)
2017-06-02 17:40:34.896304: step 9710, loss = 0.82 (2319.3 examples/sec; 0.055 sec/batch)
2017-06-02 17:40:35.989134: step 9720, loss = 0.74 (2194.6 examples/sec; 0.058 sec/batch)
2017-06-02 17:40:37.055409: step 9730, loss = 0.72 (2861.5 examples/sec; 0.045 sec/batch)
2017-06-02 17:40:38.205143: step 9740, loss = 0.84 (2867.8 examples/sec; 0.045 sec/batch)
2017-06-02 17:40:39.345185: step 9750, loss = 0.90 (2041.0 examples/sec; 0.063 sec/batch)
2017-06-02 17:40:40.441842: step 9760, loss = 0.75 (2348.0 examples/sec; 0.055 sec/batch)
2017-06-02 17:40:41.564849: step 9770, loss = 0.79 (2208.3 examples/sec; 0.058 sec/batch)
2017-06-02 17:40:42.665851: step 9780, loss = 0.67 (2415.6 examples/sec; 0.053 sec/batch)
2017-06-02 17:40:43.772426: step 9790, loss = 0.71 (2189.6 examples/sec; 0.058 sec/batch)
2017-06-02 17:40:44.855472: step 9800, loss = 0.87 (2446.1 examples/sec; 0.052 sec/batch)
2017-06-02 17:40:46.017857: step 9810, loss = 0.75 (2263.2 examples/sec; 0.057 sec/batch)
2017-06-02 17:40:47.105073: step 9820, loss = 0.69 (2122.4 examples/sec; 0.060 sec/batch)
2017-06-02 17:40:48.256613: step 9830, loss = 0.76 (2172.7 examples/sec; 0.059 sec/batch)
2017-06-02 17:40:49.329486: step 9840, loss = 0.71 (2338.0 examples/sec; 0.055 sec/batch)
2017-06-02 17:40:50.382912: step 9850, loss = 0.87 (2403.3 examples/sec; 0.053 sec/batch)
2017-06-02 17:40:51.459241: step 9860, loss = 0.63 (2375.0 examples/sec; 0.054 sec/batch)
2017-06-02 17:40:52.565639: step 9870, loss = 0.66 (1654.4 examples/sec; 0.077 sec/batch)
2017-06-02 17:40:53.896988: step 9880, loss = 0.79 (1995.5 examples/sec; 0.064 sec/batch)
2017-06-02 17:40:54.948217: step 9890, loss = 0.80 (2680.6 examples/sec; 0.048 sec/batch)
2017-06-02 17:40:56.049511: step 9900, loss = 0.73 (2555.3 examples/sec; 0.050 sec/batch)
2017-06-02 17:40:57.271800: step 9910, loss = 0.71 (2207.0 examples/sec; 0.058 sec/batch)
2017-06-02 17:40:58.350037: step 9920, loss = 0.80 (2578.1 examples/sec; 0.050 sec/batch)
2017-06-02 17:40:59.505979: step 9930, loss = 0.81 (2198.1 examples/sec; 0.058 sec/batch)
2017-06-02 17:41:00.594600: step 9940, loss = 0.77 (2123.4 examples/sec; 0.060 sec/batch)
2017-06-02 17:41:01.669905: step 9950, loss = 0.74 (2166.6 examples/sec; 0.059 sec/batch)
2017-06-02 17:41:02.739453: step 9960, loss = 0.85 (2476.9 examples/sec; 0.052 sec/batch)
2017-06-02 17:41:03.907701: step 9970, loss = 0.63 (2247.0 examples/sec; 0.057 sec/batch)
2017-06-02 17:41:04.985889: step 9980, loss = 0.93 (2476.0 examples/sec; 0.052 sec/batch)
2017-06-02 17:41:06.052791: step 9990, loss = 0.70 (2276.3 examples/sec; 0.056 sec/batch)
