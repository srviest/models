Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-06-02 15:31:30.133263: step 0, loss = 4.68 (66.3 examples/sec; 1.930 sec/batch)
2017-06-02 15:31:30.899170: step 10, loss = 4.62 (1671.1 examples/sec; 0.077 sec/batch)
2017-06-02 15:31:31.776251: step 20, loss = 4.37 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:31:32.668686: step 30, loss = 4.35 (1434.3 examples/sec; 0.089 sec/batch)
2017-06-02 15:31:33.552299: step 40, loss = 4.35 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:31:34.428233: step 50, loss = 4.26 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:31:35.299510: step 60, loss = 4.17 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 15:31:36.181320: step 70, loss = 4.23 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:31:37.069249: step 80, loss = 4.13 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 15:31:37.941622: step 90, loss = 4.07 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 15:31:38.937722: step 100, loss = 4.13 (1285.0 examples/sec; 0.100 sec/batch)
2017-06-02 15:31:39.695465: step 110, loss = 4.06 (1689.2 examples/sec; 0.076 sec/batch)
2017-06-02 15:31:40.575453: step 120, loss = 4.19 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:31:41.460648: step 130, loss = 3.90 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 15:31:42.340355: step 140, loss = 4.12 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:31:43.234767: step 150, loss = 4.07 (1431.1 examples/sec; 0.089 sec/batch)
2017-06-02 15:31:44.121827: step 160, loss = 4.01 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 15:31:45.003966: step 170, loss = 4.01 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:31:45.879213: step 180, loss = 3.91 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:31:46.766617: step 190, loss = 3.85 (1442.4 examples/sec; 0.089 sec/batch)
2017-06-02 15:31:47.771709: step 200, loss = 3.83 (1273.5 examples/sec; 0.101 sec/batch)
2017-06-02 15:31:48.527814: step 210, loss = 3.70 (1692.9 examples/sec; 0.076 sec/batch)
2017-06-02 15:31:49.405345: step 220, loss = 3.61 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:31:50.284833: step 230, loss = 3.59 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:31:51.158315: step 240, loss = 3.67 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:31:52.035055: step 250, loss = 3.54 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:31:52.908149: step 260, loss = 3.74 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 15:31:53.785077: step 270, loss = 3.70 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:31:54.663613: step 280, loss = 3.61 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:31:55.551337: step 290, loss = 3.62 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 15:31:56.569096: step 300, loss = 3.60 (1257.7 examples/sec; 0.102 sec/batch)
2017-06-02 15:31:57.329033: step 310, loss = 3.80 (1684.3 examples/sec; 0.076 sec/batch)
2017-06-02 15:31:58.205215: step 320, loss = 3.55 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:31:59.078002: step 330, loss = 3.67 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 15:31:59.940678: step 340, loss = 3.50 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 15:32:00.812362: step 350, loss = 3.51 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:32:01.700450: step 360, loss = 3.47 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 15:32:02.590109: step 370, loss = 3.34 (1438.8 examples/sec; 0.089 sec/batch)
2017-06-02 15:32:03.484332: step 380, loss = 3.48 (1431.4 examples/sec; 0.089 sec/batch)
2017-06-02 15:32:04.369004: step 390, loss = 3.47 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:32:05.388401: step 400, loss = 3.32 (1255.7 examples/sec; 0.102 sec/batch)
2017-06-02 15:32:06.164414: step 410, loss = 3.51 (1649.5 examples/sec; 0.078 sec/batch)
2017-06-02 15:32:07.050037: step 420, loss = 3.48 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 15:32:07.944397: step 430, loss = 3.27 (1431.2 examples/sec; 0.089 sec/batch)
2017-06-02 15:32:08.816148: step 440, loss = 3.20 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 15:32:09.692101: step 450, loss = 3.23 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:32:10.576123: step 460, loss = 3.16 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:32:11.466922: step 470, loss = 3.18 (1436.9 examples/sec; 0.089 sec/batch)
2017-06-02 15:32:12.347494: step 480, loss = 3.12 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:32:13.233028: step 490, loss = 3.18 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 15:32:14.236829: step 500, loss = 3.16 (1275.1 examples/sec; 0.100 sec/batch)
2017-06-02 15:32:14.998216: step 510, loss = 3.27 (1681.1 examples/sec; 0.076 sec/batch)
2017-06-02 15:32:15.883114: step 520, loss = 3.35 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:32:16.762708: step 530, loss = 3.23 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:32:17.636219: step 540, loss = 3.05 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 15:32:18.509065: step 550, loss = 3.06 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 15:32:19.390045: step 560, loss = 3.00 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:32:20.283345: step 570, loss = 3.13 (1432.9 examples/sec; 0.089 sec/batch)
2017-06-02 15:32:21.163014: step 580, loss = 3.09 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:32:22.033107: step 590, loss = 3.11 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 15:32:23.010608: step 600, loss = 3.17 (1309.5 examples/sec; 0.098 sec/batch)
2017-06-02 15:32:23.773294: step 610, loss = 3.11 (1678.3 examples/sec; 0.076 sec/batch)
2017-06-02 15:32:24.650836: step 620, loss = 3.00 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:32:25.516310: step 630, loss = 2.97 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 15:32:26.409705: step 640, loss = 2.89 (1432.7 examples/sec; 0.089 sec/batch)
2017-06-02 15:32:27.292458: step 650, loss = 2.95 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:32:28.178193: step 660, loss = 2.96 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 15:32:29.066470: step 670, loss = 2.76 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 15:32:29.944347: step 680, loss = 2.88 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:32:30.827526: step 690, loss = 2.89 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:32:31.855495: step 700, loss = 2.92 (1245.2 examples/sec; 0.103 sec/batch)
2017-06-02 15:32:32.580606: step 710, loss = 2.85 (1765.2 examples/sec; 0.073 sec/batch)
2017-06-02 15:32:33.467602: step 720, loss = 2.68 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 15:32:34.335301: step 730, loss = 2.89 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 15:32:35.209893: step 740, loss = 2.87 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 15:32:36.091854: step 750, loss = 2.65 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:32:36.975912: step 760, loss = 2.70 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:32:37.857073: step 770, loss = 2.66 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:32:38.744319: step 780, loss = 2.47 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 15:32:39.621937: step 790, loss = 2.57 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:32:40.640633: step 800, loss = 2.74 (1256.5 examples/sec; 0.102 sec/batch)
2017-06-02 15:32:41.389186: step 810, loss = 2.61 (1710.0 examples/sec; 0.075 sec/batch)
2017-06-02 15:32:42.274797: step 820, loss = 2.69 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 15:32:43.166350: step 830, loss = 2.72 (1435.7 examples/sec; 0.089 sec/batch)
2017-06-02 15:32:44.062756: step 840, loss = 2.44 (1427.9 examples/sec; 0.090 sec/batch)
2017-06-02 15:32:44.937901: step 850, loss = 2.71 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:32:45.817071: step 860, loss = 2.66 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:32:46.680902: step 870, loss = 2.63 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 15:32:47.555314: step 880, loss = 2.79 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 15:32:48.433132: step 890, loss = 2.51 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:32:49.425374: step 900, loss = 2.59 (1290.0 examples/sec; 0.099 sec/batch)
2017-06-02 15:32:50.181777: step 910, loss = 2.89 (1692.2 examples/sec; 0.076 sec/batch)
2017-06-02 15:32:51.076114: step 920, loss = 2.46 (1431.2 examples/sec; 0.089 sec/batch)
2017-06-02 15:32:51.973299: step 930, loss = 2.42 (1426.7 examples/sec; 0.090 sec/batch)
2017-06-02 15:32:52.846076: step 940, loss = 2.43 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 15:32:53.734950: step 950, loss = 2.69 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 15:32:54.619064: step 960, loss = 2.57 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:32:55.507370: step 970, loss = 2.45 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 15:32:56.395765: step 980, loss = 2.51 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 15:32:57.266055: step 990, loss = 2.30 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 15:32:58.253724: step 1000, loss = 2.34 (1296.0 examples/sec; 0.099 sec/batch)
2017-06-02 15:32:59.047124: step 1010, loss = 2.50 (1613.3 examples/sec; 0.079 sec/batch)
2017-06-02 15:32:59.933581: step 1020, loss = 2.44 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 15:33:00.828705: step 1030, loss = 2.44 (1430.0 examples/sec; 0.090 sec/batch)
2017-06-02 15:33:01.713123: step 1040, loss = 2.37 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:02.589711: step 1050, loss = 2.60 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:03.487805: step 1060, loss = 2.54 (1425.2 examples/sec; 0.090 sec/batch)
2017-06-02 15:33:04.370954: step 1070, loss = 2.52 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:05.253140: step 1080, loss = 2.55 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:06.129077: step 1090, loss = 2.40 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:07.106213: step 1100, loss = 2.37 (1310.0 examples/sec; 0.098 sec/batch)
2017-06-02 15:33:07.879512: step 1110, loss = 2.25 (1655.2 examples/sec; 0.077 sec/batch)
2017-06-02 15:33:08.774118: step 1120, loss = 2.35 (1430.8 examples/sec; 0.089 sec/batch)
2017-06-02 15:33:09.642785: step 1130, loss = 2.35 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 15:33:10.521560: step 1140, loss = 2.29 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:11.387353: step 1150, loss = 2.37 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:33:12.264395: step 1160, loss = 2.32 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:13.147059: step 1170, loss = 2.25 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:14.035267: step 1180, loss = 2.25 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 15:33:14.913188: step 1190, loss = 2.19 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:15.902269: step 1200, loss = 2.08 (1294.1 examples/sec; 0.099 sec/batch)
2017-06-02 15:33:16.675582: step 1210, loss = 2.09 (1655.2 examples/sec; 0.077 sec/batch)
2017-06-02 15:33:17.560235: step 1220, loss = 2.37 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:18.427464: step 1230, loss = 1.98 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 15:33:19.311709: step 1240, loss = 2.24 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:20.185396: step 1250, loss = 2.37 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 15:33:21.082511: step 1260, loss = 2.10 (1426.8 examples/sec; 0.090 sec/batch)
2017-06-02 15:33:21.960808: step 1270, loss = 2.22 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:22.838930: step 1280, loss = 2.23 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:23.720741: step 1290, loss = 2.33 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:24.725100: step 1300, loss = 2.18 (1274.4 examples/sec; 0.100 sec/batch)
2017-06-02 15:33:25.491934: step 1310, loss = 2.21 (1669.2 examples/sec; 0.077 sec/batch)
2017-06-02 15:33:26.354586: step 1320, loss = 2.02 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 15:33:27.235378: step 1330, loss = 2.06 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:28.108966: step 1340, loss = 2.09 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 15:33:28.996754: step 1350, loss = 2.10 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 15:33:29.879851: step 1360, loss = 1.94 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:30.781618: step 1370, loss = 2.10 (1419.5 examples/sec; 0.090 sec/batch)
2017-06-02 15:33:31.663836: step 1380, loss = 2.30 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:32.535179: step 1390, loss = 2.09 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 15:33:33.508658: step 1400, loss = 2.13 (1314.9 examples/sec; 0.097 sec/batch)
2017-06-02 15:33:34.280609: step 1410, loss = 2.04 (1658.1 examples/sec; 0.077 sec/batch)
2017-06-02 15:33:35.164414: step 1420, loss = 1.97 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:36.043824: step 1430, loss = 2.07 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:36.936620: step 1440, loss = 1.94 (1433.7 examples/sec; 0.089 sec/batch)
2017-06-02 15:33:37.823311: step 1450, loss = 2.21 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 15:33:38.701029: step 1460, loss = 2.07 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:39.585325: step 1470, loss = 1.86 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:40.467010: step 1480, loss = 1.99 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:41.348847: step 1490, loss = 2.16 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:42.334791: step 1500, loss = 1.76 (1298.3 examples/sec; 0.099 sec/batch)
2017-06-02 15:33:43.107227: step 1510, loss = 2.42 (1657.1 examples/sec; 0.077 sec/batch)
2017-06-02 15:33:44.000450: step 1520, loss = 1.98 (1433.0 examples/sec; 0.089 sec/batch)
2017-06-02 15:33:44.884194: step 1530, loss = 1.94 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:45.775160: step 1540, loss = 2.01 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 15:33:46.656256: step 1550, loss = 1.82 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:47.553318: step 1560, loss = 1.79 (1426.9 examples/sec; 0.090 sec/batch)
2017-06-02 15:33:48.443118: step 1570, loss = 2.24 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 15:33:49.337713: step 1580, loss = 1.84 (1430.8 examples/sec; 0.089 sec/batch)
2017-06-02 15:33:50.225510: step 1590, loss = 1.95 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 15:33:51.220864: step 1600, loss = 1.90 (1286.0 examples/sec; 0.100 sec/batch)
2017-06-02 15:33:52.000689: step 1610, loss = 1.87 (1641.4 examples/sec; 0.078 sec/batch)
2017-06-02 15:33:52.879853: step 1620, loss = 1.76 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:53.776246: step 1630, loss = 1.80 (1428.0 examples/sec; 0.090 sec/batch)
2017-06-02 15:33:54.664700: step 1640, loss = 1.88 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 15:33:55.546033: step 1650, loss = 1.79 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:56.418121: step 1660, loss = 1.82 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 15:33:57.298256: step 1670, loss = 1.92 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:33:58.184877: step 1680, loss = 1.73 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 15:33:59.075097: step 1690, loss = 1.82 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 15:34:00.068494: step 1700, loss = 1.58 (1288.5 examples/sec; 0.099 sec/batch)
2017-06-02 15:34:00.850436: step 1710, loss = 1.96 (1636.9 examples/sec; 0.078 sec/batch)
2017-06-02 15:34:01.729931: step 1720, loss = 1.78 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:34:02.603970: step 1730, loss = 1.88 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 15:34:03.495690: step 1740, loss = 1.83 (1435.4 examples/sec; 0.089 sec/batch)
2017-06-02 15:34:04.389683: step 1750, loss = 1.73 (1431.8 examples/sec; 0.089 sec/batch)
2017-06-02 15:34:05.293154: step 1760, loss = 1.71 (1416.8 examples/sec; 0.090 sec/batch)
2017-06-02 15:34:06.177079: step 1770, loss = 1.72 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:34:07.088002: step 1780, loss = 1.91 (1405.2 examples/sec; 0.091 sec/batch)
2017-06-02 15:34:07.964682: step 1790, loss = 1.84 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:34:08.952321: step 1800, loss = 1.71 (1296.0 examples/sec; 0.099 sec/batch)
2017-06-02 15:34:09.722572: step 1810, loss = 1.79 (1661.8 examples/sec; 0.077 sec/batch)
2017-06-02 15:34:10.594948: step 1820, loss = 1.70 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 15:34:11.484573: step 1830, loss = 1.74 (1438.8 examples/sec; 0.089 sec/batch)
2017-06-02 15:34:12.368420: step 1840, loss = 1.63 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:34:13.265914: step 1850, loss = 1.99 (1426.2 examples/sec; 0.090 sec/batch)
2017-06-02 15:34:14.140145: step 1860, loss = 1.76 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 15:34:15.022404: step 1870, loss = 1.83 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:34:15.918128: step 1880, loss = 1.77 (1429.0 examples/sec; 0.090 sec/batch)
2017-06-02 15:34:16.798355: step 1890, loss = 1.88 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:34:17.788516: step 1900, loss = 1.64 (1292.7 examples/sec; 0.099 sec/batch)
2017-06-02 15:34:18.559728: step 1910, loss = 1.66 (1659.7 examples/sec; 0.077 sec/batch)
2017-06-02 15:34:19.442110: step 1920, loss = 1.79 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:34:20.331773: step 1930, loss = 1.89 (1438.8 examples/sec; 0.089 sec/batch)
2017-06-02 15:34:21.206406: step 1940, loss = 1.44 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:34:22.089679: step 1950, loss = 1.73 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:34:22.970898: step 1960, loss = 1.72 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:34:23.850605: step 1970, loss = 1.53 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:34:24.729271: step 1980, loss = 1.77 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:34:25.620149: step 1990, loss = 1.65 (1436.8 examples/sec; 0.089 sec/batch)
2017-06-02 15:34:26.618525: step 2000, loss = 1.63 (1282.1 examples/sec; 0.100 sec/batch)
2017-06-02 15:34:27.383252: step 2010, loss = 1.51 (1673.8 examples/sec; 0.076 sec/batch)
2017-06-02 15:34:28.262102: step 2020, loss = 1.66 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:34:29.153932: step 2030, loss = 1.67 (1435.2 examples/sec; 0.089 sec/batch)
2017-06-02 15:34:30.063881: step 2040, loss = 1.76 (1406.7 examples/sec; 0.091 sec/batch)
2017-06-02 15:34:30.944602: step 2050, loss = 1.50 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:34:31.816998: step 2060, loss = 1.73 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 15:34:32.701644: step 2070, loss = 1.65 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:34:33.594961: step 2080, loss = 1.52 (1432.9 examples/sec; 0.089 sec/batch)
2017-06-02 15:34:34.466804: step 2090, loss = 1.70 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 15:34:35.464406: step 2100, loss = 1.47 (1283.1 examples/sec; 0.100 sec/batch)
2017-06-02 15:34:36.220466: step 2110, loss = 1.68 (1693.0 examples/sec; 0.076 sec/batch)
2017-06-02 15:34:37.103376: step 2120, loss = 1.65 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:34:37.972761: step 2130, loss = 1.40 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 15:34:38.856412: step 2140, loss = 1.58 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:34:39.727803: step 2150, loss = 1.48 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 15:34:40.603688: step 2160, loss = 1.54 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:34:41.471651: step 2170, loss = 1.56 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 15:34:42.354947: step 2180, loss = 1.42 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:34:43.229533: step 2190, loss = 1.58 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 15:34:44.205495: step 2200, loss = 1.60 (1311.5 examples/sec; 0.098 sec/batch)
2017-06-02 15:34:44.982906: step 2210, loss = 1.61 (1646.5 examples/sec; 0.078 sec/batch)
2017-06-02 15:34:45.855815: step 2220, loss = 1.55 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:34:46.743266: step 2230, loss = 1.44 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 15:34:47.628156: step 2240, loss = 1.44 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:34:48.494027: step 2250, loss = 1.61 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 15:34:49.366880: step 2260, loss = 1.59 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:34:50.239741: step 2270, loss = 1.36 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 15:34:51.110643: step 2280, loss = 1.44 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 15:34:51.992412: step 2290, loss = 1.55 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:34:52.971224: step 2300, loss = 1.38 (1307.7 examples/sec; 0.098 sec/batch)
2017-06-02 15:34:53.740537: step 2310, loss = 1.47 (1663.8 examples/sec; 0.077 sec/batch)
2017-06-02 15:34:54.615655: step 2320, loss = 1.83 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:34:55.501606: step 2330, loss = 1.44 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 15:34:56.374967: step 2340, loss = 1.62 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 15:34:57.254718: step 2350, loss = 1.59 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:34:58.135856: step 2360, loss = 1.48 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:34:59.006546: step 2370, loss = 1.44 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 15:34:59.881531: step 2380, loss = 1.37 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 15:35:00.753756: step 2390, loss = 1.59 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 15:35:01.731632: step 2400, loss = 1.47 (1309.0 examples/sec; 0.098 sec/batch)
2017-06-02 15:35:02.501643: step 2410, loss = 1.41 (1662.3 examples/sec; 0.077 sec/batch)
2017-06-02 15:35:03.377759: step 2420, loss = 1.45 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:35:04.247298: step 2430, loss = 1.17 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 15:35:05.115350: step 2440, loss = 1.36 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 15:35:05.995054: step 2450, loss = 1.45 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:35:06.875672: step 2460, loss = 1.43 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:35:07.753278: step 2470, loss = 1.39 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:35:08.620423: step 2480, loss = 1.36 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 15:35:09.511123: step 2490, loss = 1.41 (1437.1 examples/sec; 0.089 sec/batch)
2017-06-02 15:35:10.507828: step 2500, loss = 1.35 (1284.2 examples/sec; 0.100 sec/batch)
2017-06-02 15:35:11.270393: step 2510, loss = 1.43 (1678.5 examples/sec; 0.076 sec/batch)
2017-06-02 15:35:12.150845: step 2520, loss = 1.38 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:35:13.035919: step 2530, loss = 1.39 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 15:35:13.919617: step 2540, loss = 1.47 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:35:14.795902: step 2550, loss = 1.45 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:35:15.676196: step 2560, loss = 1.42 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:35:16.545932: step 2570, loss = 1.25 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 15:35:17.433310: step 2580, loss = 1.17 (1442.4 examples/sec; 0.089 sec/batch)
2017-06-02 15:35:18.303416: step 2590, loss = 1.22 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 15:35:19.294655: step 2600, loss = 1.39 (1291.3 examples/sec; 0.099 sec/batch)
2017-06-02 15:35:20.086826: step 2610, loss = 1.43 (1615.8 examples/sec; 0.079 sec/batch)
2017-06-02 15:35:20.955878: step 2620, loss = 1.29 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 15:35:21.836183: step 2630, loss = 1.25 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:35:22.716024: step 2640, loss = 1.31 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:35:23.599291: step 2650, loss = 1.37 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:35:24.476759: step 2660, loss = 1.37 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:35:25.358919: step 2670, loss = 1.33 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:35:26.232837: step 2680, loss = 1.33 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 15:35:27.102120: step 2690, loss = 1.43 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 15:35:28.080574: step 2700, loss = 1.24 (1308.2 examples/sec; 0.098 sec/batch)
2017-06-02 15:35:28.850660: step 2710, loss = 1.57 (1662.2 examples/sec; 0.077 sec/batch)
2017-06-02 15:35:29.734969: step 2720, loss = 1.38 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:35:30.618799: step 2730, loss = 1.44 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:35:31.494881: step 2740, loss = 1.31 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:35:32.388943: step 2750, loss = 1.37 (1431.7 examples/sec; 0.089 sec/batch)
2017-06-02 15:35:33.272800: step 2760, loss = 1.20 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:35:34.139202: step 2770, loss = 1.38 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:35:35.027577: step 2780, loss = 1.30 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 15:35:35.907198: step 2790, loss = 1.41 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:35:36.889714: step 2800, loss = 1.61 (1302.8 examples/sec; 0.098 sec/batch)
2017-06-02 15:35:37.668956: step 2810, loss = 1.32 (1642.6 examples/sec; 0.078 sec/batch)
2017-06-02 15:35:38.556977: step 2820, loss = 1.48 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 15:35:39.435299: step 2830, loss = 1.31 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:35:40.313378: step 2840, loss = 1.24 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:35:41.187135: step 2850, loss = 1.20 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 15:35:42.071446: step 2860, loss = 1.47 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:35:42.959910: step 2870, loss = 1.47 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 15:35:43.836226: step 2880, loss = 1.25 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:35:44.706072: step 2890, loss = 1.42 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 15:35:45.698523: step 2900, loss = 1.25 (1289.7 examples/sec; 0.099 sec/batch)
2017-06-02 15:35:46.474230: step 2910, loss = 1.38 (1650.1 examples/sec; 0.078 sec/batch)
2017-06-02 15:35:47.352878: step 2920, loss = 1.24 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:35:48.224806: step 2930, loss = 1.32 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 15:35:49.101937: step 2940, loss = 1.20 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:35:49.971361: step 2950, loss = 1.44 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 15:35:50.860941: step 2960, loss = 1.32 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 15:35:51.731875: step 2970, loss = 1.22 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 15:35:52.605086: step 2980, loss = 1.42 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 15:35:53.495619: step 2990, loss = 1.28 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 15:35:54.469602: step 3000, loss = 1.47 (1314.2 examples/sec; 0.097 sec/batch)
2017-06-02 15:35:55.248487: step 3010, loss = 1.35 (1643.4 examples/sec; 0.078 sec/batch)
2017-06-02 15:35:56.117030: step 3020, loss = 1.30 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 15:35:56.992451: step 3030, loss = 1.26 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:35:57.870182: step 3040, loss = 1.28 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:35:58.746905: step 3050, loss = 1.48 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:35:59.628082: step 3060, loss = 1.33 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:36:00.501121: step 3070, loss = 1.34 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 15:36:01.375979: step 3080, loss = 1.20 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 15:36:02.249247: step 3090, loss = 1.28 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 15:36:03.236846: step 3100, loss = 1.25 (1296.1 examples/sec; 0.099 sec/batch)
2017-06-02 15:36:04.012235: step 3110, loss = 1.34 (1650.8 examples/sec; 0.078 sec/batch)
2017-06-02 15:36:04.884932: step 3120, loss = 1.16 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 15:36:05.771120: step 3130, loss = 1.31 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 15:36:06.654460: step 3140, loss = 1.21 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:36:07.525672: step 3150, loss = 1.24 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 15:36:08.390207: step 3160, loss = 1.18 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 15:36:09.262353: step 3170, loss = 1.22 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 15:36:10.137206: step 3180, loss = 1.11 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 15:36:11.028325: step 3190, loss = 1.37 (1436.4 examples/sec; 0.089 sec/batch)
2017-06-02 15:36:12.008341: step 3200, loss = 1.28 (1306.1 examples/sec; 0.098 sec/batch)
2017-06-02 15:36:12.778811: step 3210, loss = 1.28 (1661.4 examples/sec; 0.077 sec/batch)
2017-06-02 15:36:13.665222: step 3220, loss = 1.16 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 15:36:14.528113: step 3230, loss = 1.20 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 15:36:15.389976: step 3240, loss = 1.19 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 15:36:16.292673: step 3250, loss = 1.24 (1418.0 examples/sec; 0.090 sec/batch)
2017-06-02 15:36:17.167862: step 3260, loss = 1.09 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:36:18.043865: step 3270, loss = 1.16 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:36:18.931785: step 3280, loss = 1.04 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 15:36:19.829222: step 3290, loss = 1.22 (1426.3 examples/sec; 0.090 sec/batch)
2017-06-02 15:36:20.799698: step 3300, loss = 1.24 (1319.0 examples/sec; 0.097 sec/batch)
2017-06-02 15:36:21.569442: step 3310, loss = 1.22 (1662.9 examples/sec; 0.077 sec/batch)
2017-06-02 15:36:22.439185: step 3320, loss = 1.48 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 15:36:23.320488: step 3330, loss = 1.15 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:36:24.208744: step 3340, loss = 1.36 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 15:36:25.077372: step 3350, loss = 1.38 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 15:36:25.959081: step 3360, loss = 1.22 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:36:26.857327: step 3370, loss = 1.37 (1425.0 examples/sec; 0.090 sec/batch)
2017-06-02 15:36:27.727971: step 3380, loss = 1.26 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 15:36:28.604280: step 3390, loss = 1.02 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:36:29.583543: step 3400, loss = 1.32 (1307.1 examples/sec; 0.098 sec/batch)
2017-06-02 15:36:30.348382: step 3410, loss = 1.08 (1673.6 examples/sec; 0.076 sec/batch)
2017-06-02 15:36:31.224953: step 3420, loss = 1.06 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:36:32.090197: step 3430, loss = 1.32 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:36:32.967025: step 3440, loss = 1.39 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:36:33.841436: step 3450, loss = 1.12 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 15:36:34.709338: step 3460, loss = 0.89 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 15:36:35.597018: step 3470, loss = 1.25 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 15:36:36.467489: step 3480, loss = 1.02 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 15:36:37.349234: step 3490, loss = 1.37 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:36:38.334897: step 3500, loss = 1.18 (1298.6 examples/sec; 0.099 sec/batch)
2017-06-02 15:36:39.081416: step 3510, loss = 1.04 (1714.6 examples/sec; 0.075 sec/batch)
2017-06-02 15:36:39.963758: step 3520, loss = 1.26 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:36:40.840645: step 3530, loss = 1.28 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:36:41.722818: step 3540, loss = 1.06 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:36:42.612629: step 3550, loss = 1.12 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 15:36:43.492672: step 3560, loss = 1.13 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:36:44.369826: step 3570, loss = 1.16 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:36:45.262627: step 3580, loss = 1.13 (1433.7 examples/sec; 0.089 sec/batch)
2017-06-02 15:36:46.138098: step 3590, loss = 1.18 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:36:47.131940: step 3600, loss = 1.28 (1287.9 examples/sec; 0.099 sec/batch)
2017-06-02 15:36:47.911417: step 3610, loss = 1.25 (1642.1 examples/sec; 0.078 sec/batch)
2017-06-02 15:36:48.796810: step 3620, loss = 1.20 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 15:36:49.685816: step 3630, loss = 1.35 (1439.8 examples/sec; 0.089 sec/batch)
2017-06-02 15:36:50.569925: step 3640, loss = 1.11 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:36:51.460561: step 3650, loss = 1.16 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 15:36:52.347841: step 3660, loss = 1.18 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 15:36:53.235346: step 3670, loss = 1.11 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 15:36:54.114099: step 3680, loss = 1.19 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:36:54.985219: step 3690, loss = 1.11 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:36:55.961194: step 3700, loss = 1.14 (1311.5 examples/sec; 0.098 sec/batch)
2017-06-02 15:36:56.746709: step 3710, loss = 1.17 (1629.5 examples/sec; 0.079 sec/batch)
2017-06-02 15:36:57.622669: step 3720, loss = 1.02 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:36:58.495709: step 3730, loss = 1.08 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 15:36:59.389344: step 3740, loss = 1.14 (1432.4 examples/sec; 0.089 sec/batch)
2017-06-02 15:37:00.271514: step 3750, loss = 1.06 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:01.160268: step 3760, loss = 1.04 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 15:37:02.035162: step 3770, loss = 1.17 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 15:37:02.922715: step 3780, loss = 1.03 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 15:37:03.794319: step 3790, loss = 1.16 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 15:37:04.780614: step 3800, loss = 1.18 (1297.8 examples/sec; 0.099 sec/batch)
2017-06-02 15:37:05.556505: step 3810, loss = 1.06 (1649.7 examples/sec; 0.078 sec/batch)
2017-06-02 15:37:06.437817: step 3820, loss = 1.04 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:07.312560: step 3830, loss = 1.16 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 15:37:08.187651: step 3840, loss = 1.22 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:09.067782: step 3850, loss = 0.99 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:09.949487: step 3860, loss = 1.15 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:10.820240: step 3870, loss = 1.13 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 15:37:11.691200: step 3880, loss = 1.20 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 15:37:12.569766: step 3890, loss = 1.08 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:13.561754: step 3900, loss = 1.11 (1290.3 examples/sec; 0.099 sec/batch)
2017-06-02 15:37:14.338169: step 3910, loss = 1.14 (1648.6 examples/sec; 0.078 sec/batch)
2017-06-02 15:37:15.224331: step 3920, loss = 1.21 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 15:37:16.100801: step 3930, loss = 1.06 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:16.977061: step 3940, loss = 1.09 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:17.856224: step 3950, loss = 0.98 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:18.750845: step 3960, loss = 1.13 (1430.8 examples/sec; 0.089 sec/batch)
2017-06-02 15:37:19.628119: step 3970, loss = 1.11 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:20.506624: step 3980, loss = 1.09 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:21.404712: step 3990, loss = 1.14 (1425.2 examples/sec; 0.090 sec/batch)
2017-06-02 15:37:22.376035: step 4000, loss = 0.99 (1317.8 examples/sec; 0.097 sec/batch)
2017-06-02 15:37:23.162803: step 4010, loss = 1.18 (1626.9 examples/sec; 0.079 sec/batch)
2017-06-02 15:37:24.052320: step 4020, loss = 1.03 (1439.0 examples/sec; 0.089 sec/batch)
2017-06-02 15:37:24.926551: step 4030, loss = 0.96 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 15:37:25.809439: step 4040, loss = 1.28 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:26.689870: step 4050, loss = 1.04 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:27.570123: step 4060, loss = 1.30 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:28.446923: step 4070, loss = 0.90 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:29.325314: step 4080, loss = 1.36 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:30.191201: step 4090, loss = 1.19 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 15:37:31.188865: step 4100, loss = 1.10 (1283.0 examples/sec; 0.100 sec/batch)
2017-06-02 15:37:31.965856: step 4110, loss = 1.15 (1647.4 examples/sec; 0.078 sec/batch)
2017-06-02 15:37:32.839392: step 4120, loss = 1.26 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 15:37:33.727504: step 4130, loss = 1.10 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 15:37:34.606558: step 4140, loss = 1.02 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:35.490790: step 4150, loss = 1.20 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:36.371724: step 4160, loss = 0.93 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:37.274230: step 4170, loss = 1.28 (1418.3 examples/sec; 0.090 sec/batch)
2017-06-02 15:37:38.139463: step 4180, loss = 1.08 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:37:39.036798: step 4190, loss = 0.91 (1426.4 examples/sec; 0.090 sec/batch)
2017-06-02 15:37:40.030640: step 4200, loss = 1.12 (1287.9 examples/sec; 0.099 sec/batch)
2017-06-02 15:37:40.826389: step 4210, loss = 1.20 (1608.5 examples/sec; 0.080 sec/batch)
2017-06-02 15:37:41.725702: step 4220, loss = 1.07 (1423.3 examples/sec; 0.090 sec/batch)
2017-06-02 15:37:42.606706: step 4230, loss = 1.19 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:43.488249: step 4240, loss = 1.08 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:44.375767: step 4250, loss = 1.18 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 15:37:45.257382: step 4260, loss = 1.08 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:46.137635: step 4270, loss = 1.04 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:47.044790: step 4280, loss = 1.00 (1411.0 examples/sec; 0.091 sec/batch)
2017-06-02 15:37:47.922929: step 4290, loss = 1.41 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:48.909644: step 4300, loss = 1.05 (1297.2 examples/sec; 0.099 sec/batch)
2017-06-02 15:37:49.686589: step 4310, loss = 1.08 (1647.5 examples/sec; 0.078 sec/batch)
2017-06-02 15:37:50.562510: step 4320, loss = 1.11 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:51.446277: step 4330, loss = 1.04 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:52.338660: step 4340, loss = 1.37 (1434.4 examples/sec; 0.089 sec/batch)
2017-06-02 15:37:53.217395: step 4350, loss = 0.96 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:54.106006: step 4360, loss = 0.90 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 15:37:54.995450: step 4370, loss = 1.13 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 15:37:55.870966: step 4380, loss = 0.90 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:37:56.760075: step 4390, loss = 0.87 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 15:37:57.739585: step 4400, loss = 1.01 (1306.8 examples/sec; 0.098 sec/batch)
2017-06-02 15:37:58.504003: step 4410, loss = 0.93 (1674.5 examples/sec; 0.076 sec/batch)
2017-06-02 15:37:59.379023: step 4420, loss = 1.27 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:38:00.267873: step 4430, loss = 1.06 (1440.1 examples/sec; 0.089 sec/batch)
2017-06-02 15:38:01.142907: step 4440, loss = 1.01 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:38:02.032685: step 4450, loss = 1.06 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 15:38:02.911811: step 4460, loss = 1.22 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:38:03.795108: step 4470, loss = 1.05 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:38:04.663515: step 4480, loss = 0.96 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 15:38:05.562136: step 4490, loss = 1.03 (1424.4 examples/sec; 0.090 sec/batch)
2017-06-02 15:38:06.533469: step 4500, loss = 1.00 (1317.8 examples/sec; 0.097 sec/batch)
2017-06-02 15:38:07.304152: step 4510, loss = 0.94 (1660.9 examples/sec; 0.077 sec/batch)
2017-06-02 15:38:08.207554: step 4520, loss = 1.07 (1416.9 examples/sec; 0.090 sec/batch)
2017-06-02 15:38:09.081271: step 4530, loss = 1.26 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 15:38:09.959434: step 4540, loss = 1.15 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:38:10.839187: step 4550, loss = 0.92 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:38:11.728622: step 4560, loss = 1.14 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 15:38:12.610044: step 4570, loss = 0.83 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:38:13.494466: step 4580, loss = 1.05 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:38:14.371479: step 4590, loss = 1.08 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:38:15.346694: step 4600, loss = 0.97 (1312.5 examples/sec; 0.098 sec/batch)
2017-06-02 15:38:16.132373: step 4610, loss = 1.07 (1629.2 examples/sec; 0.079 sec/batch)
2017-06-02 15:38:17.008349: step 4620, loss = 0.88 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:38:17.890923: step 4630, loss = 1.05 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:38:18.767719: step 4640, loss = 1.06 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:38:19.637270: step 4650, loss = 0.87 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 15:38:20.513753: step 4660, loss = 1.21 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:38:21.402314: step 4670, loss = 0.99 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 15:38:22.282532: step 4680, loss = 1.02 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:38:23.148892: step 4690, loss = 1.13 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 15:38:24.137189: step 4700, loss = 1.18 (1295.2 examples/sec; 0.099 sec/batch)
2017-06-02 15:38:25.175282: step 4710, loss = 0.99 (1233.0 examples/sec; 0.104 sec/batch)
2017-06-02 15:38:26.540965: step 4720, loss = 1.12 (937.3 examples/sec; 0.137 sec/batch)
2017-06-02 15:38:27.567747: step 4730, loss = 1.12 (1246.6 examples/sec; 0.103 sec/batch)
2017-06-02 15:38:28.484709: step 4740, loss = 1.15 (1395.9 examples/sec; 0.092 sec/batch)
2017-06-02 15:38:29.392411: step 4750, loss = 1.02 (1410.1 examples/sec; 0.091 sec/batch)
2017-06-02 15:38:30.426711: step 4760, loss = 0.84 (1237.6 examples/sec; 0.103 sec/batch)
2017-06-02 15:38:31.289983: step 4770, loss = 1.19 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 15:38:32.170065: step 4780, loss = 0.89 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:38:33.038817: step 4790, loss = 1.05 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:38:34.031107: step 4800, loss = 1.19 (1289.9 examples/sec; 0.099 sec/batch)
2017-06-02 15:38:34.824987: step 4810, loss = 1.15 (1612.3 examples/sec; 0.079 sec/batch)
2017-06-02 15:38:35.709535: step 4820, loss = 0.93 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:38:36.601940: step 4830, loss = 1.03 (1434.3 examples/sec; 0.089 sec/batch)
2017-06-02 15:38:37.479241: step 4840, loss = 1.12 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:38:38.339132: step 4850, loss = 1.10 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 15:38:39.210757: step 4860, loss = 1.06 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 15:38:40.074960: step 4870, loss = 1.05 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 15:38:40.950489: step 4880, loss = 1.01 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:38:41.831190: step 4890, loss = 0.97 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:38:42.823721: step 4900, loss = 0.86 (1289.6 examples/sec; 0.099 sec/batch)
2017-06-02 15:38:43.604302: step 4910, loss = 1.09 (1639.8 examples/sec; 0.078 sec/batch)
2017-06-02 15:38:44.489857: step 4920, loss = 1.09 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 15:38:45.369778: step 4930, loss = 1.03 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:38:46.238376: step 4940, loss = 0.97 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 15:38:47.129171: step 4950, loss = 0.96 (1436.9 examples/sec; 0.089 sec/batch)
2017-06-02 15:38:48.007303: step 4960, loss = 1.04 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:38:48.888948: step 4970, loss = 1.05 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:38:49.769001: step 4980, loss = 1.33 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:38:50.633426: step 4990, loss = 1.03 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 15:38:51.620679: step 5000, loss = 0.90 (1296.5 examples/sec; 0.099 sec/batch)
2017-06-02 15:38:52.396678: step 5010, loss = 0.86 (1649.5 examples/sec; 0.078 sec/batch)
2017-06-02 15:38:53.263994: step 5020, loss = 1.07 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 15:38:54.144960: step 5030, loss = 1.04 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:38:55.026964: step 5040, loss = 1.12 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:38:55.905106: step 5050, loss = 0.99 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:38:56.775774: step 5060, loss = 0.99 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 15:38:57.659482: step 5070, loss = 0.98 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:38:58.531983: step 5080, loss = 1.03 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 15:38:59.419969: step 5090, loss = 1.05 (1441.5 examples/sec; 0.089 sec/batch)
2017-06-02 15:39:00.402822: step 5100, loss = 1.04 (1302.3 examples/sec; 0.098 sec/batch)
2017-06-02 15:39:01.184056: step 5110, loss = 1.11 (1638.4 examples/sec; 0.078 sec/batch)
2017-06-02 15:39:02.055850: step 5120, loss = 1.09 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 15:39:02.935911: step 5130, loss = 0.87 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:39:03.806830: step 5140, loss = 1.05 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 15:39:04.694481: step 5150, loss = 0.94 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 15:39:05.588202: step 5160, loss = 0.98 (1432.2 examples/sec; 0.089 sec/batch)
2017-06-02 15:39:06.465720: step 5170, loss = 1.18 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:39:07.338825: step 5180, loss = 0.94 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 15:39:08.219035: step 5190, loss = 0.87 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:39:09.205861: step 5200, loss = 1.00 (1297.1 examples/sec; 0.099 sec/batch)
2017-06-02 15:39:09.984700: step 5210, loss = 0.90 (1643.5 examples/sec; 0.078 sec/batch)
2017-06-02 15:39:10.861889: step 5220, loss = 1.00 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:39:11.743450: step 5230, loss = 0.94 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:39:12.618343: step 5240, loss = 1.05 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 15:39:13.507861: step 5250, loss = 1.09 (1439.0 examples/sec; 0.089 sec/batch)
2017-06-02 15:39:14.387568: step 5260, loss = 1.01 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:39:15.258745: step 5270, loss = 0.85 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 15:39:16.143097: step 5280, loss = 0.83 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:39:17.025449: step 5290, loss = 1.02 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:39:18.005577: step 5300, loss = 1.05 (1306.0 examples/sec; 0.098 sec/batch)
2017-06-02 15:39:18.789095: step 5310, loss = 0.97 (1633.7 examples/sec; 0.078 sec/batch)
2017-06-02 15:39:19.663881: step 5320, loss = 0.95 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 15:39:20.538952: step 5330, loss = 1.28 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:39:21.405857: step 5340, loss = 0.96 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 15:39:22.298495: step 5350, loss = 0.86 (1434.0 examples/sec; 0.089 sec/batch)
2017-06-02 15:39:23.173539: step 5360, loss = 0.90 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:39:24.062184: step 5370, loss = 0.88 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 15:39:24.942233: step 5380, loss = 1.12 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:39:25.828478: step 5390, loss = 0.89 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 15:39:26.810442: step 5400, loss = 1.10 (1303.5 examples/sec; 0.098 sec/batch)
2017-06-02 15:39:27.586663: step 5410, loss = 0.92 (1649.0 examples/sec; 0.078 sec/batch)
2017-06-02 15:39:28.457291: step 5420, loss = 1.11 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 15:39:29.332266: step 5430, loss = 1.00 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 15:39:30.206946: step 5440, loss = 0.85 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:39:31.081947: step 5450, loss = 1.04 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 15:39:31.945506: step 5460, loss = 1.03 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 15:39:32.813660: step 5470, loss = 0.84 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:39:33.692220: step 5480, loss = 0.96 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:39:34.563764: step 5490, loss = 1.02 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 15:39:35.547856: step 5500, loss = 0.88 (1300.7 examples/sec; 0.098 sec/batch)
2017-06-02 15:39:36.315374: step 5510, loss = 0.98 (1667.7 examples/sec; 0.077 sec/batch)
2017-06-02 15:39:37.203783: step 5520, loss = 0.83 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 15:39:38.079554: step 5530, loss = 1.10 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:39:38.967113: step 5540, loss = 1.09 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 15:39:39.847021: step 5550, loss = 0.96 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:39:40.729613: step 5560, loss = 0.99 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:39:41.601172: step 5570, loss = 1.06 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 15:39:42.476690: step 5580, loss = 1.09 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:39:43.357300: step 5590, loss = 0.83 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:39:44.347241: step 5600, loss = 0.88 (1293.0 examples/sec; 0.099 sec/batch)
2017-06-02 15:39:45.140329: step 5610, loss = 1.06 (1614.0 examples/sec; 0.079 sec/batch)
2017-06-02 15:39:46.008141: step 5620, loss = 0.96 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 15:39:46.881807: step 5630, loss = 1.08 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 15:39:47.759690: step 5640, loss = 0.88 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:39:48.637417: step 5650, loss = 1.07 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:39:49.512342: step 5660, loss = 0.87 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 15:39:50.408390: step 5670, loss = 0.96 (1428.5 examples/sec; 0.090 sec/batch)
2017-06-02 15:39:51.290993: step 5680, loss = 0.83 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:39:52.168042: step 5690, loss = 1.18 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:39:53.145908: step 5700, loss = 0.99 (1309.0 examples/sec; 0.098 sec/batch)
2017-06-02 15:39:53.914430: step 5710, loss = 1.14 (1665.5 examples/sec; 0.077 sec/batch)
2017-06-02 15:39:54.779885: step 5720, loss = 0.93 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 15:39:55.655954: step 5730, loss = 0.90 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:39:56.528325: step 5740, loss = 0.94 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 15:39:57.392345: step 5750, loss = 1.05 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 15:39:58.252603: step 5760, loss = 1.05 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 15:39:59.117917: step 5770, loss = 1.06 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 15:40:00.000154: step 5780, loss = 0.92 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:40:00.875662: step 5790, loss = 1.00 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:40:01.852026: step 5800, loss = 1.04 (1311.0 examples/sec; 0.098 sec/batch)
2017-06-02 15:40:02.630492: step 5810, loss = 0.85 (1644.3 examples/sec; 0.078 sec/batch)
2017-06-02 15:40:03.500133: step 5820, loss = 0.87 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 15:40:04.380480: step 5830, loss = 1.02 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:40:05.254368: step 5840, loss = 1.02 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 15:40:06.132343: step 5850, loss = 1.03 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:40:06.996193: step 5860, loss = 0.84 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 15:40:07.880002: step 5870, loss = 0.78 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:40:08.750938: step 5880, loss = 0.93 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 15:40:09.634465: step 5890, loss = 1.05 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:40:10.612913: step 5900, loss = 0.88 (1308.2 examples/sec; 0.098 sec/batch)
2017-06-02 15:40:11.387711: step 5910, loss = 1.02 (1652.1 examples/sec; 0.077 sec/batch)
2017-06-02 15:40:12.267519: step 5920, loss = 0.97 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:40:13.142136: step 5930, loss = 1.09 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 15:40:14.025118: step 5940, loss = 1.11 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:40:14.893820: step 5950, loss = 0.99 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 15:40:15.757743: step 5960, loss = 0.83 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 15:40:16.645932: step 5970, loss = 0.97 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 15:40:17.517808: step 5980, loss = 0.88 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 15:40:18.407438: step 5990, loss = 1.07 (1438.8 examples/sec; 0.089 sec/batch)
2017-06-02 15:40:19.392531: step 6000, loss = 1.27 (1299.4 examples/sec; 0.099 sec/batch)
2017-06-02 15:40:20.172708: step 6010, loss = 0.93 (1640.7 examples/sec; 0.078 sec/batch)
2017-06-02 15:40:21.050896: step 6020, loss = 1.01 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:40:21.935416: step 6030, loss = 0.88 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:40:22.808702: step 6040, loss = 0.95 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 15:40:23.684852: step 6050, loss = 0.97 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:40:24.560692: step 6060, loss = 1.01 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:40:25.424857: step 6070, loss = 1.32 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 15:40:26.286413: step 6080, loss = 1.02 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 15:40:27.150391: step 6090, loss = 0.92 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 15:40:28.123099: step 6100, loss = 0.96 (1315.9 examples/sec; 0.097 sec/batch)
2017-06-02 15:40:28.894321: step 6110, loss = 1.06 (1659.7 examples/sec; 0.077 sec/batch)
2017-06-02 15:40:29.787915: step 6120, loss = 0.91 (1432.4 examples/sec; 0.089 sec/batch)
2017-06-02 15:40:30.661402: step 6130, loss = 0.89 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:40:31.551306: step 6140, loss = 1.05 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 15:40:32.435037: step 6150, loss = 0.97 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:40:33.314158: step 6160, loss = 0.80 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:40:34.184429: step 6170, loss = 1.11 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 15:40:35.077829: step 6180, loss = 0.93 (1432.7 examples/sec; 0.089 sec/batch)
2017-06-02 15:40:35.944353: step 6190, loss = 0.90 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 15:40:36.933076: step 6200, loss = 0.94 (1294.6 examples/sec; 0.099 sec/batch)
2017-06-02 15:40:37.707707: step 6210, loss = 1.05 (1652.4 examples/sec; 0.077 sec/batch)
2017-06-02 15:40:38.592883: step 6220, loss = 0.83 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 15:40:39.478838: step 6230, loss = 0.96 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 15:40:40.364485: step 6240, loss = 1.17 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 15:40:41.232344: step 6250, loss = 0.91 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 15:40:42.124157: step 6260, loss = 0.96 (1435.3 examples/sec; 0.089 sec/batch)
2017-06-02 15:40:42.996033: step 6270, loss = 1.00 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 15:40:43.882323: step 6280, loss = 0.93 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 15:40:44.757079: step 6290, loss = 0.97 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 15:40:45.743764: step 6300, loss = 0.99 (1297.2 examples/sec; 0.099 sec/batch)
2017-06-02 15:40:46.517727: step 6310, loss = 1.00 (1653.8 examples/sec; 0.077 sec/batch)
2017-06-02 15:40:47.405960: step 6320, loss = 1.16 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 15:40:48.292507: step 6330, loss = 1.00 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 15:40:49.169283: step 6340, loss = 0.98 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:40:50.047965: step 6350, loss = 0.93 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:40:50.929864: step 6360, loss = 1.21 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:40:51.814281: step 6370, loss = 0.99 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:40:52.703718: step 6380, loss = 0.95 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 15:40:53.589764: step 6390, loss = 1.14 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 15:40:54.609543: step 6400, loss = 1.05 (1255.1 examples/sec; 0.102 sec/batch)
2017-06-02 15:40:55.350219: step 6410, loss = 0.90 (1728.2 examples/sec; 0.074 sec/batch)
2017-06-02 15:40:56.228118: step 6420, loss = 0.94 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:40:57.091582: step 6430, loss = 0.84 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 15:40:57.971071: step 6440, loss = 0.79 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:40:58.859338: step 6450, loss = 1.07 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 15:40:59.725152: step 6460, loss = 0.90 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:41:00.604257: step 6470, loss = 0.72 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:41:01.484904: step 6480, loss = 0.86 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:41:02.357891: step 6490, loss = 0.97 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 15:41:03.354978: step 6500, loss = 0.96 (1283.7 examples/sec; 0.100 sec/batch)
2017-06-02 15:41:04.138512: step 6510, loss = 0.89 (1633.6 examples/sec; 0.078 sec/batch)
2017-06-02 15:41:05.029945: step 6520, loss = 0.93 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 15:41:05.910885: step 6530, loss = 1.03 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:41:06.781081: step 6540, loss = 1.07 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 15:41:07.655035: step 6550, loss = 0.83 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 15:41:08.544527: step 6560, loss = 1.05 (1439.0 examples/sec; 0.089 sec/batch)
2017-06-02 15:41:09.423461: step 6570, loss = 1.02 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:41:10.298443: step 6580, loss = 0.70 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 15:41:11.177555: step 6590, loss = 0.95 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:41:12.154791: step 6600, loss = 0.83 (1309.8 examples/sec; 0.098 sec/batch)
2017-06-02 15:41:12.932393: step 6610, loss = 1.08 (1646.1 examples/sec; 0.078 sec/batch)
2017-06-02 15:41:13.814596: step 6620, loss = 0.83 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:41:14.695905: step 6630, loss = 0.93 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:41:15.584523: step 6640, loss = 0.91 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 15:41:16.479164: step 6650, loss = 0.97 (1430.7 examples/sec; 0.089 sec/batch)
2017-06-02 15:41:17.364925: step 6660, loss = 1.00 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 15:41:18.233034: step 6670, loss = 0.94 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 15:41:19.125195: step 6680, loss = 0.79 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 15:41:19.989135: step 6690, loss = 0.76 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 15:41:20.965518: step 6700, loss = 1.04 (1311.0 examples/sec; 0.098 sec/batch)
2017-06-02 15:41:21.752368: step 6710, loss = 0.83 (1626.7 examples/sec; 0.079 sec/batch)
2017-06-02 15:41:22.617925: step 6720, loss = 0.96 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 15:41:23.501016: step 6730, loss = 0.98 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:41:24.391589: step 6740, loss = 1.07 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 15:41:25.271198: step 6750, loss = 0.75 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:41:26.153705: step 6760, loss = 0.97 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:41:27.034262: step 6770, loss = 0.89 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:41:27.908114: step 6780, loss = 0.94 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 15:41:28.781024: step 6790, loss = 0.71 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 15:41:29.772679: step 6800, loss = 0.73 (1290.8 examples/sec; 0.099 sec/batch)
2017-06-02 15:41:30.603115: step 6810, loss = 0.92 (1541.4 examples/sec; 0.083 sec/batch)
2017-06-02 15:41:31.480839: step 6820, loss = 1.11 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:41:32.356880: step 6830, loss = 0.87 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:41:33.250125: step 6840, loss = 0.81 (1433.0 examples/sec; 0.089 sec/batch)
2017-06-02 15:41:34.136432: step 6850, loss = 0.93 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 15:41:35.016031: step 6860, loss = 0.99 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:41:35.908202: step 6870, loss = 0.91 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 15:41:36.789795: step 6880, loss = 1.04 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:41:37.667274: step 6890, loss = 0.90 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:41:38.651172: step 6900, loss = 0.88 (1300.9 examples/sec; 0.098 sec/batch)
2017-06-02 15:41:39.421954: step 6910, loss = 1.05 (1660.7 examples/sec; 0.077 sec/batch)
2017-06-02 15:41:40.303632: step 6920, loss = 0.98 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:41:41.179196: step 6930, loss = 0.91 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:41:42.083807: step 6940, loss = 0.99 (1415.0 examples/sec; 0.090 sec/batch)
2017-06-02 15:41:42.966578: step 6950, loss = 0.88 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:41:43.848209: step 6960, loss = 0.89 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:41:44.733129: step 6970, loss = 0.87 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:41:45.619789: step 6980, loss = 0.89 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 15:41:46.904682: step 6990, loss = 1.14 (996.2 examples/sec; 0.128 sec/batch)
2017-06-02 15:41:48.204925: step 7000, loss = 0.90 (984.4 examples/sec; 0.130 sec/batch)
2017-06-02 15:41:49.023028: step 7010, loss = 0.84 (1564.6 examples/sec; 0.082 sec/batch)
2017-06-02 15:41:49.944253: step 7020, loss = 1.04 (1389.5 examples/sec; 0.092 sec/batch)
2017-06-02 15:41:50.846333: step 7030, loss = 0.83 (1418.9 examples/sec; 0.090 sec/batch)
2017-06-02 15:41:51.892113: step 7040, loss = 0.91 (1224.0 examples/sec; 0.105 sec/batch)
2017-06-02 15:41:52.774587: step 7050, loss = 0.83 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:41:53.652685: step 7060, loss = 0.93 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:41:54.531771: step 7070, loss = 0.95 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:41:55.412577: step 7080, loss = 1.05 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:41:56.290684: step 7090, loss = 0.95 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:41:57.267926: step 7100, loss = 0.83 (1309.8 examples/sec; 0.098 sec/batch)
2017-06-02 15:41:58.036845: step 7110, loss = 1.05 (1664.7 examples/sec; 0.077 sec/batch)
2017-06-02 15:41:58.915123: step 7120, loss = 0.92 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:41:59.782572: step 7130, loss = 0.94 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 15:42:00.666848: step 7140, loss = 1.03 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:01.568053: step 7150, loss = 0.93 (1420.3 examples/sec; 0.090 sec/batch)
2017-06-02 15:42:02.450290: step 7160, loss = 0.98 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:03.317167: step 7170, loss = 0.87 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 15:42:04.196426: step 7180, loss = 1.11 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:05.068539: step 7190, loss = 1.07 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 15:42:06.064766: step 7200, loss = 0.83 (1284.9 examples/sec; 0.100 sec/batch)
2017-06-02 15:42:06.830975: step 7210, loss = 0.76 (1670.6 examples/sec; 0.077 sec/batch)
2017-06-02 15:42:07.711232: step 7220, loss = 1.05 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:08.601637: step 7230, loss = 1.02 (1437.6 examples/sec; 0.089 sec/batch)
2017-06-02 15:42:09.482171: step 7240, loss = 0.84 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:10.362392: step 7250, loss = 0.99 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:11.249781: step 7260, loss = 1.04 (1442.4 examples/sec; 0.089 sec/batch)
2017-06-02 15:42:12.135996: step 7270, loss = 0.98 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 15:42:13.017011: step 7280, loss = 0.85 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:13.897774: step 7290, loss = 0.82 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:14.877705: step 7300, loss = 0.92 (1306.2 examples/sec; 0.098 sec/batch)
2017-06-02 15:42:15.661613: step 7310, loss = 0.98 (1632.8 examples/sec; 0.078 sec/batch)
2017-06-02 15:42:16.545315: step 7320, loss = 0.80 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:17.430668: step 7330, loss = 0.89 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 15:42:18.313480: step 7340, loss = 0.88 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:19.200211: step 7350, loss = 0.87 (1443.5 examples/sec; 0.089 sec/batch)
2017-06-02 15:42:20.068128: step 7360, loss = 0.96 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 15:42:20.952088: step 7370, loss = 1.14 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:21.855420: step 7380, loss = 1.09 (1417.0 examples/sec; 0.090 sec/batch)
2017-06-02 15:42:22.735532: step 7390, loss = 0.88 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:23.727599: step 7400, loss = 0.98 (1290.2 examples/sec; 0.099 sec/batch)
2017-06-02 15:42:24.495568: step 7410, loss = 1.09 (1666.7 examples/sec; 0.077 sec/batch)
2017-06-02 15:42:25.369546: step 7420, loss = 0.86 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 15:42:26.234142: step 7430, loss = 0.94 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 15:42:27.110602: step 7440, loss = 0.79 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:27.989148: step 7450, loss = 0.75 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:28.866739: step 7460, loss = 0.91 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:29.738867: step 7470, loss = 0.87 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 15:42:30.617855: step 7480, loss = 1.11 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:31.487904: step 7490, loss = 0.87 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 15:42:32.466670: step 7500, loss = 0.90 (1307.8 examples/sec; 0.098 sec/batch)
2017-06-02 15:42:33.238833: step 7510, loss = 1.11 (1657.7 examples/sec; 0.077 sec/batch)
2017-06-02 15:42:34.121966: step 7520, loss = 0.92 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:34.995114: step 7530, loss = 0.91 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 15:42:35.877960: step 7540, loss = 0.91 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:36.764460: step 7550, loss = 0.98 (1443.9 examples/sec; 0.089 sec/batch)
2017-06-02 15:42:37.660047: step 7560, loss = 0.89 (1429.2 examples/sec; 0.090 sec/batch)
2017-06-02 15:42:38.542873: step 7570, loss = 0.96 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:39.413912: step 7580, loss = 0.88 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 15:42:40.294102: step 7590, loss = 0.93 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:41.279978: step 7600, loss = 1.03 (1298.3 examples/sec; 0.099 sec/batch)
2017-06-02 15:42:42.049722: step 7610, loss = 0.72 (1662.9 examples/sec; 0.077 sec/batch)
2017-06-02 15:42:42.929649: step 7620, loss = 1.01 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:43.822937: step 7630, loss = 1.07 (1432.9 examples/sec; 0.089 sec/batch)
2017-06-02 15:42:44.691089: step 7640, loss = 0.92 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:42:45.570244: step 7650, loss = 0.91 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:46.448800: step 7660, loss = 0.99 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:47.327515: step 7670, loss = 0.81 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:48.209559: step 7680, loss = 0.80 (1451.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:49.095673: step 7690, loss = 0.79 (1444.5 examples/sec; 0.089 sec/batch)
2017-06-02 15:42:50.090289: step 7700, loss = 0.99 (1286.9 examples/sec; 0.099 sec/batch)
2017-06-02 15:42:50.844420: step 7710, loss = 0.92 (1697.3 examples/sec; 0.075 sec/batch)
2017-06-02 15:42:51.722297: step 7720, loss = 0.91 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:52.598365: step 7730, loss = 0.87 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:53.477254: step 7740, loss = 0.95 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:54.357543: step 7750, loss = 0.92 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:55.246590: step 7760, loss = 0.79 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 15:42:56.137348: step 7770, loss = 1.03 (1437.0 examples/sec; 0.089 sec/batch)
2017-06-02 15:42:57.031087: step 7780, loss = 0.87 (1432.2 examples/sec; 0.089 sec/batch)
2017-06-02 15:42:57.906997: step 7790, loss = 0.80 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:42:58.884983: step 7800, loss = 0.92 (1308.8 examples/sec; 0.098 sec/batch)
2017-06-02 15:42:59.647263: step 7810, loss = 0.82 (1679.2 examples/sec; 0.076 sec/batch)
2017-06-02 15:43:00.539600: step 7820, loss = 0.98 (1434.5 examples/sec; 0.089 sec/batch)
2017-06-02 15:43:01.434719: step 7830, loss = 0.94 (1430.0 examples/sec; 0.090 sec/batch)
2017-06-02 15:43:02.315177: step 7840, loss = 0.77 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:43:03.204328: step 7850, loss = 0.86 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 15:43:04.079336: step 7860, loss = 0.88 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:43:04.951650: step 7870, loss = 1.09 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:43:05.828881: step 7880, loss = 1.00 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:43:06.720983: step 7890, loss = 0.84 (1434.8 examples/sec; 0.089 sec/batch)
2017-06-02 15:43:07.718676: step 7900, loss = 0.85 (1283.0 examples/sec; 0.100 sec/batch)
2017-06-02 15:43:08.468209: step 7910, loss = 0.96 (1707.7 examples/sec; 0.075 sec/batch)
2017-06-02 15:43:09.346106: step 7920, loss = 0.77 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:43:10.206511: step 7930, loss = 0.89 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 15:43:11.089378: step 7940, loss = 0.75 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:43:11.965767: step 7950, loss = 0.90 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:43:12.846817: step 7960, loss = 1.04 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:43:13.730245: step 7970, loss = 0.82 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:43:14.603061: step 7980, loss = 0.92 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 15:43:15.467312: step 7990, loss = 1.02 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 15:43:16.441173: step 8000, loss = 0.83 (1314.4 examples/sec; 0.097 sec/batch)
2017-06-02 15:43:17.201345: step 8010, loss = 0.86 (1683.8 examples/sec; 0.076 sec/batch)
2017-06-02 15:43:18.083209: step 8020, loss = 0.85 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:43:18.962635: step 8030, loss = 0.80 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:43:19.844107: step 8040, loss = 0.83 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:43:20.728612: step 8050, loss = 0.93 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:43:21.595628: step 8060, loss = 0.99 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 15:43:22.470078: step 8070, loss = 1.07 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 15:43:23.346017: step 8080, loss = 0.97 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:43:24.211722: step 8090, loss = 0.85 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 15:43:25.194476: step 8100, loss = 0.76 (1302.5 examples/sec; 0.098 sec/batch)
2017-06-02 15:43:25.952900: step 8110, loss = 0.81 (1687.7 examples/sec; 0.076 sec/batch)
2017-06-02 15:43:26.830911: step 8120, loss = 0.91 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:43:27.704594: step 8130, loss = 1.00 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 15:43:28.582112: step 8140, loss = 0.87 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:43:29.452632: step 8150, loss = 1.03 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:43:30.320520: step 8160, loss = 1.00 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 15:43:31.200251: step 8170, loss = 0.97 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:43:32.077901: step 8180, loss = 1.08 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:43:32.949589: step 8190, loss = 1.00 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:43:33.933405: step 8200, loss = 0.90 (1301.1 examples/sec; 0.098 sec/batch)
2017-06-02 15:43:34.709294: step 8210, loss = 0.89 (1649.7 examples/sec; 0.078 sec/batch)
2017-06-02 15:43:35.583776: step 8220, loss = 0.99 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 15:43:36.456450: step 8230, loss = 0.89 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 15:43:37.335580: step 8240, loss = 1.01 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:43:38.211807: step 8250, loss = 1.02 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:43:39.078178: step 8260, loss = 0.85 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:43:39.973946: step 8270, loss = 0.95 (1428.9 examples/sec; 0.090 sec/batch)
2017-06-02 15:43:40.852569: step 8280, loss = 0.91 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:43:41.727772: step 8290, loss = 0.91 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:43:42.704272: step 8300, loss = 0.82 (1310.8 examples/sec; 0.098 sec/batch)
2017-06-02 15:43:43.483299: step 8310, loss = 1.00 (1643.1 examples/sec; 0.078 sec/batch)
2017-06-02 15:43:44.353739: step 8320, loss = 0.95 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 15:43:45.226771: step 8330, loss = 0.76 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 15:43:46.087219: step 8340, loss = 0.96 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 15:43:46.961439: step 8350, loss = 0.95 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 15:43:47.825870: step 8360, loss = 0.91 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 15:43:48.715966: step 8370, loss = 0.97 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 15:43:49.589589: step 8380, loss = 0.87 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 15:43:50.450432: step 8390, loss = 1.05 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 15:43:51.451883: step 8400, loss = 0.81 (1278.1 examples/sec; 0.100 sec/batch)
2017-06-02 15:43:52.199772: step 8410, loss = 0.85 (1711.5 examples/sec; 0.075 sec/batch)
2017-06-02 15:43:53.085859: step 8420, loss = 1.06 (1444.5 examples/sec; 0.089 sec/batch)
2017-06-02 15:43:53.980750: step 8430, loss = 0.95 (1430.3 examples/sec; 0.089 sec/batch)
2017-06-02 15:43:54.849234: step 8440, loss = 1.04 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 15:43:55.730749: step 8450, loss = 0.84 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:43:56.628535: step 8460, loss = 0.80 (1425.7 examples/sec; 0.090 sec/batch)
2017-06-02 15:43:57.510179: step 8470, loss = 1.00 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:43:58.396965: step 8480, loss = 0.83 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 15:43:59.287879: step 8490, loss = 0.90 (1436.7 examples/sec; 0.089 sec/batch)
2017-06-02 15:44:00.273317: step 8500, loss = 0.93 (1298.9 examples/sec; 0.099 sec/batch)
2017-06-02 15:44:01.050767: step 8510, loss = 1.03 (1646.4 examples/sec; 0.078 sec/batch)
2017-06-02 15:44:01.927011: step 8520, loss = 1.07 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:02.798889: step 8530, loss = 0.93 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 15:44:03.691607: step 8540, loss = 1.13 (1433.8 examples/sec; 0.089 sec/batch)
2017-06-02 15:44:04.557538: step 8550, loss = 0.85 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 15:44:05.433155: step 8560, loss = 0.95 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:06.301984: step 8570, loss = 0.92 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 15:44:07.184487: step 8580, loss = 0.90 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:08.066373: step 8590, loss = 0.81 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:09.050401: step 8600, loss = 0.90 (1300.8 examples/sec; 0.098 sec/batch)
2017-06-02 15:44:09.826490: step 8610, loss = 0.90 (1649.3 examples/sec; 0.078 sec/batch)
2017-06-02 15:44:10.706960: step 8620, loss = 0.86 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:11.591658: step 8630, loss = 0.86 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:12.480225: step 8640, loss = 0.84 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 15:44:13.351370: step 8650, loss = 0.91 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 15:44:14.224980: step 8660, loss = 1.00 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 15:44:15.104763: step 8670, loss = 0.91 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:15.982729: step 8680, loss = 0.93 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:16.864970: step 8690, loss = 1.00 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:17.851445: step 8700, loss = 0.84 (1297.5 examples/sec; 0.099 sec/batch)
2017-06-02 15:44:18.620702: step 8710, loss = 0.83 (1663.9 examples/sec; 0.077 sec/batch)
2017-06-02 15:44:19.495119: step 8720, loss = 0.99 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 15:44:20.375207: step 8730, loss = 0.81 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:21.254998: step 8740, loss = 0.71 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:22.118066: step 8750, loss = 0.89 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 15:44:22.993453: step 8760, loss = 0.93 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:23.873129: step 8770, loss = 0.81 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:24.755237: step 8780, loss = 0.76 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:25.623302: step 8790, loss = 1.04 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 15:44:26.612080: step 8800, loss = 0.82 (1294.5 examples/sec; 0.099 sec/batch)
2017-06-02 15:44:27.405962: step 8810, loss = 1.01 (1612.3 examples/sec; 0.079 sec/batch)
2017-06-02 15:44:28.285400: step 8820, loss = 0.88 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:29.172739: step 8830, loss = 0.82 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 15:44:30.046438: step 8840, loss = 1.11 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 15:44:30.933288: step 8850, loss = 0.76 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 15:44:31.806920: step 8860, loss = 1.01 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 15:44:32.680751: step 8870, loss = 1.00 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 15:44:33.556317: step 8880, loss = 0.84 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:34.434164: step 8890, loss = 0.91 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:35.420689: step 8900, loss = 0.89 (1297.5 examples/sec; 0.099 sec/batch)
2017-06-02 15:44:36.199397: step 8910, loss = 0.82 (1643.7 examples/sec; 0.078 sec/batch)
2017-06-02 15:44:37.077622: step 8920, loss = 1.06 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:37.967127: step 8930, loss = 1.02 (1439.0 examples/sec; 0.089 sec/batch)
2017-06-02 15:44:38.840182: step 8940, loss = 0.83 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 15:44:39.726528: step 8950, loss = 1.03 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 15:44:40.601651: step 8960, loss = 0.86 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:41.482336: step 8970, loss = 0.83 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:42.365371: step 8980, loss = 0.85 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:43.252173: step 8990, loss = 0.77 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 15:44:44.225617: step 9000, loss = 1.06 (1314.9 examples/sec; 0.097 sec/batch)
2017-06-02 15:44:45.012333: step 9010, loss = 0.90 (1627.0 examples/sec; 0.079 sec/batch)
2017-06-02 15:44:45.879019: step 9020, loss = 0.97 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 15:44:46.755301: step 9030, loss = 1.03 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:47.636439: step 9040, loss = 0.90 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:48.510759: step 9050, loss = 0.91 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 15:44:49.388474: step 9060, loss = 0.73 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:50.267204: step 9070, loss = 0.78 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:51.145421: step 9080, loss = 0.90 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:52.026940: step 9090, loss = 0.91 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:53.042420: step 9100, loss = 0.99 (1260.5 examples/sec; 0.102 sec/batch)
2017-06-02 15:44:53.782849: step 9110, loss = 0.94 (1728.7 examples/sec; 0.074 sec/batch)
2017-06-02 15:44:54.665890: step 9120, loss = 0.89 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:44:55.554376: step 9130, loss = 0.75 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 15:44:56.440313: step 9140, loss = 0.90 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 15:44:57.329992: step 9150, loss = 0.81 (1438.7 examples/sec; 0.089 sec/batch)
2017-06-02 15:44:58.197388: step 9160, loss = 0.80 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 15:44:59.086373: step 9170, loss = 1.01 (1439.8 examples/sec; 0.089 sec/batch)
2017-06-02 15:44:59.956304: step 9180, loss = 0.75 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:45:00.839642: step 9190, loss = 0.88 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:45:01.809851: step 9200, loss = 0.81 (1319.3 examples/sec; 0.097 sec/batch)
2017-06-02 15:45:02.591656: step 9210, loss = 0.77 (1637.2 examples/sec; 0.078 sec/batch)
2017-06-02 15:45:03.468177: step 9220, loss = 0.86 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:45:04.340641: step 9230, loss = 0.91 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 15:45:05.210136: step 9240, loss = 1.00 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 15:45:06.090885: step 9250, loss = 0.90 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:45:06.957236: step 9260, loss = 1.10 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 15:45:07.831671: step 9270, loss = 0.78 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 15:45:08.710461: step 9280, loss = 0.84 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:45:09.586890: step 9290, loss = 0.81 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:45:10.565070: step 9300, loss = 0.92 (1308.6 examples/sec; 0.098 sec/batch)
2017-06-02 15:45:11.347995: step 9310, loss = 1.08 (1634.9 examples/sec; 0.078 sec/batch)
2017-06-02 15:45:12.225320: step 9320, loss = 1.02 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:45:13.097455: step 9330, loss = 0.87 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 15:45:13.968916: step 9340, loss = 0.94 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 15:45:14.835868: step 9350, loss = 0.99 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:45:15.715930: step 9360, loss = 0.82 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:45:16.603183: step 9370, loss = 0.98 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 15:45:17.486950: step 9380, loss = 0.84 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:45:18.377082: step 9390, loss = 0.72 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 15:45:19.401865: step 9400, loss = 0.96 (1250.3 examples/sec; 0.102 sec/batch)
2017-06-02 15:45:20.157040: step 9410, loss = 1.00 (1692.7 examples/sec; 0.076 sec/batch)
2017-06-02 15:45:21.042989: step 9420, loss = 0.96 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 15:45:21.920184: step 9430, loss = 0.70 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:45:22.790105: step 9440, loss = 1.10 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:45:23.680179: step 9450, loss = 0.92 (1438.1 examples/sec; 0.089 sec/batch)
2017-06-02 15:45:24.548592: step 9460, loss = 0.92 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 15:45:25.429000: step 9470, loss = 0.94 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 15:45:26.295444: step 9480, loss = 0.84 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 15:45:27.172630: step 9490, loss = 0.82 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 15:45:28.146038: step 9500, loss = 1.05 (1315.0 examples/sec; 0.097 sec/batch)
2017-06-02 15:45:28.915905: step 9510, loss = 0.87 (1662.6 examples/sec; 0.077 sec/batch)
2017-06-02 15:45:29.781329: step 9520, loss = 0.90 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 15:45:30.654718: step 9530, loss = 0.83 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 15:45:31.539036: step 9540, loss = 1.05 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 15:45:32.417483: step 9550, loss = 0.76 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:45:33.300033: step 9560, loss = 0.86 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:45:34.176684: step 9570, loss = 0.69 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:45:35.044899: step 9580, loss = 1.03 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 15:45:35.919386: step 9590, loss = 0.97 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 15:45:36.899907: step 9600, loss = 1.06 (1305.4 examples/sec; 0.098 sec/batch)
2017-06-02 15:45:37.685292: step 9610, loss = 1.04 (1629.8 examples/sec; 0.079 sec/batch)
2017-06-02 15:45:38.567598: step 9620, loss = 0.91 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 15:45:39.445513: step 9630, loss = 0.91 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:45:40.311085: step 9640, loss = 0.79 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 15:45:41.196595: step 9650, loss = 0.95 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 15:45:42.070626: step 9660, loss = 0.75 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 15:45:42.944059: step 9670, loss = 0.98 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 15:45:43.824366: step 9680, loss = 1.02 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 15:45:44.705574: step 9690, loss = 0.70 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:45:45.692279: step 9700, loss = 1.00 (1297.2 examples/sec; 0.099 sec/batch)
2017-06-02 15:45:46.462931: step 9710, loss = 0.91 (1660.9 examples/sec; 0.077 sec/batch)
2017-06-02 15:45:47.350945: step 9720, loss = 0.77 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 15:45:48.242294: step 9730, loss = 0.92 (1436.0 examples/sec; 0.089 sec/batch)
2017-06-02 15:45:49.116631: step 9740, loss = 0.75 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 15:45:49.986759: step 9750, loss = 0.95 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 15:45:50.857838: step 9760, loss = 0.95 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:45:51.739629: step 9770, loss = 0.88 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 15:45:52.625162: step 9780, loss = 1.14 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 15:45:53.524960: step 9790, loss = 0.76 (1422.5 examples/sec; 0.090 sec/batch)
2017-06-02 15:45:54.499597: step 9800, loss = 0.89 (1313.3 examples/sec; 0.097 sec/batch)
2017-06-02 15:45:55.269992: step 9810, loss = 0.95 (1661.5 examples/sec; 0.077 sec/batch)
2017-06-02 15:45:56.142309: step 9820, loss = 0.97 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:45:57.020054: step 9830, loss = 0.97 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 15:45:57.892146: step 9840, loss = 0.95 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 15:45:59.100474: step 9850, loss = 0.85 (1059.3 examples/sec; 0.121 sec/batch)
2017-06-02 15:46:00.405255: step 9860, loss = 0.82 (981.0 examples/sec; 0.130 sec/batch)
2017-06-02 15:46:01.292787: step 9870, loss = 0.97 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 15:46:02.162176: step 9880, loss = 0.88 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 15:46:03.043635: step 9890, loss = 0.75 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 15:46:04.021558: step 9900, loss = 0.90 (1308.9 examples/sec; 0.098 sec/batch)
2017-06-02 15:46:04.795862: step 9910, loss = 0.99 (1653.1 examples/sec; 0.077 sec/batch)
2017-06-02 15:46:05.669926: step 9920, loss = 0.75 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:46:06.547569: step 9930, loss = 0.97 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:46:07.420432: step 9940, loss = 0.80 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:46:08.307774: step 9950, loss = 0.89 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 15:46:09.196028: step 9960, loss = 0.78 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 15:46:10.073051: step 9970, loss = 0.99 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 15:46:10.941198: step 9980, loss = 1.01 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 15:46:11.822764: step 9990, loss = 0.98 (1452.0 examples/sec; 0.088 sec/batch)
