Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-06-02 02:39:50.628444: step 0, loss = 4.68 (69.5 examples/sec; 1.843 sec/batch)
2017-06-02 02:39:51.382247: step 10, loss = 4.60 (1698.0 examples/sec; 0.075 sec/batch)
2017-06-02 02:39:52.261502: step 20, loss = 4.51 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:39:53.151267: step 30, loss = 4.38 (1438.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:39:54.034692: step 40, loss = 4.34 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:39:54.927137: step 50, loss = 4.27 (1434.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:39:55.812914: step 60, loss = 4.20 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:39:56.691027: step 70, loss = 4.32 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:39:57.570790: step 80, loss = 4.14 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:39:58.463590: step 90, loss = 4.08 (1433.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:39:59.474929: step 100, loss = 4.02 (1265.6 examples/sec; 0.101 sec/batch)
2017-06-02 02:40:00.233157: step 110, loss = 3.98 (1688.2 examples/sec; 0.076 sec/batch)
2017-06-02 02:40:01.121498: step 120, loss = 4.01 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:40:02.015462: step 130, loss = 3.97 (1431.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:40:02.891544: step 140, loss = 3.99 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:40:03.795577: step 150, loss = 4.04 (1415.9 examples/sec; 0.090 sec/batch)
2017-06-02 02:40:04.672278: step 160, loss = 4.04 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:40:05.560835: step 170, loss = 3.77 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:40:06.434243: step 180, loss = 4.06 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 02:40:07.339965: step 190, loss = 3.86 (1413.2 examples/sec; 0.091 sec/batch)
2017-06-02 02:40:08.340603: step 200, loss = 3.84 (1279.2 examples/sec; 0.100 sec/batch)
2017-06-02 02:40:09.111136: step 210, loss = 3.83 (1661.2 examples/sec; 0.077 sec/batch)
2017-06-02 02:40:10.003126: step 220, loss = 3.69 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:40:10.891899: step 230, loss = 3.75 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:40:11.776154: step 240, loss = 3.66 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:40:12.660136: step 250, loss = 3.66 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:40:13.541834: step 260, loss = 3.65 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:40:14.434824: step 270, loss = 3.52 (1433.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:40:15.317004: step 280, loss = 3.71 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:40:16.199991: step 290, loss = 3.57 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:40:17.195842: step 300, loss = 3.57 (1285.3 examples/sec; 0.100 sec/batch)
2017-06-02 02:40:17.955686: step 310, loss = 3.58 (1684.6 examples/sec; 0.076 sec/batch)
2017-06-02 02:40:18.838681: step 320, loss = 3.46 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:40:19.732502: step 330, loss = 3.42 (1432.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:40:20.636793: step 340, loss = 3.39 (1415.5 examples/sec; 0.090 sec/batch)
2017-06-02 02:40:21.511239: step 350, loss = 3.53 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 02:40:22.397308: step 360, loss = 3.45 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:40:23.293359: step 370, loss = 3.35 (1428.5 examples/sec; 0.090 sec/batch)
2017-06-02 02:40:24.193421: step 380, loss = 3.42 (1422.1 examples/sec; 0.090 sec/batch)
2017-06-02 02:40:25.094270: step 390, loss = 3.36 (1420.9 examples/sec; 0.090 sec/batch)
2017-06-02 02:40:26.095839: step 400, loss = 3.34 (1278.0 examples/sec; 0.100 sec/batch)
2017-06-02 02:40:26.856328: step 410, loss = 3.37 (1683.1 examples/sec; 0.076 sec/batch)
2017-06-02 02:40:27.736346: step 420, loss = 3.30 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:40:28.620776: step 430, loss = 3.35 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:40:29.541464: step 440, loss = 3.44 (1390.3 examples/sec; 0.092 sec/batch)
2017-06-02 02:40:30.439580: step 450, loss = 3.22 (1425.2 examples/sec; 0.090 sec/batch)
2017-06-02 02:40:31.339574: step 460, loss = 3.19 (1422.2 examples/sec; 0.090 sec/batch)
2017-06-02 02:40:32.259724: step 470, loss = 3.22 (1391.1 examples/sec; 0.092 sec/batch)
2017-06-02 02:40:33.159816: step 480, loss = 3.37 (1422.1 examples/sec; 0.090 sec/batch)
2017-06-02 02:40:34.028910: step 490, loss = 3.25 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 02:40:35.042815: step 500, loss = 3.10 (1262.5 examples/sec; 0.101 sec/batch)
2017-06-02 02:40:35.820289: step 510, loss = 3.13 (1646.3 examples/sec; 0.078 sec/batch)
2017-06-02 02:40:36.708647: step 520, loss = 3.10 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:40:37.624214: step 530, loss = 3.00 (1398.0 examples/sec; 0.092 sec/batch)
2017-06-02 02:40:38.505195: step 540, loss = 3.21 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:40:39.406168: step 550, loss = 3.10 (1420.7 examples/sec; 0.090 sec/batch)
2017-06-02 02:40:40.291821: step 560, loss = 2.92 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 02:40:41.176836: step 570, loss = 3.04 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 02:40:42.067547: step 580, loss = 2.97 (1437.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:40:42.957369: step 590, loss = 3.09 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:40:43.957975: step 600, loss = 3.12 (1279.2 examples/sec; 0.100 sec/batch)
2017-06-02 02:40:44.726540: step 610, loss = 2.82 (1665.4 examples/sec; 0.077 sec/batch)
2017-06-02 02:40:45.605952: step 620, loss = 2.89 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:40:46.482530: step 630, loss = 2.97 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:40:47.357556: step 640, loss = 3.01 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:40:48.233801: step 650, loss = 2.81 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:40:49.116858: step 660, loss = 2.80 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:40:49.991335: step 670, loss = 2.85 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 02:40:50.883906: step 680, loss = 2.90 (1434.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:40:51.771175: step 690, loss = 3.06 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:40:52.779438: step 700, loss = 2.96 (1269.5 examples/sec; 0.101 sec/batch)
2017-06-02 02:40:53.536954: step 710, loss = 2.68 (1689.7 examples/sec; 0.076 sec/batch)
2017-06-02 02:40:54.420593: step 720, loss = 3.00 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:40:55.304626: step 730, loss = 2.83 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:40:56.182026: step 740, loss = 2.68 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:40:57.069800: step 750, loss = 2.74 (1441.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:40:57.967715: step 760, loss = 2.94 (1425.5 examples/sec; 0.090 sec/batch)
2017-06-02 02:40:58.846306: step 770, loss = 2.92 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:40:59.742576: step 780, loss = 2.66 (1428.1 examples/sec; 0.090 sec/batch)
2017-06-02 02:41:00.628935: step 790, loss = 2.63 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:41:01.627711: step 800, loss = 2.87 (1281.6 examples/sec; 0.100 sec/batch)
2017-06-02 02:41:02.396118: step 810, loss = 2.79 (1665.8 examples/sec; 0.077 sec/batch)
2017-06-02 02:41:03.283176: step 820, loss = 2.82 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:41:04.169850: step 830, loss = 2.58 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:41:05.061099: step 840, loss = 2.70 (1436.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:41:05.934515: step 850, loss = 2.63 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 02:41:06.824852: step 860, loss = 2.56 (1437.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:41:07.708963: step 870, loss = 2.68 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:41:08.593916: step 880, loss = 2.55 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:41:09.482461: step 890, loss = 2.62 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:41:10.476765: step 900, loss = 2.45 (1287.3 examples/sec; 0.099 sec/batch)
2017-06-02 02:41:11.253801: step 910, loss = 2.61 (1647.3 examples/sec; 0.078 sec/batch)
2017-06-02 02:41:12.138292: step 920, loss = 2.74 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:41:13.015263: step 930, loss = 2.39 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:41:13.904130: step 940, loss = 2.73 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:41:14.780329: step 950, loss = 2.37 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:41:15.669262: step 960, loss = 2.63 (1439.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:41:16.561842: step 970, loss = 2.21 (1434.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:41:17.448965: step 980, loss = 2.38 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:41:18.326405: step 990, loss = 2.67 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:41:19.339407: step 1000, loss = 2.38 (1263.6 examples/sec; 0.101 sec/batch)
2017-06-02 02:41:20.111726: step 1010, loss = 2.30 (1657.4 examples/sec; 0.077 sec/batch)
2017-06-02 02:41:21.007889: step 1020, loss = 2.52 (1428.3 examples/sec; 0.090 sec/batch)
2017-06-02 02:41:21.898685: step 1030, loss = 2.38 (1436.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:41:22.799347: step 1040, loss = 2.44 (1421.2 examples/sec; 0.090 sec/batch)
2017-06-02 02:41:23.697345: step 1050, loss = 2.36 (1425.4 examples/sec; 0.090 sec/batch)
2017-06-02 02:41:24.611879: step 1060, loss = 2.39 (1399.6 examples/sec; 0.091 sec/batch)
2017-06-02 02:41:25.499600: step 1070, loss = 2.37 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:41:26.382524: step 1080, loss = 2.32 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:41:27.282391: step 1090, loss = 2.29 (1422.5 examples/sec; 0.090 sec/batch)
2017-06-02 02:41:28.279208: step 1100, loss = 2.42 (1284.1 examples/sec; 0.100 sec/batch)
2017-06-02 02:41:29.045828: step 1110, loss = 2.43 (1669.7 examples/sec; 0.077 sec/batch)
2017-06-02 02:41:29.932117: step 1120, loss = 2.31 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:41:30.825764: step 1130, loss = 2.23 (1432.3 examples/sec; 0.089 sec/batch)
2017-06-02 02:41:31.705197: step 1140, loss = 2.23 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:41:32.589073: step 1150, loss = 2.56 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:41:33.468521: step 1160, loss = 2.44 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:41:34.347169: step 1170, loss = 2.23 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:41:35.229682: step 1180, loss = 2.11 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:41:36.110334: step 1190, loss = 1.99 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:41:37.123248: step 1200, loss = 2.05 (1263.7 examples/sec; 0.101 sec/batch)
2017-06-02 02:41:37.907130: step 1210, loss = 2.19 (1632.9 examples/sec; 0.078 sec/batch)
2017-06-02 02:41:38.785581: step 1220, loss = 2.18 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:41:39.682987: step 1230, loss = 2.14 (1426.3 examples/sec; 0.090 sec/batch)
2017-06-02 02:41:40.569022: step 1240, loss = 2.29 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:41:41.451240: step 1250, loss = 2.18 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:41:42.327576: step 1260, loss = 2.30 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:41:43.220967: step 1270, loss = 2.06 (1432.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:41:44.100319: step 1280, loss = 2.22 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:41:44.995963: step 1290, loss = 2.31 (1429.1 examples/sec; 0.090 sec/batch)
2017-06-02 02:41:46.002141: step 1300, loss = 2.23 (1272.1 examples/sec; 0.101 sec/batch)
2017-06-02 02:41:46.776197: step 1310, loss = 2.08 (1653.6 examples/sec; 0.077 sec/batch)
2017-06-02 02:41:47.660005: step 1320, loss = 2.00 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:41:48.537054: step 1330, loss = 2.02 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:41:49.420304: step 1340, loss = 2.08 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:41:50.305140: step 1350, loss = 2.22 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:41:51.184978: step 1360, loss = 2.04 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:41:52.081273: step 1370, loss = 2.11 (1428.1 examples/sec; 0.090 sec/batch)
2017-06-02 02:41:52.972276: step 1380, loss = 2.03 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:41:53.861656: step 1390, loss = 2.03 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:41:54.858249: step 1400, loss = 2.05 (1284.4 examples/sec; 0.100 sec/batch)
2017-06-02 02:41:55.640895: step 1410, loss = 2.02 (1635.5 examples/sec; 0.078 sec/batch)
2017-06-02 02:41:56.523623: step 1420, loss = 2.14 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:41:57.400176: step 1430, loss = 2.07 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:41:58.282688: step 1440, loss = 2.04 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:41:59.170190: step 1450, loss = 1.84 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:00.063473: step 1460, loss = 2.02 (1432.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:00.952311: step 1470, loss = 1.98 (1440.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:01.838744: step 1480, loss = 1.98 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:02.716064: step 1490, loss = 1.98 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:42:03.700487: step 1500, loss = 1.88 (1300.3 examples/sec; 0.098 sec/batch)
2017-06-02 02:42:04.480571: step 1510, loss = 1.93 (1640.8 examples/sec; 0.078 sec/batch)
2017-06-02 02:42:05.370352: step 1520, loss = 2.26 (1438.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:06.257869: step 1530, loss = 1.77 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:07.145864: step 1540, loss = 2.01 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:08.026160: step 1550, loss = 1.97 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:42:08.916621: step 1560, loss = 2.15 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:09.808853: step 1570, loss = 1.96 (1434.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:10.688677: step 1580, loss = 1.83 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:42:11.565994: step 1590, loss = 2.00 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:42:12.551876: step 1600, loss = 1.70 (1298.3 examples/sec; 0.099 sec/batch)
2017-06-02 02:42:13.328156: step 1610, loss = 1.78 (1648.9 examples/sec; 0.078 sec/batch)
2017-06-02 02:42:14.193215: step 1620, loss = 1.84 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 02:42:15.071238: step 1630, loss = 1.94 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:42:15.963597: step 1640, loss = 1.79 (1434.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:16.855126: step 1650, loss = 1.94 (1435.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:17.732320: step 1660, loss = 1.97 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:42:18.616383: step 1670, loss = 1.65 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:42:19.505616: step 1680, loss = 1.69 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:20.384504: step 1690, loss = 2.08 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:42:21.384920: step 1700, loss = 1.89 (1279.5 examples/sec; 0.100 sec/batch)
2017-06-02 02:42:22.169529: step 1710, loss = 1.64 (1631.4 examples/sec; 0.078 sec/batch)
2017-06-02 02:42:23.056466: step 1720, loss = 1.63 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:23.933769: step 1730, loss = 1.91 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:42:24.820806: step 1740, loss = 1.70 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:25.702637: step 1750, loss = 1.93 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:42:26.584347: step 1760, loss = 1.89 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:42:27.468173: step 1770, loss = 1.70 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:42:28.342771: step 1780, loss = 1.61 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 02:42:29.234599: step 1790, loss = 1.52 (1435.3 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:30.237006: step 1800, loss = 1.91 (1276.9 examples/sec; 0.100 sec/batch)
2017-06-02 02:42:30.996224: step 1810, loss = 1.69 (1685.9 examples/sec; 0.076 sec/batch)
2017-06-02 02:42:31.879224: step 1820, loss = 2.03 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:42:32.768998: step 1830, loss = 1.61 (1438.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:33.653195: step 1840, loss = 1.72 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:42:34.538499: step 1850, loss = 1.63 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:35.430346: step 1860, loss = 1.63 (1435.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:36.315306: step 1870, loss = 1.81 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:42:37.200811: step 1880, loss = 1.79 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:38.080780: step 1890, loss = 1.74 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:42:39.074685: step 1900, loss = 1.72 (1287.8 examples/sec; 0.099 sec/batch)
2017-06-02 02:42:39.851846: step 1910, loss = 1.78 (1647.0 examples/sec; 0.078 sec/batch)
2017-06-02 02:42:40.751527: step 1920, loss = 1.72 (1422.8 examples/sec; 0.090 sec/batch)
2017-06-02 02:42:41.643680: step 1930, loss = 1.66 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:42.533603: step 1940, loss = 1.87 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:43.413660: step 1950, loss = 1.57 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:42:44.300078: step 1960, loss = 1.67 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:45.191197: step 1970, loss = 1.66 (1436.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:46.098229: step 1980, loss = 1.72 (1411.2 examples/sec; 0.091 sec/batch)
2017-06-02 02:42:46.980108: step 1990, loss = 1.75 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:42:47.978617: step 2000, loss = 1.67 (1281.9 examples/sec; 0.100 sec/batch)
2017-06-02 02:42:48.741808: step 2010, loss = 1.78 (1677.2 examples/sec; 0.076 sec/batch)
2017-06-02 02:42:49.628856: step 2020, loss = 1.62 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:50.523794: step 2030, loss = 1.59 (1430.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:51.408330: step 2040, loss = 1.71 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:42:52.297402: step 2050, loss = 1.52 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:53.184175: step 2060, loss = 1.54 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:54.062829: step 2070, loss = 1.57 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:42:54.939799: step 2080, loss = 1.59 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:42:55.835956: step 2090, loss = 1.58 (1428.3 examples/sec; 0.090 sec/batch)
2017-06-02 02:42:56.819529: step 2100, loss = 1.76 (1301.4 examples/sec; 0.098 sec/batch)
2017-06-02 02:42:57.595289: step 2110, loss = 1.67 (1650.0 examples/sec; 0.078 sec/batch)
2017-06-02 02:42:58.484229: step 2120, loss = 1.58 (1439.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:42:59.365044: step 2130, loss = 1.54 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:00.257694: step 2140, loss = 1.54 (1433.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:43:01.142650: step 2150, loss = 1.75 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:02.026230: step 2160, loss = 1.40 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:02.905179: step 2170, loss = 1.41 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:03.787374: step 2180, loss = 1.36 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:04.673570: step 2190, loss = 1.46 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:43:05.657509: step 2200, loss = 1.38 (1300.9 examples/sec; 0.098 sec/batch)
2017-06-02 02:43:06.438884: step 2210, loss = 1.62 (1638.1 examples/sec; 0.078 sec/batch)
2017-06-02 02:43:07.323702: step 2220, loss = 1.43 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:08.199525: step 2230, loss = 1.58 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:09.084647: step 2240, loss = 1.49 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:43:09.962896: step 2250, loss = 1.50 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:10.835993: step 2260, loss = 1.40 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 02:43:11.717428: step 2270, loss = 1.43 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:12.611666: step 2280, loss = 1.39 (1431.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:43:13.495520: step 2290, loss = 1.52 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:14.480831: step 2300, loss = 1.49 (1299.1 examples/sec; 0.099 sec/batch)
2017-06-02 02:43:15.259452: step 2310, loss = 1.53 (1643.9 examples/sec; 0.078 sec/batch)
2017-06-02 02:43:16.134346: step 2320, loss = 1.38 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 02:43:17.013857: step 2330, loss = 1.45 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:17.906465: step 2340, loss = 1.62 (1434.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:43:18.805815: step 2350, loss = 1.43 (1423.2 examples/sec; 0.090 sec/batch)
2017-06-02 02:43:19.680583: step 2360, loss = 1.53 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 02:43:20.575138: step 2370, loss = 1.36 (1430.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:43:21.457375: step 2380, loss = 1.45 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:22.349975: step 2390, loss = 1.43 (1434.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:43:23.334875: step 2400, loss = 1.64 (1299.6 examples/sec; 0.098 sec/batch)
2017-06-02 02:43:24.135304: step 2410, loss = 1.33 (1599.1 examples/sec; 0.080 sec/batch)
2017-06-02 02:43:25.018111: step 2420, loss = 1.39 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:25.907529: step 2430, loss = 1.37 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:43:26.779489: step 2440, loss = 1.48 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 02:43:27.666455: step 2450, loss = 1.39 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:43:28.553289: step 2460, loss = 1.17 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:43:29.430529: step 2470, loss = 1.43 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:30.302026: step 2480, loss = 1.57 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 02:43:31.201058: step 2490, loss = 1.43 (1423.8 examples/sec; 0.090 sec/batch)
2017-06-02 02:43:32.197118: step 2500, loss = 1.47 (1285.1 examples/sec; 0.100 sec/batch)
2017-06-02 02:43:32.971890: step 2510, loss = 1.45 (1652.1 examples/sec; 0.077 sec/batch)
2017-06-02 02:43:33.873582: step 2520, loss = 1.48 (1419.6 examples/sec; 0.090 sec/batch)
2017-06-02 02:43:34.756587: step 2530, loss = 1.64 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:35.651334: step 2540, loss = 1.38 (1430.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:43:36.534453: step 2550, loss = 1.50 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:37.417689: step 2560, loss = 1.56 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:38.295700: step 2570, loss = 1.43 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:39.171981: step 2580, loss = 1.43 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:40.068244: step 2590, loss = 1.43 (1428.1 examples/sec; 0.090 sec/batch)
2017-06-02 02:43:41.056557: step 2600, loss = 1.61 (1295.2 examples/sec; 0.099 sec/batch)
2017-06-02 02:43:41.835060: step 2610, loss = 1.17 (1644.1 examples/sec; 0.078 sec/batch)
2017-06-02 02:43:42.726751: step 2620, loss = 1.58 (1435.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:43:43.611379: step 2630, loss = 1.34 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:44.499716: step 2640, loss = 1.22 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:43:45.375561: step 2650, loss = 1.39 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:46.273984: step 2660, loss = 1.47 (1424.7 examples/sec; 0.090 sec/batch)
2017-06-02 02:43:47.153657: step 2670, loss = 1.32 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:48.042281: step 2680, loss = 1.30 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:43:48.923482: step 2690, loss = 1.37 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:49.915686: step 2700, loss = 1.28 (1290.0 examples/sec; 0.099 sec/batch)
2017-06-02 02:43:50.695691: step 2710, loss = 1.26 (1641.0 examples/sec; 0.078 sec/batch)
2017-06-02 02:43:51.590070: step 2720, loss = 1.40 (1431.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:43:52.472791: step 2730, loss = 1.37 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:53.355269: step 2740, loss = 1.38 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:54.233237: step 2750, loss = 1.31 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:55.119657: step 2760, loss = 1.32 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:43:55.996887: step 2770, loss = 1.26 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:56.874578: step 2780, loss = 1.37 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:43:57.764110: step 2790, loss = 1.37 (1439.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:43:58.743365: step 2800, loss = 1.39 (1307.1 examples/sec; 0.098 sec/batch)
2017-06-02 02:43:59.538573: step 2810, loss = 1.36 (1609.7 examples/sec; 0.080 sec/batch)
2017-06-02 02:44:00.427385: step 2820, loss = 1.17 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:44:01.323157: step 2830, loss = 1.15 (1428.9 examples/sec; 0.090 sec/batch)
2017-06-02 02:44:02.215389: step 2840, loss = 1.70 (1434.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:44:03.094517: step 2850, loss = 1.27 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:44:03.976236: step 2860, loss = 1.26 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:44:04.867384: step 2870, loss = 1.43 (1436.3 examples/sec; 0.089 sec/batch)
2017-06-02 02:44:05.748327: step 2880, loss = 1.33 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:44:06.635460: step 2890, loss = 1.28 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:44:07.624933: step 2900, loss = 1.29 (1293.6 examples/sec; 0.099 sec/batch)
2017-06-02 02:44:08.422951: step 2910, loss = 1.33 (1604.0 examples/sec; 0.080 sec/batch)
2017-06-02 02:44:09.329992: step 2920, loss = 1.30 (1411.2 examples/sec; 0.091 sec/batch)
2017-06-02 02:44:10.194980: step 2930, loss = 1.31 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 02:44:11.089592: step 2940, loss = 1.33 (1430.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:44:11.996818: step 2950, loss = 1.47 (1410.9 examples/sec; 0.091 sec/batch)
2017-06-02 02:44:12.897869: step 2960, loss = 1.33 (1420.6 examples/sec; 0.090 sec/batch)
2017-06-02 02:44:13.785606: step 2970, loss = 1.41 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:44:14.675922: step 2980, loss = 1.43 (1437.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:44:15.568569: step 2990, loss = 1.39 (1433.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:44:16.577178: step 3000, loss = 1.34 (1269.1 examples/sec; 0.101 sec/batch)
2017-06-02 02:44:17.349555: step 3010, loss = 1.27 (1657.2 examples/sec; 0.077 sec/batch)
2017-06-02 02:44:18.234237: step 3020, loss = 1.59 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:44:19.120300: step 3030, loss = 1.15 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:44:20.021810: step 3040, loss = 1.16 (1419.8 examples/sec; 0.090 sec/batch)
2017-06-02 02:44:20.903964: step 3050, loss = 1.21 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:44:21.790141: step 3060, loss = 1.26 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:44:22.669930: step 3070, loss = 1.31 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:44:23.577097: step 3080, loss = 1.39 (1411.0 examples/sec; 0.091 sec/batch)
2017-06-02 02:44:24.467758: step 3090, loss = 1.21 (1437.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:44:25.461585: step 3100, loss = 1.29 (1287.9 examples/sec; 0.099 sec/batch)
2017-06-02 02:44:26.232721: step 3110, loss = 1.25 (1659.9 examples/sec; 0.077 sec/batch)
2017-06-02 02:44:27.117199: step 3120, loss = 1.13 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:44:28.013559: step 3130, loss = 1.26 (1428.0 examples/sec; 0.090 sec/batch)
2017-06-02 02:44:28.913090: step 3140, loss = 1.29 (1423.0 examples/sec; 0.090 sec/batch)
2017-06-02 02:44:29.796120: step 3150, loss = 1.26 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:44:30.673341: step 3160, loss = 1.30 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:44:31.563575: step 3170, loss = 1.01 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:44:32.442216: step 3180, loss = 1.27 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:44:33.320904: step 3190, loss = 1.21 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:44:34.314160: step 3200, loss = 1.17 (1288.7 examples/sec; 0.099 sec/batch)
2017-06-02 02:44:35.111550: step 3210, loss = 1.44 (1605.2 examples/sec; 0.080 sec/batch)
2017-06-02 02:44:36.009852: step 3220, loss = 1.09 (1424.9 examples/sec; 0.090 sec/batch)
2017-06-02 02:44:36.900903: step 3230, loss = 1.26 (1436.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:44:37.779098: step 3240, loss = 1.39 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:44:38.657668: step 3250, loss = 1.23 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:44:39.551645: step 3260, loss = 1.12 (1431.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:44:40.438573: step 3270, loss = 1.36 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:44:41.331075: step 3280, loss = 1.25 (1434.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:44:42.226074: step 3290, loss = 1.01 (1430.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:44:43.261744: step 3300, loss = 1.21 (1235.9 examples/sec; 0.104 sec/batch)
2017-06-02 02:44:44.010603: step 3310, loss = 1.16 (1709.3 examples/sec; 0.075 sec/batch)
2017-06-02 02:44:44.899170: step 3320, loss = 1.28 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:44:45.787268: step 3330, loss = 1.21 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 02:44:46.668807: step 3340, loss = 1.06 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:44:47.563204: step 3350, loss = 1.18 (1431.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:44:48.457898: step 3360, loss = 1.19 (1430.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:44:49.350860: step 3370, loss = 1.11 (1433.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:44:50.251739: step 3380, loss = 1.28 (1420.8 examples/sec; 0.090 sec/batch)
2017-06-02 02:44:51.145919: step 3390, loss = 1.04 (1431.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:44:52.138419: step 3400, loss = 1.06 (1289.7 examples/sec; 0.099 sec/batch)
2017-06-02 02:44:52.923176: step 3410, loss = 1.33 (1631.1 examples/sec; 0.078 sec/batch)
2017-06-02 02:44:53.812364: step 3420, loss = 1.05 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:44:54.702398: step 3430, loss = 1.21 (1438.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:44:55.596283: step 3440, loss = 1.11 (1432.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:44:56.482373: step 3450, loss = 1.09 (1444.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:44:57.379785: step 3460, loss = 1.39 (1426.3 examples/sec; 0.090 sec/batch)
2017-06-02 02:44:58.258133: step 3470, loss = 1.01 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:44:59.137984: step 3480, loss = 1.18 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:45:00.028826: step 3490, loss = 1.15 (1436.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:45:01.013442: step 3500, loss = 1.13 (1300.0 examples/sec; 0.098 sec/batch)
2017-06-02 02:45:01.807208: step 3510, loss = 1.27 (1612.6 examples/sec; 0.079 sec/batch)
2017-06-02 02:45:02.692722: step 3520, loss = 0.88 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:45:03.576808: step 3530, loss = 1.29 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:45:04.459811: step 3540, loss = 1.12 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:45:05.353806: step 3550, loss = 1.02 (1431.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:45:06.237947: step 3560, loss = 1.12 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:45:07.129968: step 3570, loss = 1.03 (1434.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:45:08.017139: step 3580, loss = 1.34 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:45:08.900971: step 3590, loss = 0.93 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:45:09.889431: step 3600, loss = 1.03 (1294.9 examples/sec; 0.099 sec/batch)
2017-06-02 02:45:10.671586: step 3610, loss = 1.15 (1636.5 examples/sec; 0.078 sec/batch)
2017-06-02 02:45:11.559469: step 3620, loss = 1.10 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:45:12.442242: step 3630, loss = 1.07 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:45:13.326653: step 3640, loss = 1.10 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:45:14.206225: step 3650, loss = 1.04 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:45:15.096192: step 3660, loss = 1.11 (1438.3 examples/sec; 0.089 sec/batch)
2017-06-02 02:45:16.006336: step 3670, loss = 1.15 (1406.4 examples/sec; 0.091 sec/batch)
2017-06-02 02:45:16.888013: step 3680, loss = 1.12 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:45:17.788147: step 3690, loss = 1.10 (1422.0 examples/sec; 0.090 sec/batch)
2017-06-02 02:45:18.780378: step 3700, loss = 1.11 (1290.0 examples/sec; 0.099 sec/batch)
2017-06-02 02:45:19.555470: step 3710, loss = 1.22 (1651.4 examples/sec; 0.078 sec/batch)
2017-06-02 02:45:20.449962: step 3720, loss = 1.30 (1431.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:45:21.333927: step 3730, loss = 1.30 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:45:22.207666: step 3740, loss = 1.23 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 02:45:23.099098: step 3750, loss = 1.15 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:45:23.992777: step 3760, loss = 1.26 (1432.3 examples/sec; 0.089 sec/batch)
2017-06-02 02:45:24.871648: step 3770, loss = 1.20 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:45:25.758907: step 3780, loss = 1.18 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:45:26.647186: step 3790, loss = 1.27 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:45:27.650464: step 3800, loss = 1.38 (1275.8 examples/sec; 0.100 sec/batch)
2017-06-02 02:45:28.427383: step 3810, loss = 1.22 (1647.6 examples/sec; 0.078 sec/batch)
2017-06-02 02:45:29.320136: step 3820, loss = 1.12 (1433.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:45:30.210372: step 3830, loss = 1.38 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:45:31.113057: step 3840, loss = 1.24 (1418.0 examples/sec; 0.090 sec/batch)
2017-06-02 02:45:31.997756: step 3850, loss = 1.13 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:45:32.890486: step 3860, loss = 1.14 (1433.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:45:33.785357: step 3870, loss = 1.31 (1430.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:45:34.668045: step 3880, loss = 0.91 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:45:35.552710: step 3890, loss = 1.17 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:45:36.563243: step 3900, loss = 1.07 (1266.6 examples/sec; 0.101 sec/batch)
2017-06-02 02:45:37.328038: step 3910, loss = 1.25 (1673.7 examples/sec; 0.076 sec/batch)
2017-06-02 02:45:38.206073: step 3920, loss = 1.16 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:45:39.087974: step 3930, loss = 1.09 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:45:39.961298: step 3940, loss = 1.27 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 02:45:40.841281: step 3950, loss = 1.04 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:45:41.722346: step 3960, loss = 1.39 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:45:42.587463: step 3970, loss = 1.05 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 02:45:43.481694: step 3980, loss = 1.16 (1431.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:45:44.357476: step 3990, loss = 1.38 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:45:45.347916: step 4000, loss = 1.28 (1292.3 examples/sec; 0.099 sec/batch)
2017-06-02 02:45:46.118365: step 4010, loss = 1.02 (1661.4 examples/sec; 0.077 sec/batch)
2017-06-02 02:45:47.001804: step 4020, loss = 1.03 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:45:47.887034: step 4030, loss = 1.26 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:45:48.782905: step 4040, loss = 1.24 (1428.8 examples/sec; 0.090 sec/batch)
2017-06-02 02:45:49.658511: step 4050, loss = 1.23 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:45:50.540350: step 4060, loss = 1.33 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:45:51.421756: step 4070, loss = 1.11 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:45:52.306858: step 4080, loss = 1.15 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:45:53.190294: step 4090, loss = 0.85 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:45:54.167775: step 4100, loss = 1.30 (1309.5 examples/sec; 0.098 sec/batch)
2017-06-02 02:45:54.941058: step 4110, loss = 1.13 (1655.3 examples/sec; 0.077 sec/batch)
2017-06-02 02:45:55.833200: step 4120, loss = 1.20 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:45:56.720510: step 4130, loss = 1.18 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:45:57.599960: step 4140, loss = 1.03 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:45:58.485500: step 4150, loss = 0.94 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:45:59.378904: step 4160, loss = 1.05 (1432.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:46:00.274647: step 4170, loss = 1.22 (1429.0 examples/sec; 0.090 sec/batch)
2017-06-02 02:46:01.162957: step 4180, loss = 1.07 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:46:02.039748: step 4190, loss = 1.17 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:46:03.017653: step 4200, loss = 1.03 (1308.9 examples/sec; 0.098 sec/batch)
2017-06-02 02:46:03.810895: step 4210, loss = 1.24 (1613.6 examples/sec; 0.079 sec/batch)
2017-06-02 02:46:04.682091: step 4220, loss = 0.95 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 02:46:05.576941: step 4230, loss = 0.97 (1430.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:46:06.452594: step 4240, loss = 1.18 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:46:07.340393: step 4250, loss = 1.00 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:46:08.226599: step 4260, loss = 1.00 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:46:09.106027: step 4270, loss = 1.06 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:46:09.987062: step 4280, loss = 1.17 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:46:10.887056: step 4290, loss = 1.15 (1422.2 examples/sec; 0.090 sec/batch)
2017-06-02 02:46:11.889102: step 4300, loss = 1.21 (1277.4 examples/sec; 0.100 sec/batch)
2017-06-02 02:46:12.671101: step 4310, loss = 0.98 (1636.8 examples/sec; 0.078 sec/batch)
2017-06-02 02:46:13.556492: step 4320, loss = 1.13 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:46:14.443589: step 4330, loss = 1.11 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:46:15.331877: step 4340, loss = 1.01 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:46:16.213225: step 4350, loss = 1.04 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:46:17.095678: step 4360, loss = 1.28 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:46:17.978068: step 4370, loss = 1.11 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:46:18.858322: step 4380, loss = 1.11 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:46:19.733343: step 4390, loss = 1.18 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:46:20.728356: step 4400, loss = 1.19 (1286.4 examples/sec; 0.100 sec/batch)
2017-06-02 02:46:21.515276: step 4410, loss = 1.08 (1626.6 examples/sec; 0.079 sec/batch)
2017-06-02 02:46:22.386653: step 4420, loss = 1.04 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 02:46:23.286378: step 4430, loss = 1.09 (1422.7 examples/sec; 0.090 sec/batch)
2017-06-02 02:46:24.156246: step 4440, loss = 0.97 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 02:46:25.045992: step 4450, loss = 1.09 (1438.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:46:25.921016: step 4460, loss = 1.05 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:46:26.816964: step 4470, loss = 1.06 (1428.7 examples/sec; 0.090 sec/batch)
2017-06-02 02:46:27.694534: step 4480, loss = 0.94 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:46:28.580819: step 4490, loss = 1.14 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:46:29.567859: step 4500, loss = 0.93 (1296.8 examples/sec; 0.099 sec/batch)
2017-06-02 02:46:30.347376: step 4510, loss = 0.96 (1642.0 examples/sec; 0.078 sec/batch)
2017-06-02 02:46:31.232385: step 4520, loss = 1.09 (1446.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:46:32.126042: step 4530, loss = 1.11 (1432.3 examples/sec; 0.089 sec/batch)
2017-06-02 02:46:33.006705: step 4540, loss = 1.05 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:46:33.885869: step 4550, loss = 0.91 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:46:34.767739: step 4560, loss = 0.98 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:46:35.659324: step 4570, loss = 0.94 (1435.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:46:36.544646: step 4580, loss = 1.05 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:46:37.436671: step 4590, loss = 1.01 (1434.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:46:38.416444: step 4600, loss = 1.09 (1306.4 examples/sec; 0.098 sec/batch)
2017-06-02 02:46:39.193286: step 4610, loss = 1.25 (1647.7 examples/sec; 0.078 sec/batch)
2017-06-02 02:46:40.087311: step 4620, loss = 1.07 (1431.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:46:40.962277: step 4630, loss = 0.95 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 02:46:41.856511: step 4640, loss = 1.14 (1431.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:46:42.740459: step 4650, loss = 0.98 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:46:43.628061: step 4660, loss = 1.09 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:46:44.500220: step 4670, loss = 0.86 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 02:46:45.396909: step 4680, loss = 0.85 (1427.5 examples/sec; 0.090 sec/batch)
2017-06-02 02:46:46.279642: step 4690, loss = 0.98 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:46:47.255325: step 4700, loss = 0.97 (1311.9 examples/sec; 0.098 sec/batch)
2017-06-02 02:46:48.040653: step 4710, loss = 0.85 (1629.9 examples/sec; 0.079 sec/batch)
2017-06-02 02:46:48.921837: step 4720, loss = 0.87 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:46:49.808500: step 4730, loss = 1.13 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:46:50.691687: step 4740, loss = 0.87 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:46:51.582676: step 4750, loss = 1.10 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:46:52.472549: step 4760, loss = 1.07 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:46:53.356214: step 4770, loss = 0.92 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:46:54.244590: step 4780, loss = 0.99 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:46:55.118376: step 4790, loss = 0.93 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 02:46:56.099800: step 4800, loss = 0.86 (1304.2 examples/sec; 0.098 sec/batch)
2017-06-02 02:46:56.888183: step 4810, loss = 0.94 (1623.6 examples/sec; 0.079 sec/batch)
2017-06-02 02:46:57.772429: step 4820, loss = 1.19 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:46:58.659530: step 4830, loss = 1.07 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:46:59.560057: step 4840, loss = 1.01 (1421.4 examples/sec; 0.090 sec/batch)
2017-06-02 02:47:00.444759: step 4850, loss = 0.89 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:01.324646: step 4860, loss = 1.23 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:02.202284: step 4870, loss = 1.14 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:03.076847: step 4880, loss = 1.05 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 02:47:03.967719: step 4890, loss = 0.99 (1436.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:47:04.952402: step 4900, loss = 1.09 (1299.9 examples/sec; 0.098 sec/batch)
2017-06-02 02:47:05.733137: step 4910, loss = 1.04 (1639.5 examples/sec; 0.078 sec/batch)
2017-06-02 02:47:06.608814: step 4920, loss = 1.06 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:07.491225: step 4930, loss = 1.05 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:08.377078: step 4940, loss = 1.38 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:47:09.259154: step 4950, loss = 1.08 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:10.137292: step 4960, loss = 0.85 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:11.014966: step 4970, loss = 1.00 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:11.895573: step 4980, loss = 1.05 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:12.771361: step 4990, loss = 1.15 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:13.761721: step 5000, loss = 1.05 (1292.5 examples/sec; 0.099 sec/batch)
2017-06-02 02:47:14.536492: step 5010, loss = 1.15 (1652.1 examples/sec; 0.077 sec/batch)
2017-06-02 02:47:15.430445: step 5020, loss = 0.95 (1431.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:47:16.309471: step 5030, loss = 0.97 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:17.179067: step 5040, loss = 1.02 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 02:47:18.061442: step 5050, loss = 1.03 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:18.956145: step 5060, loss = 1.07 (1430.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:47:19.827540: step 5070, loss = 1.03 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 02:47:20.721942: step 5080, loss = 1.30 (1431.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:47:21.596361: step 5090, loss = 0.99 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 02:47:22.574135: step 5100, loss = 1.02 (1309.1 examples/sec; 0.098 sec/batch)
2017-06-02 02:47:23.351627: step 5110, loss = 0.79 (1646.4 examples/sec; 0.078 sec/batch)
2017-06-02 02:47:24.234450: step 5120, loss = 1.06 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:25.118504: step 5130, loss = 1.01 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:25.996221: step 5140, loss = 1.17 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:26.894807: step 5150, loss = 0.97 (1424.4 examples/sec; 0.090 sec/batch)
2017-06-02 02:47:27.787804: step 5160, loss = 0.97 (1433.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:47:28.668473: step 5170, loss = 0.98 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:29.551844: step 5180, loss = 1.01 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:30.434703: step 5190, loss = 1.10 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:31.417130: step 5200, loss = 0.93 (1302.9 examples/sec; 0.098 sec/batch)
2017-06-02 02:47:32.188922: step 5210, loss = 1.14 (1658.5 examples/sec; 0.077 sec/batch)
2017-06-02 02:47:33.070595: step 5220, loss = 1.07 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:33.959156: step 5230, loss = 1.13 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:47:34.838397: step 5240, loss = 0.98 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:35.722066: step 5250, loss = 1.01 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:36.627795: step 5260, loss = 0.97 (1413.2 examples/sec; 0.091 sec/batch)
2017-06-02 02:47:37.521042: step 5270, loss = 0.92 (1433.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:47:38.388987: step 5280, loss = 0.98 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 02:47:39.275690: step 5290, loss = 1.07 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:47:40.266949: step 5300, loss = 0.90 (1291.3 examples/sec; 0.099 sec/batch)
2017-06-02 02:47:41.041390: step 5310, loss = 0.99 (1652.8 examples/sec; 0.077 sec/batch)
2017-06-02 02:47:41.923861: step 5320, loss = 0.93 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:42.809133: step 5330, loss = 0.86 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:47:43.699980: step 5340, loss = 1.01 (1436.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:47:44.577486: step 5350, loss = 0.91 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:45.467018: step 5360, loss = 1.02 (1439.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:47:46.355628: step 5370, loss = 1.12 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:47:47.239836: step 5380, loss = 0.98 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:48.128811: step 5390, loss = 0.94 (1439.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:47:49.118105: step 5400, loss = 0.96 (1293.8 examples/sec; 0.099 sec/batch)
2017-06-02 02:47:49.898500: step 5410, loss = 0.88 (1640.2 examples/sec; 0.078 sec/batch)
2017-06-02 02:47:50.777994: step 5420, loss = 0.89 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:51.656656: step 5430, loss = 0.95 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:52.537343: step 5440, loss = 0.97 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:53.424693: step 5450, loss = 0.83 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:47:54.303684: step 5460, loss = 0.94 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:47:55.196996: step 5470, loss = 0.95 (1432.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:47:56.084798: step 5480, loss = 0.92 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:47:56.957537: step 5490, loss = 0.99 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 02:47:57.945993: step 5500, loss = 0.88 (1294.9 examples/sec; 0.099 sec/batch)
2017-06-02 02:47:58.726766: step 5510, loss = 1.00 (1639.4 examples/sec; 0.078 sec/batch)
2017-06-02 02:47:59.616089: step 5520, loss = 1.10 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 02:48:00.502267: step 5530, loss = 1.04 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:48:01.389268: step 5540, loss = 1.05 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:48:02.272495: step 5550, loss = 0.84 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:48:03.159717: step 5560, loss = 1.13 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:48:04.056398: step 5570, loss = 1.06 (1427.5 examples/sec; 0.090 sec/batch)
2017-06-02 02:48:04.936611: step 5580, loss = 1.12 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:48:05.825266: step 5590, loss = 1.05 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:48:06.810222: step 5600, loss = 1.09 (1299.5 examples/sec; 0.098 sec/batch)
2017-06-02 02:48:07.604760: step 5610, loss = 1.13 (1611.0 examples/sec; 0.079 sec/batch)
2017-06-02 02:48:08.493222: step 5620, loss = 0.93 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:48:09.368460: step 5630, loss = 1.00 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:48:10.250919: step 5640, loss = 1.20 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:48:11.136610: step 5650, loss = 1.05 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:48:12.029942: step 5660, loss = 0.93 (1432.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:48:12.924161: step 5670, loss = 0.92 (1431.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:48:13.794623: step 5680, loss = 1.05 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 02:48:14.680411: step 5690, loss = 1.06 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:48:15.679073: step 5700, loss = 1.11 (1281.7 examples/sec; 0.100 sec/batch)
2017-06-02 02:48:16.476418: step 5710, loss = 0.98 (1605.3 examples/sec; 0.080 sec/batch)
2017-06-02 02:48:17.356557: step 5720, loss = 0.98 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:48:18.238111: step 5730, loss = 1.06 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:48:19.143324: step 5740, loss = 0.96 (1414.0 examples/sec; 0.091 sec/batch)
2017-06-02 02:48:20.023815: step 5750, loss = 0.92 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:48:20.909623: step 5760, loss = 0.94 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:48:21.797040: step 5770, loss = 1.06 (1442.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:48:22.693534: step 5780, loss = 0.99 (1427.8 examples/sec; 0.090 sec/batch)
2017-06-02 02:48:23.579988: step 5790, loss = 0.99 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:48:24.584264: step 5800, loss = 1.05 (1274.5 examples/sec; 0.100 sec/batch)
2017-06-02 02:48:25.364240: step 5810, loss = 0.99 (1641.1 examples/sec; 0.078 sec/batch)
2017-06-02 02:48:26.242084: step 5820, loss = 0.97 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:48:27.129720: step 5830, loss = 1.09 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:48:28.015598: step 5840, loss = 1.14 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:48:28.905691: step 5850, loss = 0.94 (1438.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:48:29.788546: step 5860, loss = 0.92 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:48:30.666158: step 5870, loss = 0.85 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:48:31.547131: step 5880, loss = 1.38 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:48:32.431648: step 5890, loss = 1.06 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:48:33.405515: step 5900, loss = 1.08 (1314.4 examples/sec; 0.097 sec/batch)
2017-06-02 02:48:34.190171: step 5910, loss = 0.93 (1631.3 examples/sec; 0.078 sec/batch)
2017-06-02 02:48:35.097440: step 5920, loss = 1.07 (1410.8 examples/sec; 0.091 sec/batch)
2017-06-02 02:48:35.981182: step 5930, loss = 0.94 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:48:36.871287: step 5940, loss = 1.10 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:48:37.748389: step 5950, loss = 0.78 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:48:38.633722: step 5960, loss = 1.13 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:48:39.538233: step 5970, loss = 1.04 (1415.1 examples/sec; 0.090 sec/batch)
2017-06-02 02:48:40.425558: step 5980, loss = 0.77 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:48:41.316518: step 5990, loss = 0.96 (1436.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:48:42.302824: step 6000, loss = 0.86 (1297.8 examples/sec; 0.099 sec/batch)
2017-06-02 02:48:43.091850: step 6010, loss = 0.96 (1622.3 examples/sec; 0.079 sec/batch)
2017-06-02 02:48:43.979899: step 6020, loss = 0.98 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:48:44.864130: step 6030, loss = 1.03 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:48:45.748908: step 6040, loss = 0.94 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:48:46.627151: step 6050, loss = 1.04 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:48:47.518147: step 6060, loss = 1.02 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:48:48.405266: step 6070, loss = 0.90 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:48:49.292163: step 6080, loss = 1.23 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:48:50.173859: step 6090, loss = 0.80 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:48:51.172224: step 6100, loss = 1.17 (1282.1 examples/sec; 0.100 sec/batch)
2017-06-02 02:48:51.962044: step 6110, loss = 0.86 (1620.6 examples/sec; 0.079 sec/batch)
2017-06-02 02:48:52.845638: step 6120, loss = 0.87 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:48:53.737398: step 6130, loss = 0.80 (1435.3 examples/sec; 0.089 sec/batch)
2017-06-02 02:48:54.615869: step 6140, loss = 0.94 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:48:55.496331: step 6150, loss = 0.85 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:48:56.383585: step 6160, loss = 0.79 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:48:57.260659: step 6170, loss = 1.02 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:48:58.154176: step 6180, loss = 0.95 (1432.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:48:59.036312: step 6190, loss = 0.95 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:49:00.020402: step 6200, loss = 0.87 (1300.7 examples/sec; 0.098 sec/batch)
2017-06-02 02:49:00.800563: step 6210, loss = 1.03 (1640.7 examples/sec; 0.078 sec/batch)
2017-06-02 02:49:01.679829: step 6220, loss = 0.97 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:49:02.565300: step 6230, loss = 1.06 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:49:03.450235: step 6240, loss = 0.88 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:49:04.318994: step 6250, loss = 0.85 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 02:49:05.206288: step 6260, loss = 0.91 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:49:06.092945: step 6270, loss = 0.80 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:49:06.979526: step 6280, loss = 0.98 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:49:07.863797: step 6290, loss = 0.91 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:49:08.848352: step 6300, loss = 0.85 (1300.1 examples/sec; 0.098 sec/batch)
2017-06-02 02:49:09.634696: step 6310, loss = 0.97 (1627.8 examples/sec; 0.079 sec/batch)
2017-06-02 02:49:10.518974: step 6320, loss = 1.27 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:49:11.403767: step 6330, loss = 0.97 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:49:12.300685: step 6340, loss = 0.90 (1427.1 examples/sec; 0.090 sec/batch)
2017-06-02 02:49:13.180010: step 6350, loss = 0.86 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:49:14.079824: step 6360, loss = 0.94 (1422.5 examples/sec; 0.090 sec/batch)
2017-06-02 02:49:14.973911: step 6370, loss = 0.93 (1431.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:49:15.853065: step 6380, loss = 1.04 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:49:16.744700: step 6390, loss = 1.01 (1435.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:49:17.724436: step 6400, loss = 1.07 (1306.5 examples/sec; 0.098 sec/batch)
2017-06-02 02:49:18.517186: step 6410, loss = 0.85 (1614.6 examples/sec; 0.079 sec/batch)
2017-06-02 02:49:19.414450: step 6420, loss = 1.05 (1426.6 examples/sec; 0.090 sec/batch)
2017-06-02 02:49:20.309658: step 6430, loss = 0.90 (1429.8 examples/sec; 0.090 sec/batch)
2017-06-02 02:49:21.186291: step 6440, loss = 1.03 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:49:22.069781: step 6450, loss = 0.91 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:49:22.959332: step 6460, loss = 0.85 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:49:23.846129: step 6470, loss = 1.01 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:49:24.724226: step 6480, loss = 0.91 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:49:25.602395: step 6490, loss = 0.78 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:49:26.588704: step 6500, loss = 0.96 (1297.8 examples/sec; 0.099 sec/batch)
2017-06-02 02:49:27.387438: step 6510, loss = 1.00 (1602.5 examples/sec; 0.080 sec/batch)
2017-06-02 02:49:28.272214: step 6520, loss = 0.97 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:49:29.159318: step 6530, loss = 0.78 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:49:30.043571: step 6540, loss = 1.00 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:49:30.924991: step 6550, loss = 0.86 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:49:31.812293: step 6560, loss = 1.01 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:49:32.687424: step 6570, loss = 0.82 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:49:33.574152: step 6580, loss = 1.20 (1443.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:49:34.449480: step 6590, loss = 0.99 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:49:35.443666: step 6600, loss = 1.02 (1287.5 examples/sec; 0.099 sec/batch)
2017-06-02 02:49:36.213278: step 6610, loss = 1.00 (1663.5 examples/sec; 0.077 sec/batch)
2017-06-02 02:49:37.102940: step 6620, loss = 0.89 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:49:37.983457: step 6630, loss = 1.08 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:49:38.873563: step 6640, loss = 1.13 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:49:39.768567: step 6650, loss = 0.97 (1430.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:49:40.655465: step 6660, loss = 0.91 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:49:41.546597: step 6670, loss = 1.20 (1436.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:49:42.421900: step 6680, loss = 1.02 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:49:43.330205: step 6690, loss = 0.84 (1409.2 examples/sec; 0.091 sec/batch)
2017-06-02 02:49:44.312797: step 6700, loss = 0.99 (1302.7 examples/sec; 0.098 sec/batch)
2017-06-02 02:49:45.102531: step 6710, loss = 0.88 (1620.8 examples/sec; 0.079 sec/batch)
2017-06-02 02:49:45.994096: step 6720, loss = 0.85 (1435.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:49:46.883775: step 6730, loss = 1.11 (1438.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:49:47.770385: step 6740, loss = 0.86 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:49:48.651491: step 6750, loss = 1.19 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:49:49.537675: step 6760, loss = 0.92 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:49:50.413966: step 6770, loss = 0.82 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:49:51.360683: step 6780, loss = 0.92 (1352.0 examples/sec; 0.095 sec/batch)
2017-06-02 02:49:52.241906: step 6790, loss = 0.90 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:49:53.216011: step 6800, loss = 0.97 (1314.0 examples/sec; 0.097 sec/batch)
2017-06-02 02:49:54.004202: step 6810, loss = 0.91 (1624.0 examples/sec; 0.079 sec/batch)
2017-06-02 02:49:54.894012: step 6820, loss = 0.91 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:49:55.772073: step 6830, loss = 1.00 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:49:56.668499: step 6840, loss = 0.99 (1427.9 examples/sec; 0.090 sec/batch)
2017-06-02 02:49:57.555919: step 6850, loss = 1.08 (1442.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:49:58.441764: step 6860, loss = 0.87 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:49:59.332565: step 6870, loss = 0.78 (1436.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:50:00.236430: step 6880, loss = 0.97 (1416.2 examples/sec; 0.090 sec/batch)
2017-06-02 02:50:01.135377: step 6890, loss = 0.94 (1423.9 examples/sec; 0.090 sec/batch)
2017-06-02 02:50:02.127676: step 6900, loss = 1.00 (1289.9 examples/sec; 0.099 sec/batch)
2017-06-02 02:50:02.919459: step 6910, loss = 0.84 (1616.6 examples/sec; 0.079 sec/batch)
2017-06-02 02:50:03.804543: step 6920, loss = 1.01 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:50:04.686344: step 6930, loss = 1.00 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:50:05.568365: step 6940, loss = 1.04 (1451.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:50:06.452973: step 6950, loss = 1.01 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:50:07.336126: step 6960, loss = 0.92 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:50:08.226337: step 6970, loss = 0.94 (1437.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:50:09.104052: step 6980, loss = 1.01 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:50:09.972251: step 6990, loss = 0.93 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 02:50:10.976756: step 7000, loss = 0.87 (1274.3 examples/sec; 0.100 sec/batch)
2017-06-02 02:50:11.763754: step 7010, loss = 0.94 (1626.5 examples/sec; 0.079 sec/batch)
2017-06-02 02:50:12.634701: step 7020, loss = 1.33 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 02:50:13.519158: step 7030, loss = 0.97 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:50:14.396073: step 7040, loss = 1.00 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:50:15.277425: step 7050, loss = 1.05 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:50:16.162344: step 7060, loss = 0.96 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:50:17.037073: step 7070, loss = 1.01 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 02:50:17.921178: step 7080, loss = 0.91 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:50:18.807171: step 7090, loss = 1.09 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:50:19.790338: step 7100, loss = 0.72 (1301.9 examples/sec; 0.098 sec/batch)
2017-06-02 02:50:20.574433: step 7110, loss = 0.82 (1632.4 examples/sec; 0.078 sec/batch)
2017-06-02 02:50:21.453354: step 7120, loss = 0.95 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:50:22.339905: step 7130, loss = 0.86 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:50:23.237216: step 7140, loss = 0.87 (1426.5 examples/sec; 0.090 sec/batch)
2017-06-02 02:50:24.146519: step 7150, loss = 1.08 (1407.7 examples/sec; 0.091 sec/batch)
2017-06-02 02:50:25.042572: step 7160, loss = 0.93 (1428.5 examples/sec; 0.090 sec/batch)
2017-06-02 02:50:25.948339: step 7170, loss = 1.01 (1413.2 examples/sec; 0.091 sec/batch)
2017-06-02 02:50:26.834959: step 7180, loss = 0.91 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:50:27.741438: step 7190, loss = 0.86 (1412.1 examples/sec; 0.091 sec/batch)
2017-06-02 02:50:28.742862: step 7200, loss = 1.05 (1278.2 examples/sec; 0.100 sec/batch)
2017-06-02 02:50:29.528070: step 7210, loss = 1.05 (1630.1 examples/sec; 0.079 sec/batch)
2017-06-02 02:50:30.397301: step 7220, loss = 0.71 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 02:50:31.287595: step 7230, loss = 1.18 (1437.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:50:32.178674: step 7240, loss = 0.90 (1436.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:50:33.059966: step 7250, loss = 0.88 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:50:33.936892: step 7260, loss = 1.00 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:50:34.822910: step 7270, loss = 1.03 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:50:35.723491: step 7280, loss = 0.71 (1421.3 examples/sec; 0.090 sec/batch)
2017-06-02 02:50:36.610028: step 7290, loss = 0.96 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:50:37.595997: step 7300, loss = 1.20 (1298.2 examples/sec; 0.099 sec/batch)
2017-06-02 02:50:38.372957: step 7310, loss = 0.85 (1647.4 examples/sec; 0.078 sec/batch)
2017-06-02 02:50:39.263425: step 7320, loss = 0.91 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:50:40.157120: step 7330, loss = 0.91 (1432.3 examples/sec; 0.089 sec/batch)
2017-06-02 02:50:41.033854: step 7340, loss = 1.08 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:50:41.901171: step 7350, loss = 0.80 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 02:50:42.781910: step 7360, loss = 1.01 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:50:43.667334: step 7370, loss = 0.89 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:50:44.575698: step 7380, loss = 1.01 (1409.1 examples/sec; 0.091 sec/batch)
2017-06-02 02:50:45.468507: step 7390, loss = 0.84 (1433.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:50:46.457081: step 7400, loss = 0.78 (1294.8 examples/sec; 0.099 sec/batch)
2017-06-02 02:50:47.229531: step 7410, loss = 0.92 (1657.1 examples/sec; 0.077 sec/batch)
2017-06-02 02:50:48.105441: step 7420, loss = 1.02 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:50:48.987381: step 7430, loss = 0.88 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:50:49.861666: step 7440, loss = 1.04 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 02:50:50.750137: step 7450, loss = 0.95 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:50:51.635671: step 7460, loss = 0.95 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:50:52.528236: step 7470, loss = 0.81 (1434.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:50:53.409890: step 7480, loss = 0.92 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:50:54.292144: step 7490, loss = 0.79 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:50:55.274947: step 7500, loss = 0.85 (1302.4 examples/sec; 0.098 sec/batch)
2017-06-02 02:50:56.062676: step 7510, loss = 0.93 (1624.9 examples/sec; 0.079 sec/batch)
2017-06-02 02:50:56.952761: step 7520, loss = 0.90 (1438.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:50:57.832201: step 7530, loss = 0.77 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:50:58.712763: step 7540, loss = 0.83 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:50:59.603390: step 7550, loss = 0.83 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:51:00.518183: step 7560, loss = 0.96 (1399.2 examples/sec; 0.091 sec/batch)
2017-06-02 02:51:01.398000: step 7570, loss = 0.83 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:51:02.280749: step 7580, loss = 0.93 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:51:03.175964: step 7590, loss = 0.98 (1429.8 examples/sec; 0.090 sec/batch)
2017-06-02 02:51:04.166972: step 7600, loss = 1.24 (1291.6 examples/sec; 0.099 sec/batch)
2017-06-02 02:51:04.957670: step 7610, loss = 1.00 (1618.8 examples/sec; 0.079 sec/batch)
2017-06-02 02:51:05.843222: step 7620, loss = 0.77 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:51:06.741062: step 7630, loss = 1.13 (1425.6 examples/sec; 0.090 sec/batch)
2017-06-02 02:51:07.635890: step 7640, loss = 0.96 (1430.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:51:08.535180: step 7650, loss = 0.81 (1423.4 examples/sec; 0.090 sec/batch)
2017-06-02 02:51:09.439799: step 7660, loss = 0.87 (1415.0 examples/sec; 0.090 sec/batch)
2017-06-02 02:51:10.337313: step 7670, loss = 0.87 (1426.2 examples/sec; 0.090 sec/batch)
2017-06-02 02:51:11.220372: step 7680, loss = 0.76 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:51:12.106527: step 7690, loss = 1.10 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:51:13.119132: step 7700, loss = 0.77 (1264.1 examples/sec; 0.101 sec/batch)
2017-06-02 02:51:13.876587: step 7710, loss = 0.85 (1689.9 examples/sec; 0.076 sec/batch)
2017-06-02 02:51:14.761525: step 7720, loss = 0.94 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:51:15.658596: step 7730, loss = 0.99 (1426.9 examples/sec; 0.090 sec/batch)
2017-06-02 02:51:16.540945: step 7740, loss = 0.82 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:51:17.436352: step 7750, loss = 0.79 (1429.5 examples/sec; 0.090 sec/batch)
2017-06-02 02:51:18.324588: step 7760, loss = 1.11 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:51:19.214515: step 7770, loss = 0.85 (1438.3 examples/sec; 0.089 sec/batch)
2017-06-02 02:51:20.113629: step 7780, loss = 1.07 (1423.7 examples/sec; 0.090 sec/batch)
2017-06-02 02:51:20.999404: step 7790, loss = 0.97 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:51:21.987489: step 7800, loss = 0.80 (1295.4 examples/sec; 0.099 sec/batch)
2017-06-02 02:51:22.771827: step 7810, loss = 0.93 (1631.9 examples/sec; 0.078 sec/batch)
2017-06-02 02:51:23.658728: step 7820, loss = 0.93 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:51:24.541964: step 7830, loss = 0.85 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:51:25.432425: step 7840, loss = 0.81 (1437.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:51:26.314589: step 7850, loss = 1.02 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:51:27.213094: step 7860, loss = 0.80 (1424.6 examples/sec; 0.090 sec/batch)
2017-06-02 02:51:28.092514: step 7870, loss = 0.97 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:51:28.976182: step 7880, loss = 0.79 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:51:29.856947: step 7890, loss = 0.82 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:51:30.841877: step 7900, loss = 0.78 (1299.6 examples/sec; 0.098 sec/batch)
2017-06-02 02:51:31.634609: step 7910, loss = 1.09 (1614.7 examples/sec; 0.079 sec/batch)
2017-06-02 02:51:32.529254: step 7920, loss = 0.80 (1430.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:51:33.406548: step 7930, loss = 1.14 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:51:34.283036: step 7940, loss = 0.83 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:51:35.163525: step 7950, loss = 0.94 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:51:36.061140: step 7960, loss = 0.85 (1426.0 examples/sec; 0.090 sec/batch)
2017-06-02 02:51:36.950265: step 7970, loss = 0.82 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:51:37.834272: step 7980, loss = 1.09 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:51:38.711805: step 7990, loss = 0.97 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:51:39.691350: step 8000, loss = 1.02 (1306.7 examples/sec; 0.098 sec/batch)
2017-06-02 02:51:40.486599: step 8010, loss = 1.07 (1609.6 examples/sec; 0.080 sec/batch)
2017-06-02 02:51:41.371657: step 8020, loss = 0.96 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:51:42.256790: step 8030, loss = 0.84 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:51:43.136140: step 8040, loss = 1.01 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:51:44.020777: step 8050, loss = 0.97 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:51:44.901786: step 8060, loss = 1.06 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:51:45.785946: step 8070, loss = 0.79 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:51:46.671750: step 8080, loss = 0.97 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:51:47.555383: step 8090, loss = 1.07 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:51:48.554893: step 8100, loss = 0.85 (1280.6 examples/sec; 0.100 sec/batch)
2017-06-02 02:51:49.335080: step 8110, loss = 0.82 (1640.7 examples/sec; 0.078 sec/batch)
2017-06-02 02:51:50.205941: step 8120, loss = 0.92 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 02:51:51.084182: step 8130, loss = 0.89 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:51:51.964597: step 8140, loss = 0.91 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:51:52.844313: step 8150, loss = 0.79 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:51:53.723063: step 8160, loss = 1.03 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:51:54.615606: step 8170, loss = 0.84 (1434.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:51:55.508117: step 8180, loss = 1.01 (1434.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:51:56.379475: step 8190, loss = 0.93 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 02:51:57.368446: step 8200, loss = 0.97 (1294.3 examples/sec; 0.099 sec/batch)
2017-06-02 02:51:58.161973: step 8210, loss = 0.85 (1613.0 examples/sec; 0.079 sec/batch)
2017-06-02 02:51:59.050199: step 8220, loss = 0.98 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:51:59.935446: step 8230, loss = 0.88 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:52:00.822105: step 8240, loss = 0.82 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:52:01.711832: step 8250, loss = 0.71 (1438.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:52:02.593264: step 8260, loss = 0.92 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:52:03.495265: step 8270, loss = 0.91 (1419.1 examples/sec; 0.090 sec/batch)
2017-06-02 02:52:04.395675: step 8280, loss = 0.95 (1421.6 examples/sec; 0.090 sec/batch)
2017-06-02 02:52:05.282342: step 8290, loss = 1.08 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:52:06.257812: step 8300, loss = 0.96 (1312.2 examples/sec; 0.098 sec/batch)
2017-06-02 02:52:07.041568: step 8310, loss = 0.83 (1633.2 examples/sec; 0.078 sec/batch)
2017-06-02 02:52:07.928114: step 8320, loss = 0.89 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:52:08.814035: step 8330, loss = 0.94 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:52:09.704487: step 8340, loss = 0.98 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:52:10.582359: step 8350, loss = 0.82 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:52:11.463320: step 8360, loss = 0.82 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:52:12.354176: step 8370, loss = 0.80 (1436.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:52:13.254269: step 8380, loss = 0.89 (1422.1 examples/sec; 0.090 sec/batch)
2017-06-02 02:52:14.153180: step 8390, loss = 0.88 (1424.0 examples/sec; 0.090 sec/batch)
2017-06-02 02:52:15.131169: step 8400, loss = 0.87 (1308.8 examples/sec; 0.098 sec/batch)
2017-06-02 02:52:15.929360: step 8410, loss = 0.83 (1603.6 examples/sec; 0.080 sec/batch)
2017-06-02 02:52:16.803494: step 8420, loss = 0.78 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 02:52:17.692645: step 8430, loss = 0.89 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:52:18.556686: step 8440, loss = 1.01 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 02:52:19.445240: step 8450, loss = 0.99 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:52:20.334715: step 8460, loss = 1.00 (1439.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:52:21.234051: step 8470, loss = 0.78 (1423.3 examples/sec; 0.090 sec/batch)
2017-06-02 02:52:22.116813: step 8480, loss = 0.94 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:52:23.000144: step 8490, loss = 1.05 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:52:23.987043: step 8500, loss = 0.95 (1297.0 examples/sec; 0.099 sec/batch)
2017-06-02 02:52:24.771440: step 8510, loss = 0.90 (1631.8 examples/sec; 0.078 sec/batch)
2017-06-02 02:52:25.667990: step 8520, loss = 0.88 (1427.7 examples/sec; 0.090 sec/batch)
2017-06-02 02:52:26.563459: step 8530, loss = 0.93 (1429.4 examples/sec; 0.090 sec/batch)
2017-06-02 02:52:27.462111: step 8540, loss = 0.83 (1424.4 examples/sec; 0.090 sec/batch)
2017-06-02 02:52:28.355065: step 8550, loss = 0.83 (1433.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:52:29.234191: step 8560, loss = 0.88 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:52:30.114948: step 8570, loss = 1.15 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:52:31.006424: step 8580, loss = 0.87 (1435.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:52:31.893479: step 8590, loss = 0.88 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:52:32.870155: step 8600, loss = 0.90 (1310.6 examples/sec; 0.098 sec/batch)
2017-06-02 02:52:33.663003: step 8610, loss = 1.04 (1614.4 examples/sec; 0.079 sec/batch)
2017-06-02 02:52:34.571749: step 8620, loss = 0.89 (1408.5 examples/sec; 0.091 sec/batch)
2017-06-02 02:52:35.471166: step 8630, loss = 0.91 (1423.1 examples/sec; 0.090 sec/batch)
2017-06-02 02:52:36.365497: step 8640, loss = 0.97 (1431.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:52:37.262139: step 8650, loss = 0.78 (1427.6 examples/sec; 0.090 sec/batch)
2017-06-02 02:52:38.136550: step 8660, loss = 0.81 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 02:52:39.019062: step 8670, loss = 0.97 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:52:39.916709: step 8680, loss = 0.83 (1425.9 examples/sec; 0.090 sec/batch)
2017-06-02 02:52:40.794450: step 8690, loss = 0.80 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:52:41.784992: step 8700, loss = 0.92 (1292.2 examples/sec; 0.099 sec/batch)
2017-06-02 02:52:42.557991: step 8710, loss = 1.02 (1655.9 examples/sec; 0.077 sec/batch)
2017-06-02 02:52:43.443360: step 8720, loss = 0.93 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:52:44.350180: step 8730, loss = 0.75 (1411.5 examples/sec; 0.091 sec/batch)
2017-06-02 02:52:45.235030: step 8740, loss = 0.96 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:52:46.121387: step 8750, loss = 0.99 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:52:47.017079: step 8760, loss = 0.92 (1429.1 examples/sec; 0.090 sec/batch)
2017-06-02 02:52:47.898381: step 8770, loss = 1.05 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:52:48.790553: step 8780, loss = 1.03 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:52:49.678761: step 8790, loss = 1.07 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:52:50.660865: step 8800, loss = 0.91 (1303.3 examples/sec; 0.098 sec/batch)
2017-06-02 02:52:51.449846: step 8810, loss = 0.97 (1622.4 examples/sec; 0.079 sec/batch)
2017-06-02 02:52:52.337260: step 8820, loss = 0.84 (1442.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:52:53.220402: step 8830, loss = 0.88 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:52:54.108724: step 8840, loss = 0.79 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:52:54.989806: step 8850, loss = 0.87 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:52:55.888912: step 8860, loss = 0.99 (1423.6 examples/sec; 0.090 sec/batch)
2017-06-02 02:52:56.781836: step 8870, loss = 1.04 (1433.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:52:57.671409: step 8880, loss = 0.85 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:52:58.556843: step 8890, loss = 0.85 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:52:59.539581: step 8900, loss = 0.70 (1302.5 examples/sec; 0.098 sec/batch)
2017-06-02 02:53:00.329033: step 8910, loss = 0.90 (1621.4 examples/sec; 0.079 sec/batch)
2017-06-02 02:53:01.206492: step 8920, loss = 1.00 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:53:02.108538: step 8930, loss = 0.97 (1419.0 examples/sec; 0.090 sec/batch)
2017-06-02 02:53:02.997690: step 8940, loss = 0.90 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:53:03.884219: step 8950, loss = 0.85 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:53:04.769376: step 8960, loss = 0.87 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:53:05.666487: step 8970, loss = 1.04 (1426.8 examples/sec; 0.090 sec/batch)
2017-06-02 02:53:06.552391: step 8980, loss = 0.92 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:53:07.443176: step 8990, loss = 0.87 (1436.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:53:08.453865: step 9000, loss = 0.80 (1266.5 examples/sec; 0.101 sec/batch)
2017-06-02 02:53:09.215946: step 9010, loss = 1.00 (1679.6 examples/sec; 0.076 sec/batch)
2017-06-02 02:53:10.107615: step 9020, loss = 0.88 (1435.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:53:10.995681: step 9030, loss = 0.96 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 02:53:11.891365: step 9040, loss = 0.81 (1429.1 examples/sec; 0.090 sec/batch)
2017-06-02 02:53:12.800700: step 9050, loss = 0.87 (1407.6 examples/sec; 0.091 sec/batch)
2017-06-02 02:53:13.706701: step 9060, loss = 0.71 (1412.8 examples/sec; 0.091 sec/batch)
2017-06-02 02:53:14.590568: step 9070, loss = 0.96 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:53:15.496345: step 9080, loss = 1.01 (1413.2 examples/sec; 0.091 sec/batch)
2017-06-02 02:53:16.385680: step 9090, loss = 0.84 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 02:53:17.371448: step 9100, loss = 0.79 (1298.5 examples/sec; 0.099 sec/batch)
2017-06-02 02:53:18.162368: step 9110, loss = 1.02 (1618.3 examples/sec; 0.079 sec/batch)
2017-06-02 02:53:19.044922: step 9120, loss = 0.84 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:53:19.938643: step 9130, loss = 0.86 (1432.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:53:20.822974: step 9140, loss = 1.00 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:53:21.707741: step 9150, loss = 0.84 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:53:22.586050: step 9160, loss = 1.10 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:53:23.480995: step 9170, loss = 1.10 (1430.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:53:24.366977: step 9180, loss = 0.88 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:53:25.251728: step 9190, loss = 0.92 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:53:26.292646: step 9200, loss = 0.92 (1229.7 examples/sec; 0.104 sec/batch)
2017-06-02 02:53:27.050331: step 9210, loss = 0.93 (1689.4 examples/sec; 0.076 sec/batch)
2017-06-02 02:53:27.938230: step 9220, loss = 0.87 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:53:28.835011: step 9230, loss = 0.78 (1427.3 examples/sec; 0.090 sec/batch)
2017-06-02 02:53:29.734122: step 9240, loss = 0.93 (1423.6 examples/sec; 0.090 sec/batch)
2017-06-02 02:53:30.621030: step 9250, loss = 1.05 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:53:31.533140: step 9260, loss = 0.98 (1403.3 examples/sec; 0.091 sec/batch)
2017-06-02 02:53:32.423959: step 9270, loss = 0.90 (1436.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:53:33.302488: step 9280, loss = 0.87 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:53:34.205372: step 9290, loss = 0.81 (1417.7 examples/sec; 0.090 sec/batch)
2017-06-02 02:53:35.180981: step 9300, loss = 0.95 (1312.0 examples/sec; 0.098 sec/batch)
2017-06-02 02:53:35.976103: step 9310, loss = 1.04 (1609.8 examples/sec; 0.080 sec/batch)
2017-06-02 02:53:36.854725: step 9320, loss = 0.90 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:53:37.745151: step 9330, loss = 0.95 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:53:38.626431: step 9340, loss = 0.86 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:53:39.521890: step 9350, loss = 0.71 (1429.4 examples/sec; 0.090 sec/batch)
2017-06-02 02:53:40.412269: step 9360, loss = 1.04 (1437.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:53:41.291716: step 9370, loss = 0.89 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:53:42.171466: step 9380, loss = 0.91 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:53:43.057526: step 9390, loss = 0.78 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:53:44.036553: step 9400, loss = 0.71 (1307.4 examples/sec; 0.098 sec/batch)
2017-06-02 02:53:44.814521: step 9410, loss = 0.78 (1645.4 examples/sec; 0.078 sec/batch)
2017-06-02 02:53:45.696950: step 9420, loss = 0.85 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:53:46.574812: step 9430, loss = 1.02 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:53:47.466455: step 9440, loss = 1.05 (1435.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:53:48.353500: step 9450, loss = 0.92 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:53:49.253755: step 9460, loss = 0.76 (1421.8 examples/sec; 0.090 sec/batch)
2017-06-02 02:53:50.152011: step 9470, loss = 0.83 (1425.0 examples/sec; 0.090 sec/batch)
2017-06-02 02:53:51.030569: step 9480, loss = 0.97 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:53:51.917208: step 9490, loss = 0.80 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:53:52.915711: step 9500, loss = 1.07 (1281.9 examples/sec; 0.100 sec/batch)
2017-06-02 02:53:53.709238: step 9510, loss = 0.96 (1613.0 examples/sec; 0.079 sec/batch)
2017-06-02 02:53:54.596658: step 9520, loss = 1.09 (1442.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:53:55.496083: step 9530, loss = 0.97 (1423.1 examples/sec; 0.090 sec/batch)
2017-06-02 02:53:56.379461: step 9540, loss = 1.00 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:53:57.274237: step 9550, loss = 0.87 (1430.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:53:58.159179: step 9560, loss = 0.93 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:53:59.045719: step 9570, loss = 0.82 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:53:59.943098: step 9580, loss = 0.72 (1426.4 examples/sec; 0.090 sec/batch)
2017-06-02 02:54:00.840523: step 9590, loss = 0.80 (1426.3 examples/sec; 0.090 sec/batch)
2017-06-02 02:54:01.838128: step 9600, loss = 0.78 (1283.1 examples/sec; 0.100 sec/batch)
2017-06-02 02:54:02.624219: step 9610, loss = 0.86 (1628.3 examples/sec; 0.079 sec/batch)
2017-06-02 02:54:03.509183: step 9620, loss = 0.83 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:54:04.403093: step 9630, loss = 0.88 (1431.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:54:05.293026: step 9640, loss = 1.08 (1438.3 examples/sec; 0.089 sec/batch)
2017-06-02 02:54:06.200214: step 9650, loss = 0.85 (1410.9 examples/sec; 0.091 sec/batch)
2017-06-02 02:54:07.090827: step 9660, loss = 0.86 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:54:07.985950: step 9670, loss = 0.94 (1430.0 examples/sec; 0.090 sec/batch)
2017-06-02 02:54:08.869460: step 9680, loss = 0.77 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:54:09.758861: step 9690, loss = 0.91 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:54:10.743016: step 9700, loss = 0.90 (1300.6 examples/sec; 0.098 sec/batch)
2017-06-02 02:54:11.536274: step 9710, loss = 1.00 (1613.6 examples/sec; 0.079 sec/batch)
2017-06-02 02:54:12.420908: step 9720, loss = 1.10 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:54:13.294812: step 9730, loss = 0.96 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 02:54:14.190357: step 9740, loss = 0.83 (1429.3 examples/sec; 0.090 sec/batch)
2017-06-02 02:54:15.085051: step 9750, loss = 0.97 (1430.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:54:15.967663: step 9760, loss = 0.85 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:54:16.869783: step 9770, loss = 0.92 (1418.9 examples/sec; 0.090 sec/batch)
2017-06-02 02:54:17.748782: step 9780, loss = 0.94 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:54:18.632972: step 9790, loss = 0.84 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:54:19.622952: step 9800, loss = 0.79 (1292.9 examples/sec; 0.099 sec/batch)
2017-06-02 02:54:20.415662: step 9810, loss = 0.82 (1614.7 examples/sec; 0.079 sec/batch)
2017-06-02 02:54:21.302321: step 9820, loss = 0.84 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:54:22.188154: step 9830, loss = 0.99 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:54:23.084466: step 9840, loss = 0.90 (1428.1 examples/sec; 0.090 sec/batch)
2017-06-02 02:54:23.962319: step 9850, loss = 0.89 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:54:24.846206: step 9860, loss = 0.79 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:54:25.722316: step 9870, loss = 0.85 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:54:26.606112: step 9880, loss = 0.87 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:54:27.498310: step 9890, loss = 0.80 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:54:28.484822: step 9900, loss = 0.84 (1297.5 examples/sec; 0.099 sec/batch)
2017-06-02 02:54:29.270435: step 9910, loss = 0.78 (1629.3 examples/sec; 0.079 sec/batch)
2017-06-02 02:54:30.152951: step 9920, loss = 0.94 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:54:31.038890: step 9930, loss = 1.01 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:54:31.933151: step 9940, loss = 0.89 (1431.3 examples/sec; 0.089 sec/batch)
2017-06-02 02:54:32.820970: step 9950, loss = 0.78 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:54:33.712984: step 9960, loss = 0.91 (1434.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:54:34.593491: step 9970, loss = 1.05 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:54:35.479439: step 9980, loss = 1.07 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:54:36.354276: step 9990, loss = 1.06 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 02:54:37.340721: step 10000, loss = 0.90 (1297.6 examples/sec; 0.099 sec/batch)
2017-06-02 02:54:38.123504: step 10010, loss = 1.06 (1635.2 examples/sec; 0.078 sec/batch)
2017-06-02 02:54:39.006477: step 10020, loss = 0.92 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:54:39.891002: step 10030, loss = 1.12 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:54:40.787592: step 10040, loss = 0.81 (1427.6 examples/sec; 0.090 sec/batch)
2017-06-02 02:54:41.671692: step 10050, loss = 0.96 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:54:42.554272: step 10060, loss = 1.11 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:54:43.440974: step 10070, loss = 0.79 (1443.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:54:44.329091: step 10080, loss = 0.92 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 02:54:45.210664: step 10090, loss = 0.78 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:54:46.194520: step 10100, loss = 0.86 (1301.0 examples/sec; 0.098 sec/batch)
2017-06-02 02:54:46.971505: step 10110, loss = 0.86 (1647.4 examples/sec; 0.078 sec/batch)
2017-06-02 02:54:47.857223: step 10120, loss = 0.95 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:54:48.745231: step 10130, loss = 0.87 (1441.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:54:49.623658: step 10140, loss = 1.16 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:54:50.493625: step 10150, loss = 0.80 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 02:54:51.374072: step 10160, loss = 0.89 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:54:52.259679: step 10170, loss = 0.96 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 02:54:53.155479: step 10180, loss = 0.83 (1428.9 examples/sec; 0.090 sec/batch)
2017-06-02 02:54:54.034045: step 10190, loss = 1.07 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:54:55.018774: step 10200, loss = 0.95 (1299.8 examples/sec; 0.098 sec/batch)
2017-06-02 02:54:55.805977: step 10210, loss = 0.96 (1626.0 examples/sec; 0.079 sec/batch)
2017-06-02 02:54:56.686977: step 10220, loss = 1.00 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:54:57.571531: step 10230, loss = 1.06 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:54:58.451965: step 10240, loss = 0.97 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:54:59.344584: step 10250, loss = 0.88 (1434.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:55:00.229261: step 10260, loss = 0.82 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:55:01.110918: step 10270, loss = 0.70 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:55:01.998715: step 10280, loss = 0.85 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:55:02.880245: step 10290, loss = 1.01 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:55:03.880352: step 10300, loss = 0.77 (1279.9 examples/sec; 0.100 sec/batch)
2017-06-02 02:55:04.652066: step 10310, loss = 0.88 (1658.7 examples/sec; 0.077 sec/batch)
2017-06-02 02:55:05.544064: step 10320, loss = 1.07 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:55:06.436164: step 10330, loss = 0.87 (1434.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:55:07.318430: step 10340, loss = 0.85 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:55:08.213703: step 10350, loss = 0.82 (1429.8 examples/sec; 0.090 sec/batch)
2017-06-02 02:55:09.135713: step 10360, loss = 0.85 (1388.2 examples/sec; 0.092 sec/batch)
2017-06-02 02:55:10.027538: step 10370, loss = 0.82 (1435.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:55:10.920382: step 10380, loss = 0.87 (1433.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:55:11.806061: step 10390, loss = 1.10 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:55:12.792092: step 10400, loss = 0.92 (1298.1 examples/sec; 0.099 sec/batch)
2017-06-02 02:55:13.591275: step 10410, loss = 0.85 (1601.6 examples/sec; 0.080 sec/batch)
2017-06-02 02:55:14.466693: step 10420, loss = 0.86 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:55:15.350494: step 10430, loss = 0.94 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:55:16.259835: step 10440, loss = 1.01 (1407.6 examples/sec; 0.091 sec/batch)
2017-06-02 02:55:17.145706: step 10450, loss = 0.85 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:55:18.021775: step 10460, loss = 0.85 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:55:18.890515: step 10470, loss = 0.92 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 02:55:19.773481: step 10480, loss = 0.91 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:55:20.669943: step 10490, loss = 0.98 (1427.8 examples/sec; 0.090 sec/batch)
2017-06-02 02:55:21.658822: step 10500, loss = 0.85 (1294.4 examples/sec; 0.099 sec/batch)
2017-06-02 02:55:22.440168: step 10510, loss = 0.97 (1638.2 examples/sec; 0.078 sec/batch)
2017-06-02 02:55:23.324268: step 10520, loss = 0.79 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:55:24.213914: step 10530, loss = 0.79 (1438.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:55:25.101431: step 10540, loss = 0.82 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:55:26.001508: step 10550, loss = 0.85 (1422.1 examples/sec; 0.090 sec/batch)
2017-06-02 02:55:26.900397: step 10560, loss = 0.82 (1424.0 examples/sec; 0.090 sec/batch)
2017-06-02 02:55:27.788974: step 10570, loss = 0.89 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:55:28.679185: step 10580, loss = 0.91 (1437.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:55:29.575031: step 10590, loss = 0.87 (1428.8 examples/sec; 0.090 sec/batch)
2017-06-02 02:55:30.553909: step 10600, loss = 0.87 (1307.6 examples/sec; 0.098 sec/batch)
2017-06-02 02:55:31.335739: step 10610, loss = 0.77 (1637.2 examples/sec; 0.078 sec/batch)
2017-06-02 02:55:32.208452: step 10620, loss = 0.77 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 02:55:33.102904: step 10630, loss = 0.78 (1430.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:55:34.005417: step 10640, loss = 0.93 (1418.3 examples/sec; 0.090 sec/batch)
2017-06-02 02:55:34.890633: step 10650, loss = 0.98 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:55:35.784189: step 10660, loss = 0.89 (1432.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:55:36.671702: step 10670, loss = 0.81 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:55:37.557806: step 10680, loss = 0.78 (1444.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:55:38.437617: step 10690, loss = 0.94 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:55:39.419130: step 10700, loss = 0.94 (1304.1 examples/sec; 0.098 sec/batch)
2017-06-02 02:55:40.213727: step 10710, loss = 1.00 (1610.9 examples/sec; 0.079 sec/batch)
2017-06-02 02:55:41.096412: step 10720, loss = 0.89 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:55:41.985043: step 10730, loss = 0.87 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:55:42.868177: step 10740, loss = 0.90 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:55:43.748674: step 10750, loss = 0.79 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:55:44.642509: step 10760, loss = 0.92 (1432.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:55:45.524828: step 10770, loss = 0.97 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:55:46.403739: step 10780, loss = 0.96 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:55:47.301296: step 10790, loss = 0.83 (1426.1 examples/sec; 0.090 sec/batch)
2017-06-02 02:55:48.306290: step 10800, loss = 0.76 (1273.6 examples/sec; 0.100 sec/batch)
2017-06-02 02:55:49.082419: step 10810, loss = 0.90 (1649.2 examples/sec; 0.078 sec/batch)
2017-06-02 02:55:49.964560: step 10820, loss = 0.92 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:55:50.843063: step 10830, loss = 0.76 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:55:51.727927: step 10840, loss = 0.95 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:55:52.632396: step 10850, loss = 1.03 (1415.2 examples/sec; 0.090 sec/batch)
2017-06-02 02:55:53.513840: step 10860, loss = 0.75 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:55:54.387455: step 10870, loss = 0.79 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 02:55:55.272676: step 10880, loss = 0.91 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:55:56.141146: step 10890, loss = 0.99 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 02:55:57.139857: step 10900, loss = 1.12 (1281.7 examples/sec; 0.100 sec/batch)
2017-06-02 02:55:57.914147: step 10910, loss = 0.79 (1653.1 examples/sec; 0.077 sec/batch)
2017-06-02 02:55:58.787661: step 10920, loss = 0.78 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 02:55:59.674460: step 10930, loss = 0.75 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:00.565284: step 10940, loss = 0.85 (1436.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:01.468263: step 10950, loss = 0.97 (1417.5 examples/sec; 0.090 sec/batch)
2017-06-02 02:56:02.358527: step 10960, loss = 0.76 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:03.230219: step 10970, loss = 0.96 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 02:56:04.124837: step 10980, loss = 1.14 (1430.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:05.011644: step 10990, loss = 0.86 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:05.994307: step 11000, loss = 0.87 (1302.6 examples/sec; 0.098 sec/batch)
2017-06-02 02:56:06.777505: step 11010, loss = 0.87 (1634.3 examples/sec; 0.078 sec/batch)
2017-06-02 02:56:07.669110: step 11020, loss = 0.77 (1435.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:08.559216: step 11030, loss = 1.05 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:09.448472: step 11040, loss = 0.74 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:10.337530: step 11050, loss = 1.09 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:11.228163: step 11060, loss = 0.89 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:12.118030: step 11070, loss = 0.92 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:12.985664: step 11080, loss = 0.81 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 02:56:13.885897: step 11090, loss = 1.05 (1421.8 examples/sec; 0.090 sec/batch)
2017-06-02 02:56:14.866370: step 11100, loss = 0.99 (1305.5 examples/sec; 0.098 sec/batch)
2017-06-02 02:56:15.652810: step 11110, loss = 0.95 (1627.6 examples/sec; 0.079 sec/batch)
2017-06-02 02:56:16.539121: step 11120, loss = 0.76 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:17.427409: step 11130, loss = 0.90 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:18.308713: step 11140, loss = 0.99 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:56:19.203877: step 11150, loss = 0.91 (1429.9 examples/sec; 0.090 sec/batch)
2017-06-02 02:56:20.093387: step 11160, loss = 0.89 (1439.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:20.982884: step 11170, loss = 1.01 (1439.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:21.864639: step 11180, loss = 0.86 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:56:22.740283: step 11190, loss = 0.84 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:56:23.731103: step 11200, loss = 0.97 (1291.9 examples/sec; 0.099 sec/batch)
2017-06-02 02:56:24.525249: step 11210, loss = 0.86 (1611.8 examples/sec; 0.079 sec/batch)
2017-06-02 02:56:25.411190: step 11220, loss = 0.72 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:26.301141: step 11230, loss = 0.75 (1438.3 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:27.193690: step 11240, loss = 0.85 (1434.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:28.087993: step 11250, loss = 0.84 (1431.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:28.987134: step 11260, loss = 0.86 (1423.6 examples/sec; 0.090 sec/batch)
2017-06-02 02:56:29.871974: step 11270, loss = 0.89 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:56:30.753871: step 11280, loss = 0.73 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:56:31.652876: step 11290, loss = 0.89 (1423.8 examples/sec; 0.090 sec/batch)
2017-06-02 02:56:32.634984: step 11300, loss = 0.85 (1303.3 examples/sec; 0.098 sec/batch)
2017-06-02 02:56:33.432036: step 11310, loss = 0.85 (1605.9 examples/sec; 0.080 sec/batch)
2017-06-02 02:56:34.309588: step 11320, loss = 0.83 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:56:35.200153: step 11330, loss = 0.79 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:36.092940: step 11340, loss = 0.68 (1433.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:36.981426: step 11350, loss = 0.83 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:37.859087: step 11360, loss = 0.83 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:56:38.743681: step 11370, loss = 0.84 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:56:39.648700: step 11380, loss = 0.88 (1414.3 examples/sec; 0.091 sec/batch)
2017-06-02 02:56:40.543143: step 11390, loss = 0.83 (1431.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:41.551280: step 11400, loss = 0.92 (1269.7 examples/sec; 0.101 sec/batch)
2017-06-02 02:56:42.343232: step 11410, loss = 0.96 (1616.3 examples/sec; 0.079 sec/batch)
2017-06-02 02:56:43.230315: step 11420, loss = 0.76 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:44.119098: step 11430, loss = 0.88 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:44.999684: step 11440, loss = 0.93 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:56:45.905181: step 11450, loss = 0.77 (1413.6 examples/sec; 0.091 sec/batch)
2017-06-02 02:56:46.783222: step 11460, loss = 0.90 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:56:47.677765: step 11470, loss = 0.89 (1430.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:48.574622: step 11480, loss = 0.92 (1427.2 examples/sec; 0.090 sec/batch)
2017-06-02 02:56:49.469324: step 11490, loss = 1.07 (1430.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:50.453794: step 11500, loss = 0.80 (1300.2 examples/sec; 0.098 sec/batch)
2017-06-02 02:56:51.237943: step 11510, loss = 0.89 (1632.4 examples/sec; 0.078 sec/batch)
2017-06-02 02:56:52.131468: step 11520, loss = 0.87 (1432.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:53.029270: step 11530, loss = 0.85 (1425.7 examples/sec; 0.090 sec/batch)
2017-06-02 02:56:53.906617: step 11540, loss = 0.86 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:56:54.796682: step 11550, loss = 0.82 (1438.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:55.687135: step 11560, loss = 0.97 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:56:56.570311: step 11570, loss = 1.04 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:56:57.472154: step 11580, loss = 0.85 (1419.3 examples/sec; 0.090 sec/batch)
2017-06-02 02:56:58.348462: step 11590, loss = 1.06 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:56:59.326697: step 11600, loss = 0.94 (1308.5 examples/sec; 0.098 sec/batch)
2017-06-02 02:57:00.121526: step 11610, loss = 0.87 (1610.4 examples/sec; 0.079 sec/batch)
2017-06-02 02:57:01.010141: step 11620, loss = 0.90 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:57:01.886603: step 11630, loss = 0.99 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:57:02.777245: step 11640, loss = 0.83 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:57:03.676119: step 11650, loss = 0.80 (1424.0 examples/sec; 0.090 sec/batch)
2017-06-02 02:57:04.563179: step 11660, loss = 0.87 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:57:05.463237: step 11670, loss = 0.88 (1422.1 examples/sec; 0.090 sec/batch)
2017-06-02 02:57:06.344530: step 11680, loss = 0.76 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:57:07.243033: step 11690, loss = 0.87 (1424.6 examples/sec; 0.090 sec/batch)
2017-06-02 02:57:08.230048: step 11700, loss = 0.95 (1296.8 examples/sec; 0.099 sec/batch)
2017-06-02 02:57:09.030381: step 11710, loss = 1.06 (1599.4 examples/sec; 0.080 sec/batch)
2017-06-02 02:57:09.905243: step 11720, loss = 0.85 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 02:57:10.793576: step 11730, loss = 0.77 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:57:11.680239: step 11740, loss = 0.98 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:57:12.570083: step 11750, loss = 0.76 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:57:13.466704: step 11760, loss = 0.96 (1427.6 examples/sec; 0.090 sec/batch)
2017-06-02 02:57:14.344972: step 11770, loss = 0.90 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:57:15.235378: step 11780, loss = 0.91 (1437.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:57:16.113355: step 11790, loss = 0.76 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:57:17.101977: step 11800, loss = 0.88 (1294.7 examples/sec; 0.099 sec/batch)
2017-06-02 02:57:17.889613: step 11810, loss = 0.73 (1625.1 examples/sec; 0.079 sec/batch)
2017-06-02 02:57:18.773591: step 11820, loss = 1.02 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:57:19.663646: step 11830, loss = 0.85 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:57:20.562739: step 11840, loss = 0.80 (1423.6 examples/sec; 0.090 sec/batch)
2017-06-02 02:57:21.454061: step 11850, loss = 1.00 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:57:22.351972: step 11860, loss = 0.79 (1425.5 examples/sec; 0.090 sec/batch)
2017-06-02 02:57:23.242268: step 11870, loss = 0.86 (1437.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:57:24.127493: step 11880, loss = 0.73 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:57:25.023617: step 11890, loss = 0.75 (1428.3 examples/sec; 0.090 sec/batch)
2017-06-02 02:57:26.025834: step 11900, loss = 1.09 (1277.2 examples/sec; 0.100 sec/batch)
2017-06-02 02:57:26.795400: step 11910, loss = 0.89 (1663.3 examples/sec; 0.077 sec/batch)
2017-06-02 02:57:27.676128: step 11920, loss = 0.95 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:57:28.563589: step 11930, loss = 0.73 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 02:57:29.449144: step 11940, loss = 0.83 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:57:30.335970: step 11950, loss = 0.87 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:57:31.233340: step 11960, loss = 0.92 (1426.4 examples/sec; 0.090 sec/batch)
2017-06-02 02:57:32.124882: step 11970, loss = 0.93 (1435.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:57:33.032035: step 11980, loss = 1.01 (1411.0 examples/sec; 0.091 sec/batch)
2017-06-02 02:57:33.937774: step 11990, loss = 0.98 (1413.2 examples/sec; 0.091 sec/batch)
2017-06-02 02:57:34.941982: step 12000, loss = 0.83 (1274.6 examples/sec; 0.100 sec/batch)
2017-06-02 02:57:35.731970: step 12010, loss = 0.83 (1620.3 examples/sec; 0.079 sec/batch)
2017-06-02 02:57:36.621423: step 12020, loss = 0.93 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:57:37.524763: step 12030, loss = 1.01 (1417.0 examples/sec; 0.090 sec/batch)
2017-06-02 02:57:38.415706: step 12040, loss = 0.74 (1436.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:57:39.302288: step 12050, loss = 0.89 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:57:40.194962: step 12060, loss = 0.85 (1433.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:57:41.079400: step 12070, loss = 0.85 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 02:57:41.967183: step 12080, loss = 1.01 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:57:42.862910: step 12090, loss = 0.83 (1429.0 examples/sec; 0.090 sec/batch)
2017-06-02 02:57:43.911989: step 12100, loss = 0.71 (1220.1 examples/sec; 0.105 sec/batch)
2017-06-02 02:57:44.639640: step 12110, loss = 0.74 (1759.1 examples/sec; 0.073 sec/batch)
2017-06-02 02:57:45.531610: step 12120, loss = 1.01 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:57:46.422632: step 12130, loss = 0.87 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:57:47.313604: step 12140, loss = 0.68 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:57:48.198126: step 12150, loss = 0.99 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:57:49.104340: step 12160, loss = 0.85 (1412.5 examples/sec; 0.091 sec/batch)
2017-06-02 02:57:49.986057: step 12170, loss = 0.83 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:57:50.869975: step 12180, loss = 0.94 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:57:51.763421: step 12190, loss = 0.96 (1432.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:57:52.738388: step 12200, loss = 0.88 (1312.9 examples/sec; 0.097 sec/batch)
2017-06-02 02:57:53.522886: step 12210, loss = 0.72 (1631.6 examples/sec; 0.078 sec/batch)
2017-06-02 02:57:54.410737: step 12220, loss = 0.83 (1441.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:57:55.297863: step 12230, loss = 0.82 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:57:56.181376: step 12240, loss = 1.07 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:57:57.068428: step 12250, loss = 0.82 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:57:57.944756: step 12260, loss = 1.13 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:57:58.826764: step 12270, loss = 0.77 (1451.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:57:59.726415: step 12280, loss = 1.08 (1422.8 examples/sec; 0.090 sec/batch)
2017-06-02 02:58:00.604587: step 12290, loss = 0.79 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:58:01.595007: step 12300, loss = 0.82 (1292.4 examples/sec; 0.099 sec/batch)
2017-06-02 02:58:02.385253: step 12310, loss = 1.02 (1619.7 examples/sec; 0.079 sec/batch)
2017-06-02 02:58:03.264434: step 12320, loss = 0.78 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:58:04.146208: step 12330, loss = 0.88 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:58:05.026672: step 12340, loss = 0.73 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:58:05.912924: step 12350, loss = 0.93 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 02:58:06.792533: step 12360, loss = 0.93 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:58:07.680608: step 12370, loss = 0.82 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 02:58:08.564963: step 12380, loss = 0.81 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:58:09.458022: step 12390, loss = 0.81 (1433.3 examples/sec; 0.089 sec/batch)
2017-06-02 02:58:10.440367: step 12400, loss = 1.02 (1303.0 examples/sec; 0.098 sec/batch)
2017-06-02 02:58:11.221032: step 12410, loss = 1.09 (1639.6 examples/sec; 0.078 sec/batch)
2017-06-02 02:58:12.103720: step 12420, loss = 0.99 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:58:13.004236: step 12430, loss = 1.09 (1421.4 examples/sec; 0.090 sec/batch)
2017-06-02 02:58:13.892140: step 12440, loss = 1.05 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:58:14.778880: step 12450, loss = 0.90 (1443.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:58:15.675195: step 12460, loss = 1.02 (1428.1 examples/sec; 0.090 sec/batch)
2017-06-02 02:58:16.564602: step 12470, loss = 0.86 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:58:17.442220: step 12480, loss = 0.80 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:58:18.323779: step 12490, loss = 1.00 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 02:58:19.322405: step 12500, loss = 0.69 (1281.8 examples/sec; 0.100 sec/batch)
2017-06-02 02:58:20.110688: step 12510, loss = 0.74 (1623.8 examples/sec; 0.079 sec/batch)
2017-06-02 02:58:20.993671: step 12520, loss = 0.94 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:58:21.874964: step 12530, loss = 0.78 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 02:58:22.752949: step 12540, loss = 0.77 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:58:23.648218: step 12550, loss = 0.93 (1429.7 examples/sec; 0.090 sec/batch)
2017-06-02 02:58:24.539698: step 12560, loss = 0.87 (1435.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:58:25.427399: step 12570, loss = 0.72 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:58:26.319277: step 12580, loss = 0.89 (1435.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:58:27.202163: step 12590, loss = 0.91 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:58:28.191091: step 12600, loss = 0.78 (1294.3 examples/sec; 0.099 sec/batch)
2017-06-02 02:58:28.984400: step 12610, loss = 0.79 (1613.5 examples/sec; 0.079 sec/batch)
2017-06-02 02:58:29.872117: step 12620, loss = 0.98 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:58:30.760780: step 12630, loss = 0.91 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:58:31.661183: step 12640, loss = 0.72 (1421.6 examples/sec; 0.090 sec/batch)
2017-06-02 02:58:32.545884: step 12650, loss = 0.83 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:58:33.435945: step 12660, loss = 0.63 (1438.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:58:34.321147: step 12670, loss = 0.94 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:58:35.213726: step 12680, loss = 0.86 (1434.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:58:36.101535: step 12690, loss = 0.79 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:58:37.087829: step 12700, loss = 0.87 (1297.8 examples/sec; 0.099 sec/batch)
2017-06-02 02:58:37.888689: step 12710, loss = 0.89 (1598.3 examples/sec; 0.080 sec/batch)
2017-06-02 02:58:38.765860: step 12720, loss = 0.89 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:58:39.658195: step 12730, loss = 0.87 (1434.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:58:40.548200: step 12740, loss = 0.71 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:58:41.433721: step 12750, loss = 0.91 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:58:42.307483: step 12760, loss = 0.92 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 02:58:43.197861: step 12770, loss = 0.89 (1437.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:58:44.086821: step 12780, loss = 0.84 (1439.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:58:44.976018: step 12790, loss = 0.87 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:58:45.952510: step 12800, loss = 0.86 (1310.8 examples/sec; 0.098 sec/batch)
2017-06-02 02:58:46.742200: step 12810, loss = 0.87 (1620.9 examples/sec; 0.079 sec/batch)
2017-06-02 02:58:47.641464: step 12820, loss = 1.24 (1423.4 examples/sec; 0.090 sec/batch)
2017-06-02 02:58:48.531287: step 12830, loss = 0.95 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:58:49.411769: step 12840, loss = 0.91 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:58:50.299180: step 12850, loss = 0.90 (1442.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:58:51.185879: step 12860, loss = 0.86 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:58:52.092214: step 12870, loss = 0.94 (1412.3 examples/sec; 0.091 sec/batch)
2017-06-02 02:58:52.981611: step 12880, loss = 0.91 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:58:53.863358: step 12890, loss = 0.81 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:58:54.853201: step 12900, loss = 0.93 (1293.1 examples/sec; 0.099 sec/batch)
2017-06-02 02:58:55.670761: step 12910, loss = 0.99 (1565.6 examples/sec; 0.082 sec/batch)
2017-06-02 02:58:56.554644: step 12920, loss = 0.94 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 02:58:57.433086: step 12930, loss = 1.11 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:58:58.306724: step 12940, loss = 0.99 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 02:58:59.191625: step 12950, loss = 0.80 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:59:00.096736: step 12960, loss = 0.74 (1414.2 examples/sec; 0.091 sec/batch)
2017-06-02 02:59:00.973538: step 12970, loss = 0.91 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:59:01.864697: step 12980, loss = 0.75 (1436.3 examples/sec; 0.089 sec/batch)
2017-06-02 02:59:02.752495: step 12990, loss = 0.76 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:59:03.751852: step 13000, loss = 0.86 (1280.8 examples/sec; 0.100 sec/batch)
2017-06-02 02:59:04.541703: step 13010, loss = 0.89 (1620.6 examples/sec; 0.079 sec/batch)
2017-06-02 02:59:05.424660: step 13020, loss = 0.91 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:59:06.319049: step 13030, loss = 0.83 (1431.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:59:07.208329: step 13040, loss = 0.84 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 02:59:08.098198: step 13050, loss = 0.89 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:59:08.993163: step 13060, loss = 0.75 (1430.2 examples/sec; 0.089 sec/batch)
2017-06-02 02:59:09.870800: step 13070, loss = 0.81 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:59:10.757202: step 13080, loss = 0.99 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:59:11.657410: step 13090, loss = 0.86 (1421.9 examples/sec; 0.090 sec/batch)
2017-06-02 02:59:12.639289: step 13100, loss = 0.95 (1303.6 examples/sec; 0.098 sec/batch)
2017-06-02 02:59:13.428150: step 13110, loss = 0.94 (1622.6 examples/sec; 0.079 sec/batch)
2017-06-02 02:59:14.313458: step 13120, loss = 0.85 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:59:15.205358: step 13130, loss = 0.83 (1435.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:59:16.087229: step 13140, loss = 0.91 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 02:59:16.969577: step 13150, loss = 0.75 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 02:59:17.864307: step 13160, loss = 0.80 (1430.6 examples/sec; 0.089 sec/batch)
2017-06-02 02:59:18.754420: step 13170, loss = 0.72 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:59:19.650132: step 13180, loss = 0.77 (1429.1 examples/sec; 0.090 sec/batch)
2017-06-02 02:59:20.554245: step 13190, loss = 0.94 (1415.7 examples/sec; 0.090 sec/batch)
2017-06-02 02:59:21.537186: step 13200, loss = 0.91 (1302.2 examples/sec; 0.098 sec/batch)
2017-06-02 02:59:22.328275: step 13210, loss = 0.93 (1618.0 examples/sec; 0.079 sec/batch)
2017-06-02 02:59:23.225790: step 13220, loss = 0.85 (1426.2 examples/sec; 0.090 sec/batch)
2017-06-02 02:59:24.124591: step 13230, loss = 0.81 (1424.1 examples/sec; 0.090 sec/batch)
2017-06-02 02:59:25.023096: step 13240, loss = 0.92 (1424.6 examples/sec; 0.090 sec/batch)
2017-06-02 02:59:25.927251: step 13250, loss = 0.96 (1415.7 examples/sec; 0.090 sec/batch)
2017-06-02 02:59:26.819857: step 13260, loss = 0.85 (1434.0 examples/sec; 0.089 sec/batch)
2017-06-02 02:59:27.720399: step 13270, loss = 0.94 (1421.4 examples/sec; 0.090 sec/batch)
2017-06-02 02:59:28.596573: step 13280, loss = 0.85 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:59:29.507059: step 13290, loss = 0.73 (1405.8 examples/sec; 0.091 sec/batch)
2017-06-02 02:59:30.486930: step 13300, loss = 0.70 (1306.3 examples/sec; 0.098 sec/batch)
2017-06-02 02:59:31.263556: step 13310, loss = 0.95 (1648.2 examples/sec; 0.078 sec/batch)
2017-06-02 02:59:32.158531: step 13320, loss = 0.78 (1430.1 examples/sec; 0.090 sec/batch)
2017-06-02 02:59:33.054197: step 13330, loss = 0.73 (1429.1 examples/sec; 0.090 sec/batch)
2017-06-02 02:59:33.942749: step 13340, loss = 0.74 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 02:59:34.831166: step 13350, loss = 0.87 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:59:35.715835: step 13360, loss = 0.99 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 02:59:36.593307: step 13370, loss = 0.74 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 02:59:37.500256: step 13380, loss = 0.79 (1411.3 examples/sec; 0.091 sec/batch)
2017-06-02 02:59:38.389094: step 13390, loss = 0.85 (1440.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:59:39.384635: step 13400, loss = 0.72 (1285.7 examples/sec; 0.100 sec/batch)
2017-06-02 02:59:40.163710: step 13410, loss = 0.85 (1643.0 examples/sec; 0.078 sec/batch)
2017-06-02 02:59:41.058708: step 13420, loss = 0.83 (1430.1 examples/sec; 0.090 sec/batch)
2017-06-02 02:59:41.948387: step 13430, loss = 0.74 (1438.7 examples/sec; 0.089 sec/batch)
2017-06-02 02:59:42.833207: step 13440, loss = 0.87 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 02:59:43.720305: step 13450, loss = 0.73 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:59:44.616865: step 13460, loss = 0.82 (1427.7 examples/sec; 0.090 sec/batch)
2017-06-02 02:59:45.516387: step 13470, loss = 0.93 (1423.0 examples/sec; 0.090 sec/batch)
2017-06-02 02:59:46.409694: step 13480, loss = 0.91 (1432.9 examples/sec; 0.089 sec/batch)
2017-06-02 02:59:47.300830: step 13490, loss = 0.86 (1436.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:59:48.281610: step 13500, loss = 0.81 (1305.1 examples/sec; 0.098 sec/batch)
2017-06-02 02:59:49.064742: step 13510, loss = 0.92 (1634.5 examples/sec; 0.078 sec/batch)
2017-06-02 02:59:49.949227: step 13520, loss = 0.89 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 02:59:50.899027: step 13530, loss = 0.70 (1347.7 examples/sec; 0.095 sec/batch)
2017-06-02 02:59:51.784177: step 13540, loss = 0.82 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:59:52.689090: step 13550, loss = 0.91 (1414.5 examples/sec; 0.090 sec/batch)
2017-06-02 02:59:53.591765: step 13560, loss = 0.92 (1418.0 examples/sec; 0.090 sec/batch)
2017-06-02 02:59:54.479344: step 13570, loss = 0.72 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 02:59:55.374388: step 13580, loss = 1.10 (1430.1 examples/sec; 0.090 sec/batch)
2017-06-02 02:59:56.265283: step 13590, loss = 0.88 (1436.8 examples/sec; 0.089 sec/batch)
2017-06-02 02:59:57.254264: step 13600, loss = 0.88 (1294.2 examples/sec; 0.099 sec/batch)
2017-06-02 02:59:58.046345: step 13610, loss = 0.85 (1616.0 examples/sec; 0.079 sec/batch)
2017-06-02 02:59:58.938725: step 13620, loss = 0.94 (1434.4 examples/sec; 0.089 sec/batch)
2017-06-02 02:59:59.814828: step 13630, loss = 0.91 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:00:00.707139: step 13640, loss = 0.93 (1434.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:00:01.584535: step 13650, loss = 0.81 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:00:02.471792: step 13660, loss = 1.04 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:00:03.368597: step 13670, loss = 0.75 (1427.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:00:04.267612: step 13680, loss = 0.86 (1423.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:00:05.152080: step 13690, loss = 0.78 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:00:06.134987: step 13700, loss = 0.74 (1302.3 examples/sec; 0.098 sec/batch)
2017-06-02 03:00:06.926370: step 13710, loss = 0.80 (1617.4 examples/sec; 0.079 sec/batch)
2017-06-02 03:00:07.811061: step 13720, loss = 0.94 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:00:08.692258: step 13730, loss = 0.96 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:00:09.580252: step 13740, loss = 0.79 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:00:10.466021: step 13750, loss = 0.85 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:00:11.344122: step 13760, loss = 0.89 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:00:12.227204: step 13770, loss = 0.90 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:00:13.127996: step 13780, loss = 0.80 (1421.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:00:14.018550: step 13790, loss = 0.96 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:00:15.004598: step 13800, loss = 0.97 (1298.1 examples/sec; 0.099 sec/batch)
2017-06-02 03:00:15.799827: step 13810, loss = 0.98 (1609.6 examples/sec; 0.080 sec/batch)
2017-06-02 03:00:16.716856: step 13820, loss = 1.03 (1395.8 examples/sec; 0.092 sec/batch)
2017-06-02 03:00:17.616399: step 13830, loss = 0.92 (1422.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:00:18.506408: step 13840, loss = 0.82 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:00:19.396091: step 13850, loss = 0.98 (1438.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:00:20.296193: step 13860, loss = 0.76 (1422.1 examples/sec; 0.090 sec/batch)
2017-06-02 03:00:21.200637: step 13870, loss = 0.83 (1415.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:00:22.070965: step 13880, loss = 0.84 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:00:22.969088: step 13890, loss = 0.88 (1425.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:00:23.976758: step 13900, loss = 0.95 (1270.3 examples/sec; 0.101 sec/batch)
2017-06-02 03:00:24.769824: step 13910, loss = 0.76 (1614.0 examples/sec; 0.079 sec/batch)
2017-06-02 03:00:25.669627: step 13920, loss = 0.65 (1422.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:00:26.553825: step 13930, loss = 1.01 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:00:27.448078: step 13940, loss = 0.83 (1431.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:00:28.330411: step 13950, loss = 0.93 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:00:29.210160: step 13960, loss = 0.77 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:00:30.084610: step 13970, loss = 0.92 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:00:30.964062: step 13980, loss = 0.72 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:00:31.846613: step 13990, loss = 0.93 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:00:32.828820: step 14000, loss = 0.85 (1303.2 examples/sec; 0.098 sec/batch)
2017-06-02 03:00:33.619128: step 14010, loss = 0.95 (1619.6 examples/sec; 0.079 sec/batch)
2017-06-02 03:00:34.502853: step 14020, loss = 0.79 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:00:35.396133: step 14030, loss = 0.94 (1432.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:00:36.285315: step 14040, loss = 0.79 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:00:37.169085: step 14050, loss = 0.76 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:00:38.051025: step 14060, loss = 0.85 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:00:38.931652: step 14070, loss = 0.80 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:00:39.823502: step 14080, loss = 0.66 (1435.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:00:40.713710: step 14090, loss = 0.61 (1437.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:00:41.685597: step 14100, loss = 0.65 (1317.0 examples/sec; 0.097 sec/batch)
2017-06-02 03:00:42.478506: step 14110, loss = 0.70 (1614.3 examples/sec; 0.079 sec/batch)
2017-06-02 03:00:43.369136: step 14120, loss = 0.87 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:00:44.269399: step 14130, loss = 0.96 (1421.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:00:45.144857: step 14140, loss = 0.88 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:00:46.031017: step 14150, loss = 0.86 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:00:46.921860: step 14160, loss = 0.76 (1436.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:00:47.808869: step 14170, loss = 0.77 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:00:48.704103: step 14180, loss = 0.86 (1429.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:00:49.570821: step 14190, loss = 0.83 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:00:50.560041: step 14200, loss = 0.99 (1294.0 examples/sec; 0.099 sec/batch)
2017-06-02 03:00:51.350237: step 14210, loss = 0.74 (1619.8 examples/sec; 0.079 sec/batch)
2017-06-02 03:00:52.241947: step 14220, loss = 0.84 (1435.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:00:53.126393: step 14230, loss = 0.85 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:00:54.017858: step 14240, loss = 0.90 (1435.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:00:54.896954: step 14250, loss = 0.96 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:00:55.777214: step 14260, loss = 0.80 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:00:56.664941: step 14270, loss = 0.94 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:00:57.557894: step 14280, loss = 0.82 (1433.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:00:58.447240: step 14290, loss = 0.72 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:00:59.437631: step 14300, loss = 1.12 (1292.4 examples/sec; 0.099 sec/batch)
2017-06-02 03:01:00.237376: step 14310, loss = 0.85 (1600.5 examples/sec; 0.080 sec/batch)
2017-06-02 03:01:01.131999: step 14320, loss = 0.83 (1430.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:01:02.018879: step 14330, loss = 0.81 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:01:02.902076: step 14340, loss = 0.80 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:01:03.786213: step 14350, loss = 0.88 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:01:04.678396: step 14360, loss = 0.81 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:01:05.554897: step 14370, loss = 0.91 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:01:06.439879: step 14380, loss = 0.85 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:01:07.341892: step 14390, loss = 0.97 (1419.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:01:08.338108: step 14400, loss = 0.75 (1284.9 examples/sec; 0.100 sec/batch)
2017-06-02 03:01:09.127441: step 14410, loss = 0.71 (1621.6 examples/sec; 0.079 sec/batch)
2017-06-02 03:01:10.020567: step 14420, loss = 0.87 (1433.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:01:10.919182: step 14430, loss = 0.68 (1424.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:01:11.799360: step 14440, loss = 0.92 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:01:12.680063: step 14450, loss = 0.89 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:01:13.574215: step 14460, loss = 0.85 (1431.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:01:14.456165: step 14470, loss = 0.80 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:01:15.346846: step 14480, loss = 0.82 (1437.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:01:16.240957: step 14490, loss = 0.83 (1431.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:01:17.224917: step 14500, loss = 0.77 (1300.9 examples/sec; 0.098 sec/batch)
2017-06-02 03:01:18.013678: step 14510, loss = 0.77 (1622.8 examples/sec; 0.079 sec/batch)
2017-06-02 03:01:18.911993: step 14520, loss = 0.97 (1424.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:01:19.793095: step 14530, loss = 0.98 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:01:20.675571: step 14540, loss = 0.80 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:01:21.574866: step 14550, loss = 0.86 (1423.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:01:22.462724: step 14560, loss = 0.97 (1441.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:01:23.338497: step 14570, loss = 0.83 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:01:24.227660: step 14580, loss = 0.97 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:01:25.110653: step 14590, loss = 0.77 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:01:26.092414: step 14600, loss = 1.04 (1303.8 examples/sec; 0.098 sec/batch)
2017-06-02 03:01:26.875457: step 14610, loss = 0.97 (1634.7 examples/sec; 0.078 sec/batch)
2017-06-02 03:01:27.772187: step 14620, loss = 0.84 (1427.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:01:28.656518: step 14630, loss = 1.00 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:01:29.544307: step 14640, loss = 0.79 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:01:30.438025: step 14650, loss = 0.80 (1432.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:01:31.339207: step 14660, loss = 0.93 (1420.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:01:32.217948: step 14670, loss = 0.77 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:01:33.110345: step 14680, loss = 0.78 (1434.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:01:33.999802: step 14690, loss = 0.83 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:01:34.995051: step 14700, loss = 0.79 (1286.1 examples/sec; 0.100 sec/batch)
2017-06-02 03:01:35.792381: step 14710, loss = 0.76 (1605.4 examples/sec; 0.080 sec/batch)
2017-06-02 03:01:36.666500: step 14720, loss = 0.90 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:01:37.550980: step 14730, loss = 0.80 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:01:38.434519: step 14740, loss = 0.67 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:01:39.342124: step 14750, loss = 0.84 (1410.3 examples/sec; 0.091 sec/batch)
2017-06-02 03:01:40.234468: step 14760, loss = 0.94 (1434.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:01:41.130931: step 14770, loss = 0.88 (1427.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:01:42.011024: step 14780, loss = 0.90 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:01:42.910491: step 14790, loss = 0.92 (1423.1 examples/sec; 0.090 sec/batch)
2017-06-02 03:01:43.908452: step 14800, loss = 0.75 (1282.6 examples/sec; 0.100 sec/batch)
2017-06-02 03:01:44.710084: step 14810, loss = 0.92 (1596.7 examples/sec; 0.080 sec/batch)
2017-06-02 03:01:45.602656: step 14820, loss = 0.79 (1434.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:01:46.486036: step 14830, loss = 0.86 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:01:47.380396: step 14840, loss = 1.00 (1431.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:01:48.277965: step 14850, loss = 0.87 (1426.1 examples/sec; 0.090 sec/batch)
2017-06-02 03:01:49.164853: step 14860, loss = 0.93 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:01:50.057345: step 14870, loss = 0.94 (1434.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:01:50.938787: step 14880, loss = 0.71 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:01:51.824714: step 14890, loss = 0.75 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:01:52.828945: step 14900, loss = 0.88 (1274.6 examples/sec; 0.100 sec/batch)
2017-06-02 03:01:53.631065: step 14910, loss = 0.75 (1595.8 examples/sec; 0.080 sec/batch)
2017-06-02 03:01:54.520436: step 14920, loss = 0.85 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:01:55.416234: step 14930, loss = 0.82 (1428.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:01:56.308972: step 14940, loss = 0.93 (1433.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:01:57.194905: step 14950, loss = 0.88 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:01:58.094159: step 14960, loss = 0.84 (1423.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:01:58.994499: step 14970, loss = 0.80 (1421.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:01:59.874461: step 14980, loss = 0.79 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:02:00.761633: step 14990, loss = 0.88 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:02:01.743667: step 15000, loss = 0.85 (1303.4 examples/sec; 0.098 sec/batch)
2017-06-02 03:02:02.547876: step 15010, loss = 0.79 (1591.6 examples/sec; 0.080 sec/batch)
2017-06-02 03:02:03.458287: step 15020, loss = 0.90 (1406.0 examples/sec; 0.091 sec/batch)
2017-06-02 03:02:04.351079: step 15030, loss = 0.86 (1433.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:02:05.233467: step 15040, loss = 0.85 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:02:06.136699: step 15050, loss = 0.93 (1417.1 examples/sec; 0.090 sec/batch)
2017-06-02 03:02:07.049331: step 15060, loss = 0.80 (1402.5 examples/sec; 0.091 sec/batch)
2017-06-02 03:02:07.953922: step 15070, loss = 0.98 (1415.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:02:08.861220: step 15080, loss = 0.81 (1410.8 examples/sec; 0.091 sec/batch)
2017-06-02 03:02:09.742501: step 15090, loss = 0.91 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:02:10.745905: step 15100, loss = 1.06 (1275.6 examples/sec; 0.100 sec/batch)
2017-06-02 03:02:11.537782: step 15110, loss = 0.85 (1616.4 examples/sec; 0.079 sec/batch)
2017-06-02 03:02:12.432166: step 15120, loss = 0.91 (1431.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:02:13.330965: step 15130, loss = 1.12 (1424.1 examples/sec; 0.090 sec/batch)
2017-06-02 03:02:14.219718: step 15140, loss = 0.84 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:02:15.113217: step 15150, loss = 0.66 (1432.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:02:16.010071: step 15160, loss = 0.85 (1427.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:02:16.909903: step 15170, loss = 0.83 (1422.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:02:17.784886: step 15180, loss = 0.91 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:02:18.674024: step 15190, loss = 0.94 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:02:19.669948: step 15200, loss = 0.79 (1285.2 examples/sec; 0.100 sec/batch)
2017-06-02 03:02:20.458185: step 15210, loss = 0.68 (1623.9 examples/sec; 0.079 sec/batch)
2017-06-02 03:02:21.355818: step 15220, loss = 0.93 (1426.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:02:22.241973: step 15230, loss = 0.80 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:02:23.132416: step 15240, loss = 0.73 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:02:24.027442: step 15250, loss = 0.93 (1430.1 examples/sec; 0.090 sec/batch)
2017-06-02 03:02:24.915334: step 15260, loss = 0.98 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:02:25.808183: step 15270, loss = 0.83 (1433.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:02:26.693384: step 15280, loss = 0.81 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:02:27.568333: step 15290, loss = 0.72 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:02:28.560160: step 15300, loss = 0.80 (1290.5 examples/sec; 0.099 sec/batch)
2017-06-02 03:02:29.343489: step 15310, loss = 0.98 (1634.0 examples/sec; 0.078 sec/batch)
2017-06-02 03:02:30.234576: step 15320, loss = 0.85 (1436.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:02:31.118802: step 15330, loss = 0.80 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:02:32.002256: step 15340, loss = 0.81 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:02:32.878862: step 15350, loss = 0.83 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:02:33.763385: step 15360, loss = 0.83 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:02:34.654674: step 15370, loss = 0.73 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:02:35.554656: step 15380, loss = 0.85 (1422.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:02:36.440286: step 15390, loss = 0.99 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:02:37.442376: step 15400, loss = 0.87 (1277.3 examples/sec; 0.100 sec/batch)
2017-06-02 03:02:38.218370: step 15410, loss = 0.86 (1649.5 examples/sec; 0.078 sec/batch)
2017-06-02 03:02:39.108500: step 15420, loss = 0.88 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:02:39.997262: step 15430, loss = 1.03 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:02:40.884702: step 15440, loss = 0.83 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:02:41.764187: step 15450, loss = 0.92 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:02:42.645741: step 15460, loss = 0.78 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:02:43.529317: step 15470, loss = 0.76 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:02:44.420259: step 15480, loss = 0.80 (1436.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:02:45.312435: step 15490, loss = 0.94 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:02:46.319944: step 15500, loss = 0.85 (1270.5 examples/sec; 0.101 sec/batch)
2017-06-02 03:02:47.118388: step 15510, loss = 0.92 (1603.1 examples/sec; 0.080 sec/batch)
2017-06-02 03:02:47.998194: step 15520, loss = 0.82 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:02:48.882609: step 15530, loss = 0.82 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:02:49.769594: step 15540, loss = 0.82 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:02:50.650334: step 15550, loss = 0.77 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:02:51.544488: step 15560, loss = 0.64 (1431.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:02:52.452633: step 15570, loss = 1.01 (1409.5 examples/sec; 0.091 sec/batch)
2017-06-02 03:02:53.353762: step 15580, loss = 0.67 (1420.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:02:54.232320: step 15590, loss = 0.88 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:02:55.217545: step 15600, loss = 0.97 (1299.2 examples/sec; 0.099 sec/batch)
2017-06-02 03:02:56.017645: step 15610, loss = 0.93 (1599.8 examples/sec; 0.080 sec/batch)
2017-06-02 03:02:56.904184: step 15620, loss = 0.85 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:02:57.794928: step 15630, loss = 0.86 (1437.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:02:58.680890: step 15640, loss = 0.91 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:02:59.589358: step 15650, loss = 0.96 (1409.0 examples/sec; 0.091 sec/batch)
2017-06-02 03:03:00.487982: step 15660, loss = 0.88 (1424.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:03:01.369751: step 15670, loss = 0.83 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:03:02.255670: step 15680, loss = 0.77 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:03:03.140274: step 15690, loss = 0.91 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:03:04.120910: step 15700, loss = 0.89 (1305.3 examples/sec; 0.098 sec/batch)
2017-06-02 03:03:04.898562: step 15710, loss = 0.84 (1646.0 examples/sec; 0.078 sec/batch)
2017-06-02 03:03:05.776822: step 15720, loss = 0.80 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:03:06.654213: step 15730, loss = 0.95 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:03:07.529916: step 15740, loss = 0.95 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:03:08.416942: step 15750, loss = 0.75 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:03:09.294164: step 15760, loss = 0.87 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:03:10.168838: step 15770, loss = 0.83 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:03:11.063180: step 15780, loss = 0.86 (1431.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:03:11.948019: step 15790, loss = 0.77 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:03:12.925803: step 15800, loss = 0.81 (1309.1 examples/sec; 0.098 sec/batch)
2017-06-02 03:03:13.712936: step 15810, loss = 1.13 (1626.1 examples/sec; 0.079 sec/batch)
2017-06-02 03:03:14.607821: step 15820, loss = 0.92 (1430.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:03:15.495639: step 15830, loss = 0.81 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:03:16.386808: step 15840, loss = 0.75 (1436.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:03:17.278818: step 15850, loss = 0.84 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:03:18.161989: step 15860, loss = 0.89 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:03:19.048257: step 15870, loss = 0.97 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:03:19.939148: step 15880, loss = 0.92 (1436.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:03:20.828147: step 15890, loss = 0.92 (1439.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:03:21.825181: step 15900, loss = 0.81 (1283.8 examples/sec; 0.100 sec/batch)
2017-06-02 03:03:22.609105: step 15910, loss = 0.97 (1632.8 examples/sec; 0.078 sec/batch)
2017-06-02 03:03:23.502642: step 15920, loss = 0.92 (1432.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:03:24.413335: step 15930, loss = 1.05 (1405.5 examples/sec; 0.091 sec/batch)
2017-06-02 03:03:25.304524: step 15940, loss = 0.73 (1436.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:03:26.199842: step 15950, loss = 0.81 (1429.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:03:27.104301: step 15960, loss = 0.85 (1415.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:03:27.999692: step 15970, loss = 0.88 (1429.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:03:28.898726: step 15980, loss = 0.88 (1423.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:03:29.796305: step 15990, loss = 0.98 (1426.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:03:30.795756: step 16000, loss = 0.86 (1280.7 examples/sec; 0.100 sec/batch)
2017-06-02 03:03:31.582491: step 16010, loss = 0.86 (1627.0 examples/sec; 0.079 sec/batch)
2017-06-02 03:03:32.475981: step 16020, loss = 0.85 (1432.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:03:33.359210: step 16030, loss = 0.82 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:03:34.254698: step 16040, loss = 0.93 (1429.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:03:35.160959: step 16050, loss = 0.80 (1412.4 examples/sec; 0.091 sec/batch)
2017-06-02 03:03:36.052641: step 16060, loss = 0.73 (1435.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:03:36.947126: step 16070, loss = 0.75 (1431.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:03:37.838444: step 16080, loss = 0.69 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:03:38.721356: step 16090, loss = 0.81 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:03:39.715941: step 16100, loss = 0.69 (1287.0 examples/sec; 0.099 sec/batch)
2017-06-02 03:03:40.506467: step 16110, loss = 0.78 (1619.2 examples/sec; 0.079 sec/batch)
2017-06-02 03:03:41.416092: step 16120, loss = 0.84 (1407.2 examples/sec; 0.091 sec/batch)
2017-06-02 03:03:42.319091: step 16130, loss = 0.74 (1417.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:03:43.202881: step 16140, loss = 0.78 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:03:44.104593: step 16150, loss = 0.93 (1419.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:03:44.994596: step 16160, loss = 0.66 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:03:45.884249: step 16170, loss = 0.76 (1438.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:03:46.764631: step 16180, loss = 0.75 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:03:47.648433: step 16190, loss = 0.87 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:03:48.670296: step 16200, loss = 0.96 (1252.6 examples/sec; 0.102 sec/batch)
2017-06-02 03:03:49.459167: step 16210, loss = 0.89 (1622.6 examples/sec; 0.079 sec/batch)
2017-06-02 03:03:50.356432: step 16220, loss = 0.89 (1426.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:03:51.248697: step 16230, loss = 0.84 (1434.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:03:52.148882: step 16240, loss = 0.85 (1421.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:03:53.035634: step 16250, loss = 0.85 (1443.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:03:53.918993: step 16260, loss = 0.83 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:03:54.802541: step 16270, loss = 0.80 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:03:55.679109: step 16280, loss = 0.90 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:03:56.567935: step 16290, loss = 0.78 (1440.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:03:57.545353: step 16300, loss = 0.85 (1309.6 examples/sec; 0.098 sec/batch)
2017-06-02 03:03:58.336310: step 16310, loss = 0.86 (1618.3 examples/sec; 0.079 sec/batch)
2017-06-02 03:03:59.228210: step 16320, loss = 0.79 (1435.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:04:00.104182: step 16330, loss = 0.75 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:04:00.990710: step 16340, loss = 0.74 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:04:01.876892: step 16350, loss = 1.14 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:04:02.771472: step 16360, loss = 0.92 (1430.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:04:03.646536: step 16370, loss = 0.88 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:04:04.527650: step 16380, loss = 0.84 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:04:05.423437: step 16390, loss = 0.78 (1428.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:04:06.392624: step 16400, loss = 0.90 (1320.7 examples/sec; 0.097 sec/batch)
2017-06-02 03:04:07.180572: step 16410, loss = 0.98 (1624.5 examples/sec; 0.079 sec/batch)
2017-06-02 03:04:08.082747: step 16420, loss = 0.70 (1418.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:04:08.987824: step 16430, loss = 0.91 (1414.2 examples/sec; 0.091 sec/batch)
2017-06-02 03:04:09.876742: step 16440, loss = 0.81 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:04:10.770998: step 16450, loss = 0.82 (1431.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:04:11.667489: step 16460, loss = 0.94 (1427.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:04:12.556551: step 16470, loss = 0.74 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:04:13.432908: step 16480, loss = 0.82 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:04:14.317923: step 16490, loss = 0.97 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:04:15.302866: step 16500, loss = 0.66 (1299.6 examples/sec; 0.098 sec/batch)
2017-06-02 03:04:16.081063: step 16510, loss = 0.79 (1644.8 examples/sec; 0.078 sec/batch)
2017-06-02 03:04:16.967173: step 16520, loss = 0.77 (1444.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:04:17.846638: step 16530, loss = 0.73 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:04:18.734895: step 16540, loss = 0.69 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:04:19.608289: step 16550, loss = 0.67 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:04:20.491572: step 16560, loss = 0.68 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:04:21.376775: step 16570, loss = 0.84 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:04:22.259597: step 16580, loss = 0.84 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:04:23.133381: step 16590, loss = 0.70 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:04:24.136280: step 16600, loss = 1.06 (1276.3 examples/sec; 0.100 sec/batch)
2017-06-02 03:04:24.908995: step 16610, loss = 0.89 (1656.5 examples/sec; 0.077 sec/batch)
2017-06-02 03:04:25.798783: step 16620, loss = 0.81 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:04:26.679978: step 16630, loss = 0.89 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:04:27.563936: step 16640, loss = 0.88 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:04:28.449017: step 16650, loss = 0.86 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:04:29.349490: step 16660, loss = 0.87 (1421.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:04:30.239244: step 16670, loss = 0.89 (1438.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:04:31.118168: step 16680, loss = 0.72 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:04:32.001607: step 16690, loss = 1.04 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:04:32.979204: step 16700, loss = 0.73 (1309.3 examples/sec; 0.098 sec/batch)
2017-06-02 03:04:33.769227: step 16710, loss = 0.78 (1620.2 examples/sec; 0.079 sec/batch)
2017-06-02 03:04:34.649182: step 16720, loss = 0.83 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:04:35.523395: step 16730, loss = 0.73 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:04:36.410067: step 16740, loss = 0.72 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:04:37.288783: step 16750, loss = 0.77 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:04:38.172123: step 16760, loss = 0.85 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:04:39.041687: step 16770, loss = 0.77 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:04:39.938467: step 16780, loss = 0.92 (1427.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:04:40.833956: step 16790, loss = 0.59 (1429.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:04:41.817007: step 16800, loss = 1.13 (1302.1 examples/sec; 0.098 sec/batch)
2017-06-02 03:04:42.611576: step 16810, loss = 0.85 (1610.9 examples/sec; 0.079 sec/batch)
2017-06-02 03:04:43.497485: step 16820, loss = 0.71 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:04:44.387710: step 16830, loss = 0.76 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:04:45.267283: step 16840, loss = 0.69 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:04:46.149144: step 16850, loss = 0.77 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:04:47.018049: step 16860, loss = 0.76 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:04:47.905183: step 16870, loss = 0.82 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:04:48.796113: step 16880, loss = 0.85 (1436.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:04:49.693479: step 16890, loss = 0.93 (1426.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:04:50.703258: step 16900, loss = 0.90 (1267.6 examples/sec; 0.101 sec/batch)
2017-06-02 03:04:51.467862: step 16910, loss = 0.73 (1674.1 examples/sec; 0.076 sec/batch)
2017-06-02 03:04:52.353315: step 16920, loss = 0.91 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:04:53.253984: step 16930, loss = 0.77 (1421.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:04:54.127204: step 16940, loss = 0.99 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:04:55.007474: step 16950, loss = 0.90 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:04:55.921326: step 16960, loss = 0.74 (1400.7 examples/sec; 0.091 sec/batch)
2017-06-02 03:04:56.820023: step 16970, loss = 0.82 (1424.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:04:57.707330: step 16980, loss = 0.85 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:04:58.582233: step 16990, loss = 0.83 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:04:59.585318: step 17000, loss = 0.91 (1276.1 examples/sec; 0.100 sec/batch)
2017-06-02 03:05:00.353218: step 17010, loss = 0.82 (1666.9 examples/sec; 0.077 sec/batch)
2017-06-02 03:05:01.252865: step 17020, loss = 0.73 (1422.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:05:02.153979: step 17030, loss = 1.02 (1420.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:05:03.036004: step 17040, loss = 0.84 (1451.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:05:03.914792: step 17050, loss = 1.07 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:05:04.803523: step 17060, loss = 0.92 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:05:05.686754: step 17070, loss = 0.70 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:05:06.571994: step 17080, loss = 1.06 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:05:07.464121: step 17090, loss = 0.81 (1434.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:05:08.455867: step 17100, loss = 0.90 (1290.7 examples/sec; 0.099 sec/batch)
2017-06-02 03:05:09.250531: step 17110, loss = 0.86 (1610.7 examples/sec; 0.079 sec/batch)
2017-06-02 03:05:10.118891: step 17120, loss = 0.77 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:05:11.012138: step 17130, loss = 1.05 (1433.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:05:11.899719: step 17140, loss = 0.75 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:05:12.788353: step 17150, loss = 0.85 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:05:13.663270: step 17160, loss = 0.71 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:05:14.539376: step 17170, loss = 0.78 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:05:15.427544: step 17180, loss = 0.80 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:05:16.314851: step 17190, loss = 0.81 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:05:17.309646: step 17200, loss = 0.72 (1286.7 examples/sec; 0.099 sec/batch)
2017-06-02 03:05:18.093022: step 17210, loss = 0.93 (1634.0 examples/sec; 0.078 sec/batch)
2017-06-02 03:05:18.974254: step 17220, loss = 0.66 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:05:19.849378: step 17230, loss = 0.91 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:05:20.732461: step 17240, loss = 0.84 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:05:21.619797: step 17250, loss = 1.02 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:05:22.506334: step 17260, loss = 0.73 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:05:23.400324: step 17270, loss = 0.88 (1431.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:05:24.286532: step 17280, loss = 0.88 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:05:25.174220: step 17290, loss = 0.95 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:05:26.180107: step 17300, loss = 0.81 (1272.5 examples/sec; 0.101 sec/batch)
2017-06-02 03:05:26.943413: step 17310, loss = 0.82 (1676.9 examples/sec; 0.076 sec/batch)
2017-06-02 03:05:27.829521: step 17320, loss = 0.81 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:05:28.722503: step 17330, loss = 0.91 (1433.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:05:29.597979: step 17340, loss = 0.73 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:05:30.479005: step 17350, loss = 0.84 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:05:31.371674: step 17360, loss = 0.78 (1433.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:05:32.260954: step 17370, loss = 0.74 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:05:33.148552: step 17380, loss = 0.86 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:05:34.028787: step 17390, loss = 0.91 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:05:35.025437: step 17400, loss = 0.94 (1284.3 examples/sec; 0.100 sec/batch)
2017-06-02 03:05:35.791565: step 17410, loss = 0.70 (1670.8 examples/sec; 0.077 sec/batch)
2017-06-02 03:05:36.676395: step 17420, loss = 0.87 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:05:37.559240: step 17430, loss = 0.75 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:05:38.441056: step 17440, loss = 0.78 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:05:39.323296: step 17450, loss = 0.82 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:05:40.196829: step 17460, loss = 0.72 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:05:41.078837: step 17470, loss = 0.94 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:05:41.964907: step 17480, loss = 0.89 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:05:42.837596: step 17490, loss = 0.72 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:05:43.820087: step 17500, loss = 0.77 (1302.8 examples/sec; 0.098 sec/batch)
2017-06-02 03:05:44.604935: step 17510, loss = 0.96 (1630.9 examples/sec; 0.078 sec/batch)
2017-06-02 03:05:45.492512: step 17520, loss = 0.86 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:05:46.362938: step 17530, loss = 0.82 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:05:47.252399: step 17540, loss = 0.88 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:05:48.139184: step 17550, loss = 0.85 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:05:49.010992: step 17560, loss = 0.86 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:05:49.881228: step 17570, loss = 0.91 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:05:50.772188: step 17580, loss = 0.81 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:05:51.648289: step 17590, loss = 0.83 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:05:52.623976: step 17600, loss = 0.82 (1311.9 examples/sec; 0.098 sec/batch)
2017-06-02 03:05:53.409551: step 17610, loss = 0.98 (1629.4 examples/sec; 0.079 sec/batch)
2017-06-02 03:05:54.292935: step 17620, loss = 0.91 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:05:55.184282: step 17630, loss = 0.79 (1436.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:05:56.058713: step 17640, loss = 0.85 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:05:56.944298: step 17650, loss = 0.86 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:05:57.820286: step 17660, loss = 0.70 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:05:58.702168: step 17670, loss = 0.69 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:05:59.581915: step 17680, loss = 0.72 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:06:00.472862: step 17690, loss = 0.91 (1436.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:06:01.452380: step 17700, loss = 0.72 (1306.8 examples/sec; 0.098 sec/batch)
2017-06-02 03:06:02.228910: step 17710, loss = 1.02 (1648.3 examples/sec; 0.078 sec/batch)
2017-06-02 03:06:03.116566: step 17720, loss = 0.81 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:06:03.996405: step 17730, loss = 0.80 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:06:04.872511: step 17740, loss = 0.68 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:06:05.754954: step 17750, loss = 0.99 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:06:06.629288: step 17760, loss = 0.92 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:06:07.503129: step 17770, loss = 0.75 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:06:08.390326: step 17780, loss = 0.79 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:06:09.268063: step 17790, loss = 0.98 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:06:10.253134: step 17800, loss = 0.86 (1299.4 examples/sec; 0.099 sec/batch)
2017-06-02 03:06:11.026140: step 17810, loss = 0.87 (1655.9 examples/sec; 0.077 sec/batch)
2017-06-02 03:06:11.908661: step 17820, loss = 0.89 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:06:12.792877: step 17830, loss = 0.80 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:06:13.673422: step 17840, loss = 0.88 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:06:14.546501: step 17850, loss = 0.76 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:06:15.424307: step 17860, loss = 0.80 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:06:16.305228: step 17870, loss = 0.73 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:06:17.193608: step 17880, loss = 0.90 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:06:18.066319: step 17890, loss = 0.88 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:06:19.114778: step 17900, loss = 0.92 (1220.8 examples/sec; 0.105 sec/batch)
2017-06-02 03:06:19.835523: step 17910, loss = 0.93 (1776.0 examples/sec; 0.072 sec/batch)
2017-06-02 03:06:20.706912: step 17920, loss = 0.90 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:06:21.590713: step 17930, loss = 0.81 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:06:22.472300: step 17940, loss = 0.89 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:06:23.350118: step 17950, loss = 0.90 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:06:24.234362: step 17960, loss = 0.95 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:06:25.109566: step 17970, loss = 0.84 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:06:25.976587: step 17980, loss = 0.80 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:06:26.850895: step 17990, loss = 0.84 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:06:27.844105: step 18000, loss = 0.97 (1288.8 examples/sec; 0.099 sec/batch)
2017-06-02 03:06:28.623911: step 18010, loss = 0.91 (1641.4 examples/sec; 0.078 sec/batch)
2017-06-02 03:06:29.513647: step 18020, loss = 0.71 (1438.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:06:30.399647: step 18030, loss = 0.77 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:06:31.275746: step 18040, loss = 0.86 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:06:32.161043: step 18050, loss = 0.88 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:06:33.058144: step 18060, loss = 0.80 (1426.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:06:33.934209: step 18070, loss = 0.90 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:06:34.813757: step 18080, loss = 0.69 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:06:35.702930: step 18090, loss = 0.76 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:06:36.688867: step 18100, loss = 0.87 (1298.3 examples/sec; 0.099 sec/batch)
2017-06-02 03:06:37.478797: step 18110, loss = 0.84 (1620.4 examples/sec; 0.079 sec/batch)
2017-06-02 03:06:38.366657: step 18120, loss = 0.96 (1441.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:06:39.265680: step 18130, loss = 0.82 (1423.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:06:40.149162: step 18140, loss = 1.05 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:06:41.037819: step 18150, loss = 0.84 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:06:41.925452: step 18160, loss = 0.82 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:06:42.806895: step 18170, loss = 0.78 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:06:43.692112: step 18180, loss = 0.78 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:06:44.583546: step 18190, loss = 0.72 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:06:45.559993: step 18200, loss = 0.74 (1310.9 examples/sec; 0.098 sec/batch)
2017-06-02 03:06:46.338186: step 18210, loss = 0.87 (1644.8 examples/sec; 0.078 sec/batch)
2017-06-02 03:06:47.217612: step 18220, loss = 0.72 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:06:48.084421: step 18230, loss = 0.87 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:06:48.982755: step 18240, loss = 0.93 (1424.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:06:49.863983: step 18250, loss = 0.84 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:06:50.751036: step 18260, loss = 0.84 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:06:51.643032: step 18270, loss = 0.94 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:06:52.523606: step 18280, loss = 0.90 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:06:53.413984: step 18290, loss = 0.81 (1437.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:06:54.387050: step 18300, loss = 0.80 (1315.4 examples/sec; 0.097 sec/batch)
2017-06-02 03:06:55.182053: step 18310, loss = 0.89 (1610.0 examples/sec; 0.080 sec/batch)
2017-06-02 03:06:56.062697: step 18320, loss = 0.90 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:06:56.961637: step 18330, loss = 0.66 (1423.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:06:57.838092: step 18340, loss = 0.79 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:06:58.719498: step 18350, loss = 0.96 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:06:59.618630: step 18360, loss = 0.70 (1423.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:07:00.512677: step 18370, loss = 0.79 (1431.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:07:01.404785: step 18380, loss = 0.59 (1434.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:07:02.274280: step 18390, loss = 0.86 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:07:03.256234: step 18400, loss = 0.91 (1303.5 examples/sec; 0.098 sec/batch)
2017-06-02 03:07:04.030897: step 18410, loss = 0.87 (1652.3 examples/sec; 0.077 sec/batch)
2017-06-02 03:07:04.912960: step 18420, loss = 0.91 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:07:05.796271: step 18430, loss = 0.75 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:07:06.678691: step 18440, loss = 0.85 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:07:07.561246: step 18450, loss = 0.70 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:07:08.452599: step 18460, loss = 0.89 (1436.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:07:09.327269: step 18470, loss = 0.79 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:07:10.209898: step 18480, loss = 0.83 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:07:11.100879: step 18490, loss = 1.00 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:07:12.086994: step 18500, loss = 0.83 (1298.0 examples/sec; 0.099 sec/batch)
2017-06-02 03:07:12.879800: step 18510, loss = 0.75 (1614.5 examples/sec; 0.079 sec/batch)
2017-06-02 03:07:13.767413: step 18520, loss = 0.77 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:07:14.644022: step 18530, loss = 0.88 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:07:15.531212: step 18540, loss = 0.95 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:07:16.416419: step 18550, loss = 0.92 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:07:17.301445: step 18560, loss = 0.93 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:07:18.194327: step 18570, loss = 0.88 (1433.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:07:19.070955: step 18580, loss = 0.67 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:07:19.958110: step 18590, loss = 0.83 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:07:20.948357: step 18600, loss = 0.72 (1292.6 examples/sec; 0.099 sec/batch)
2017-06-02 03:07:21.738607: step 18610, loss = 0.90 (1619.8 examples/sec; 0.079 sec/batch)
2017-06-02 03:07:22.611475: step 18620, loss = 0.88 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:07:23.496800: step 18630, loss = 0.99 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:07:24.379780: step 18640, loss = 0.84 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:07:25.270002: step 18650, loss = 1.03 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:07:26.155582: step 18660, loss = 0.78 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:07:27.045803: step 18670, loss = 0.80 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:07:27.924449: step 18680, loss = 0.92 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:07:28.808372: step 18690, loss = 0.76 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:07:29.796548: step 18700, loss = 0.85 (1295.3 examples/sec; 0.099 sec/batch)
2017-06-02 03:07:30.581813: step 18710, loss = 0.80 (1630.0 examples/sec; 0.079 sec/batch)
2017-06-02 03:07:31.471422: step 18720, loss = 0.83 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:07:32.351008: step 18730, loss = 0.77 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:07:33.231714: step 18740, loss = 0.98 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:07:34.104345: step 18750, loss = 0.74 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:07:34.989859: step 18760, loss = 0.72 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:07:35.870940: step 18770, loss = 0.78 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:07:36.745516: step 18780, loss = 0.80 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:07:37.625142: step 18790, loss = 0.75 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:07:38.602526: step 18800, loss = 0.79 (1309.6 examples/sec; 0.098 sec/batch)
2017-06-02 03:07:39.382057: step 18810, loss = 0.87 (1642.0 examples/sec; 0.078 sec/batch)
2017-06-02 03:07:40.272711: step 18820, loss = 0.76 (1437.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:07:41.158092: step 18830, loss = 0.73 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:07:42.042819: step 18840, loss = 0.74 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:07:42.924743: step 18850, loss = 0.83 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:07:43.808458: step 18860, loss = 0.74 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:07:44.690573: step 18870, loss = 0.97 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:07:45.578681: step 18880, loss = 0.86 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:07:46.445717: step 18890, loss = 0.86 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:07:47.423848: step 18900, loss = 0.73 (1308.6 examples/sec; 0.098 sec/batch)
2017-06-02 03:07:48.200763: step 18910, loss = 0.89 (1647.6 examples/sec; 0.078 sec/batch)
2017-06-02 03:07:49.081604: step 18920, loss = 0.75 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:07:49.973517: step 18930, loss = 0.82 (1435.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:07:50.850709: step 18940, loss = 0.85 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:07:51.731715: step 18950, loss = 0.83 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:07:52.621556: step 18960, loss = 0.78 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:07:53.527103: step 18970, loss = 0.87 (1413.5 examples/sec; 0.091 sec/batch)
2017-06-02 03:07:54.417372: step 18980, loss = 0.88 (1437.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:07:55.312144: step 18990, loss = 0.94 (1430.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:07:56.311480: step 19000, loss = 0.79 (1280.8 examples/sec; 0.100 sec/batch)
2017-06-02 03:07:57.089309: step 19010, loss = 0.76 (1645.6 examples/sec; 0.078 sec/batch)
2017-06-02 03:07:57.978765: step 19020, loss = 0.70 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:07:58.860640: step 19030, loss = 0.96 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:07:59.743931: step 19040, loss = 0.82 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:08:00.634323: step 19050, loss = 0.81 (1437.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:08:01.527465: step 19060, loss = 0.84 (1433.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:08:02.402876: step 19070, loss = 0.96 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:08:03.294811: step 19080, loss = 0.90 (1435.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:08:04.179277: step 19090, loss = 0.77 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:08:05.160268: step 19100, loss = 0.70 (1304.8 examples/sec; 0.098 sec/batch)
2017-06-02 03:08:05.935570: step 19110, loss = 0.75 (1651.0 examples/sec; 0.078 sec/batch)
2017-06-02 03:08:06.818319: step 19120, loss = 0.82 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:08:07.705397: step 19130, loss = 0.74 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:08:08.589103: step 19140, loss = 0.92 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:08:09.471201: step 19150, loss = 0.67 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:08:10.345432: step 19160, loss = 0.74 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:08:11.230954: step 19170, loss = 0.73 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:08:12.128925: step 19180, loss = 0.82 (1425.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:08:13.009781: step 19190, loss = 0.86 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:08:13.983287: step 19200, loss = 0.91 (1314.9 examples/sec; 0.097 sec/batch)
2017-06-02 03:08:14.771603: step 19210, loss = 0.78 (1623.7 examples/sec; 0.079 sec/batch)
2017-06-02 03:08:15.682460: step 19220, loss = 0.91 (1405.3 examples/sec; 0.091 sec/batch)
2017-06-02 03:08:16.565950: step 19230, loss = 0.82 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:08:17.447581: step 19240, loss = 0.90 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:08:18.325990: step 19250, loss = 0.83 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:08:19.204983: step 19260, loss = 0.66 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:08:20.075445: step 19270, loss = 0.75 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:08:20.946024: step 19280, loss = 0.99 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:08:21.826775: step 19290, loss = 0.92 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:08:22.805714: step 19300, loss = 0.85 (1307.5 examples/sec; 0.098 sec/batch)
2017-06-02 03:08:23.588491: step 19310, loss = 0.72 (1635.2 examples/sec; 0.078 sec/batch)
2017-06-02 03:08:24.482617: step 19320, loss = 0.88 (1431.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:08:25.358088: step 19330, loss = 0.83 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:08:26.238332: step 19340, loss = 0.75 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:08:27.137322: step 19350, loss = 0.94 (1423.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:08:28.052200: step 19360, loss = 0.74 (1399.1 examples/sec; 0.091 sec/batch)
2017-06-02 03:08:28.934965: step 19370, loss = 0.79 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:08:29.824156: step 19380, loss = 0.87 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:08:30.716812: step 19390, loss = 0.80 (1433.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:08:31.713146: step 19400, loss = 0.96 (1284.7 examples/sec; 0.100 sec/batch)
2017-06-02 03:08:32.501007: step 19410, loss = 0.83 (1624.6 examples/sec; 0.079 sec/batch)
2017-06-02 03:08:33.392616: step 19420, loss = 0.86 (1435.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:08:34.279959: step 19430, loss = 0.95 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:08:35.166461: step 19440, loss = 0.79 (1443.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:08:36.045403: step 19450, loss = 0.62 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:08:36.920122: step 19460, loss = 0.75 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:08:37.809013: step 19470, loss = 0.82 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:08:38.698701: step 19480, loss = 0.92 (1438.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:08:39.577871: step 19490, loss = 1.01 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:08:40.558868: step 19500, loss = 0.91 (1304.8 examples/sec; 0.098 sec/batch)
2017-06-02 03:08:41.342910: step 19510, loss = 0.74 (1632.6 examples/sec; 0.078 sec/batch)
2017-06-02 03:08:42.227843: step 19520, loss = 0.76 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:08:43.103674: step 19530, loss = 0.78 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:08:43.997315: step 19540, loss = 0.86 (1432.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:08:44.875318: step 19550, loss = 0.77 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:08:45.760471: step 19560, loss = 0.89 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:08:46.640546: step 19570, loss = 0.84 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:08:47.527266: step 19580, loss = 0.94 (1443.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:08:48.410225: step 19590, loss = 0.80 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:08:49.407930: step 19600, loss = 0.66 (1282.9 examples/sec; 0.100 sec/batch)
2017-06-02 03:08:50.194915: step 19610, loss = 0.79 (1626.5 examples/sec; 0.079 sec/batch)
2017-06-02 03:08:51.065449: step 19620, loss = 0.85 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:08:51.952533: step 19630, loss = 0.75 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:08:52.835608: step 19640, loss = 0.86 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:08:53.717201: step 19650, loss = 0.84 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:08:54.605152: step 19660, loss = 0.80 (1441.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:08:55.484387: step 19670, loss = 0.91 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:08:56.370478: step 19680, loss = 0.81 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:08:57.252461: step 19690, loss = 0.98 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:08:58.234566: step 19700, loss = 0.93 (1303.3 examples/sec; 0.098 sec/batch)
2017-06-02 03:08:59.020665: step 19710, loss = 0.77 (1628.3 examples/sec; 0.079 sec/batch)
2017-06-02 03:08:59.897929: step 19720, loss = 1.03 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:09:00.784026: step 19730, loss = 0.83 (1444.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:09:01.666596: step 19740, loss = 0.85 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:09:02.554870: step 19750, loss = 0.80 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:09:03.437928: step 19760, loss = 1.08 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:09:04.335062: step 19770, loss = 0.93 (1426.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:09:05.211842: step 19780, loss = 0.87 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:09:06.107119: step 19790, loss = 0.77 (1429.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:09:07.093625: step 19800, loss = 0.69 (1297.5 examples/sec; 0.099 sec/batch)
2017-06-02 03:09:07.887726: step 19810, loss = 0.85 (1611.9 examples/sec; 0.079 sec/batch)
2017-06-02 03:09:08.759966: step 19820, loss = 0.96 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:09:09.640562: step 19830, loss = 0.63 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:09:10.518782: step 19840, loss = 0.94 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:09:11.411771: step 19850, loss = 0.75 (1433.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:09:12.286224: step 19860, loss = 0.91 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:09:13.179815: step 19870, loss = 0.84 (1432.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:09:14.061200: step 19880, loss = 0.70 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:09:14.940202: step 19890, loss = 0.84 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:09:15.922090: step 19900, loss = 0.80 (1303.6 examples/sec; 0.098 sec/batch)
2017-06-02 03:09:16.708552: step 19910, loss = 0.78 (1627.5 examples/sec; 0.079 sec/batch)
2017-06-02 03:09:17.590203: step 19920, loss = 0.87 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:09:18.463009: step 19930, loss = 0.96 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:09:19.329979: step 19940, loss = 0.93 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:09:20.204033: step 19950, loss = 0.78 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:09:21.083973: step 19960, loss = 0.78 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:09:21.980453: step 19970, loss = 0.83 (1427.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:09:22.861050: step 19980, loss = 0.99 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:09:23.739094: step 19990, loss = 0.93 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:09:24.711488: step 20000, loss = 0.82 (1316.3 examples/sec; 0.097 sec/batch)
2017-06-02 03:09:25.499576: step 20010, loss = 0.80 (1624.2 examples/sec; 0.079 sec/batch)
2017-06-02 03:09:26.374872: step 20020, loss = 0.64 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:09:27.264947: step 20030, loss = 0.72 (1438.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:09:28.148934: step 20040, loss = 0.69 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:09:29.045580: step 20050, loss = 0.96 (1427.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:09:29.923244: step 20060, loss = 0.83 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:09:30.807996: step 20070, loss = 0.90 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:09:31.694569: step 20080, loss = 0.80 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:09:32.584334: step 20090, loss = 0.75 (1438.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:09:33.561681: step 20100, loss = 0.88 (1309.7 examples/sec; 0.098 sec/batch)
2017-06-02 03:09:34.341724: step 20110, loss = 0.76 (1640.9 examples/sec; 0.078 sec/batch)
2017-06-02 03:09:35.225806: step 20120, loss = 0.83 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:09:36.118465: step 20130, loss = 0.89 (1433.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:09:37.015594: step 20140, loss = 0.74 (1426.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:09:37.903638: step 20150, loss = 0.78 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:09:38.794765: step 20160, loss = 0.92 (1436.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:09:39.699383: step 20170, loss = 0.80 (1415.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:09:40.582718: step 20180, loss = 0.85 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:09:41.466675: step 20190, loss = 0.70 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:09:42.442829: step 20200, loss = 0.85 (1311.3 examples/sec; 0.098 sec/batch)
2017-06-02 03:09:43.226387: step 20210, loss = 0.85 (1633.6 examples/sec; 0.078 sec/batch)
2017-06-02 03:09:44.110582: step 20220, loss = 0.75 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:09:44.990098: step 20230, loss = 0.67 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:09:45.883834: step 20240, loss = 0.84 (1432.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:09:46.768351: step 20250, loss = 0.74 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:09:47.653098: step 20260, loss = 0.89 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:09:48.541510: step 20270, loss = 0.81 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:09:49.413753: step 20280, loss = 0.80 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:09:50.301164: step 20290, loss = 0.87 (1442.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:09:51.364611: step 20300, loss = 0.88 (1203.7 examples/sec; 0.106 sec/batch)
2017-06-02 03:09:52.163255: step 20310, loss = 0.94 (1602.6 examples/sec; 0.080 sec/batch)
2017-06-02 03:09:53.047481: step 20320, loss = 0.72 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:09:53.938089: step 20330, loss = 0.78 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:09:54.823499: step 20340, loss = 0.84 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:09:55.709550: step 20350, loss = 0.90 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:09:56.596428: step 20360, loss = 0.79 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:09:57.487312: step 20370, loss = 0.78 (1436.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:09:58.365672: step 20380, loss = 0.74 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:09:59.257439: step 20390, loss = 0.89 (1435.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:10:00.255443: step 20400, loss = 0.81 (1282.5 examples/sec; 0.100 sec/batch)
2017-06-02 03:10:01.031712: step 20410, loss = 0.79 (1648.9 examples/sec; 0.078 sec/batch)
2017-06-02 03:10:01.922122: step 20420, loss = 0.93 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:10:02.810972: step 20430, loss = 0.86 (1440.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:10:03.693287: step 20440, loss = 0.77 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:10:04.583295: step 20450, loss = 0.85 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:10:05.473304: step 20460, loss = 0.90 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:10:06.351924: step 20470, loss = 0.83 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:10:07.237832: step 20480, loss = 0.67 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:10:08.135624: step 20490, loss = 0.82 (1425.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:10:09.113608: step 20500, loss = 0.74 (1308.8 examples/sec; 0.098 sec/batch)
2017-06-02 03:10:09.900196: step 20510, loss = 0.90 (1627.3 examples/sec; 0.079 sec/batch)
2017-06-02 03:10:10.793699: step 20520, loss = 0.72 (1432.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:10:11.672931: step 20530, loss = 0.68 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:10:12.554683: step 20540, loss = 0.80 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:10:13.431530: step 20550, loss = 0.81 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:10:14.312201: step 20560, loss = 0.87 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:10:15.203084: step 20570, loss = 0.77 (1436.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:10:16.111027: step 20580, loss = 0.88 (1409.8 examples/sec; 0.091 sec/batch)
2017-06-02 03:10:17.016831: step 20590, loss = 0.90 (1413.1 examples/sec; 0.091 sec/batch)
2017-06-02 03:10:18.015369: step 20600, loss = 0.99 (1281.9 examples/sec; 0.100 sec/batch)
2017-06-02 03:10:18.794787: step 20610, loss = 0.76 (1642.3 examples/sec; 0.078 sec/batch)
2017-06-02 03:10:19.678975: step 20620, loss = 0.71 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:10:20.562539: step 20630, loss = 0.96 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:10:21.454508: step 20640, loss = 0.67 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:10:22.339212: step 20650, loss = 0.98 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:10:23.228890: step 20660, loss = 0.93 (1438.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:10:24.123274: step 20670, loss = 0.72 (1431.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:10:25.010276: step 20680, loss = 0.78 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:10:25.914368: step 20690, loss = 0.73 (1415.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:10:26.909637: step 20700, loss = 0.94 (1286.1 examples/sec; 0.100 sec/batch)
2017-06-02 03:10:27.706494: step 20710, loss = 0.88 (1606.3 examples/sec; 0.080 sec/batch)
2017-06-02 03:10:28.618454: step 20720, loss = 1.02 (1403.6 examples/sec; 0.091 sec/batch)
2017-06-02 03:10:29.509504: step 20730, loss = 0.96 (1436.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:10:30.402989: step 20740, loss = 0.90 (1432.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:10:31.303011: step 20750, loss = 0.72 (1422.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:10:32.179426: step 20760, loss = 0.73 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:10:33.067061: step 20770, loss = 0.82 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:10:33.939110: step 20780, loss = 0.70 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:10:34.827351: step 20790, loss = 0.93 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:10:35.802201: step 20800, loss = 0.68 (1313.0 examples/sec; 0.097 sec/batch)
2017-06-02 03:10:36.599531: step 20810, loss = 0.71 (1605.4 examples/sec; 0.080 sec/batch)
2017-06-02 03:10:37.484115: step 20820, loss = 0.99 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:10:38.369906: step 20830, loss = 0.79 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:10:39.269405: step 20840, loss = 0.88 (1423.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:10:40.154990: step 20850, loss = 0.90 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:10:41.046450: step 20860, loss = 0.75 (1435.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:10:41.929458: step 20870, loss = 0.78 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:10:42.810791: step 20880, loss = 0.87 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:10:43.706358: step 20890, loss = 0.89 (1429.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:10:44.704204: step 20900, loss = 0.91 (1282.8 examples/sec; 0.100 sec/batch)
2017-06-02 03:10:45.491940: step 20910, loss = 0.86 (1624.9 examples/sec; 0.079 sec/batch)
2017-06-02 03:10:46.379425: step 20920, loss = 0.87 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:10:47.279128: step 20930, loss = 0.72 (1422.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:10:48.169740: step 20940, loss = 0.92 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:10:49.045381: step 20950, loss = 0.92 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:10:49.927862: step 20960, loss = 0.91 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:10:50.823352: step 20970, loss = 0.70 (1429.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:10:51.704064: step 20980, loss = 0.73 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:10:52.585013: step 20990, loss = 0.90 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:10:53.566028: step 21000, loss = 0.74 (1304.8 examples/sec; 0.098 sec/batch)
2017-06-02 03:10:54.361754: step 21010, loss = 0.71 (1608.6 examples/sec; 0.080 sec/batch)
2017-06-02 03:10:55.259496: step 21020, loss = 0.77 (1425.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:10:56.151679: step 21030, loss = 0.84 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:10:57.058202: step 21040, loss = 0.88 (1412.0 examples/sec; 0.091 sec/batch)
2017-06-02 03:10:57.950779: step 21050, loss = 0.73 (1434.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:10:58.834891: step 21060, loss = 1.04 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:10:59.727616: step 21070, loss = 0.72 (1433.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:11:00.627373: step 21080, loss = 0.74 (1422.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:11:01.516248: step 21090, loss = 0.94 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:11:02.507702: step 21100, loss = 1.07 (1291.0 examples/sec; 0.099 sec/batch)
2017-06-02 03:11:03.291571: step 21110, loss = 0.94 (1632.9 examples/sec; 0.078 sec/batch)
2017-06-02 03:11:04.175615: step 21120, loss = 0.80 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:11:05.057866: step 21130, loss = 0.85 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:11:05.938692: step 21140, loss = 0.95 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:11:06.820967: step 21150, loss = 0.90 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:11:07.718040: step 21160, loss = 0.98 (1426.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:11:08.608067: step 21170, loss = 0.62 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:11:09.491581: step 21180, loss = 0.78 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:11:10.375634: step 21190, loss = 0.86 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:11:11.369769: step 21200, loss = 0.77 (1287.6 examples/sec; 0.099 sec/batch)
2017-06-02 03:11:12.164550: step 21210, loss = 0.74 (1610.5 examples/sec; 0.079 sec/batch)
2017-06-02 03:11:13.055266: step 21220, loss = 0.66 (1437.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:11:13.942401: step 21230, loss = 0.86 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:11:14.833994: step 21240, loss = 0.77 (1435.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:11:15.723096: step 21250, loss = 0.68 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:11:16.608560: step 21260, loss = 1.00 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:11:17.505252: step 21270, loss = 1.04 (1427.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:11:18.382940: step 21280, loss = 0.89 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:11:19.262538: step 21290, loss = 0.61 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:11:20.272116: step 21300, loss = 0.91 (1267.9 examples/sec; 0.101 sec/batch)
2017-06-02 03:11:21.078239: step 21310, loss = 0.90 (1587.9 examples/sec; 0.081 sec/batch)
2017-06-02 03:11:21.954717: step 21320, loss = 0.72 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:11:22.850354: step 21330, loss = 0.83 (1429.1 examples/sec; 0.090 sec/batch)
2017-06-02 03:11:23.744106: step 21340, loss = 0.77 (1432.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:11:24.638766: step 21350, loss = 0.86 (1430.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:11:25.522989: step 21360, loss = 1.00 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:11:26.414306: step 21370, loss = 0.66 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:11:27.289848: step 21380, loss = 0.84 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:11:28.173812: step 21390, loss = 0.81 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:11:29.167536: step 21400, loss = 0.76 (1288.1 examples/sec; 0.099 sec/batch)
2017-06-02 03:11:29.958145: step 21410, loss = 0.85 (1619.0 examples/sec; 0.079 sec/batch)
2017-06-02 03:11:30.843656: step 21420, loss = 0.66 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:11:31.731670: step 21430, loss = 0.88 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:11:32.625700: step 21440, loss = 0.70 (1431.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:11:33.514213: step 21450, loss = 0.77 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:11:34.396706: step 21460, loss = 0.64 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:11:35.298292: step 21470, loss = 0.72 (1419.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:11:36.181010: step 21480, loss = 0.79 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:11:37.060509: step 21490, loss = 0.70 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:11:38.049625: step 21500, loss = 0.75 (1294.1 examples/sec; 0.099 sec/batch)
2017-06-02 03:11:38.842110: step 21510, loss = 0.65 (1615.2 examples/sec; 0.079 sec/batch)
2017-06-02 03:11:39.725467: step 21520, loss = 0.77 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:11:40.621791: step 21530, loss = 0.86 (1428.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:11:41.517805: step 21540, loss = 0.88 (1428.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:11:42.397496: step 21550, loss = 0.94 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:11:43.287659: step 21560, loss = 0.81 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:11:44.172704: step 21570, loss = 0.90 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:11:45.065872: step 21580, loss = 0.85 (1433.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:11:45.964802: step 21590, loss = 0.76 (1423.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:11:46.961675: step 21600, loss = 0.75 (1284.0 examples/sec; 0.100 sec/batch)
2017-06-02 03:11:47.735549: step 21610, loss = 1.01 (1654.0 examples/sec; 0.077 sec/batch)
2017-06-02 03:11:48.610354: step 21620, loss = 0.85 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:11:49.487729: step 21630, loss = 0.72 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:11:50.370506: step 21640, loss = 0.81 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:11:51.267010: step 21650, loss = 0.75 (1427.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:11:52.161174: step 21660, loss = 0.79 (1431.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:11:53.052265: step 21670, loss = 0.72 (1436.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:11:53.930608: step 21680, loss = 0.83 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:11:54.814944: step 21690, loss = 0.91 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:11:55.833864: step 21700, loss = 0.83 (1256.2 examples/sec; 0.102 sec/batch)
2017-06-02 03:11:56.621091: step 21710, loss = 0.76 (1626.0 examples/sec; 0.079 sec/batch)
2017-06-02 03:11:57.507642: step 21720, loss = 0.69 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:11:58.401841: step 21730, loss = 0.86 (1431.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:11:59.309797: step 21740, loss = 0.77 (1409.7 examples/sec; 0.091 sec/batch)
2017-06-02 03:12:00.195865: step 21750, loss = 0.66 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:01.086401: step 21760, loss = 0.76 (1437.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:01.978701: step 21770, loss = 0.82 (1434.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:02.868814: step 21780, loss = 0.79 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:03.763940: step 21790, loss = 0.78 (1430.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:12:04.753765: step 21800, loss = 0.81 (1293.2 examples/sec; 0.099 sec/batch)
2017-06-02 03:12:05.548712: step 21810, loss = 0.88 (1610.2 examples/sec; 0.079 sec/batch)
2017-06-02 03:12:06.424318: step 21820, loss = 0.84 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:12:07.326117: step 21830, loss = 0.66 (1419.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:12:08.217058: step 21840, loss = 0.87 (1436.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:09.110787: step 21850, loss = 0.70 (1432.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:10.004586: step 21860, loss = 0.61 (1432.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:10.896387: step 21870, loss = 0.57 (1435.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:11.783933: step 21880, loss = 0.73 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:12.672549: step 21890, loss = 0.74 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:13.663311: step 21900, loss = 0.86 (1291.9 examples/sec; 0.099 sec/batch)
2017-06-02 03:12:14.449032: step 21910, loss = 0.78 (1629.1 examples/sec; 0.079 sec/batch)
2017-06-02 03:12:15.334981: step 21920, loss = 0.82 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:16.222331: step 21930, loss = 0.78 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:17.120728: step 21940, loss = 0.93 (1424.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:12:18.010448: step 21950, loss = 0.87 (1438.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:18.883390: step 21960, loss = 0.85 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:12:19.778512: step 21970, loss = 0.76 (1430.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:12:20.669945: step 21980, loss = 0.85 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:21.551338: step 21990, loss = 0.76 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:12:22.530837: step 22000, loss = 0.95 (1306.8 examples/sec; 0.098 sec/batch)
2017-06-02 03:12:23.322454: step 22010, loss = 0.90 (1616.9 examples/sec; 0.079 sec/batch)
2017-06-02 03:12:24.206334: step 22020, loss = 0.86 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:12:25.101429: step 22030, loss = 0.86 (1430.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:12:25.987960: step 22040, loss = 0.69 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:26.876788: step 22050, loss = 0.76 (1440.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:27.764821: step 22060, loss = 0.74 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:28.655128: step 22070, loss = 0.85 (1437.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:29.542788: step 22080, loss = 0.77 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:30.431343: step 22090, loss = 0.78 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:31.430724: step 22100, loss = 0.81 (1280.8 examples/sec; 0.100 sec/batch)
2017-06-02 03:12:32.218572: step 22110, loss = 0.91 (1624.7 examples/sec; 0.079 sec/batch)
2017-06-02 03:12:33.110264: step 22120, loss = 0.93 (1435.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:33.984982: step 22130, loss = 0.78 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:12:34.879772: step 22140, loss = 0.99 (1430.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:35.750740: step 22150, loss = 0.81 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:12:36.651773: step 22160, loss = 0.83 (1420.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:12:37.531839: step 22170, loss = 0.95 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:12:38.414120: step 22180, loss = 0.94 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:12:39.314469: step 22190, loss = 0.70 (1421.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:12:40.297973: step 22200, loss = 0.83 (1301.5 examples/sec; 0.098 sec/batch)
2017-06-02 03:12:41.084039: step 22210, loss = 0.83 (1628.4 examples/sec; 0.079 sec/batch)
2017-06-02 03:12:41.974328: step 22220, loss = 0.72 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:42.858421: step 22230, loss = 0.80 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:12:43.759534: step 22240, loss = 0.77 (1420.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:12:44.660330: step 22250, loss = 1.16 (1421.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:12:45.542875: step 22260, loss = 0.92 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:12:46.434097: step 22270, loss = 0.89 (1436.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:47.311355: step 22280, loss = 0.92 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:12:48.196065: step 22290, loss = 0.89 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:12:49.180906: step 22300, loss = 0.91 (1299.7 examples/sec; 0.098 sec/batch)
2017-06-02 03:12:49.972715: step 22310, loss = 0.84 (1616.6 examples/sec; 0.079 sec/batch)
2017-06-02 03:12:50.858117: step 22320, loss = 0.74 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:51.751589: step 22330, loss = 0.81 (1432.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:52.634021: step 22340, loss = 0.77 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:12:53.528287: step 22350, loss = 0.76 (1431.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:54.415943: step 22360, loss = 0.84 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:55.299980: step 22370, loss = 0.84 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:12:56.202563: step 22380, loss = 0.82 (1418.1 examples/sec; 0.090 sec/batch)
2017-06-02 03:12:57.096024: step 22390, loss = 0.88 (1432.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:12:58.088616: step 22400, loss = 0.88 (1289.5 examples/sec; 0.099 sec/batch)
2017-06-02 03:12:58.864860: step 22410, loss = 0.77 (1649.0 examples/sec; 0.078 sec/batch)
2017-06-02 03:12:59.769320: step 22420, loss = 0.67 (1415.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:13:00.644965: step 22430, loss = 0.80 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:13:01.526453: step 22440, loss = 0.89 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:13:02.410177: step 22450, loss = 1.04 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:13:03.288171: step 22460, loss = 0.95 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:13:04.184833: step 22470, loss = 0.77 (1427.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:13:05.097273: step 22480, loss = 0.84 (1402.8 examples/sec; 0.091 sec/batch)
2017-06-02 03:13:05.980424: step 22490, loss = 0.80 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:13:06.972717: step 22500, loss = 0.93 (1289.9 examples/sec; 0.099 sec/batch)
2017-06-02 03:13:07.778067: step 22510, loss = 0.99 (1589.4 examples/sec; 0.081 sec/batch)
2017-06-02 03:13:08.681828: step 22520, loss = 0.77 (1416.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:13:09.583215: step 22530, loss = 0.65 (1420.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:13:10.478644: step 22540, loss = 0.77 (1429.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:13:11.387562: step 22550, loss = 0.92 (1408.2 examples/sec; 0.091 sec/batch)
2017-06-02 03:13:12.275067: step 22560, loss = 0.82 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:13:13.168985: step 22570, loss = 0.82 (1431.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:13:14.056058: step 22580, loss = 0.68 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:13:14.951864: step 22590, loss = 0.81 (1428.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:13:15.936854: step 22600, loss = 0.78 (1299.5 examples/sec; 0.098 sec/batch)
2017-06-02 03:13:16.727929: step 22610, loss = 0.77 (1618.0 examples/sec; 0.079 sec/batch)
2017-06-02 03:13:17.615347: step 22620, loss = 0.84 (1442.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:13:18.507033: step 22630, loss = 0.86 (1435.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:13:19.389711: step 22640, loss = 0.92 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:13:20.283616: step 22650, loss = 0.96 (1431.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:13:21.181477: step 22660, loss = 0.89 (1425.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:13:22.077171: step 22670, loss = 0.92 (1429.1 examples/sec; 0.090 sec/batch)
2017-06-02 03:13:22.973896: step 22680, loss = 0.84 (1427.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:13:23.881352: step 22690, loss = 0.78 (1410.5 examples/sec; 0.091 sec/batch)
2017-06-02 03:13:24.864291: step 22700, loss = 0.81 (1302.2 examples/sec; 0.098 sec/batch)
2017-06-02 03:13:25.662031: step 22710, loss = 0.77 (1604.5 examples/sec; 0.080 sec/batch)
2017-06-02 03:13:26.554486: step 22720, loss = 0.94 (1434.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:13:27.432405: step 22730, loss = 0.73 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:13:28.330890: step 22740, loss = 0.83 (1424.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:13:29.207601: step 22750, loss = 0.81 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:13:30.088760: step 22760, loss = 0.75 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:13:30.976514: step 22770, loss = 0.79 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:13:31.864076: step 22780, loss = 0.74 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:13:32.765888: step 22790, loss = 0.88 (1419.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:13:33.768427: step 22800, loss = 0.99 (1276.8 examples/sec; 0.100 sec/batch)
2017-06-02 03:13:34.553363: step 22810, loss = 0.69 (1630.7 examples/sec; 0.078 sec/batch)
2017-06-02 03:13:35.442025: step 22820, loss = 0.80 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:13:36.315237: step 22830, loss = 1.00 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:13:37.196673: step 22840, loss = 0.91 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:13:38.071309: step 22850, loss = 0.84 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:13:38.970175: step 22860, loss = 0.79 (1424.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:13:39.858671: step 22870, loss = 0.74 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:13:40.739867: step 22880, loss = 0.99 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:13:41.635934: step 22890, loss = 0.83 (1428.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:13:42.624235: step 22900, loss = 0.71 (1295.1 examples/sec; 0.099 sec/batch)
2017-06-02 03:13:43.407171: step 22910, loss = 0.76 (1634.9 examples/sec; 0.078 sec/batch)
2017-06-02 03:13:44.300387: step 22920, loss = 0.81 (1433.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:13:45.188529: step 22930, loss = 0.81 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:13:46.064891: step 22940, loss = 0.95 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:13:46.966252: step 22950, loss = 0.78 (1420.1 examples/sec; 0.090 sec/batch)
2017-06-02 03:13:47.853722: step 22960, loss = 0.89 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:13:48.737201: step 22970, loss = 0.60 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:13:49.626453: step 22980, loss = 0.85 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:13:50.508855: step 22990, loss = 0.77 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:13:51.494170: step 23000, loss = 0.98 (1299.1 examples/sec; 0.099 sec/batch)
2017-06-02 03:13:52.289706: step 23010, loss = 0.72 (1609.0 examples/sec; 0.080 sec/batch)
2017-06-02 03:13:53.180598: step 23020, loss = 0.81 (1436.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:13:54.071469: step 23030, loss = 0.98 (1436.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:13:54.950954: step 23040, loss = 0.97 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:13:55.840369: step 23050, loss = 0.81 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:13:56.737522: step 23060, loss = 0.66 (1426.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:13:57.623755: step 23070, loss = 0.72 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:13:58.500566: step 23080, loss = 0.86 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:13:59.381794: step 23090, loss = 0.76 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:14:00.378457: step 23100, loss = 0.81 (1284.3 examples/sec; 0.100 sec/batch)
2017-06-02 03:14:01.167631: step 23110, loss = 0.86 (1621.9 examples/sec; 0.079 sec/batch)
2017-06-02 03:14:02.046464: step 23120, loss = 0.79 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:14:02.935995: step 23130, loss = 0.85 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:14:03.822940: step 23140, loss = 0.80 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:14:04.734560: step 23150, loss = 0.76 (1404.1 examples/sec; 0.091 sec/batch)
2017-06-02 03:14:05.615251: step 23160, loss = 0.99 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:14:06.495406: step 23170, loss = 0.85 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:14:07.386783: step 23180, loss = 0.87 (1436.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:14:08.285545: step 23190, loss = 0.79 (1424.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:14:09.271348: step 23200, loss = 0.88 (1298.4 examples/sec; 0.099 sec/batch)
2017-06-02 03:14:10.061266: step 23210, loss = 0.82 (1620.4 examples/sec; 0.079 sec/batch)
2017-06-02 03:14:10.948251: step 23220, loss = 0.78 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:14:11.843776: step 23230, loss = 0.86 (1429.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:14:12.726960: step 23240, loss = 0.78 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:14:13.629131: step 23250, loss = 1.01 (1418.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:14:14.514451: step 23260, loss = 0.88 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:14:15.402238: step 23270, loss = 0.74 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:14:16.294186: step 23280, loss = 0.82 (1435.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:14:17.177172: step 23290, loss = 0.84 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:14:18.175456: step 23300, loss = 0.81 (1282.2 examples/sec; 0.100 sec/batch)
2017-06-02 03:14:18.967319: step 23310, loss = 0.72 (1616.4 examples/sec; 0.079 sec/batch)
2017-06-02 03:14:19.865654: step 23320, loss = 0.72 (1424.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:14:20.765258: step 23330, loss = 0.84 (1422.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:14:21.662531: step 23340, loss = 0.90 (1426.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:14:22.542813: step 23350, loss = 0.79 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:14:23.440136: step 23360, loss = 0.78 (1426.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:14:24.321830: step 23370, loss = 0.77 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:14:25.203277: step 23380, loss = 0.83 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:14:26.099365: step 23390, loss = 0.74 (1428.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:14:27.087804: step 23400, loss = 0.83 (1295.0 examples/sec; 0.099 sec/batch)
2017-06-02 03:14:27.880574: step 23410, loss = 0.94 (1614.6 examples/sec; 0.079 sec/batch)
2017-06-02 03:14:28.764687: step 23420, loss = 0.93 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:14:29.667650: step 23430, loss = 0.88 (1417.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:14:30.556244: step 23440, loss = 0.88 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:14:31.441454: step 23450, loss = 0.76 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:14:32.334202: step 23460, loss = 0.64 (1433.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:14:33.213924: step 23470, loss = 0.82 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:14:34.114798: step 23480, loss = 0.85 (1420.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:14:35.021158: step 23490, loss = 0.80 (1412.2 examples/sec; 0.091 sec/batch)
2017-06-02 03:14:35.997562: step 23500, loss = 0.81 (1310.9 examples/sec; 0.098 sec/batch)
2017-06-02 03:14:36.788149: step 23510, loss = 0.84 (1619.1 examples/sec; 0.079 sec/batch)
2017-06-02 03:14:37.675302: step 23520, loss = 0.93 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:14:38.561894: step 23530, loss = 0.78 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:14:39.451136: step 23540, loss = 0.93 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:14:40.326946: step 23550, loss = 0.65 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:14:41.207258: step 23560, loss = 0.97 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:14:42.093617: step 23570, loss = 0.86 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:14:43.003018: step 23580, loss = 0.78 (1407.6 examples/sec; 0.091 sec/batch)
2017-06-02 03:14:43.896032: step 23590, loss = 0.80 (1433.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:14:44.877974: step 23600, loss = 0.91 (1303.5 examples/sec; 0.098 sec/batch)
2017-06-02 03:14:45.676131: step 23610, loss = 0.92 (1603.7 examples/sec; 0.080 sec/batch)
2017-06-02 03:14:46.571350: step 23620, loss = 0.79 (1429.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:14:47.454831: step 23630, loss = 0.77 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:14:48.348725: step 23640, loss = 0.87 (1431.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:14:49.244408: step 23650, loss = 0.88 (1429.1 examples/sec; 0.090 sec/batch)
2017-06-02 03:14:50.125005: step 23660, loss = 0.75 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:14:51.019394: step 23670, loss = 0.77 (1431.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:14:51.916470: step 23680, loss = 0.75 (1426.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:14:52.814688: step 23690, loss = 0.63 (1425.1 examples/sec; 0.090 sec/batch)
2017-06-02 03:14:53.811747: step 23700, loss = 0.85 (1283.8 examples/sec; 0.100 sec/batch)
2017-06-02 03:14:54.603078: step 23710, loss = 0.70 (1617.5 examples/sec; 0.079 sec/batch)
2017-06-02 03:14:55.476988: step 23720, loss = 0.73 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:14:56.365458: step 23730, loss = 0.73 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:14:57.254094: step 23740, loss = 0.76 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:14:58.127953: step 23750, loss = 0.81 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:14:59.014308: step 23760, loss = 0.73 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:14:59.906704: step 23770, loss = 0.73 (1434.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:15:00.785004: step 23780, loss = 0.73 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:15:01.676327: step 23790, loss = 0.80 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:15:02.662580: step 23800, loss = 0.96 (1297.8 examples/sec; 0.099 sec/batch)
2017-06-02 03:15:03.470121: step 23810, loss = 0.76 (1585.1 examples/sec; 0.081 sec/batch)
2017-06-02 03:15:04.347505: step 23820, loss = 0.84 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:15:05.232220: step 23830, loss = 0.81 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:15:06.107078: step 23840, loss = 0.66 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:15:06.992178: step 23850, loss = 0.72 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:15:07.875659: step 23860, loss = 0.86 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:15:08.769333: step 23870, loss = 0.80 (1432.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:15:09.668586: step 23880, loss = 0.74 (1423.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:15:10.555101: step 23890, loss = 0.62 (1443.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:15:11.539304: step 23900, loss = 0.87 (1300.5 examples/sec; 0.098 sec/batch)
2017-06-02 03:15:12.334363: step 23910, loss = 0.80 (1609.9 examples/sec; 0.080 sec/batch)
2017-06-02 03:15:13.230080: step 23920, loss = 0.88 (1429.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:15:14.121967: step 23930, loss = 0.79 (1435.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:15:15.013406: step 23940, loss = 0.80 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:15:15.893369: step 23950, loss = 0.95 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:15:16.778075: step 23960, loss = 0.93 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:15:17.656553: step 23970, loss = 0.77 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:15:18.526956: step 23980, loss = 0.75 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:15:19.416209: step 23990, loss = 0.80 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:15:20.399217: step 24000, loss = 0.68 (1302.1 examples/sec; 0.098 sec/batch)
2017-06-02 03:15:21.192373: step 24010, loss = 0.81 (1613.8 examples/sec; 0.079 sec/batch)
2017-06-02 03:15:22.091362: step 24020, loss = 0.66 (1423.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:15:22.975816: step 24030, loss = 0.72 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:15:23.859064: step 24040, loss = 0.69 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:15:24.752079: step 24050, loss = 0.76 (1433.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:15:25.623344: step 24060, loss = 0.79 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:15:26.509862: step 24070, loss = 0.55 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:15:27.396326: step 24080, loss = 0.84 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:15:28.282804: step 24090, loss = 0.62 (1443.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:15:29.270571: step 24100, loss = 0.93 (1295.9 examples/sec; 0.099 sec/batch)
2017-06-02 03:15:30.059644: step 24110, loss = 0.76 (1622.1 examples/sec; 0.079 sec/batch)
2017-06-02 03:15:30.945771: step 24120, loss = 0.90 (1444.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:15:31.821632: step 24130, loss = 0.76 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:15:32.709624: step 24140, loss = 1.09 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:15:33.616707: step 24150, loss = 0.80 (1411.1 examples/sec; 0.091 sec/batch)
2017-06-02 03:15:34.505933: step 24160, loss = 0.64 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:15:35.388991: step 24170, loss = 0.79 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:15:36.277899: step 24180, loss = 0.62 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:15:37.167809: step 24190, loss = 0.87 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:15:38.139497: step 24200, loss = 0.81 (1317.3 examples/sec; 0.097 sec/batch)
2017-06-02 03:15:38.920742: step 24210, loss = 0.99 (1638.4 examples/sec; 0.078 sec/batch)
2017-06-02 03:15:39.806144: step 24220, loss = 0.71 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:15:40.694277: step 24230, loss = 0.85 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:15:41.571034: step 24240, loss = 0.91 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:15:42.458253: step 24250, loss = 0.84 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:15:43.354602: step 24260, loss = 0.88 (1428.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:15:44.256400: step 24270, loss = 0.83 (1419.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:15:45.143715: step 24280, loss = 0.84 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:15:46.042774: step 24290, loss = 0.80 (1423.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:15:47.024451: step 24300, loss = 0.85 (1303.9 examples/sec; 0.098 sec/batch)
2017-06-02 03:15:47.822444: step 24310, loss = 0.86 (1604.0 examples/sec; 0.080 sec/batch)
2017-06-02 03:15:48.707035: step 24320, loss = 0.98 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:15:49.589531: step 24330, loss = 0.69 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:15:50.473306: step 24340, loss = 0.82 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:15:51.355854: step 24350, loss = 0.95 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:15:52.244595: step 24360, loss = 0.85 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:15:53.138439: step 24370, loss = 0.88 (1432.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:15:54.014603: step 24380, loss = 0.77 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:15:54.906605: step 24390, loss = 0.91 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:15:55.905615: step 24400, loss = 0.73 (1281.3 examples/sec; 0.100 sec/batch)
2017-06-02 03:15:56.723216: step 24410, loss = 0.92 (1565.6 examples/sec; 0.082 sec/batch)
2017-06-02 03:15:57.618255: step 24420, loss = 0.77 (1430.1 examples/sec; 0.090 sec/batch)
2017-06-02 03:15:58.493941: step 24430, loss = 0.84 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:15:59.385253: step 24440, loss = 0.86 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:16:00.264208: step 24450, loss = 0.83 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:16:01.144036: step 24460, loss = 0.89 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:16:02.027274: step 24470, loss = 0.87 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:16:02.901836: step 24480, loss = 0.74 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:16:03.784462: step 24490, loss = 0.82 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:16:04.777629: step 24500, loss = 0.91 (1288.8 examples/sec; 0.099 sec/batch)
2017-06-02 03:16:05.552576: step 24510, loss = 0.86 (1651.7 examples/sec; 0.077 sec/batch)
2017-06-02 03:16:06.442267: step 24520, loss = 0.68 (1438.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:16:07.332858: step 24530, loss = 0.87 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:16:08.222382: step 24540, loss = 0.78 (1439.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:16:09.100985: step 24550, loss = 0.80 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:16:09.974246: step 24560, loss = 0.98 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:16:10.866329: step 24570, loss = 0.79 (1434.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:16:11.762921: step 24580, loss = 0.83 (1427.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:16:12.660273: step 24590, loss = 0.88 (1426.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:16:13.639942: step 24600, loss = 0.69 (1306.6 examples/sec; 0.098 sec/batch)
2017-06-02 03:16:14.424633: step 24610, loss = 0.71 (1631.2 examples/sec; 0.078 sec/batch)
2017-06-02 03:16:15.300953: step 24620, loss = 0.85 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:16:16.186406: step 24630, loss = 0.87 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:16:17.064359: step 24640, loss = 0.69 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:16:17.963054: step 24650, loss = 0.72 (1424.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:16:18.849536: step 24660, loss = 0.69 (1443.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:16:19.743775: step 24670, loss = 0.71 (1431.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:16:20.642112: step 24680, loss = 0.83 (1424.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:16:21.518928: step 24690, loss = 0.89 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:16:22.496496: step 24700, loss = 0.76 (1309.4 examples/sec; 0.098 sec/batch)
2017-06-02 03:16:23.292385: step 24710, loss = 0.96 (1608.3 examples/sec; 0.080 sec/batch)
2017-06-02 03:16:24.173011: step 24720, loss = 0.68 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:16:25.053968: step 24730, loss = 0.81 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:16:25.940217: step 24740, loss = 0.78 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:16:26.820162: step 24750, loss = 0.83 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:16:27.702639: step 24760, loss = 0.87 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:16:28.592480: step 24770, loss = 0.63 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:16:29.482791: step 24780, loss = 0.85 (1437.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:16:30.374041: step 24790, loss = 0.83 (1436.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:16:31.361141: step 24800, loss = 1.13 (1296.7 examples/sec; 0.099 sec/batch)
2017-06-02 03:16:32.146037: step 24810, loss = 0.82 (1630.8 examples/sec; 0.078 sec/batch)
2017-06-02 03:16:33.050518: step 24820, loss = 0.66 (1415.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:16:33.942712: step 24830, loss = 0.83 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:16:34.837855: step 24840, loss = 0.77 (1429.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:16:35.736187: step 24850, loss = 0.82 (1424.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:16:36.628088: step 24860, loss = 0.87 (1435.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:16:37.515987: step 24870, loss = 0.69 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:16:38.400879: step 24880, loss = 0.83 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:16:39.286628: step 24890, loss = 0.96 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:16:40.270544: step 24900, loss = 0.86 (1300.9 examples/sec; 0.098 sec/batch)
2017-06-02 03:16:41.056209: step 24910, loss = 0.65 (1629.2 examples/sec; 0.079 sec/batch)
2017-06-02 03:16:41.931047: step 24920, loss = 0.78 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:16:42.812092: step 24930, loss = 0.80 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:16:43.700640: step 24940, loss = 0.88 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:16:44.585361: step 24950, loss = 0.99 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:16:45.465499: step 24960, loss = 0.72 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:16:46.354629: step 24970, loss = 0.76 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:16:47.267777: step 24980, loss = 0.78 (1401.7 examples/sec; 0.091 sec/batch)
2017-06-02 03:16:48.147544: step 24990, loss = 0.77 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:16:49.135113: step 25000, loss = 0.78 (1296.1 examples/sec; 0.099 sec/batch)
2017-06-02 03:16:49.917883: step 25010, loss = 0.75 (1635.2 examples/sec; 0.078 sec/batch)
2017-06-02 03:16:50.812975: step 25020, loss = 0.82 (1430.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:16:51.697336: step 25030, loss = 0.67 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:16:52.589093: step 25040, loss = 0.96 (1435.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:16:53.468194: step 25050, loss = 0.99 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:16:54.354364: step 25060, loss = 0.65 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:16:55.237983: step 25070, loss = 0.68 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:16:56.126258: step 25080, loss = 0.83 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:16:57.020500: step 25090, loss = 0.77 (1431.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:16:58.019205: step 25100, loss = 0.87 (1281.7 examples/sec; 0.100 sec/batch)
2017-06-02 03:16:58.805520: step 25110, loss = 0.71 (1627.8 examples/sec; 0.079 sec/batch)
2017-06-02 03:16:59.697298: step 25120, loss = 0.86 (1435.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:17:00.584635: step 25130, loss = 0.68 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:17:01.464969: step 25140, loss = 1.02 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:17:02.349012: step 25150, loss = 0.80 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:17:03.243852: step 25160, loss = 0.74 (1430.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:17:04.135043: step 25170, loss = 0.83 (1436.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:17:05.011200: step 25180, loss = 0.75 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:17:05.884169: step 25190, loss = 0.91 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:17:06.871904: step 25200, loss = 0.73 (1295.9 examples/sec; 0.099 sec/batch)
2017-06-02 03:17:07.658646: step 25210, loss = 0.81 (1627.0 examples/sec; 0.079 sec/batch)
2017-06-02 03:17:08.541385: step 25220, loss = 0.64 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:17:09.422577: step 25230, loss = 0.86 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:17:10.303949: step 25240, loss = 0.81 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:17:11.186283: step 25250, loss = 0.78 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:17:12.067409: step 25260, loss = 0.76 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:17:12.947221: step 25270, loss = 0.64 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:17:13.840092: step 25280, loss = 0.88 (1433.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:17:14.725479: step 25290, loss = 0.78 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:17:15.723296: step 25300, loss = 0.72 (1282.8 examples/sec; 0.100 sec/batch)
2017-06-02 03:17:16.518912: step 25310, loss = 0.86 (1608.8 examples/sec; 0.080 sec/batch)
2017-06-02 03:17:17.404953: step 25320, loss = 0.92 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:17:18.283710: step 25330, loss = 0.83 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:17:19.183453: step 25340, loss = 0.76 (1422.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:17:20.072270: step 25350, loss = 0.84 (1440.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:17:20.954118: step 25360, loss = 0.77 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:17:21.837654: step 25370, loss = 0.72 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:17:22.722945: step 25380, loss = 1.02 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:17:23.613661: step 25390, loss = 0.82 (1437.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:17:24.599824: step 25400, loss = 0.72 (1298.0 examples/sec; 0.099 sec/batch)
2017-06-02 03:17:25.394259: step 25410, loss = 0.76 (1611.2 examples/sec; 0.079 sec/batch)
2017-06-02 03:17:26.270042: step 25420, loss = 0.96 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:17:27.158139: step 25430, loss = 0.86 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:17:28.042474: step 25440, loss = 0.88 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:17:28.933771: step 25450, loss = 0.68 (1436.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:17:29.820992: step 25460, loss = 0.80 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:17:30.702006: step 25470, loss = 0.85 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:17:31.576158: step 25480, loss = 0.83 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:17:32.462570: step 25490, loss = 0.87 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:17:33.445541: step 25500, loss = 0.83 (1302.2 examples/sec; 0.098 sec/batch)
2017-06-02 03:17:34.229792: step 25510, loss = 0.90 (1632.2 examples/sec; 0.078 sec/batch)
2017-06-02 03:17:35.108659: step 25520, loss = 0.64 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:17:35.996045: step 25530, loss = 0.76 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:17:36.884721: step 25540, loss = 0.75 (1440.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:17:37.765558: step 25550, loss = 0.82 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:17:38.659484: step 25560, loss = 0.73 (1431.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:17:39.560284: step 25570, loss = 0.80 (1420.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:17:40.434771: step 25580, loss = 0.89 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:17:41.315358: step 25590, loss = 0.76 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:17:42.288834: step 25600, loss = 0.89 (1314.9 examples/sec; 0.097 sec/batch)
2017-06-02 03:17:43.104184: step 25610, loss = 0.96 (1569.8 examples/sec; 0.082 sec/batch)
2017-06-02 03:17:44.003815: step 25620, loss = 0.87 (1422.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:17:44.896559: step 25630, loss = 0.71 (1433.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:17:45.782945: step 25640, loss = 0.79 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:17:46.664985: step 25650, loss = 0.85 (1451.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:17:47.543928: step 25660, loss = 0.90 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:17:48.418248: step 25670, loss = 0.89 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:17:49.308879: step 25680, loss = 0.92 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:17:50.182773: step 25690, loss = 0.86 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:17:51.160947: step 25700, loss = 0.85 (1308.5 examples/sec; 0.098 sec/batch)
2017-06-02 03:17:51.949727: step 25710, loss = 0.78 (1622.8 examples/sec; 0.079 sec/batch)
2017-06-02 03:17:52.833800: step 25720, loss = 0.72 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:17:53.724806: step 25730, loss = 0.73 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:17:54.603088: step 25740, loss = 0.77 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:17:55.503210: step 25750, loss = 0.61 (1422.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:17:56.380843: step 25760, loss = 0.81 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:17:57.283624: step 25770, loss = 0.78 (1417.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:17:58.161494: step 25780, loss = 0.71 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:17:59.049255: step 25790, loss = 0.72 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:18:00.035266: step 25800, loss = 0.62 (1298.2 examples/sec; 0.099 sec/batch)
2017-06-02 03:18:00.824493: step 25810, loss = 0.81 (1621.8 examples/sec; 0.079 sec/batch)
2017-06-02 03:18:01.700395: step 25820, loss = 0.70 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:18:02.596091: step 25830, loss = 0.78 (1429.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:18:03.495692: step 25840, loss = 0.68 (1422.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:18:04.384180: step 25850, loss = 0.79 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:18:05.268965: step 25860, loss = 0.72 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:18:06.152899: step 25870, loss = 0.87 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:18:07.032778: step 25880, loss = 0.79 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:18:07.914133: step 25890, loss = 0.77 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:18:08.913026: step 25900, loss = 0.65 (1281.4 examples/sec; 0.100 sec/batch)
2017-06-02 03:18:09.694521: step 25910, loss = 0.60 (1637.9 examples/sec; 0.078 sec/batch)
2017-06-02 03:18:10.577209: step 25920, loss = 0.75 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:18:11.457119: step 25930, loss = 0.79 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:18:12.333912: step 25940, loss = 0.77 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:18:13.212501: step 25950, loss = 0.86 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:18:14.091851: step 25960, loss = 0.87 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:18:14.979832: step 25970, loss = 0.88 (1441.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:18:15.860805: step 25980, loss = 0.79 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:18:16.749737: step 25990, loss = 0.80 (1439.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:18:17.732270: step 26000, loss = 0.71 (1302.7 examples/sec; 0.098 sec/batch)
2017-06-02 03:18:18.511329: step 26010, loss = 0.82 (1643.0 examples/sec; 0.078 sec/batch)
2017-06-02 03:18:19.392898: step 26020, loss = 0.67 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:18:20.284218: step 26030, loss = 0.74 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:18:21.178380: step 26040, loss = 0.86 (1431.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:18:22.066253: step 26050, loss = 0.73 (1441.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:18:22.941578: step 26060, loss = 0.86 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:18:23.835581: step 26070, loss = 0.85 (1431.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:18:24.728955: step 26080, loss = 0.76 (1432.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:18:25.618211: step 26090, loss = 0.92 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:18:26.592528: step 26100, loss = 0.80 (1313.7 examples/sec; 0.097 sec/batch)
2017-06-02 03:18:27.377893: step 26110, loss = 0.84 (1629.8 examples/sec; 0.079 sec/batch)
2017-06-02 03:18:28.266802: step 26120, loss = 0.86 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:18:29.150094: step 26130, loss = 0.86 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:18:30.027587: step 26140, loss = 0.85 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:18:30.926118: step 26150, loss = 0.75 (1424.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:18:31.815004: step 26160, loss = 0.72 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:18:32.715990: step 26170, loss = 0.79 (1420.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:18:33.598450: step 26180, loss = 0.84 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:18:34.479011: step 26190, loss = 0.71 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:18:35.463268: step 26200, loss = 0.74 (1300.5 examples/sec; 0.098 sec/batch)
2017-06-02 03:18:36.241861: step 26210, loss = 0.74 (1644.0 examples/sec; 0.078 sec/batch)
2017-06-02 03:18:37.122130: step 26220, loss = 0.73 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:18:38.017272: step 26230, loss = 0.67 (1429.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:18:38.902225: step 26240, loss = 0.95 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:18:39.788070: step 26250, loss = 0.80 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:18:40.683409: step 26260, loss = 0.83 (1429.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:18:41.565575: step 26270, loss = 0.95 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:18:42.447852: step 26280, loss = 0.88 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:18:43.334098: step 26290, loss = 0.69 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:18:44.311846: step 26300, loss = 0.73 (1309.1 examples/sec; 0.098 sec/batch)
2017-06-02 03:18:45.100098: step 26310, loss = 0.62 (1623.9 examples/sec; 0.079 sec/batch)
2017-06-02 03:18:45.992963: step 26320, loss = 0.79 (1433.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:18:46.881462: step 26330, loss = 0.76 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:18:47.757364: step 26340, loss = 0.89 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:18:48.643951: step 26350, loss = 0.87 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:18:49.538135: step 26360, loss = 0.74 (1431.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:18:50.425875: step 26370, loss = 0.81 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:18:51.302903: step 26380, loss = 0.64 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:18:52.180536: step 26390, loss = 0.94 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:18:53.167012: step 26400, loss = 0.69 (1297.5 examples/sec; 0.099 sec/batch)
2017-06-02 03:18:53.939414: step 26410, loss = 0.79 (1657.2 examples/sec; 0.077 sec/batch)
2017-06-02 03:18:54.824256: step 26420, loss = 0.86 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:18:55.714138: step 26430, loss = 0.87 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:18:56.593441: step 26440, loss = 0.85 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:18:57.482747: step 26450, loss = 0.94 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:18:58.371795: step 26460, loss = 0.61 (1439.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:18:59.260891: step 26470, loss = 0.86 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:19:00.144756: step 26480, loss = 0.76 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:19:01.018898: step 26490, loss = 0.67 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:19:02.010117: step 26500, loss = 0.91 (1291.3 examples/sec; 0.099 sec/batch)
2017-06-02 03:19:02.786513: step 26510, loss = 0.68 (1648.6 examples/sec; 0.078 sec/batch)
2017-06-02 03:19:03.668771: step 26520, loss = 0.67 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:19:04.547067: step 26530, loss = 0.87 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:19:05.439111: step 26540, loss = 0.83 (1434.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:19:06.319653: step 26550, loss = 0.84 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:19:07.208190: step 26560, loss = 0.92 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:19:08.098604: step 26570, loss = 0.92 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:19:08.988399: step 26580, loss = 0.64 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:19:09.876649: step 26590, loss = 0.79 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:19:10.862708: step 26600, loss = 0.72 (1298.1 examples/sec; 0.099 sec/batch)
2017-06-02 03:19:11.655232: step 26610, loss = 0.73 (1615.1 examples/sec; 0.079 sec/batch)
2017-06-02 03:19:12.540715: step 26620, loss = 0.55 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:19:13.436412: step 26630, loss = 0.73 (1429.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:19:14.311420: step 26640, loss = 0.77 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:19:15.197822: step 26650, loss = 0.85 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:19:16.077289: step 26660, loss = 0.68 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:19:16.953275: step 26670, loss = 0.75 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:19:17.833874: step 26680, loss = 0.79 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:19:18.714103: step 26690, loss = 0.90 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:19:19.704159: step 26700, loss = 0.63 (1292.9 examples/sec; 0.099 sec/batch)
2017-06-02 03:19:20.486681: step 26710, loss = 0.76 (1635.7 examples/sec; 0.078 sec/batch)
2017-06-02 03:19:21.362829: step 26720, loss = 0.83 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:19:22.252131: step 26730, loss = 0.71 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:19:23.147514: step 26740, loss = 0.85 (1429.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:19:24.029790: step 26750, loss = 0.89 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:19:24.916994: step 26760, loss = 0.67 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:19:25.805359: step 26770, loss = 0.72 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:19:26.678058: step 26780, loss = 0.74 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:19:27.555939: step 26790, loss = 0.85 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:19:28.531333: step 26800, loss = 0.87 (1312.3 examples/sec; 0.098 sec/batch)
2017-06-02 03:19:29.314413: step 26810, loss = 0.88 (1634.6 examples/sec; 0.078 sec/batch)
2017-06-02 03:19:30.207472: step 26820, loss = 0.89 (1433.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:19:31.091297: step 26830, loss = 0.75 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:19:31.967295: step 26840, loss = 0.81 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:19:32.858951: step 26850, loss = 0.96 (1435.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:19:33.746174: step 26860, loss = 0.74 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:19:34.619865: step 26870, loss = 0.78 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:19:35.498257: step 26880, loss = 0.93 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:19:36.384137: step 26890, loss = 0.97 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:19:37.364082: step 26900, loss = 0.67 (1306.2 examples/sec; 0.098 sec/batch)
2017-06-02 03:19:38.152208: step 26910, loss = 0.74 (1624.1 examples/sec; 0.079 sec/batch)
2017-06-02 03:19:39.035801: step 26920, loss = 0.90 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:19:39.917746: step 26930, loss = 0.73 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:19:40.809450: step 26940, loss = 0.80 (1435.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:19:41.689573: step 26950, loss = 0.68 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:19:42.573187: step 26960, loss = 0.76 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:19:43.468427: step 26970, loss = 0.73 (1429.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:19:44.351272: step 26980, loss = 0.67 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:19:45.242680: step 26990, loss = 0.72 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:19:46.225632: step 27000, loss = 0.77 (1302.2 examples/sec; 0.098 sec/batch)
2017-06-02 03:19:46.990820: step 27010, loss = 0.86 (1672.8 examples/sec; 0.077 sec/batch)
2017-06-02 03:19:47.876645: step 27020, loss = 0.75 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:19:48.761397: step 27030, loss = 0.90 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:19:49.656399: step 27040, loss = 0.74 (1430.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:19:50.529561: step 27050, loss = 0.67 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:19:51.484261: step 27060, loss = 0.82 (1340.8 examples/sec; 0.095 sec/batch)
2017-06-02 03:19:52.367374: step 27070, loss = 0.95 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:19:53.259691: step 27080, loss = 0.89 (1434.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:19:54.134220: step 27090, loss = 0.75 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:19:55.109553: step 27100, loss = 0.80 (1312.4 examples/sec; 0.098 sec/batch)
2017-06-02 03:19:55.919077: step 27110, loss = 0.94 (1581.2 examples/sec; 0.081 sec/batch)
2017-06-02 03:19:56.795171: step 27120, loss = 0.78 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:19:57.684599: step 27130, loss = 0.80 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:19:58.562862: step 27140, loss = 0.76 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:19:59.454487: step 27150, loss = 0.87 (1435.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:20:00.336110: step 27160, loss = 0.64 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:20:01.210209: step 27170, loss = 0.85 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:20:02.091350: step 27180, loss = 0.82 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:20:02.987060: step 27190, loss = 0.83 (1429.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:20:03.969580: step 27200, loss = 0.71 (1302.8 examples/sec; 0.098 sec/batch)
2017-06-02 03:20:04.767851: step 27210, loss = 0.78 (1603.5 examples/sec; 0.080 sec/batch)
2017-06-02 03:20:05.641674: step 27220, loss = 0.81 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:20:06.520322: step 27230, loss = 0.74 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:20:07.399393: step 27240, loss = 0.75 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:20:08.282963: step 27250, loss = 0.75 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:20:09.186791: step 27260, loss = 0.80 (1416.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:20:10.073006: step 27270, loss = 0.93 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:20:10.956831: step 27280, loss = 0.81 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:20:11.831839: step 27290, loss = 0.82 (1462.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:20:12.814694: step 27300, loss = 0.69 (1302.3 examples/sec; 0.098 sec/batch)
2017-06-02 03:20:13.606147: step 27310, loss = 0.81 (1617.3 examples/sec; 0.079 sec/batch)
2017-06-02 03:20:14.495368: step 27320, loss = 0.79 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:20:15.384670: step 27330, loss = 0.81 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:20:16.284411: step 27340, loss = 0.77 (1422.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:20:17.176892: step 27350, loss = 0.73 (1434.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:20:18.049357: step 27360, loss = 0.93 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:20:18.932929: step 27370, loss = 0.89 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:20:19.821594: step 27380, loss = 0.63 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:20:20.714942: step 27390, loss = 0.75 (1432.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:20:21.706370: step 27400, loss = 0.82 (1291.1 examples/sec; 0.099 sec/batch)
2017-06-02 03:20:22.495274: step 27410, loss = 0.85 (1622.5 examples/sec; 0.079 sec/batch)
2017-06-02 03:20:23.379540: step 27420, loss = 0.81 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:20:24.276802: step 27430, loss = 0.77 (1426.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:20:25.164978: step 27440, loss = 0.69 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:20:26.040993: step 27450, loss = 0.90 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:20:26.938060: step 27460, loss = 0.85 (1426.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:20:27.827500: step 27470, loss = 0.75 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:20:28.715653: step 27480, loss = 0.90 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:20:29.597009: step 27490, loss = 0.76 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:20:30.573255: step 27500, loss = 0.86 (1311.1 examples/sec; 0.098 sec/batch)
2017-06-02 03:20:31.385304: step 27510, loss = 0.94 (1576.3 examples/sec; 0.081 sec/batch)
2017-06-02 03:20:32.277801: step 27520, loss = 0.80 (1434.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:20:33.168059: step 27530, loss = 0.73 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:20:34.067742: step 27540, loss = 0.75 (1422.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:20:34.948070: step 27550, loss = 0.93 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:20:35.850224: step 27560, loss = 0.74 (1418.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:20:36.743307: step 27570, loss = 0.82 (1433.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:20:37.633954: step 27580, loss = 0.82 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:20:38.524454: step 27590, loss = 0.71 (1437.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:20:39.505878: step 27600, loss = 0.77 (1304.2 examples/sec; 0.098 sec/batch)
2017-06-02 03:20:40.292548: step 27610, loss = 0.69 (1627.1 examples/sec; 0.079 sec/batch)
2017-06-02 03:20:41.176652: step 27620, loss = 0.82 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:20:42.063194: step 27630, loss = 0.93 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:20:42.952950: step 27640, loss = 0.70 (1438.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:20:43.840150: step 27650, loss = 0.72 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:20:44.730307: step 27660, loss = 0.84 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:20:45.610486: step 27670, loss = 0.69 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:20:46.484323: step 27680, loss = 0.80 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:20:47.372749: step 27690, loss = 0.71 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:20:48.350926: step 27700, loss = 0.88 (1308.5 examples/sec; 0.098 sec/batch)
2017-06-02 03:20:49.148346: step 27710, loss = 0.58 (1605.2 examples/sec; 0.080 sec/batch)
2017-06-02 03:20:50.030242: step 27720, loss = 0.74 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:20:50.915776: step 27730, loss = 0.94 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:20:51.809385: step 27740, loss = 0.78 (1432.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:20:52.698769: step 27750, loss = 0.81 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:20:53.586095: step 27760, loss = 0.74 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:20:54.479089: step 27770, loss = 0.81 (1433.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:20:55.359262: step 27780, loss = 0.92 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:20:56.251789: step 27790, loss = 0.76 (1434.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:20:57.232677: step 27800, loss = 0.96 (1305.0 examples/sec; 0.098 sec/batch)
2017-06-02 03:20:58.011928: step 27810, loss = 0.85 (1642.6 examples/sec; 0.078 sec/batch)
2017-06-02 03:20:58.899521: step 27820, loss = 0.83 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:20:59.799230: step 27830, loss = 0.72 (1422.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:21:00.684251: step 27840, loss = 0.76 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:21:01.568807: step 27850, loss = 0.85 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:21:02.464814: step 27860, loss = 0.74 (1428.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:21:03.341250: step 27870, loss = 0.69 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:21:04.219744: step 27880, loss = 0.90 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:21:05.106673: step 27890, loss = 0.81 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:21:06.108918: step 27900, loss = 0.73 (1277.1 examples/sec; 0.100 sec/batch)
2017-06-02 03:21:06.900213: step 27910, loss = 0.65 (1617.6 examples/sec; 0.079 sec/batch)
2017-06-02 03:21:07.812294: step 27920, loss = 0.83 (1403.4 examples/sec; 0.091 sec/batch)
2017-06-02 03:21:08.723377: step 27930, loss = 0.65 (1404.9 examples/sec; 0.091 sec/batch)
2017-06-02 03:21:09.620462: step 27940, loss = 0.82 (1426.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:21:10.504856: step 27950, loss = 0.92 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:21:11.399454: step 27960, loss = 0.73 (1430.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:21:12.281011: step 27970, loss = 0.84 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:21:13.167714: step 27980, loss = 0.63 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:21:14.047150: step 27990, loss = 0.71 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:21:15.031996: step 28000, loss = 0.96 (1299.7 examples/sec; 0.098 sec/batch)
2017-06-02 03:21:15.819156: step 28010, loss = 0.67 (1626.1 examples/sec; 0.079 sec/batch)
2017-06-02 03:21:16.706688: step 28020, loss = 0.92 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:21:17.604561: step 28030, loss = 0.63 (1425.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:21:18.483496: step 28040, loss = 0.86 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:21:19.374241: step 28050, loss = 0.67 (1437.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:21:20.240225: step 28060, loss = 0.89 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:21:21.118109: step 28070, loss = 0.87 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:21:21.996803: step 28080, loss = 0.78 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:21:22.875476: step 28090, loss = 0.90 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:21:23.887910: step 28100, loss = 0.72 (1264.3 examples/sec; 0.101 sec/batch)
2017-06-02 03:21:24.670789: step 28110, loss = 0.88 (1635.0 examples/sec; 0.078 sec/batch)
2017-06-02 03:21:25.547282: step 28120, loss = 0.81 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:21:26.424071: step 28130, loss = 0.74 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:21:27.306183: step 28140, loss = 1.02 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:21:28.198576: step 28150, loss = 0.67 (1434.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:21:29.077159: step 28160, loss = 0.76 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:21:29.950226: step 28170, loss = 0.77 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:21:30.821418: step 28180, loss = 0.58 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:21:31.706174: step 28190, loss = 0.85 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:21:32.683799: step 28200, loss = 0.67 (1309.3 examples/sec; 0.098 sec/batch)
2017-06-02 03:21:33.474611: step 28210, loss = 0.76 (1618.6 examples/sec; 0.079 sec/batch)
2017-06-02 03:21:34.360202: step 28220, loss = 0.93 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:21:35.241698: step 28230, loss = 0.84 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:21:36.130947: step 28240, loss = 0.68 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:21:37.018444: step 28250, loss = 0.78 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:21:37.906819: step 28260, loss = 0.73 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:21:38.788638: step 28270, loss = 0.81 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:21:39.669802: step 28280, loss = 0.96 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:21:40.563744: step 28290, loss = 0.77 (1431.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:21:41.540216: step 28300, loss = 0.77 (1310.8 examples/sec; 0.098 sec/batch)
2017-06-02 03:21:42.321483: step 28310, loss = 0.72 (1638.4 examples/sec; 0.078 sec/batch)
2017-06-02 03:21:43.224383: step 28320, loss = 0.91 (1417.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:21:44.108004: step 28330, loss = 0.78 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:21:44.998564: step 28340, loss = 0.74 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:21:45.895156: step 28350, loss = 0.63 (1427.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:21:46.777977: step 28360, loss = 0.78 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:21:47.671881: step 28370, loss = 0.91 (1431.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:21:48.552167: step 28380, loss = 0.78 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:21:49.439711: step 28390, loss = 0.88 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:21:50.443310: step 28400, loss = 0.82 (1275.4 examples/sec; 0.100 sec/batch)
2017-06-02 03:21:51.208777: step 28410, loss = 0.75 (1672.2 examples/sec; 0.077 sec/batch)
2017-06-02 03:21:52.100821: step 28420, loss = 0.78 (1434.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:21:52.973059: step 28430, loss = 0.74 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:21:53.860435: step 28440, loss = 0.81 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:21:54.736062: step 28450, loss = 0.71 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:21:55.631507: step 28460, loss = 0.65 (1429.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:21:56.514006: step 28470, loss = 0.71 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:21:57.397429: step 28480, loss = 0.82 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:21:58.285609: step 28490, loss = 0.89 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:21:59.262227: step 28500, loss = 0.86 (1310.7 examples/sec; 0.098 sec/batch)
2017-06-02 03:22:00.051425: step 28510, loss = 0.69 (1621.9 examples/sec; 0.079 sec/batch)
2017-06-02 03:22:00.926342: step 28520, loss = 0.81 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:22:01.812071: step 28530, loss = 0.81 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:22:02.698064: step 28540, loss = 0.82 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:22:03.598059: step 28550, loss = 0.71 (1422.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:22:04.482729: step 28560, loss = 0.76 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:22:05.360142: step 28570, loss = 0.72 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:22:06.244726: step 28580, loss = 0.68 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:22:07.134040: step 28590, loss = 0.75 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:22:08.125129: step 28600, loss = 0.75 (1291.5 examples/sec; 0.099 sec/batch)
2017-06-02 03:22:08.913568: step 28610, loss = 0.72 (1623.4 examples/sec; 0.079 sec/batch)
2017-06-02 03:22:09.801120: step 28620, loss = 0.81 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:22:10.678072: step 28630, loss = 0.79 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:22:11.574203: step 28640, loss = 0.69 (1428.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:22:12.458680: step 28650, loss = 0.81 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:22:13.354478: step 28660, loss = 0.86 (1428.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:22:14.233985: step 28670, loss = 0.71 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:22:15.114816: step 28680, loss = 0.90 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:22:16.018597: step 28690, loss = 0.79 (1416.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:22:16.993362: step 28700, loss = 0.81 (1313.1 examples/sec; 0.097 sec/batch)
2017-06-02 03:22:17.778777: step 28710, loss = 0.90 (1629.7 examples/sec; 0.079 sec/batch)
2017-06-02 03:22:18.665608: step 28720, loss = 0.68 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:22:19.563425: step 28730, loss = 1.01 (1425.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:22:20.447599: step 28740, loss = 0.85 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:22:21.319330: step 28750, loss = 0.82 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:22:22.192078: step 28760, loss = 0.76 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:22:23.079276: step 28770, loss = 1.01 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:22:23.975310: step 28780, loss = 0.83 (1428.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:22:24.863009: step 28790, loss = 0.84 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:22:25.848728: step 28800, loss = 0.87 (1298.5 examples/sec; 0.099 sec/batch)
2017-06-02 03:22:26.646715: step 28810, loss = 0.75 (1604.0 examples/sec; 0.080 sec/batch)
2017-06-02 03:22:27.528272: step 28820, loss = 0.72 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:22:28.420637: step 28830, loss = 0.78 (1434.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:22:29.312727: step 28840, loss = 0.77 (1434.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:22:30.198204: step 28850, loss = 0.68 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:22:31.079996: step 28860, loss = 0.69 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:22:31.964427: step 28870, loss = 0.91 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:22:32.849284: step 28880, loss = 0.87 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:22:33.738219: step 28890, loss = 0.75 (1439.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:22:34.722507: step 28900, loss = 0.74 (1300.4 examples/sec; 0.098 sec/batch)
2017-06-02 03:22:35.528199: step 28910, loss = 0.84 (1588.7 examples/sec; 0.081 sec/batch)
2017-06-02 03:22:36.409022: step 28920, loss = 0.82 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:22:37.289640: step 28930, loss = 0.85 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:22:38.182315: step 28940, loss = 0.61 (1433.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:22:39.070084: step 28950, loss = 0.74 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:22:39.954392: step 28960, loss = 0.71 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:22:40.834665: step 28970, loss = 0.77 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:22:41.728697: step 28980, loss = 0.89 (1431.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:22:42.614681: step 28990, loss = 0.62 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:22:43.597197: step 29000, loss = 0.80 (1302.8 examples/sec; 0.098 sec/batch)
2017-06-02 03:22:44.398294: step 29010, loss = 0.90 (1597.8 examples/sec; 0.080 sec/batch)
2017-06-02 03:22:45.290461: step 29020, loss = 0.79 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:22:46.169438: step 29030, loss = 0.76 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:22:47.041441: step 29040, loss = 0.71 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:22:47.926099: step 29050, loss = 0.67 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:22:48.825342: step 29060, loss = 0.86 (1423.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:22:49.718794: step 29070, loss = 0.71 (1432.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:22:50.606646: step 29080, loss = 0.68 (1441.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:22:51.484261: step 29090, loss = 1.00 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:22:52.482394: step 29100, loss = 0.82 (1282.4 examples/sec; 0.100 sec/batch)
2017-06-02 03:22:53.249455: step 29110, loss = 0.75 (1668.7 examples/sec; 0.077 sec/batch)
2017-06-02 03:22:54.132265: step 29120, loss = 0.89 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:22:55.024154: step 29130, loss = 1.07 (1435.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:22:55.906065: step 29140, loss = 0.93 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:22:56.788352: step 29150, loss = 0.79 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:22:57.669742: step 29160, loss = 0.73 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:22:58.553050: step 29170, loss = 0.90 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:22:59.422321: step 29180, loss = 0.96 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:23:00.318560: step 29190, loss = 0.79 (1428.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:23:01.307492: step 29200, loss = 0.77 (1294.3 examples/sec; 0.099 sec/batch)
2017-06-02 03:23:02.089281: step 29210, loss = 0.75 (1637.3 examples/sec; 0.078 sec/batch)
2017-06-02 03:23:02.980558: step 29220, loss = 0.72 (1436.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:23:03.868545: step 29230, loss = 0.82 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:23:04.766503: step 29240, loss = 0.69 (1425.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:23:05.640238: step 29250, loss = 0.86 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:23:06.520836: step 29260, loss = 0.76 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:23:07.404460: step 29270, loss = 0.85 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:23:08.288978: step 29280, loss = 0.59 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:23:09.160873: step 29290, loss = 0.85 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:23:10.137914: step 29300, loss = 0.82 (1310.1 examples/sec; 0.098 sec/batch)
2017-06-02 03:23:10.928753: step 29310, loss = 0.76 (1618.5 examples/sec; 0.079 sec/batch)
2017-06-02 03:23:11.817791: step 29320, loss = 0.78 (1439.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:23:12.701233: step 29330, loss = 0.84 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:23:13.590635: step 29340, loss = 0.84 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:23:14.470036: step 29350, loss = 0.74 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:23:15.352200: step 29360, loss = 0.76 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:23:16.251725: step 29370, loss = 0.86 (1423.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:23:17.145692: step 29380, loss = 0.73 (1431.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:23:18.018624: step 29390, loss = 0.87 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:23:19.003866: step 29400, loss = 0.83 (1299.2 examples/sec; 0.099 sec/batch)
2017-06-02 03:23:19.789560: step 29410, loss = 0.73 (1629.1 examples/sec; 0.079 sec/batch)
2017-06-02 03:23:20.679155: step 29420, loss = 0.81 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:23:21.582063: step 29430, loss = 0.84 (1417.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:23:22.470287: step 29440, loss = 0.86 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:23:23.361304: step 29450, loss = 0.83 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:23:24.246439: step 29460, loss = 0.71 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:23:25.132178: step 29470, loss = 0.84 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:23:26.021407: step 29480, loss = 0.94 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:23:26.900385: step 29490, loss = 0.78 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:23:27.885369: step 29500, loss = 0.91 (1299.5 examples/sec; 0.098 sec/batch)
2017-06-02 03:23:28.668971: step 29510, loss = 0.84 (1633.5 examples/sec; 0.078 sec/batch)
2017-06-02 03:23:29.552011: step 29520, loss = 0.77 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:23:30.430483: step 29530, loss = 0.95 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:23:31.326965: step 29540, loss = 0.67 (1427.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:23:32.211823: step 29550, loss = 0.69 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:23:33.089850: step 29560, loss = 0.76 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:23:33.972003: step 29570, loss = 0.83 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:23:34.874790: step 29580, loss = 0.78 (1417.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:23:35.767314: step 29590, loss = 0.73 (1434.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:23:36.742288: step 29600, loss = 0.90 (1312.9 examples/sec; 0.097 sec/batch)
2017-06-02 03:23:37.533126: step 29610, loss = 0.98 (1618.6 examples/sec; 0.079 sec/batch)
2017-06-02 03:23:38.422506: step 29620, loss = 0.62 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:23:39.316702: step 29630, loss = 0.88 (1431.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:23:40.210158: step 29640, loss = 0.88 (1432.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:23:41.117938: step 29650, loss = 0.64 (1410.0 examples/sec; 0.091 sec/batch)
2017-06-02 03:23:41.997790: step 29660, loss = 0.75 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:23:42.873311: step 29670, loss = 0.90 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:23:43.773108: step 29680, loss = 0.60 (1422.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:23:44.671970: step 29690, loss = 0.71 (1424.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:23:45.652409: step 29700, loss = 0.81 (1305.5 examples/sec; 0.098 sec/batch)
2017-06-02 03:23:46.441260: step 29710, loss = 0.72 (1622.6 examples/sec; 0.079 sec/batch)
2017-06-02 03:23:47.321576: step 29720, loss = 0.79 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:23:48.222114: step 29730, loss = 0.82 (1421.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:23:49.115003: step 29740, loss = 0.72 (1433.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:23:49.989397: step 29750, loss = 0.83 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:23:50.885771: step 29760, loss = 0.82 (1428.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:23:51.781323: step 29770, loss = 0.83 (1429.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:23:52.661405: step 29780, loss = 0.71 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:23:53.553889: step 29790, loss = 0.71 (1434.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:23:54.540767: step 29800, loss = 0.81 (1297.0 examples/sec; 0.099 sec/batch)
2017-06-02 03:23:55.316035: step 29810, loss = 0.75 (1651.1 examples/sec; 0.078 sec/batch)
2017-06-02 03:23:56.217537: step 29820, loss = 0.90 (1419.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:23:57.092533: step 29830, loss = 0.64 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:23:57.976451: step 29840, loss = 0.84 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:23:58.856812: step 29850, loss = 0.79 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:23:59.740798: step 29860, loss = 0.73 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:24:00.617097: step 29870, loss = 0.88 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:24:01.503326: step 29880, loss = 0.72 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:24:02.372594: step 29890, loss = 0.85 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:24:03.350435: step 29900, loss = 0.82 (1309.0 examples/sec; 0.098 sec/batch)
2017-06-02 03:24:04.137331: step 29910, loss = 0.73 (1626.6 examples/sec; 0.079 sec/batch)
2017-06-02 03:24:05.009849: step 29920, loss = 0.80 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:24:05.892597: step 29930, loss = 0.99 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:24:06.778211: step 29940, loss = 0.79 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:24:07.663951: step 29950, loss = 0.74 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:24:08.548948: step 29960, loss = 0.84 (1446.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:24:09.430056: step 29970, loss = 0.74 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:24:10.309246: step 29980, loss = 1.00 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:24:11.200563: step 29990, loss = 0.91 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:24:12.208495: step 30000, loss = 0.71 (1269.9 examples/sec; 0.101 sec/batch)
2017-06-02 03:24:12.980645: step 30010, loss = 0.81 (1657.7 examples/sec; 0.077 sec/batch)
2017-06-02 03:24:13.855412: step 30020, loss = 0.84 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:24:14.733717: step 30030, loss = 0.72 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:24:15.620146: step 30040, loss = 0.89 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:24:16.489276: step 30050, loss = 0.61 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:24:17.384477: step 30060, loss = 0.83 (1429.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:24:18.259703: step 30070, loss = 0.78 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:24:19.146588: step 30080, loss = 0.85 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:24:20.023459: step 30090, loss = 0.60 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:24:21.007599: step 30100, loss = 0.78 (1300.6 examples/sec; 0.098 sec/batch)
2017-06-02 03:24:21.791118: step 30110, loss = 0.71 (1633.7 examples/sec; 0.078 sec/batch)
2017-06-02 03:24:22.674447: step 30120, loss = 0.77 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:24:23.566791: step 30130, loss = 0.79 (1434.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:24:24.440366: step 30140, loss = 0.76 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:24:25.320287: step 30150, loss = 0.75 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:24:26.201118: step 30160, loss = 0.87 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:24:27.091208: step 30170, loss = 0.78 (1438.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:24:27.975321: step 30180, loss = 0.97 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:24:28.861286: step 30190, loss = 0.62 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:24:29.840124: step 30200, loss = 0.72 (1307.7 examples/sec; 0.098 sec/batch)
2017-06-02 03:24:30.623277: step 30210, loss = 0.77 (1634.4 examples/sec; 0.078 sec/batch)
2017-06-02 03:24:31.503640: step 30220, loss = 0.76 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:24:32.381890: step 30230, loss = 0.81 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:24:33.271287: step 30240, loss = 0.85 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:24:34.157981: step 30250, loss = 0.81 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:24:35.039127: step 30260, loss = 0.73 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:24:35.913815: step 30270, loss = 0.92 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:24:36.800717: step 30280, loss = 0.90 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:24:37.692019: step 30290, loss = 0.91 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:24:38.677212: step 30300, loss = 0.77 (1299.2 examples/sec; 0.099 sec/batch)
2017-06-02 03:24:39.445542: step 30310, loss = 0.88 (1666.0 examples/sec; 0.077 sec/batch)
2017-06-02 03:24:40.318861: step 30320, loss = 0.80 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:24:41.209834: step 30330, loss = 0.79 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:24:42.083577: step 30340, loss = 0.78 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:24:42.953813: step 30350, loss = 0.84 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:24:43.837125: step 30360, loss = 0.89 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:24:44.724435: step 30370, loss = 0.86 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:24:45.599612: step 30380, loss = 0.76 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:24:46.471382: step 30390, loss = 0.92 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:24:47.439973: step 30400, loss = 0.87 (1321.5 examples/sec; 0.097 sec/batch)
2017-06-02 03:24:48.233280: step 30410, loss = 0.64 (1613.5 examples/sec; 0.079 sec/batch)
2017-06-02 03:24:49.130437: step 30420, loss = 0.68 (1426.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:24:50.012847: step 30430, loss = 0.75 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:24:50.891599: step 30440, loss = 0.55 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:24:51.766346: step 30450, loss = 0.78 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:24:52.652672: step 30460, loss = 0.81 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:24:53.535607: step 30470, loss = 0.78 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:24:54.410020: step 30480, loss = 0.85 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:24:55.284874: step 30490, loss = 0.73 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:24:56.262138: step 30500, loss = 0.76 (1309.8 examples/sec; 0.098 sec/batch)
2017-06-02 03:24:57.036994: step 30510, loss = 0.80 (1651.9 examples/sec; 0.077 sec/batch)
2017-06-02 03:24:57.919638: step 30520, loss = 0.95 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:24:58.797999: step 30530, loss = 0.79 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:24:59.674652: step 30540, loss = 0.68 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:00.547817: step 30550, loss = 0.84 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:25:01.444014: step 30560, loss = 0.68 (1428.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:25:02.333615: step 30570, loss = 0.65 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:25:03.225103: step 30580, loss = 0.69 (1435.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:25:04.113993: step 30590, loss = 0.70 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:25:05.095125: step 30600, loss = 0.69 (1304.6 examples/sec; 0.098 sec/batch)
2017-06-02 03:25:05.873073: step 30610, loss = 0.98 (1645.4 examples/sec; 0.078 sec/batch)
2017-06-02 03:25:06.754332: step 30620, loss = 0.99 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:07.632760: step 30630, loss = 0.75 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:08.531232: step 30640, loss = 0.66 (1424.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:25:09.408697: step 30650, loss = 0.77 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:10.311550: step 30660, loss = 0.82 (1417.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:25:11.195341: step 30670, loss = 0.75 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:12.089391: step 30680, loss = 0.79 (1431.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:25:12.987624: step 30690, loss = 0.81 (1425.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:25:13.968638: step 30700, loss = 0.70 (1304.8 examples/sec; 0.098 sec/batch)
2017-06-02 03:25:14.753352: step 30710, loss = 0.73 (1631.2 examples/sec; 0.078 sec/batch)
2017-06-02 03:25:15.634614: step 30720, loss = 0.73 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:16.513113: step 30730, loss = 0.89 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:17.392385: step 30740, loss = 0.82 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:18.271368: step 30750, loss = 0.72 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:19.155223: step 30760, loss = 0.78 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:20.029921: step 30770, loss = 0.94 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:25:20.916490: step 30780, loss = 0.77 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:25:21.800989: step 30790, loss = 0.77 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:22.780625: step 30800, loss = 0.80 (1306.6 examples/sec; 0.098 sec/batch)
2017-06-02 03:25:23.570417: step 30810, loss = 0.82 (1620.7 examples/sec; 0.079 sec/batch)
2017-06-02 03:25:24.444230: step 30820, loss = 0.75 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:25:25.315759: step 30830, loss = 0.80 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:25:26.205799: step 30840, loss = 0.90 (1438.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:25:27.097038: step 30850, loss = 0.75 (1436.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:25:27.975945: step 30860, loss = 0.87 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:28.869710: step 30870, loss = 0.85 (1432.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:25:29.753727: step 30880, loss = 0.77 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:30.625039: step 30890, loss = 0.66 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:25:31.606056: step 30900, loss = 0.83 (1304.8 examples/sec; 0.098 sec/batch)
2017-06-02 03:25:32.400127: step 30910, loss = 0.95 (1611.9 examples/sec; 0.079 sec/batch)
2017-06-02 03:25:33.274817: step 30920, loss = 0.78 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:25:34.148871: step 30930, loss = 0.78 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:25:35.035805: step 30940, loss = 0.89 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:25:35.912974: step 30950, loss = 0.81 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:36.793937: step 30960, loss = 0.69 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:37.671746: step 30970, loss = 0.90 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:38.551525: step 30980, loss = 0.82 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:39.431182: step 30990, loss = 0.77 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:40.417561: step 31000, loss = 0.76 (1297.7 examples/sec; 0.099 sec/batch)
2017-06-02 03:25:41.207350: step 31010, loss = 0.77 (1620.7 examples/sec; 0.079 sec/batch)
2017-06-02 03:25:42.106370: step 31020, loss = 0.91 (1423.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:25:42.987093: step 31030, loss = 0.78 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:43.863530: step 31040, loss = 0.64 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:44.751523: step 31050, loss = 0.76 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:25:45.637260: step 31060, loss = 0.85 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:25:46.514128: step 31070, loss = 0.84 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:47.398445: step 31080, loss = 0.94 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:48.278648: step 31090, loss = 0.82 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:49.263191: step 31100, loss = 0.67 (1300.1 examples/sec; 0.098 sec/batch)
2017-06-02 03:25:50.052681: step 31110, loss = 0.68 (1621.3 examples/sec; 0.079 sec/batch)
2017-06-02 03:25:50.941941: step 31120, loss = 0.90 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:25:51.820480: step 31130, loss = 0.68 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:52.702149: step 31140, loss = 0.90 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:53.592485: step 31150, loss = 0.74 (1437.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:25:54.472447: step 31160, loss = 0.77 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:55.334509: step 31170, loss = 0.76 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 03:25:56.211256: step 31180, loss = 0.83 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:57.095876: step 31190, loss = 0.84 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:25:58.068546: step 31200, loss = 0.68 (1316.0 examples/sec; 0.097 sec/batch)
2017-06-02 03:25:58.840275: step 31210, loss = 0.82 (1658.6 examples/sec; 0.077 sec/batch)
2017-06-02 03:25:59.728948: step 31220, loss = 0.55 (1440.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:26:00.613858: step 31230, loss = 0.65 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:26:01.490288: step 31240, loss = 0.75 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:26:02.364749: step 31250, loss = 0.76 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:26:03.244515: step 31260, loss = 0.77 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:26:04.123947: step 31270, loss = 0.73 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:26:04.998682: step 31280, loss = 0.80 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:26:05.870477: step 31290, loss = 0.74 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:26:06.865675: step 31300, loss = 0.75 (1286.2 examples/sec; 0.100 sec/batch)
2017-06-02 03:26:07.643022: step 31310, loss = 0.83 (1646.6 examples/sec; 0.078 sec/batch)
2017-06-02 03:26:08.526901: step 31320, loss = 0.92 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:26:09.401361: step 31330, loss = 0.73 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:26:10.279282: step 31340, loss = 0.88 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:26:11.166267: step 31350, loss = 0.77 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:26:12.044139: step 31360, loss = 0.71 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:26:12.929159: step 31370, loss = 0.87 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:26:13.804127: step 31380, loss = 0.75 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:26:14.674455: step 31390, loss = 0.82 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:26:15.657286: step 31400, loss = 0.78 (1302.4 examples/sec; 0.098 sec/batch)
2017-06-02 03:26:16.453772: step 31410, loss = 0.79 (1607.1 examples/sec; 0.080 sec/batch)
2017-06-02 03:26:17.329819: step 31420, loss = 0.79 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:26:18.203474: step 31430, loss = 0.68 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:26:19.078749: step 31440, loss = 0.74 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:26:19.952745: step 31450, loss = 1.00 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:26:20.854058: step 31460, loss = 0.72 (1420.1 examples/sec; 0.090 sec/batch)
2017-06-02 03:26:21.741969: step 31470, loss = 0.77 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:26:22.618369: step 31480, loss = 0.79 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:26:23.497666: step 31490, loss = 0.74 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:26:24.472494: step 31500, loss = 0.80 (1313.0 examples/sec; 0.097 sec/batch)
2017-06-02 03:26:25.256632: step 31510, loss = 0.73 (1632.4 examples/sec; 0.078 sec/batch)
2017-06-02 03:26:26.126313: step 31520, loss = 0.78 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:26:26.991269: step 31530, loss = 0.87 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 03:26:27.863399: step 31540, loss = 0.71 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:26:28.746467: step 31550, loss = 0.81 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:26:29.620398: step 31560, loss = 0.98 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:26:30.500662: step 31570, loss = 0.89 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:26:31.377168: step 31580, loss = 0.74 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:26:32.270309: step 31590, loss = 0.66 (1433.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:26:33.239164: step 31600, loss = 0.86 (1321.1 examples/sec; 0.097 sec/batch)
2017-06-02 03:26:34.032605: step 31610, loss = 0.77 (1613.2 examples/sec; 0.079 sec/batch)
2017-06-02 03:26:34.917298: step 31620, loss = 0.70 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:26:35.793121: step 31630, loss = 0.95 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:26:36.683985: step 31640, loss = 0.77 (1436.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:26:37.572777: step 31650, loss = 0.67 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:26:38.443474: step 31660, loss = 0.84 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:26:39.318925: step 31670, loss = 0.82 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:26:40.195076: step 31680, loss = 0.69 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:26:41.076480: step 31690, loss = 0.78 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:26:42.056140: step 31700, loss = 0.89 (1306.6 examples/sec; 0.098 sec/batch)
2017-06-02 03:26:42.850090: step 31710, loss = 0.86 (1612.2 examples/sec; 0.079 sec/batch)
2017-06-02 03:26:43.716086: step 31720, loss = 0.76 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:26:44.584270: step 31730, loss = 0.64 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:26:45.468438: step 31740, loss = 0.78 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:26:46.345920: step 31750, loss = 0.81 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:26:47.222211: step 31760, loss = 0.92 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:26:48.096747: step 31770, loss = 0.79 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:26:48.978075: step 31780, loss = 0.89 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:26:49.858889: step 31790, loss = 0.72 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:26:50.834757: step 31800, loss = 0.74 (1311.7 examples/sec; 0.098 sec/batch)
2017-06-02 03:26:51.592612: step 31810, loss = 0.78 (1689.0 examples/sec; 0.076 sec/batch)
2017-06-02 03:26:52.469410: step 31820, loss = 0.92 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:26:53.343163: step 31830, loss = 0.75 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:26:54.223964: step 31840, loss = 0.71 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:26:55.110251: step 31850, loss = 0.67 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:26:55.981799: step 31860, loss = 0.87 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:26:56.868283: step 31870, loss = 0.70 (1443.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:26:57.738988: step 31880, loss = 0.73 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:26:58.615390: step 31890, loss = 0.67 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:26:59.593247: step 31900, loss = 0.94 (1309.0 examples/sec; 0.098 sec/batch)
2017-06-02 03:27:00.367205: step 31910, loss = 0.74 (1653.8 examples/sec; 0.077 sec/batch)
2017-06-02 03:27:01.254515: step 31920, loss = 0.71 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:27:02.130921: step 31930, loss = 0.77 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:27:03.020252: step 31940, loss = 0.84 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:27:03.899659: step 31950, loss = 0.72 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:27:04.765659: step 31960, loss = 0.83 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:27:05.670304: step 31970, loss = 0.94 (1414.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:27:06.547175: step 31980, loss = 0.93 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:27:07.427702: step 31990, loss = 0.79 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:27:08.400689: step 32000, loss = 0.73 (1315.5 examples/sec; 0.097 sec/batch)
2017-06-02 03:27:09.179013: step 32010, loss = 0.84 (1644.6 examples/sec; 0.078 sec/batch)
2017-06-02 03:27:10.064503: step 32020, loss = 0.94 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:27:10.935166: step 32030, loss = 0.70 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:27:11.812666: step 32040, loss = 0.73 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:27:12.693175: step 32050, loss = 0.71 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:27:13.573268: step 32060, loss = 0.93 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:27:14.456961: step 32070, loss = 0.73 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:27:15.352103: step 32080, loss = 0.74 (1429.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:27:16.244178: step 32090, loss = 0.78 (1434.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:27:17.220671: step 32100, loss = 0.76 (1310.8 examples/sec; 0.098 sec/batch)
2017-06-02 03:27:18.008746: step 32110, loss = 0.74 (1624.2 examples/sec; 0.079 sec/batch)
2017-06-02 03:27:18.878408: step 32120, loss = 0.74 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:27:19.737816: step 32130, loss = 0.76 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 03:27:20.615777: step 32140, loss = 0.80 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:27:21.505595: step 32150, loss = 0.74 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:27:22.386815: step 32160, loss = 0.70 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:27:23.259508: step 32170, loss = 0.86 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:27:24.127797: step 32180, loss = 0.70 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:27:25.012886: step 32190, loss = 0.78 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:27:25.987809: step 32200, loss = 0.71 (1312.9 examples/sec; 0.097 sec/batch)
2017-06-02 03:27:26.765011: step 32210, loss = 0.77 (1646.9 examples/sec; 0.078 sec/batch)
2017-06-02 03:27:27.643183: step 32220, loss = 0.92 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:27:28.524458: step 32230, loss = 0.75 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:27:29.398364: step 32240, loss = 0.78 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:27:30.265899: step 32250, loss = 0.89 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:27:31.146637: step 32260, loss = 0.84 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:27:32.027370: step 32270, loss = 0.76 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:27:32.893401: step 32280, loss = 0.76 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:27:33.776089: step 32290, loss = 0.81 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:27:34.758827: step 32300, loss = 0.98 (1302.5 examples/sec; 0.098 sec/batch)
2017-06-02 03:27:35.559096: step 32310, loss = 0.79 (1599.5 examples/sec; 0.080 sec/batch)
2017-06-02 03:27:36.456203: step 32320, loss = 0.70 (1426.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:27:37.348878: step 32330, loss = 0.62 (1433.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:27:38.219778: step 32340, loss = 0.64 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:27:39.095995: step 32350, loss = 0.93 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:27:39.979450: step 32360, loss = 1.01 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:27:40.840348: step 32370, loss = 0.91 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 03:27:41.720180: step 32380, loss = 0.80 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:27:42.596924: step 32390, loss = 0.81 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:27:43.596168: step 32400, loss = 0.86 (1281.0 examples/sec; 0.100 sec/batch)
2017-06-02 03:27:44.384548: step 32410, loss = 0.75 (1623.6 examples/sec; 0.079 sec/batch)
2017-06-02 03:27:45.263706: step 32420, loss = 0.73 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:27:46.156924: step 32430, loss = 0.69 (1433.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:27:47.045041: step 32440, loss = 0.66 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:27:47.925944: step 32450, loss = 0.67 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:27:48.794559: step 32460, loss = 0.63 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:27:49.667670: step 32470, loss = 0.71 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:27:50.548030: step 32480, loss = 0.72 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:27:51.441420: step 32490, loss = 0.72 (1432.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:27:52.429040: step 32500, loss = 0.64 (1296.0 examples/sec; 0.099 sec/batch)
2017-06-02 03:27:53.223360: step 32510, loss = 0.86 (1611.4 examples/sec; 0.079 sec/batch)
2017-06-02 03:27:54.114123: step 32520, loss = 0.82 (1437.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:27:54.998676: step 32530, loss = 0.79 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:27:55.885646: step 32540, loss = 0.83 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:27:56.765652: step 32550, loss = 0.81 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:27:57.648709: step 32560, loss = 0.74 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:27:58.528694: step 32570, loss = 0.72 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:27:59.410863: step 32580, loss = 0.80 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:00.289226: step 32590, loss = 0.72 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:01.258711: step 32600, loss = 0.75 (1320.3 examples/sec; 0.097 sec/batch)
2017-06-02 03:28:02.047478: step 32610, loss = 0.86 (1622.8 examples/sec; 0.079 sec/batch)
2017-06-02 03:28:02.925592: step 32620, loss = 0.75 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:03.809030: step 32630, loss = 0.72 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:04.693273: step 32640, loss = 0.78 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:05.571957: step 32650, loss = 0.78 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:06.455802: step 32660, loss = 0.84 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:07.339355: step 32670, loss = 0.81 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:08.219138: step 32680, loss = 0.74 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:09.103279: step 32690, loss = 0.75 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:10.070461: step 32700, loss = 0.81 (1323.4 examples/sec; 0.097 sec/batch)
2017-06-02 03:28:10.852713: step 32710, loss = 0.80 (1636.3 examples/sec; 0.078 sec/batch)
2017-06-02 03:28:11.734083: step 32720, loss = 0.72 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:12.623803: step 32730, loss = 0.80 (1438.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:28:13.508777: step 32740, loss = 0.68 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:14.377019: step 32750, loss = 0.71 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:28:15.253067: step 32760, loss = 0.69 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:16.124659: step 32770, loss = 0.71 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:28:17.011046: step 32780, loss = 0.68 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:28:17.891289: step 32790, loss = 0.88 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:18.870373: step 32800, loss = 0.74 (1307.3 examples/sec; 0.098 sec/batch)
2017-06-02 03:28:19.645749: step 32810, loss = 0.66 (1650.8 examples/sec; 0.078 sec/batch)
2017-06-02 03:28:20.523320: step 32820, loss = 0.93 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:21.392762: step 32830, loss = 0.76 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:28:22.259439: step 32840, loss = 0.58 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:28:23.167797: step 32850, loss = 0.68 (1409.1 examples/sec; 0.091 sec/batch)
2017-06-02 03:28:24.043570: step 32860, loss = 0.88 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:24.920760: step 32870, loss = 0.75 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:25.798864: step 32880, loss = 0.67 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:26.690820: step 32890, loss = 0.80 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:28:27.654761: step 32900, loss = 0.81 (1327.9 examples/sec; 0.096 sec/batch)
2017-06-02 03:28:28.424027: step 32910, loss = 0.84 (1663.9 examples/sec; 0.077 sec/batch)
2017-06-02 03:28:29.303170: step 32920, loss = 0.72 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:30.186915: step 32930, loss = 0.83 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:31.059797: step 32940, loss = 0.92 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:28:31.937225: step 32950, loss = 0.80 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:32.813626: step 32960, loss = 0.91 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:33.688079: step 32970, loss = 0.83 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:28:34.557585: step 32980, loss = 0.79 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:28:35.436087: step 32990, loss = 0.64 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:36.402227: step 33000, loss = 0.72 (1324.9 examples/sec; 0.097 sec/batch)
2017-06-02 03:28:37.181454: step 33010, loss = 0.74 (1642.7 examples/sec; 0.078 sec/batch)
2017-06-02 03:28:38.060114: step 33020, loss = 0.80 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:38.933334: step 33030, loss = 0.80 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:28:39.809564: step 33040, loss = 0.74 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:40.672924: step 33050, loss = 0.83 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 03:28:41.549691: step 33060, loss = 0.66 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:42.425022: step 33070, loss = 0.61 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:43.303774: step 33080, loss = 0.64 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:44.187891: step 33090, loss = 0.69 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:45.166894: step 33100, loss = 0.83 (1307.5 examples/sec; 0.098 sec/batch)
2017-06-02 03:28:45.949987: step 33110, loss = 0.88 (1634.5 examples/sec; 0.078 sec/batch)
2017-06-02 03:28:46.829620: step 33120, loss = 0.75 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:47.723237: step 33130, loss = 0.80 (1432.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:28:48.608491: step 33140, loss = 0.76 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:28:49.498749: step 33150, loss = 0.68 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:28:50.378319: step 33160, loss = 0.80 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:51.245604: step 33170, loss = 0.69 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:28:52.125019: step 33180, loss = 0.77 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:53.004905: step 33190, loss = 0.76 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:53.988193: step 33200, loss = 0.86 (1301.8 examples/sec; 0.098 sec/batch)
2017-06-02 03:28:54.756019: step 33210, loss = 0.82 (1667.0 examples/sec; 0.077 sec/batch)
2017-06-02 03:28:55.636183: step 33220, loss = 0.84 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:56.512332: step 33230, loss = 0.89 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:57.386349: step 33240, loss = 0.78 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:28:58.264523: step 33250, loss = 0.77 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:28:59.137522: step 33260, loss = 0.77 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:29:00.028634: step 33270, loss = 0.77 (1436.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:29:00.906979: step 33280, loss = 1.00 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:29:01.791933: step 33290, loss = 0.81 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:29:02.765839: step 33300, loss = 0.75 (1314.3 examples/sec; 0.097 sec/batch)
2017-06-02 03:29:03.539380: step 33310, loss = 0.85 (1654.7 examples/sec; 0.077 sec/batch)
2017-06-02 03:29:04.425574: step 33320, loss = 0.74 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:29:05.315268: step 33330, loss = 0.72 (1438.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:29:06.188952: step 33340, loss = 0.73 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:29:07.065946: step 33350, loss = 0.77 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:29:07.949590: step 33360, loss = 0.83 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:29:08.830010: step 33370, loss = 0.84 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:29:09.700933: step 33380, loss = 0.80 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:29:10.564404: step 33390, loss = 0.95 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 03:29:11.533969: step 33400, loss = 0.74 (1320.2 examples/sec; 0.097 sec/batch)
2017-06-02 03:29:12.302517: step 33410, loss = 0.66 (1665.5 examples/sec; 0.077 sec/batch)
2017-06-02 03:29:13.181037: step 33420, loss = 0.79 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:29:14.051773: step 33430, loss = 0.77 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:29:14.937498: step 33440, loss = 0.71 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:29:15.807241: step 33450, loss = 0.90 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:29:16.693287: step 33460, loss = 0.75 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:29:17.582736: step 33470, loss = 0.94 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:29:18.462078: step 33480, loss = 0.83 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:29:19.341420: step 33490, loss = 0.67 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:29:20.327366: step 33500, loss = 0.66 (1298.2 examples/sec; 0.099 sec/batch)
2017-06-02 03:29:21.113577: step 33510, loss = 0.83 (1628.1 examples/sec; 0.079 sec/batch)
2017-06-02 03:29:21.994998: step 33520, loss = 0.86 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:29:22.872273: step 33530, loss = 0.71 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:29:23.757654: step 33540, loss = 0.93 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:29:24.635159: step 33550, loss = 0.65 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:29:25.510427: step 33560, loss = 0.83 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:29:26.391984: step 33570, loss = 0.81 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:29:27.274401: step 33580, loss = 0.73 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:29:28.151713: step 33590, loss = 0.62 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:29:29.141059: step 33600, loss = 0.97 (1293.8 examples/sec; 0.099 sec/batch)
2017-06-02 03:29:29.910935: step 33610, loss = 0.81 (1662.6 examples/sec; 0.077 sec/batch)
2017-06-02 03:29:30.795558: step 33620, loss = 0.83 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:29:31.674754: step 33630, loss = 0.83 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:29:32.544764: step 33640, loss = 0.79 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:29:33.447531: step 33650, loss = 0.71 (1417.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:29:34.330299: step 33660, loss = 0.71 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:29:35.207708: step 33670, loss = 0.67 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:29:36.095465: step 33680, loss = 0.75 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:29:36.974837: step 33690, loss = 0.76 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:29:37.946116: step 33700, loss = 0.55 (1317.9 examples/sec; 0.097 sec/batch)
2017-06-02 03:29:38.727721: step 33710, loss = 0.88 (1637.7 examples/sec; 0.078 sec/batch)
2017-06-02 03:29:39.618754: step 33720, loss = 0.79 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:29:40.515263: step 33730, loss = 0.82 (1427.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:29:41.412117: step 33740, loss = 0.70 (1427.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:29:42.294030: step 33750, loss = 0.65 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:29:43.160172: step 33760, loss = 0.75 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:29:44.030908: step 33770, loss = 0.74 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:29:44.927979: step 33780, loss = 0.77 (1426.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:29:45.815852: step 33790, loss = 0.77 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:29:46.788034: step 33800, loss = 0.77 (1316.6 examples/sec; 0.097 sec/batch)
2017-06-02 03:29:47.587718: step 33810, loss = 0.87 (1600.6 examples/sec; 0.080 sec/batch)
2017-06-02 03:29:48.459386: step 33820, loss = 0.68 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:29:49.354870: step 33830, loss = 1.13 (1429.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:29:50.236059: step 33840, loss = 0.68 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:29:51.167736: step 33850, loss = 0.67 (1373.9 examples/sec; 0.093 sec/batch)
2017-06-02 03:29:52.046401: step 33860, loss = 0.73 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:29:52.913575: step 33870, loss = 0.85 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:29:53.798402: step 33880, loss = 0.64 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:29:54.691414: step 33890, loss = 0.79 (1433.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:29:55.675497: step 33900, loss = 0.69 (1300.7 examples/sec; 0.098 sec/batch)
2017-06-02 03:29:56.459609: step 33910, loss = 0.72 (1632.4 examples/sec; 0.078 sec/batch)
2017-06-02 03:29:57.344275: step 33920, loss = 0.90 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:29:58.216097: step 33930, loss = 0.91 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:29:59.104316: step 33940, loss = 0.84 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:29:59.992434: step 33950, loss = 0.90 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:30:00.881258: step 33960, loss = 0.88 (1440.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:30:01.777285: step 33970, loss = 0.68 (1428.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:30:02.668512: step 33980, loss = 0.64 (1436.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:30:03.550419: step 33990, loss = 0.69 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:30:04.524276: step 34000, loss = 0.78 (1314.4 examples/sec; 0.097 sec/batch)
2017-06-02 03:30:05.321692: step 34010, loss = 0.63 (1605.2 examples/sec; 0.080 sec/batch)
2017-06-02 03:30:06.205461: step 34020, loss = 0.72 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:30:07.099873: step 34030, loss = 0.72 (1431.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:30:07.979391: step 34040, loss = 0.84 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:30:08.858081: step 34050, loss = 0.77 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:30:09.739401: step 34060, loss = 0.86 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:30:10.619037: step 34070, loss = 0.76 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:30:11.519497: step 34080, loss = 0.80 (1421.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:30:12.427057: step 34090, loss = 0.75 (1410.4 examples/sec; 0.091 sec/batch)
2017-06-02 03:30:13.441614: step 34100, loss = 0.69 (1261.6 examples/sec; 0.101 sec/batch)
2017-06-02 03:30:14.219363: step 34110, loss = 0.71 (1645.8 examples/sec; 0.078 sec/batch)
2017-06-02 03:30:15.111775: step 34120, loss = 0.89 (1434.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:30:16.015394: step 34130, loss = 0.65 (1416.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:30:16.899015: step 34140, loss = 0.70 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:30:17.785058: step 34150, loss = 0.81 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:30:18.679571: step 34160, loss = 0.74 (1430.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:30:19.557126: step 34170, loss = 0.72 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:30:20.446026: step 34180, loss = 0.79 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:30:21.331572: step 34190, loss = 0.99 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:30:22.332380: step 34200, loss = 0.84 (1279.0 examples/sec; 0.100 sec/batch)
2017-06-02 03:30:23.124993: step 34210, loss = 0.66 (1614.9 examples/sec; 0.079 sec/batch)
2017-06-02 03:30:24.015160: step 34220, loss = 0.74 (1437.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:30:24.916740: step 34230, loss = 0.82 (1419.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:30:25.831700: step 34240, loss = 0.87 (1399.0 examples/sec; 0.091 sec/batch)
2017-06-02 03:30:26.706432: step 34250, loss = 0.81 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:30:27.594879: step 34260, loss = 0.98 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:30:28.481082: step 34270, loss = 0.68 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:30:29.369858: step 34280, loss = 0.66 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:30:30.258239: step 34290, loss = 0.81 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:30:31.277393: step 34300, loss = 0.67 (1255.9 examples/sec; 0.102 sec/batch)
2017-06-02 03:30:32.061759: step 34310, loss = 0.77 (1631.9 examples/sec; 0.078 sec/batch)
2017-06-02 03:30:32.946795: step 34320, loss = 0.73 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:30:33.849708: step 34330, loss = 0.84 (1417.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:30:34.737290: step 34340, loss = 0.88 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:30:35.632004: step 34350, loss = 0.66 (1430.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:30:36.533254: step 34360, loss = 0.67 (1420.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:30:37.449463: step 34370, loss = 0.71 (1397.1 examples/sec; 0.092 sec/batch)
2017-06-02 03:30:38.345110: step 34380, loss = 0.69 (1429.1 examples/sec; 0.090 sec/batch)
2017-06-02 03:30:39.243560: step 34390, loss = 0.74 (1424.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:30:40.230003: step 34400, loss = 0.77 (1297.6 examples/sec; 0.099 sec/batch)
2017-06-02 03:30:41.038132: step 34410, loss = 0.75 (1583.9 examples/sec; 0.081 sec/batch)
2017-06-02 03:30:41.941425: step 34420, loss = 0.74 (1417.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:30:42.834055: step 34430, loss = 0.83 (1434.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:30:43.721758: step 34440, loss = 0.74 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:30:44.620273: step 34450, loss = 0.83 (1424.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:30:45.511625: step 34460, loss = 0.79 (1436.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:30:46.401957: step 34470, loss = 0.77 (1437.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:30:47.291468: step 34480, loss = 0.65 (1439.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:30:48.179778: step 34490, loss = 0.86 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:30:49.175615: step 34500, loss = 0.82 (1285.3 examples/sec; 0.100 sec/batch)
2017-06-02 03:30:49.964409: step 34510, loss = 0.78 (1622.7 examples/sec; 0.079 sec/batch)
2017-06-02 03:30:50.842028: step 34520, loss = 0.93 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:30:51.764144: step 34530, loss = 0.69 (1388.1 examples/sec; 0.092 sec/batch)
2017-06-02 03:30:52.688060: step 34540, loss = 0.82 (1385.4 examples/sec; 0.092 sec/batch)
2017-06-02 03:30:53.584488: step 34550, loss = 0.80 (1427.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:30:54.485261: step 34560, loss = 0.64 (1421.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:30:55.404738: step 34570, loss = 0.74 (1392.1 examples/sec; 0.092 sec/batch)
2017-06-02 03:30:56.315106: step 34580, loss = 0.71 (1406.0 examples/sec; 0.091 sec/batch)
2017-06-02 03:30:57.216289: step 34590, loss = 0.70 (1420.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:30:58.216061: step 34600, loss = 0.80 (1280.3 examples/sec; 0.100 sec/batch)
2017-06-02 03:30:59.029708: step 34610, loss = 0.82 (1573.2 examples/sec; 0.081 sec/batch)
2017-06-02 03:30:59.941197: step 34620, loss = 0.91 (1404.3 examples/sec; 0.091 sec/batch)
2017-06-02 03:31:00.845343: step 34630, loss = 0.84 (1415.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:31:01.735886: step 34640, loss = 0.87 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:31:02.655696: step 34650, loss = 0.63 (1391.6 examples/sec; 0.092 sec/batch)
2017-06-02 03:31:03.580071: step 34660, loss = 0.77 (1384.7 examples/sec; 0.092 sec/batch)
2017-06-02 03:31:04.475456: step 34670, loss = 0.77 (1429.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:31:05.379041: step 34680, loss = 0.76 (1416.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:31:06.274307: step 34690, loss = 0.71 (1429.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:31:07.267943: step 34700, loss = 0.69 (1288.2 examples/sec; 0.099 sec/batch)
2017-06-02 03:31:08.094943: step 34710, loss = 0.90 (1547.7 examples/sec; 0.083 sec/batch)
2017-06-02 03:31:08.996556: step 34720, loss = 0.81 (1419.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:31:09.912527: step 34730, loss = 0.81 (1397.4 examples/sec; 0.092 sec/batch)
2017-06-02 03:31:10.827461: step 34740, loss = 0.65 (1399.0 examples/sec; 0.091 sec/batch)
2017-06-02 03:31:11.743755: step 34750, loss = 0.62 (1396.9 examples/sec; 0.092 sec/batch)
2017-06-02 03:31:12.635354: step 34760, loss = 0.83 (1435.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:31:13.534604: step 34770, loss = 0.78 (1423.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:31:14.444519: step 34780, loss = 0.87 (1406.7 examples/sec; 0.091 sec/batch)
2017-06-02 03:31:15.366578: step 34790, loss = 0.93 (1388.2 examples/sec; 0.092 sec/batch)
2017-06-02 03:31:16.366884: step 34800, loss = 0.71 (1279.6 examples/sec; 0.100 sec/batch)
2017-06-02 03:31:17.161286: step 34810, loss = 0.75 (1611.3 examples/sec; 0.079 sec/batch)
2017-06-02 03:31:18.052198: step 34820, loss = 0.80 (1436.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:31:18.930073: step 34830, loss = 0.81 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:31:19.837076: step 34840, loss = 0.88 (1411.2 examples/sec; 0.091 sec/batch)
2017-06-02 03:31:20.757138: step 34850, loss = 0.69 (1391.2 examples/sec; 0.092 sec/batch)
2017-06-02 03:31:21.674719: step 34860, loss = 0.73 (1395.0 examples/sec; 0.092 sec/batch)
2017-06-02 03:31:22.605572: step 34870, loss = 0.71 (1375.1 examples/sec; 0.093 sec/batch)
2017-06-02 03:31:23.503926: step 34880, loss = 0.79 (1424.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:31:24.398598: step 34890, loss = 0.65 (1430.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:31:25.380161: step 34900, loss = 0.77 (1304.0 examples/sec; 0.098 sec/batch)
2017-06-02 03:31:26.196136: step 34910, loss = 0.80 (1568.7 examples/sec; 0.082 sec/batch)
2017-06-02 03:31:27.112933: step 34920, loss = 0.88 (1396.2 examples/sec; 0.092 sec/batch)
2017-06-02 03:31:28.006687: step 34930, loss = 0.63 (1432.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:31:28.913404: step 34940, loss = 0.76 (1411.7 examples/sec; 0.091 sec/batch)
2017-06-02 03:31:29.827941: step 34950, loss = 0.76 (1399.6 examples/sec; 0.091 sec/batch)
2017-06-02 03:31:30.745516: step 34960, loss = 0.58 (1395.0 examples/sec; 0.092 sec/batch)
2017-06-02 03:31:31.661909: step 34970, loss = 0.85 (1396.8 examples/sec; 0.092 sec/batch)
2017-06-02 03:31:32.585875: step 34980, loss = 0.93 (1385.3 examples/sec; 0.092 sec/batch)
2017-06-02 03:31:33.485386: step 34990, loss = 0.83 (1423.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:31:34.472768: step 35000, loss = 0.67 (1296.4 examples/sec; 0.099 sec/batch)
2017-06-02 03:31:35.293143: step 35010, loss = 0.84 (1560.3 examples/sec; 0.082 sec/batch)
2017-06-02 03:31:36.218253: step 35020, loss = 0.76 (1383.6 examples/sec; 0.093 sec/batch)
2017-06-02 03:31:37.115461: step 35030, loss = 0.77 (1426.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:31:38.018183: step 35040, loss = 0.80 (1417.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:31:38.916782: step 35050, loss = 0.86 (1424.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:31:39.834918: step 35060, loss = 0.68 (1394.1 examples/sec; 0.092 sec/batch)
2017-06-02 03:31:40.748521: step 35070, loss = 0.74 (1401.0 examples/sec; 0.091 sec/batch)
2017-06-02 03:31:41.642194: step 35080, loss = 0.98 (1432.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:31:42.549089: step 35090, loss = 0.79 (1411.4 examples/sec; 0.091 sec/batch)
2017-06-02 03:31:43.568948: step 35100, loss = 0.89 (1255.1 examples/sec; 0.102 sec/batch)
2017-06-02 03:31:44.387222: step 35110, loss = 0.87 (1564.3 examples/sec; 0.082 sec/batch)
2017-06-02 03:31:45.285391: step 35120, loss = 1.00 (1425.1 examples/sec; 0.090 sec/batch)
2017-06-02 03:31:46.190087: step 35130, loss = 0.80 (1414.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:31:47.089219: step 35140, loss = 0.68 (1423.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:31:47.993605: step 35150, loss = 0.81 (1415.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:31:48.908567: step 35160, loss = 0.75 (1399.0 examples/sec; 0.091 sec/batch)
2017-06-02 03:31:49.808490: step 35170, loss = 0.81 (1422.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:31:50.703345: step 35180, loss = 0.85 (1430.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:31:51.601012: step 35190, loss = 0.74 (1425.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:31:52.612930: step 35200, loss = 0.63 (1264.9 examples/sec; 0.101 sec/batch)
2017-06-02 03:31:53.416930: step 35210, loss = 0.92 (1592.0 examples/sec; 0.080 sec/batch)
2017-06-02 03:31:54.318449: step 35220, loss = 0.70 (1419.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:31:55.224422: step 35230, loss = 0.87 (1412.9 examples/sec; 0.091 sec/batch)
2017-06-02 03:31:56.117307: step 35240, loss = 0.83 (1433.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:31:57.037787: step 35250, loss = 0.69 (1390.6 examples/sec; 0.092 sec/batch)
2017-06-02 03:31:57.938507: step 35260, loss = 0.76 (1421.1 examples/sec; 0.090 sec/batch)
2017-06-02 03:31:58.818951: step 35270, loss = 0.81 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:31:59.723465: step 35280, loss = 0.62 (1415.1 examples/sec; 0.090 sec/batch)
2017-06-02 03:32:00.636696: step 35290, loss = 0.70 (1401.6 examples/sec; 0.091 sec/batch)
2017-06-02 03:32:01.642158: step 35300, loss = 0.77 (1273.0 examples/sec; 0.101 sec/batch)
2017-06-02 03:32:02.437391: step 35310, loss = 0.79 (1609.6 examples/sec; 0.080 sec/batch)
2017-06-02 03:32:03.349614: step 35320, loss = 0.75 (1403.2 examples/sec; 0.091 sec/batch)
2017-06-02 03:32:04.248247: step 35330, loss = 0.85 (1424.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:32:05.150381: step 35340, loss = 0.81 (1418.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:32:06.028878: step 35350, loss = 0.77 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:32:06.926249: step 35360, loss = 0.77 (1426.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:32:07.824887: step 35370, loss = 0.75 (1424.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:32:08.735323: step 35380, loss = 0.75 (1405.9 examples/sec; 0.091 sec/batch)
2017-06-02 03:32:09.654997: step 35390, loss = 0.69 (1391.8 examples/sec; 0.092 sec/batch)
2017-06-02 03:32:10.718138: step 35400, loss = 0.75 (1204.0 examples/sec; 0.106 sec/batch)
2017-06-02 03:32:11.455404: step 35410, loss = 0.83 (1736.2 examples/sec; 0.074 sec/batch)
2017-06-02 03:32:12.347074: step 35420, loss = 0.88 (1435.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:32:13.246292: step 35430, loss = 0.80 (1423.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:32:14.152606: step 35440, loss = 0.63 (1412.3 examples/sec; 0.091 sec/batch)
2017-06-02 03:32:15.066553: step 35450, loss = 0.93 (1400.5 examples/sec; 0.091 sec/batch)
2017-06-02 03:32:15.980025: step 35460, loss = 0.61 (1401.2 examples/sec; 0.091 sec/batch)
2017-06-02 03:32:16.887764: step 35470, loss = 0.67 (1410.1 examples/sec; 0.091 sec/batch)
2017-06-02 03:32:17.806214: step 35480, loss = 0.76 (1393.6 examples/sec; 0.092 sec/batch)
2017-06-02 03:32:18.727017: step 35490, loss = 0.75 (1390.1 examples/sec; 0.092 sec/batch)
2017-06-02 03:32:19.737717: step 35500, loss = 0.78 (1266.5 examples/sec; 0.101 sec/batch)
2017-06-02 03:32:20.578058: step 35510, loss = 0.90 (1523.2 examples/sec; 0.084 sec/batch)
2017-06-02 03:32:21.491904: step 35520, loss = 0.92 (1400.7 examples/sec; 0.091 sec/batch)
2017-06-02 03:32:22.405865: step 35530, loss = 0.70 (1400.5 examples/sec; 0.091 sec/batch)
2017-06-02 03:32:23.335222: step 35540, loss = 0.81 (1377.3 examples/sec; 0.093 sec/batch)
2017-06-02 03:32:24.246979: step 35550, loss = 0.69 (1403.9 examples/sec; 0.091 sec/batch)
2017-06-02 03:32:25.171445: step 35560, loss = 0.85 (1384.6 examples/sec; 0.092 sec/batch)
2017-06-02 03:32:26.080678: step 35570, loss = 0.84 (1407.8 examples/sec; 0.091 sec/batch)
2017-06-02 03:32:26.984236: step 35580, loss = 0.61 (1416.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:32:27.887124: step 35590, loss = 0.81 (1417.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:32:28.900012: step 35600, loss = 0.85 (1263.7 examples/sec; 0.101 sec/batch)
2017-06-02 03:32:29.711366: step 35610, loss = 0.73 (1577.6 examples/sec; 0.081 sec/batch)
2017-06-02 03:32:30.598940: step 35620, loss = 0.74 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:32:31.492552: step 35630, loss = 0.81 (1432.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:32:32.395572: step 35640, loss = 0.68 (1417.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:32:33.316557: step 35650, loss = 0.66 (1389.8 examples/sec; 0.092 sec/batch)
2017-06-02 03:32:34.215071: step 35660, loss = 0.70 (1424.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:32:35.123627: step 35670, loss = 0.86 (1408.8 examples/sec; 0.091 sec/batch)
2017-06-02 03:32:36.043879: step 35680, loss = 0.76 (1390.9 examples/sec; 0.092 sec/batch)
2017-06-02 03:32:36.954931: step 35690, loss = 0.66 (1405.0 examples/sec; 0.091 sec/batch)
2017-06-02 03:32:37.953851: step 35700, loss = 0.67 (1281.4 examples/sec; 0.100 sec/batch)
2017-06-02 03:32:38.768300: step 35710, loss = 1.02 (1571.6 examples/sec; 0.081 sec/batch)
2017-06-02 03:32:39.664393: step 35720, loss = 0.87 (1428.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:32:40.584536: step 35730, loss = 0.95 (1391.1 examples/sec; 0.092 sec/batch)
2017-06-02 03:32:41.496870: step 35740, loss = 0.79 (1403.0 examples/sec; 0.091 sec/batch)
2017-06-02 03:32:42.433586: step 35750, loss = 0.81 (1366.5 examples/sec; 0.094 sec/batch)
2017-06-02 03:32:43.339175: step 35760, loss = 0.83 (1413.5 examples/sec; 0.091 sec/batch)
2017-06-02 03:32:44.257287: step 35770, loss = 0.78 (1394.1 examples/sec; 0.092 sec/batch)
2017-06-02 03:32:45.178311: step 35780, loss = 0.93 (1389.8 examples/sec; 0.092 sec/batch)
2017-06-02 03:32:46.082545: step 35790, loss = 0.87 (1415.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:32:47.142376: step 35800, loss = 1.03 (1207.7 examples/sec; 0.106 sec/batch)
2017-06-02 03:32:47.924655: step 35810, loss = 0.93 (1636.2 examples/sec; 0.078 sec/batch)
2017-06-02 03:32:48.843345: step 35820, loss = 0.78 (1393.3 examples/sec; 0.092 sec/batch)
2017-06-02 03:32:49.733501: step 35830, loss = 0.76 (1437.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:32:50.663696: step 35840, loss = 0.90 (1376.1 examples/sec; 0.093 sec/batch)
2017-06-02 03:32:51.582197: step 35850, loss = 0.90 (1393.6 examples/sec; 0.092 sec/batch)
2017-06-02 03:32:52.497288: step 35860, loss = 0.85 (1398.8 examples/sec; 0.092 sec/batch)
2017-06-02 03:32:53.416727: step 35870, loss = 0.73 (1392.1 examples/sec; 0.092 sec/batch)
2017-06-02 03:32:54.339053: step 35880, loss = 0.80 (1387.8 examples/sec; 0.092 sec/batch)
2017-06-02 03:32:55.237270: step 35890, loss = 0.93 (1425.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:32:56.229074: step 35900, loss = 0.84 (1290.6 examples/sec; 0.099 sec/batch)
2017-06-02 03:32:57.040062: step 35910, loss = 0.88 (1578.3 examples/sec; 0.081 sec/batch)
2017-06-02 03:32:57.963831: step 35920, loss = 0.79 (1385.6 examples/sec; 0.092 sec/batch)
2017-06-02 03:32:58.870356: step 35930, loss = 0.71 (1412.0 examples/sec; 0.091 sec/batch)
2017-06-02 03:32:59.814541: step 35940, loss = 0.77 (1355.7 examples/sec; 0.094 sec/batch)
2017-06-02 03:33:00.739969: step 35950, loss = 0.97 (1383.1 examples/sec; 0.093 sec/batch)
2017-06-02 03:33:01.644959: step 35960, loss = 0.79 (1414.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:33:02.549148: step 35970, loss = 0.72 (1415.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:33:03.456757: step 35980, loss = 0.81 (1410.3 examples/sec; 0.091 sec/batch)
2017-06-02 03:33:04.353785: step 35990, loss = 0.90 (1427.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:33:05.345117: step 36000, loss = 0.93 (1291.2 examples/sec; 0.099 sec/batch)
2017-06-02 03:33:06.132190: step 36010, loss = 0.76 (1626.3 examples/sec; 0.079 sec/batch)
2017-06-02 03:33:07.029390: step 36020, loss = 0.71 (1426.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:33:07.926823: step 36030, loss = 0.74 (1426.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:33:08.851553: step 36040, loss = 0.83 (1384.2 examples/sec; 0.092 sec/batch)
2017-06-02 03:33:09.745093: step 36050, loss = 0.74 (1432.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:33:10.663515: step 36060, loss = 0.69 (1393.7 examples/sec; 0.092 sec/batch)
2017-06-02 03:33:11.601659: step 36070, loss = 0.91 (1364.4 examples/sec; 0.094 sec/batch)
2017-06-02 03:33:12.530291: step 36080, loss = 0.72 (1378.4 examples/sec; 0.093 sec/batch)
2017-06-02 03:33:13.444318: step 36090, loss = 0.74 (1400.4 examples/sec; 0.091 sec/batch)
2017-06-02 03:33:14.454333: step 36100, loss = 0.75 (1267.3 examples/sec; 0.101 sec/batch)
2017-06-02 03:33:15.278008: step 36110, loss = 0.88 (1554.0 examples/sec; 0.082 sec/batch)
2017-06-02 03:33:16.207924: step 36120, loss = 0.76 (1376.5 examples/sec; 0.093 sec/batch)
2017-06-02 03:33:17.116942: step 36130, loss = 0.71 (1408.1 examples/sec; 0.091 sec/batch)
2017-06-02 03:33:18.025347: step 36140, loss = 0.86 (1409.1 examples/sec; 0.091 sec/batch)
2017-06-02 03:33:18.921214: step 36150, loss = 0.67 (1428.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:33:19.815150: step 36160, loss = 0.73 (1431.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:33:20.729644: step 36170, loss = 0.71 (1399.7 examples/sec; 0.091 sec/batch)
2017-06-02 03:33:21.625720: step 36180, loss = 0.77 (1428.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:33:22.549855: step 36190, loss = 0.67 (1385.1 examples/sec; 0.092 sec/batch)
2017-06-02 03:33:23.558041: step 36200, loss = 0.72 (1269.6 examples/sec; 0.101 sec/batch)
2017-06-02 03:33:24.356043: step 36210, loss = 0.87 (1604.0 examples/sec; 0.080 sec/batch)
2017-06-02 03:33:25.265965: step 36220, loss = 0.93 (1406.7 examples/sec; 0.091 sec/batch)
2017-06-02 03:33:26.180456: step 36230, loss = 0.83 (1399.7 examples/sec; 0.091 sec/batch)
2017-06-02 03:33:27.063317: step 36240, loss = 0.80 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:33:27.980239: step 36250, loss = 0.81 (1396.0 examples/sec; 0.092 sec/batch)
2017-06-02 03:33:28.887503: step 36260, loss = 0.83 (1410.8 examples/sec; 0.091 sec/batch)
2017-06-02 03:33:29.795933: step 36270, loss = 0.70 (1409.0 examples/sec; 0.091 sec/batch)
2017-06-02 03:33:30.688892: step 36280, loss = 0.75 (1433.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:33:31.600947: step 36290, loss = 0.69 (1403.4 examples/sec; 0.091 sec/batch)
2017-06-02 03:33:32.601285: step 36300, loss = 0.89 (1279.6 examples/sec; 0.100 sec/batch)
2017-06-02 03:33:33.404245: step 36310, loss = 0.92 (1594.1 examples/sec; 0.080 sec/batch)
2017-06-02 03:33:34.290675: step 36320, loss = 0.74 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:33:35.207911: step 36330, loss = 0.67 (1395.5 examples/sec; 0.092 sec/batch)
2017-06-02 03:33:36.114194: step 36340, loss = 0.69 (1412.4 examples/sec; 0.091 sec/batch)
2017-06-02 03:33:37.020969: step 36350, loss = 0.80 (1411.6 examples/sec; 0.091 sec/batch)
2017-06-02 03:33:37.914560: step 36360, loss = 0.78 (1432.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:33:38.808827: step 36370, loss = 0.79 (1431.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:33:39.730289: step 36380, loss = 0.64 (1389.1 examples/sec; 0.092 sec/batch)
2017-06-02 03:33:40.634438: step 36390, loss = 0.73 (1415.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:33:41.648451: step 36400, loss = 0.69 (1262.3 examples/sec; 0.101 sec/batch)
2017-06-02 03:33:42.446465: step 36410, loss = 0.77 (1604.0 examples/sec; 0.080 sec/batch)
2017-06-02 03:33:43.349695: step 36420, loss = 0.79 (1417.1 examples/sec; 0.090 sec/batch)
2017-06-02 03:33:44.258921: step 36430, loss = 0.57 (1407.8 examples/sec; 0.091 sec/batch)
2017-06-02 03:33:45.158285: step 36440, loss = 0.78 (1423.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:33:46.068754: step 36450, loss = 0.82 (1405.8 examples/sec; 0.091 sec/batch)
2017-06-02 03:33:46.975005: step 36460, loss = 0.93 (1412.4 examples/sec; 0.091 sec/batch)
2017-06-02 03:33:47.878048: step 36470, loss = 0.84 (1417.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:33:48.771228: step 36480, loss = 0.80 (1433.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:33:49.659648: step 36490, loss = 0.84 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:33:50.613844: step 36500, loss = 0.70 (1341.4 examples/sec; 0.095 sec/batch)
2017-06-02 03:33:51.419011: step 36510, loss = 0.70 (1589.7 examples/sec; 0.081 sec/batch)
2017-06-02 03:33:52.313965: step 36520, loss = 0.87 (1430.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:33:53.204432: step 36530, loss = 0.75 (1437.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:33:54.121800: step 36540, loss = 0.63 (1395.3 examples/sec; 0.092 sec/batch)
2017-06-02 03:33:55.031059: step 36550, loss = 0.74 (1407.8 examples/sec; 0.091 sec/batch)
2017-06-02 03:33:55.948800: step 36560, loss = 0.68 (1394.6 examples/sec; 0.092 sec/batch)
2017-06-02 03:33:56.868070: step 36570, loss = 0.72 (1392.4 examples/sec; 0.092 sec/batch)
2017-06-02 03:33:57.765634: step 36580, loss = 0.69 (1426.1 examples/sec; 0.090 sec/batch)
2017-06-02 03:33:58.654180: step 36590, loss = 0.64 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:33:59.656894: step 36600, loss = 0.97 (1276.5 examples/sec; 0.100 sec/batch)
2017-06-02 03:34:00.470368: step 36610, loss = 0.67 (1573.5 examples/sec; 0.081 sec/batch)
2017-06-02 03:34:01.404346: step 36620, loss = 0.82 (1370.5 examples/sec; 0.093 sec/batch)
2017-06-02 03:34:02.280918: step 36630, loss = 0.72 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:34:03.173462: step 36640, loss = 0.81 (1434.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:34:04.068426: step 36650, loss = 0.80 (1430.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:34:04.949742: step 36660, loss = 0.68 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:34:05.843533: step 36670, loss = 0.89 (1432.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:34:06.760580: step 36680, loss = 0.69 (1395.8 examples/sec; 0.092 sec/batch)
2017-06-02 03:34:07.685186: step 36690, loss = 0.75 (1384.4 examples/sec; 0.092 sec/batch)
2017-06-02 03:34:08.703724: step 36700, loss = 0.62 (1256.7 examples/sec; 0.102 sec/batch)
2017-06-02 03:34:09.505085: step 36710, loss = 0.79 (1597.3 examples/sec; 0.080 sec/batch)
2017-06-02 03:34:10.400161: step 36720, loss = 0.66 (1430.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:34:11.266146: step 36730, loss = 0.74 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:34:12.143997: step 36740, loss = 0.78 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:34:13.006831: step 36750, loss = 0.65 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 03:34:13.909554: step 36760, loss = 0.75 (1417.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:34:14.802388: step 36770, loss = 0.63 (1433.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:34:15.723757: step 36780, loss = 0.77 (1389.2 examples/sec; 0.092 sec/batch)
2017-06-02 03:34:16.612512: step 36790, loss = 0.91 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:34:17.633962: step 36800, loss = 0.93 (1253.1 examples/sec; 0.102 sec/batch)
2017-06-02 03:34:18.443723: step 36810, loss = 0.73 (1580.7 examples/sec; 0.081 sec/batch)
2017-06-02 03:34:19.339482: step 36820, loss = 1.03 (1429.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:34:20.232250: step 36830, loss = 0.86 (1433.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:34:21.139694: step 36840, loss = 0.77 (1410.6 examples/sec; 0.091 sec/batch)
2017-06-02 03:34:22.034661: step 36850, loss = 0.81 (1430.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:34:22.925936: step 36860, loss = 0.80 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:34:23.816364: step 36870, loss = 1.02 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:34:24.723343: step 36880, loss = 0.88 (1411.3 examples/sec; 0.091 sec/batch)
2017-06-02 03:34:25.611945: step 36890, loss = 0.77 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:34:26.604759: step 36900, loss = 0.70 (1289.3 examples/sec; 0.099 sec/batch)
2017-06-02 03:34:27.396038: step 36910, loss = 0.83 (1617.6 examples/sec; 0.079 sec/batch)
2017-06-02 03:34:28.314870: step 36920, loss = 0.89 (1393.1 examples/sec; 0.092 sec/batch)
2017-06-02 03:34:29.223976: step 36930, loss = 0.72 (1408.0 examples/sec; 0.091 sec/batch)
2017-06-02 03:34:30.143856: step 36940, loss = 0.84 (1391.5 examples/sec; 0.092 sec/batch)
2017-06-02 03:34:31.045548: step 36950, loss = 0.88 (1419.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:34:31.938833: step 36960, loss = 0.69 (1432.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:34:32.864173: step 36970, loss = 0.69 (1383.3 examples/sec; 0.093 sec/batch)
2017-06-02 03:34:33.764357: step 36980, loss = 0.66 (1421.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:34:34.669434: step 36990, loss = 0.75 (1414.2 examples/sec; 0.091 sec/batch)
2017-06-02 03:34:35.706836: step 37000, loss = 0.81 (1233.9 examples/sec; 0.104 sec/batch)
2017-06-02 03:34:36.456822: step 37010, loss = 0.72 (1706.7 examples/sec; 0.075 sec/batch)
2017-06-02 03:34:37.343540: step 37020, loss = 0.79 (1443.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:34:38.233001: step 37030, loss = 0.61 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:34:39.134215: step 37040, loss = 0.95 (1420.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:34:40.047707: step 37050, loss = 0.66 (1401.2 examples/sec; 0.091 sec/batch)
2017-06-02 03:34:40.963910: step 37060, loss = 0.75 (1397.1 examples/sec; 0.092 sec/batch)
2017-06-02 03:34:41.827074: step 37070, loss = 0.79 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 03:34:42.687261: step 37080, loss = 0.78 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:34:43.580330: step 37090, loss = 0.72 (1433.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:34:44.559716: step 37100, loss = 0.70 (1306.9 examples/sec; 0.098 sec/batch)
2017-06-02 03:34:45.331659: step 37110, loss = 0.61 (1658.2 examples/sec; 0.077 sec/batch)
2017-06-02 03:34:46.179912: step 37120, loss = 0.88 (1509.0 examples/sec; 0.085 sec/batch)
2017-06-02 03:34:47.061437: step 37130, loss = 0.72 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:34:47.934499: step 37140, loss = 0.79 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:34:48.809827: step 37150, loss = 0.75 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:34:49.702670: step 37160, loss = 0.65 (1433.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:34:50.581603: step 37170, loss = 0.72 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:34:51.482135: step 37180, loss = 0.84 (1421.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:34:52.390348: step 37190, loss = 0.91 (1409.4 examples/sec; 0.091 sec/batch)
2017-06-02 03:34:53.384858: step 37200, loss = 0.67 (1287.1 examples/sec; 0.099 sec/batch)
2017-06-02 03:34:54.179409: step 37210, loss = 0.71 (1611.0 examples/sec; 0.079 sec/batch)
2017-06-02 03:34:55.050394: step 37220, loss = 0.86 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:34:55.935584: step 37230, loss = 0.65 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:34:56.820250: step 37240, loss = 0.92 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:34:57.704769: step 37250, loss = 0.86 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:34:58.593413: step 37260, loss = 0.74 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:34:59.485229: step 37270, loss = 0.73 (1435.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:35:00.362636: step 37280, loss = 0.73 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:35:01.223983: step 37290, loss = 0.94 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 03:35:02.167125: step 37300, loss = 0.67 (1357.2 examples/sec; 0.094 sec/batch)
2017-06-02 03:35:02.936720: step 37310, loss = 0.88 (1663.2 examples/sec; 0.077 sec/batch)
2017-06-02 03:35:03.829147: step 37320, loss = 0.83 (1434.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:35:04.707515: step 37330, loss = 0.73 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:35:05.583678: step 37340, loss = 0.82 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:35:06.472777: step 37350, loss = 0.83 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:35:07.366193: step 37360, loss = 0.66 (1432.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:35:08.261814: step 37370, loss = 0.73 (1429.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:35:09.145384: step 37380, loss = 0.75 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:35:10.030573: step 37390, loss = 0.80 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:35:11.007976: step 37400, loss = 0.64 (1309.6 examples/sec; 0.098 sec/batch)
2017-06-02 03:35:11.783059: step 37410, loss = 0.96 (1651.4 examples/sec; 0.078 sec/batch)
2017-06-02 03:35:12.684170: step 37420, loss = 0.84 (1420.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:35:13.575500: step 37430, loss = 0.74 (1436.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:35:14.470389: step 37440, loss = 0.83 (1430.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:35:15.350207: step 37450, loss = 0.71 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:35:16.224658: step 37460, loss = 0.78 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:35:17.100360: step 37470, loss = 0.76 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:35:17.988390: step 37480, loss = 0.69 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:35:18.876766: step 37490, loss = 0.81 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:35:19.858248: step 37500, loss = 0.72 (1304.1 examples/sec; 0.098 sec/batch)
2017-06-02 03:35:20.667336: step 37510, loss = 0.66 (1582.0 examples/sec; 0.081 sec/batch)
2017-06-02 03:35:21.548384: step 37520, loss = 0.71 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:35:22.405430: step 37530, loss = 0.68 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 03:35:23.294866: step 37540, loss = 0.85 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:35:24.174395: step 37550, loss = 0.88 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:35:25.062404: step 37560, loss = 0.79 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:35:25.929542: step 37570, loss = 0.87 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:35:26.789724: step 37580, loss = 0.72 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:35:27.662371: step 37590, loss = 0.68 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:35:28.670184: step 37600, loss = 0.73 (1270.1 examples/sec; 0.101 sec/batch)
2017-06-02 03:35:29.412097: step 37610, loss = 0.65 (1725.3 examples/sec; 0.074 sec/batch)
2017-06-02 03:35:30.287277: step 37620, loss = 0.90 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:35:31.179259: step 37630, loss = 0.72 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:35:32.056117: step 37640, loss = 0.81 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:35:32.937474: step 37650, loss = 0.79 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:35:33.819455: step 37660, loss = 0.79 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:35:34.674397: step 37670, loss = 0.74 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 03:35:35.552172: step 37680, loss = 0.72 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:35:36.403456: step 37690, loss = 0.89 (1503.6 examples/sec; 0.085 sec/batch)
2017-06-02 03:35:37.408154: step 37700, loss = 0.77 (1274.0 examples/sec; 0.100 sec/batch)
2017-06-02 03:35:38.199985: step 37710, loss = 0.58 (1616.5 examples/sec; 0.079 sec/batch)
2017-06-02 03:35:39.077343: step 37720, loss = 0.75 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:35:39.955706: step 37730, loss = 0.73 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:35:40.820755: step 37740, loss = 0.73 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:35:41.694076: step 37750, loss = 0.63 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:35:42.526371: step 37760, loss = 0.71 (1537.9 examples/sec; 0.083 sec/batch)
2017-06-02 03:35:43.382298: step 37770, loss = 0.77 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 03:35:44.287591: step 37780, loss = 0.77 (1413.9 examples/sec; 0.091 sec/batch)
2017-06-02 03:35:45.157607: step 37790, loss = 0.74 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:35:46.113682: step 37800, loss = 0.92 (1338.8 examples/sec; 0.096 sec/batch)
2017-06-02 03:35:46.892366: step 37810, loss = 0.71 (1643.8 examples/sec; 0.078 sec/batch)
2017-06-02 03:35:47.746616: step 37820, loss = 0.85 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 03:35:48.600770: step 37830, loss = 0.73 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 03:35:49.480963: step 37840, loss = 0.79 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:35:50.359681: step 37850, loss = 0.68 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:35:51.254727: step 37860, loss = 0.82 (1430.1 examples/sec; 0.090 sec/batch)
2017-06-02 03:35:52.137476: step 37870, loss = 0.90 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:35:53.036625: step 37880, loss = 0.91 (1423.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:35:53.923559: step 37890, loss = 0.74 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:35:54.947085: step 37900, loss = 0.73 (1250.6 examples/sec; 0.102 sec/batch)
2017-06-02 03:35:55.707435: step 37910, loss = 0.79 (1683.4 examples/sec; 0.076 sec/batch)
2017-06-02 03:35:56.577645: step 37920, loss = 0.75 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:35:57.480517: step 37930, loss = 0.75 (1417.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:35:58.400123: step 37940, loss = 0.75 (1391.9 examples/sec; 0.092 sec/batch)
2017-06-02 03:35:59.274666: step 37950, loss = 0.85 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:36:00.158099: step 37960, loss = 0.71 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:36:01.039001: step 37970, loss = 0.77 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:36:01.921859: step 37980, loss = 0.71 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:36:02.806396: step 37990, loss = 0.84 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:36:03.764281: step 38000, loss = 0.80 (1336.3 examples/sec; 0.096 sec/batch)
2017-06-02 03:36:04.548017: step 38010, loss = 0.85 (1633.2 examples/sec; 0.078 sec/batch)
2017-06-02 03:36:05.422054: step 38020, loss = 0.82 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:36:06.329819: step 38030, loss = 0.89 (1410.1 examples/sec; 0.091 sec/batch)
2017-06-02 03:36:07.203839: step 38040, loss = 0.71 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:36:08.103789: step 38050, loss = 0.66 (1422.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:36:09.004567: step 38060, loss = 0.69 (1421.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:36:09.856157: step 38070, loss = 0.80 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 03:36:10.734661: step 38080, loss = 0.78 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:36:11.628449: step 38090, loss = 0.90 (1432.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:36:12.622585: step 38100, loss = 0.88 (1287.5 examples/sec; 0.099 sec/batch)
2017-06-02 03:36:13.416941: step 38110, loss = 0.85 (1611.4 examples/sec; 0.079 sec/batch)
2017-06-02 03:36:14.291706: step 38120, loss = 0.84 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:36:15.173841: step 38130, loss = 0.63 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:36:16.069206: step 38140, loss = 0.86 (1429.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:36:16.977477: step 38150, loss = 0.85 (1409.3 examples/sec; 0.091 sec/batch)
2017-06-02 03:36:17.853464: step 38160, loss = 0.77 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:36:18.763503: step 38170, loss = 0.74 (1406.5 examples/sec; 0.091 sec/batch)
2017-06-02 03:36:19.652555: step 38180, loss = 0.72 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:36:20.514536: step 38190, loss = 0.72 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 03:36:21.474880: step 38200, loss = 0.86 (1332.8 examples/sec; 0.096 sec/batch)
2017-06-02 03:36:22.271439: step 38210, loss = 0.78 (1606.9 examples/sec; 0.080 sec/batch)
2017-06-02 03:36:23.133040: step 38220, loss = 0.98 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 03:36:24.004682: step 38230, loss = 0.78 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:36:24.871524: step 38240, loss = 0.78 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:36:25.748371: step 38250, loss = 0.74 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:36:26.609483: step 38260, loss = 0.73 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 03:36:27.493619: step 38270, loss = 0.82 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:36:28.352346: step 38280, loss = 0.73 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 03:36:29.201412: step 38290, loss = 0.83 (1507.5 examples/sec; 0.085 sec/batch)
2017-06-02 03:36:30.174160: step 38300, loss = 0.78 (1315.9 examples/sec; 0.097 sec/batch)
2017-06-02 03:36:30.974233: step 38310, loss = 0.78 (1599.9 examples/sec; 0.080 sec/batch)
2017-06-02 03:36:31.859602: step 38320, loss = 1.01 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:36:32.733237: step 38330, loss = 0.81 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:36:33.625236: step 38340, loss = 0.72 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:36:34.498305: step 38350, loss = 0.73 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:36:35.385934: step 38360, loss = 0.61 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:36:36.267860: step 38370, loss = 0.87 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:36:37.148562: step 38380, loss = 0.71 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:36:38.035620: step 38390, loss = 0.84 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:36:39.025898: step 38400, loss = 0.83 (1292.6 examples/sec; 0.099 sec/batch)
2017-06-02 03:36:39.809937: step 38410, loss = 0.70 (1632.6 examples/sec; 0.078 sec/batch)
2017-06-02 03:36:40.693954: step 38420, loss = 0.61 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:36:41.560174: step 38430, loss = 0.66 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:36:42.444495: step 38440, loss = 0.86 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:36:43.336062: step 38450, loss = 0.73 (1435.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:36:44.236180: step 38460, loss = 0.67 (1422.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:36:45.145885: step 38470, loss = 0.90 (1407.0 examples/sec; 0.091 sec/batch)
2017-06-02 03:36:46.037169: step 38480, loss = 0.84 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:36:46.906035: step 38490, loss = 0.88 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:36:47.901401: step 38500, loss = 0.65 (1285.9 examples/sec; 0.100 sec/batch)
2017-06-02 03:36:48.700592: step 38510, loss = 0.79 (1601.6 examples/sec; 0.080 sec/batch)
2017-06-02 03:36:49.573027: step 38520, loss = 0.78 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:36:50.463428: step 38530, loss = 0.78 (1437.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:36:51.348044: step 38540, loss = 0.82 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:36:52.255153: step 38550, loss = 0.86 (1411.1 examples/sec; 0.091 sec/batch)
2017-06-02 03:36:53.144467: step 38560, loss = 0.83 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:36:54.035061: step 38570, loss = 0.82 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:36:54.889935: step 38580, loss = 0.86 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 03:36:55.757639: step 38590, loss = 0.74 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:36:56.702786: step 38600, loss = 0.67 (1354.3 examples/sec; 0.095 sec/batch)
2017-06-02 03:36:57.453791: step 38610, loss = 0.87 (1704.4 examples/sec; 0.075 sec/batch)
2017-06-02 03:36:58.339224: step 38620, loss = 0.83 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:36:59.228861: step 38630, loss = 0.78 (1438.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:37:00.102674: step 38640, loss = 0.77 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:37:00.983207: step 38650, loss = 0.67 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:37:01.847494: step 38660, loss = 0.67 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 03:37:02.705445: step 38670, loss = 0.73 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 03:37:03.536887: step 38680, loss = 0.72 (1539.5 examples/sec; 0.083 sec/batch)
2017-06-02 03:37:04.430272: step 38690, loss = 0.82 (1432.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:37:05.408099: step 38700, loss = 0.81 (1309.0 examples/sec; 0.098 sec/batch)
2017-06-02 03:37:06.186958: step 38710, loss = 0.79 (1643.4 examples/sec; 0.078 sec/batch)
2017-06-02 03:37:07.032558: step 38720, loss = 0.83 (1513.7 examples/sec; 0.085 sec/batch)
2017-06-02 03:37:07.909476: step 38730, loss = 0.56 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:37:08.777162: step 38740, loss = 0.83 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:37:09.672582: step 38750, loss = 0.75 (1429.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:37:10.524082: step 38760, loss = 0.83 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 03:37:11.426270: step 38770, loss = 0.81 (1418.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:37:12.271250: step 38780, loss = 0.82 (1514.8 examples/sec; 0.084 sec/batch)
2017-06-02 03:37:13.146588: step 38790, loss = 0.62 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:37:14.145881: step 38800, loss = 0.80 (1280.9 examples/sec; 0.100 sec/batch)
2017-06-02 03:37:14.883323: step 38810, loss = 0.73 (1735.7 examples/sec; 0.074 sec/batch)
2017-06-02 03:37:15.744174: step 38820, loss = 0.77 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 03:37:16.629396: step 38830, loss = 0.90 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:37:17.501715: step 38840, loss = 0.78 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:37:18.372187: step 38850, loss = 0.92 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:37:19.268264: step 38860, loss = 0.81 (1428.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:37:20.153688: step 38870, loss = 0.62 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:37:21.024835: step 38880, loss = 0.78 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:37:21.884841: step 38890, loss = 0.80 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 03:37:22.879658: step 38900, loss = 0.79 (1286.7 examples/sec; 0.099 sec/batch)
2017-06-02 03:37:23.599706: step 38910, loss = 0.74 (1777.7 examples/sec; 0.072 sec/batch)
2017-06-02 03:37:24.445337: step 38920, loss = 0.73 (1513.7 examples/sec; 0.085 sec/batch)
2017-06-02 03:37:25.293846: step 38930, loss = 0.90 (1508.5 examples/sec; 0.085 sec/batch)
2017-06-02 03:37:26.125106: step 38940, loss = 0.84 (1539.8 examples/sec; 0.083 sec/batch)
2017-06-02 03:37:27.007072: step 38950, loss = 0.70 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:37:27.846514: step 38960, loss = 0.93 (1524.8 examples/sec; 0.084 sec/batch)
2017-06-02 03:37:28.719513: step 38970, loss = 0.73 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:37:29.623910: step 38980, loss = 0.81 (1415.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:37:30.488973: step 38990, loss = 0.67 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:37:31.461800: step 39000, loss = 0.78 (1315.8 examples/sec; 0.097 sec/batch)
2017-06-02 03:37:32.238759: step 39010, loss = 0.69 (1647.4 examples/sec; 0.078 sec/batch)
2017-06-02 03:37:33.123283: step 39020, loss = 0.64 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:37:33.986742: step 39030, loss = 0.73 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 03:37:34.854272: step 39040, loss = 0.91 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:37:35.707710: step 39050, loss = 0.76 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 03:37:36.609327: step 39060, loss = 0.84 (1419.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:37:37.492240: step 39070, loss = 0.85 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:37:38.394373: step 39080, loss = 0.67 (1418.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:37:39.320677: step 39090, loss = 0.67 (1381.9 examples/sec; 0.093 sec/batch)
2017-06-02 03:37:40.322066: step 39100, loss = 0.83 (1278.2 examples/sec; 0.100 sec/batch)
2017-06-02 03:37:41.092303: step 39110, loss = 0.77 (1661.8 examples/sec; 0.077 sec/batch)
2017-06-02 03:37:41.997747: step 39120, loss = 0.71 (1413.7 examples/sec; 0.091 sec/batch)
2017-06-02 03:37:42.908232: step 39130, loss = 0.69 (1405.9 examples/sec; 0.091 sec/batch)
2017-06-02 03:37:43.803271: step 39140, loss = 0.69 (1430.1 examples/sec; 0.090 sec/batch)
2017-06-02 03:37:44.689600: step 39150, loss = 0.70 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:37:45.597244: step 39160, loss = 0.84 (1410.2 examples/sec; 0.091 sec/batch)
2017-06-02 03:37:46.494858: step 39170, loss = 0.88 (1426.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:37:47.393901: step 39180, loss = 0.81 (1423.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:37:48.291609: step 39190, loss = 0.67 (1425.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:37:49.252383: step 39200, loss = 0.73 (1332.2 examples/sec; 0.096 sec/batch)
2017-06-02 03:37:50.035129: step 39210, loss = 0.97 (1635.3 examples/sec; 0.078 sec/batch)
2017-06-02 03:37:50.921020: step 39220, loss = 0.84 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:37:51.818423: step 39230, loss = 0.62 (1426.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:37:52.716669: step 39240, loss = 0.69 (1425.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:37:53.606635: step 39250, loss = 0.73 (1438.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:37:54.501007: step 39260, loss = 0.75 (1431.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:37:55.378634: step 39270, loss = 0.67 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:37:56.248788: step 39280, loss = 0.87 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:37:57.124475: step 39290, loss = 0.83 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:37:58.099222: step 39300, loss = 0.79 (1313.2 examples/sec; 0.097 sec/batch)
2017-06-02 03:37:58.883340: step 39310, loss = 0.95 (1632.4 examples/sec; 0.078 sec/batch)
2017-06-02 03:37:59.767362: step 39320, loss = 0.79 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:38:00.647996: step 39330, loss = 0.80 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:38:01.534056: step 39340, loss = 0.66 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:38:02.394923: step 39350, loss = 0.84 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 03:38:03.266081: step 39360, loss = 0.91 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:38:04.140671: step 39370, loss = 0.91 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:38:05.023560: step 39380, loss = 0.97 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:38:05.918126: step 39390, loss = 0.94 (1430.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:38:06.901561: step 39400, loss = 0.75 (1301.6 examples/sec; 0.098 sec/batch)
2017-06-02 03:38:07.702603: step 39410, loss = 0.77 (1597.9 examples/sec; 0.080 sec/batch)
2017-06-02 03:38:08.593578: step 39420, loss = 0.74 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:38:09.480745: step 39430, loss = 0.76 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:38:10.369939: step 39440, loss = 0.75 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:38:11.280167: step 39450, loss = 1.01 (1406.2 examples/sec; 0.091 sec/batch)
2017-06-02 03:38:12.160830: step 39460, loss = 0.88 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:38:13.042820: step 39470, loss = 0.74 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:38:13.935707: step 39480, loss = 0.92 (1433.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:38:14.813020: step 39490, loss = 0.67 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:38:15.779264: step 39500, loss = 0.82 (1324.7 examples/sec; 0.097 sec/batch)
2017-06-02 03:38:16.566180: step 39510, loss = 0.80 (1626.6 examples/sec; 0.079 sec/batch)
2017-06-02 03:38:17.462273: step 39520, loss = 0.56 (1428.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:38:18.336232: step 39530, loss = 0.68 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:38:19.217076: step 39540, loss = 0.96 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:38:20.116380: step 39550, loss = 0.70 (1423.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:38:21.022840: step 39560, loss = 0.92 (1412.1 examples/sec; 0.091 sec/batch)
2017-06-02 03:38:21.914298: step 39570, loss = 0.74 (1435.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:38:22.812355: step 39580, loss = 0.68 (1425.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:38:23.710296: step 39590, loss = 0.58 (1425.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:38:24.702714: step 39600, loss = 0.78 (1289.8 examples/sec; 0.099 sec/batch)
2017-06-02 03:38:25.507881: step 39610, loss = 0.76 (1589.7 examples/sec; 0.081 sec/batch)
2017-06-02 03:38:26.414982: step 39620, loss = 0.76 (1411.1 examples/sec; 0.091 sec/batch)
2017-06-02 03:38:27.331990: step 39630, loss = 1.09 (1395.9 examples/sec; 0.092 sec/batch)
2017-06-02 03:38:28.205549: step 39640, loss = 0.75 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:38:29.053384: step 39650, loss = 0.76 (1509.7 examples/sec; 0.085 sec/batch)
2017-06-02 03:38:29.927249: step 39660, loss = 0.74 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:38:30.815717: step 39670, loss = 0.74 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:38:31.722570: step 39680, loss = 0.67 (1411.5 examples/sec; 0.091 sec/batch)
2017-06-02 03:38:32.581555: step 39690, loss = 0.80 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:38:33.536192: step 39700, loss = 0.77 (1340.8 examples/sec; 0.095 sec/batch)
2017-06-02 03:38:34.319008: step 39710, loss = 0.98 (1635.1 examples/sec; 0.078 sec/batch)
2017-06-02 03:38:35.204046: step 39720, loss = 0.81 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:38:36.088543: step 39730, loss = 0.75 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:38:36.956581: step 39740, loss = 0.76 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:38:37.826997: step 39750, loss = 0.68 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:38:38.700416: step 39760, loss = 0.77 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:38:39.579093: step 39770, loss = 0.73 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:38:40.437006: step 39780, loss = 0.84 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 03:38:41.310003: step 39790, loss = 0.78 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:38:42.317539: step 39800, loss = 0.87 (1270.4 examples/sec; 0.101 sec/batch)
2017-06-02 03:38:43.051016: step 39810, loss = 0.73 (1745.1 examples/sec; 0.073 sec/batch)
2017-06-02 03:38:43.926501: step 39820, loss = 0.83 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:38:44.825261: step 39830, loss = 0.97 (1424.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:38:45.675106: step 39840, loss = 0.69 (1506.2 examples/sec; 0.085 sec/batch)
2017-06-02 03:38:46.585257: step 39850, loss = 0.67 (1406.3 examples/sec; 0.091 sec/batch)
2017-06-02 03:38:47.485174: step 39860, loss = 0.80 (1422.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:38:48.349846: step 39870, loss = 0.67 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 03:38:49.252397: step 39880, loss = 0.86 (1418.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:38:50.136009: step 39890, loss = 0.73 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:38:51.124601: step 39900, loss = 0.78 (1294.8 examples/sec; 0.099 sec/batch)
2017-06-02 03:38:51.907645: step 39910, loss = 0.73 (1634.7 examples/sec; 0.078 sec/batch)
2017-06-02 03:38:52.782358: step 39920, loss = 0.77 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:38:53.656392: step 39930, loss = 0.67 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:38:54.545514: step 39940, loss = 0.88 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:38:55.440463: step 39950, loss = 0.73 (1430.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:38:56.335943: step 39960, loss = 0.87 (1429.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:38:57.210176: step 39970, loss = 0.91 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:38:58.091340: step 39980, loss = 0.66 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:38:58.959893: step 39990, loss = 0.72 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:38:59.914292: step 40000, loss = 0.70 (1341.2 examples/sec; 0.095 sec/batch)
2017-06-02 03:39:00.728121: step 40010, loss = 0.77 (1572.8 examples/sec; 0.081 sec/batch)
2017-06-02 03:39:01.617668: step 40020, loss = 0.82 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:39:02.494131: step 40030, loss = 0.75 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:39:03.350969: step 40040, loss = 0.90 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 03:39:04.243357: step 40050, loss = 0.85 (1434.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:39:05.139644: step 40060, loss = 0.78 (1428.1 examples/sec; 0.090 sec/batch)
2017-06-02 03:39:06.006990: step 40070, loss = 0.68 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:39:06.904829: step 40080, loss = 0.96 (1425.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:39:07.762249: step 40090, loss = 0.86 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 03:39:08.733684: step 40100, loss = 0.83 (1317.6 examples/sec; 0.097 sec/batch)
2017-06-02 03:39:09.501279: step 40110, loss = 0.92 (1667.6 examples/sec; 0.077 sec/batch)
2017-06-02 03:39:10.369175: step 40120, loss = 0.82 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:39:11.258293: step 40130, loss = 0.82 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:39:12.152113: step 40140, loss = 0.83 (1432.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:39:13.028360: step 40150, loss = 0.69 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:39:13.872247: step 40160, loss = 0.65 (1516.8 examples/sec; 0.084 sec/batch)
2017-06-02 03:39:14.722598: step 40170, loss = 0.69 (1505.3 examples/sec; 0.085 sec/batch)
2017-06-02 03:39:15.605703: step 40180, loss = 0.82 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:39:16.475181: step 40190, loss = 0.74 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:39:17.443917: step 40200, loss = 0.76 (1321.3 examples/sec; 0.097 sec/batch)
2017-06-02 03:39:18.228429: step 40210, loss = 0.85 (1631.6 examples/sec; 0.078 sec/batch)
2017-06-02 03:39:19.138713: step 40220, loss = 0.92 (1406.2 examples/sec; 0.091 sec/batch)
2017-06-02 03:39:20.016054: step 40230, loss = 0.72 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:39:20.915544: step 40240, loss = 0.74 (1423.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:39:21.812345: step 40250, loss = 0.87 (1427.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:39:22.692002: step 40260, loss = 0.72 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:39:23.586971: step 40270, loss = 0.75 (1430.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:39:24.453897: step 40280, loss = 0.75 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:39:25.337846: step 40290, loss = 0.71 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:39:26.311759: step 40300, loss = 0.64 (1314.3 examples/sec; 0.097 sec/batch)
2017-06-02 03:39:27.099759: step 40310, loss = 0.89 (1624.4 examples/sec; 0.079 sec/batch)
2017-06-02 03:39:27.975656: step 40320, loss = 0.79 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:39:28.859936: step 40330, loss = 0.71 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:39:29.732243: step 40340, loss = 0.75 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:39:30.607382: step 40350, loss = 0.84 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:39:31.492807: step 40360, loss = 0.69 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:39:32.368783: step 40370, loss = 0.82 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:39:33.222021: step 40380, loss = 0.87 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 03:39:34.091936: step 40390, loss = 0.83 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:39:35.061238: step 40400, loss = 0.93 (1320.5 examples/sec; 0.097 sec/batch)
2017-06-02 03:39:35.852928: step 40410, loss = 0.88 (1616.8 examples/sec; 0.079 sec/batch)
2017-06-02 03:39:36.707053: step 40420, loss = 0.82 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 03:39:37.576218: step 40430, loss = 0.84 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:39:38.442216: step 40440, loss = 0.96 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:39:39.322236: step 40450, loss = 0.76 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:39:40.150612: step 40460, loss = 0.71 (1545.2 examples/sec; 0.083 sec/batch)
2017-06-02 03:39:41.031013: step 40470, loss = 0.75 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:39:41.916942: step 40480, loss = 0.92 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:39:42.808044: step 40490, loss = 0.85 (1436.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:39:43.821317: step 40500, loss = 0.73 (1263.2 examples/sec; 0.101 sec/batch)
2017-06-02 03:39:44.546925: step 40510, loss = 0.77 (1764.0 examples/sec; 0.073 sec/batch)
2017-06-02 03:39:45.412307: step 40520, loss = 0.82 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:39:46.303232: step 40530, loss = 0.72 (1436.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:39:47.189337: step 40540, loss = 0.83 (1444.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:39:48.044160: step 40550, loss = 0.75 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 03:39:48.913355: step 40560, loss = 0.77 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:39:49.783710: step 40570, loss = 0.58 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:39:50.678389: step 40580, loss = 0.81 (1430.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:39:51.605790: step 40590, loss = 0.91 (1380.2 examples/sec; 0.093 sec/batch)
2017-06-02 03:39:52.608845: step 40600, loss = 0.74 (1276.1 examples/sec; 0.100 sec/batch)
2017-06-02 03:39:53.389744: step 40610, loss = 0.86 (1639.1 examples/sec; 0.078 sec/batch)
2017-06-02 03:39:54.292813: step 40620, loss = 0.73 (1417.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:39:55.166042: step 40630, loss = 0.89 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:39:56.020061: step 40640, loss = 0.76 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 03:39:56.897327: step 40650, loss = 0.81 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:39:57.765472: step 40660, loss = 0.87 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:39:58.627327: step 40670, loss = 0.86 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 03:39:59.499540: step 40680, loss = 0.69 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:40:00.381344: step 40690, loss = 0.76 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:40:01.349647: step 40700, loss = 0.91 (1321.9 examples/sec; 0.097 sec/batch)
2017-06-02 03:40:02.138502: step 40710, loss = 0.79 (1622.6 examples/sec; 0.079 sec/batch)
2017-06-02 03:40:02.998619: step 40720, loss = 0.83 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 03:40:03.868675: step 40730, loss = 0.69 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:40:04.741350: step 40740, loss = 0.73 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:40:05.641868: step 40750, loss = 0.83 (1421.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:40:06.523669: step 40760, loss = 0.75 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:40:07.411335: step 40770, loss = 0.80 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:40:08.323909: step 40780, loss = 0.74 (1402.6 examples/sec; 0.091 sec/batch)
2017-06-02 03:40:09.205303: step 40790, loss = 0.78 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:40:10.184972: step 40800, loss = 0.90 (1306.6 examples/sec; 0.098 sec/batch)
2017-06-02 03:40:10.963823: step 40810, loss = 0.85 (1643.4 examples/sec; 0.078 sec/batch)
2017-06-02 03:40:11.844885: step 40820, loss = 0.66 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:40:12.710439: step 40830, loss = 0.87 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:40:13.581274: step 40840, loss = 0.97 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:40:14.473641: step 40850, loss = 0.72 (1434.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:40:15.355826: step 40860, loss = 0.65 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:40:16.252892: step 40870, loss = 0.86 (1426.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:40:17.134298: step 40880, loss = 0.87 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:40:17.993051: step 40890, loss = 0.71 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 03:40:18.931534: step 40900, loss = 0.78 (1363.9 examples/sec; 0.094 sec/batch)
2017-06-02 03:40:19.715168: step 40910, loss = 0.84 (1633.4 examples/sec; 0.078 sec/batch)
2017-06-02 03:40:20.587889: step 40920, loss = 0.72 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:40:21.491625: step 40930, loss = 0.68 (1416.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:40:22.371578: step 40940, loss = 0.82 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:40:23.274248: step 40950, loss = 0.82 (1418.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:40:24.163801: step 40960, loss = 0.68 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:40:25.040649: step 40970, loss = 0.86 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:40:25.935882: step 40980, loss = 0.69 (1429.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:40:26.780932: step 40990, loss = 0.74 (1514.7 examples/sec; 0.085 sec/batch)
2017-06-02 03:40:27.760524: step 41000, loss = 0.76 (1306.7 examples/sec; 0.098 sec/batch)
2017-06-02 03:40:28.549275: step 41010, loss = 0.78 (1622.8 examples/sec; 0.079 sec/batch)
2017-06-02 03:40:29.380212: step 41020, loss = 0.81 (1540.4 examples/sec; 0.083 sec/batch)
2017-06-02 03:40:30.271655: step 41030, loss = 0.62 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:40:31.167856: step 41040, loss = 0.76 (1428.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:40:32.031967: step 41050, loss = 0.68 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 03:40:32.891566: step 41060, loss = 0.73 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:40:33.750248: step 41070, loss = 0.67 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 03:40:34.616784: step 41080, loss = 0.77 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:40:35.493515: step 41090, loss = 0.77 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:40:36.490479: step 41100, loss = 0.78 (1283.9 examples/sec; 0.100 sec/batch)
2017-06-02 03:40:37.245825: step 41110, loss = 0.78 (1694.6 examples/sec; 0.076 sec/batch)
2017-06-02 03:40:38.110554: step 41120, loss = 0.80 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 03:40:38.983814: step 41130, loss = 0.64 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:40:39.848393: step 41140, loss = 0.92 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 03:40:40.702765: step 41150, loss = 0.61 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 03:40:41.584958: step 41160, loss = 0.83 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:40:42.450171: step 41170, loss = 0.90 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:40:43.341087: step 41180, loss = 0.72 (1436.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:40:44.219640: step 41190, loss = 0.86 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:40:45.205430: step 41200, loss = 0.76 (1298.4 examples/sec; 0.099 sec/batch)
2017-06-02 03:40:46.000658: step 41210, loss = 0.82 (1609.6 examples/sec; 0.080 sec/batch)
2017-06-02 03:40:46.897721: step 41220, loss = 0.77 (1426.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:40:47.791305: step 41230, loss = 0.68 (1432.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:40:48.699711: step 41240, loss = 0.81 (1409.1 examples/sec; 0.091 sec/batch)
2017-06-02 03:40:49.588084: step 41250, loss = 0.72 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:40:50.469744: step 41260, loss = 0.73 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:40:51.353805: step 41270, loss = 1.01 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:40:52.234672: step 41280, loss = 1.00 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:40:53.136616: step 41290, loss = 0.79 (1419.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:40:54.111801: step 41300, loss = 0.78 (1312.6 examples/sec; 0.098 sec/batch)
2017-06-02 03:40:54.893280: step 41310, loss = 0.81 (1637.9 examples/sec; 0.078 sec/batch)
2017-06-02 03:40:55.792975: step 41320, loss = 0.78 (1422.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:40:56.683020: step 41330, loss = 0.77 (1438.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:40:57.559395: step 41340, loss = 0.59 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:40:58.456673: step 41350, loss = 0.74 (1426.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:40:59.365487: step 41360, loss = 0.78 (1408.4 examples/sec; 0.091 sec/batch)
2017-06-02 03:41:00.277394: step 41370, loss = 0.80 (1403.6 examples/sec; 0.091 sec/batch)
2017-06-02 03:41:01.158508: step 41380, loss = 0.63 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:41:02.046456: step 41390, loss = 0.79 (1441.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:41:03.051291: step 41400, loss = 0.82 (1273.8 examples/sec; 0.100 sec/batch)
2017-06-02 03:41:03.841826: step 41410, loss = 0.84 (1619.2 examples/sec; 0.079 sec/batch)
2017-06-02 03:41:04.725040: step 41420, loss = 0.72 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:41:05.608037: step 41430, loss = 0.90 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:41:06.502941: step 41440, loss = 0.85 (1430.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:41:07.374198: step 41450, loss = 0.81 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:41:08.277951: step 41460, loss = 0.76 (1416.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:41:09.182072: step 41470, loss = 0.87 (1415.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:41:10.048467: step 41480, loss = 0.81 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:41:10.905713: step 41490, loss = 0.68 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 03:41:11.878148: step 41500, loss = 0.64 (1316.3 examples/sec; 0.097 sec/batch)
2017-06-02 03:41:12.670187: step 41510, loss = 0.83 (1616.1 examples/sec; 0.079 sec/batch)
2017-06-02 03:41:13.550826: step 41520, loss = 0.83 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:41:14.427195: step 41530, loss = 0.83 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:41:15.306350: step 41540, loss = 0.74 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:41:16.203404: step 41550, loss = 0.76 (1426.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:41:17.083534: step 41560, loss = 0.85 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:41:17.955428: step 41570, loss = 0.75 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:41:18.818853: step 41580, loss = 0.65 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 03:41:19.656543: step 41590, loss = 0.53 (1528.0 examples/sec; 0.084 sec/batch)
2017-06-02 03:41:20.623209: step 41600, loss = 0.88 (1324.1 examples/sec; 0.097 sec/batch)
2017-06-02 03:41:21.410642: step 41610, loss = 0.67 (1625.6 examples/sec; 0.079 sec/batch)
2017-06-02 03:41:22.279600: step 41620, loss = 0.71 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:41:23.159514: step 41630, loss = 0.90 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:41:24.057643: step 41640, loss = 0.74 (1425.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:41:24.933508: step 41650, loss = 0.81 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:41:25.780616: step 41660, loss = 0.72 (1511.0 examples/sec; 0.085 sec/batch)
2017-06-02 03:41:26.641758: step 41670, loss = 0.80 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 03:41:27.512974: step 41680, loss = 0.81 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:41:28.405067: step 41690, loss = 0.75 (1434.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:41:29.435789: step 41700, loss = 0.81 (1241.9 examples/sec; 0.103 sec/batch)
2017-06-02 03:41:30.171918: step 41710, loss = 0.86 (1738.8 examples/sec; 0.074 sec/batch)
2017-06-02 03:41:31.048082: step 41720, loss = 0.82 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:41:31.948108: step 41730, loss = 0.80 (1422.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:41:32.797385: step 41740, loss = 0.61 (1507.2 examples/sec; 0.085 sec/batch)
2017-06-02 03:41:33.684942: step 41750, loss = 0.79 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:41:34.553161: step 41760, loss = 0.70 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:41:35.410141: step 41770, loss = 0.85 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 03:41:36.296055: step 41780, loss = 0.69 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:41:37.187489: step 41790, loss = 0.73 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:41:38.141707: step 41800, loss = 0.90 (1341.4 examples/sec; 0.095 sec/batch)
2017-06-02 03:41:38.935934: step 41810, loss = 0.85 (1611.6 examples/sec; 0.079 sec/batch)
2017-06-02 03:41:39.838813: step 41820, loss = 0.78 (1417.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:41:40.703174: step 41830, loss = 0.74 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 03:41:41.601379: step 41840, loss = 0.88 (1425.1 examples/sec; 0.090 sec/batch)
2017-06-02 03:41:42.491153: step 41850, loss = 0.79 (1438.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:41:43.353288: step 41860, loss = 0.67 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 03:41:44.250438: step 41870, loss = 0.90 (1426.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:41:45.121790: step 41880, loss = 0.71 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:41:46.004265: step 41890, loss = 0.75 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:41:47.001659: step 41900, loss = 0.73 (1283.3 examples/sec; 0.100 sec/batch)
2017-06-02 03:41:47.793564: step 41910, loss = 0.61 (1616.4 examples/sec; 0.079 sec/batch)
2017-06-02 03:41:48.685375: step 41920, loss = 0.85 (1435.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:41:49.565934: step 41930, loss = 0.77 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:41:50.461917: step 41940, loss = 0.84 (1428.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:41:51.364054: step 41950, loss = 0.67 (1418.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:41:52.233831: step 41960, loss = 0.66 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:41:53.101851: step 41970, loss = 0.58 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:41:53.946853: step 41980, loss = 0.73 (1514.8 examples/sec; 0.085 sec/batch)
2017-06-02 03:41:54.823200: step 41990, loss = 0.75 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:41:55.780469: step 42000, loss = 0.87 (1337.1 examples/sec; 0.096 sec/batch)
2017-06-02 03:41:56.552411: step 42010, loss = 0.82 (1658.2 examples/sec; 0.077 sec/batch)
2017-06-02 03:41:57.465825: step 42020, loss = 0.76 (1401.4 examples/sec; 0.091 sec/batch)
2017-06-02 03:41:58.357196: step 42030, loss = 0.58 (1436.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:41:59.256885: step 42040, loss = 0.75 (1422.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:42:00.101561: step 42050, loss = 0.74 (1515.4 examples/sec; 0.084 sec/batch)
2017-06-02 03:42:00.967262: step 42060, loss = 0.61 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:42:01.865272: step 42070, loss = 0.92 (1425.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:42:02.768086: step 42080, loss = 0.73 (1417.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:42:03.679437: step 42090, loss = 0.92 (1404.5 examples/sec; 0.091 sec/batch)
2017-06-02 03:42:04.678683: step 42100, loss = 0.67 (1281.0 examples/sec; 0.100 sec/batch)
2017-06-02 03:42:05.468176: step 42110, loss = 0.72 (1621.3 examples/sec; 0.079 sec/batch)
2017-06-02 03:42:06.362350: step 42120, loss = 0.73 (1431.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:42:07.231367: step 42130, loss = 0.74 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:42:08.153463: step 42140, loss = 0.85 (1388.1 examples/sec; 0.092 sec/batch)
2017-06-02 03:42:09.040036: step 42150, loss = 0.81 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:42:09.912209: step 42160, loss = 0.74 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:42:10.789562: step 42170, loss = 0.74 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:42:11.683749: step 42180, loss = 0.60 (1431.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:42:12.571299: step 42190, loss = 0.85 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:42:13.551102: step 42200, loss = 0.84 (1306.4 examples/sec; 0.098 sec/batch)
2017-06-02 03:42:14.330784: step 42210, loss = 0.75 (1641.7 examples/sec; 0.078 sec/batch)
2017-06-02 03:42:15.205604: step 42220, loss = 0.87 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:42:16.073507: step 42230, loss = 0.68 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:42:16.963329: step 42240, loss = 0.69 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:42:17.850241: step 42250, loss = 0.83 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:42:18.735960: step 42260, loss = 1.02 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:42:19.609955: step 42270, loss = 0.73 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:42:20.485394: step 42280, loss = 0.86 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:42:21.355904: step 42290, loss = 0.74 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:42:22.339403: step 42300, loss = 0.76 (1301.5 examples/sec; 0.098 sec/batch)
2017-06-02 03:42:23.123485: step 42310, loss = 0.89 (1632.5 examples/sec; 0.078 sec/batch)
2017-06-02 03:42:23.997984: step 42320, loss = 0.77 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:42:24.862918: step 42330, loss = 0.78 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 03:42:25.735521: step 42340, loss = 0.74 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:42:26.607316: step 42350, loss = 0.81 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:42:27.499422: step 42360, loss = 0.89 (1434.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:42:28.354160: step 42370, loss = 0.68 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 03:42:29.208129: step 42380, loss = 0.72 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 03:42:30.083466: step 42390, loss = 0.70 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:42:31.091886: step 42400, loss = 0.85 (1269.3 examples/sec; 0.101 sec/batch)
2017-06-02 03:42:31.811288: step 42410, loss = 0.54 (1779.3 examples/sec; 0.072 sec/batch)
2017-06-02 03:42:32.678589: step 42420, loss = 0.65 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:42:33.533632: step 42430, loss = 0.80 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 03:42:34.324643: step 42440, loss = 0.99 (1617.8 examples/sec; 0.079 sec/batch)
2017-06-02 03:42:35.211421: step 42450, loss = 0.74 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:42:36.068869: step 42460, loss = 0.69 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 03:42:36.918793: step 42470, loss = 0.93 (1506.0 examples/sec; 0.085 sec/batch)
2017-06-02 03:42:37.801512: step 42480, loss = 0.80 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:42:38.672139: step 42490, loss = 0.71 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:42:39.639567: step 42500, loss = 0.81 (1323.1 examples/sec; 0.097 sec/batch)
2017-06-02 03:42:40.421305: step 42510, loss = 0.84 (1637.4 examples/sec; 0.078 sec/batch)
2017-06-02 03:42:41.275234: step 42520, loss = 0.59 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 03:42:42.138180: step 42530, loss = 0.75 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 03:42:43.005740: step 42540, loss = 0.60 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:42:43.896244: step 42550, loss = 0.66 (1437.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:42:44.773619: step 42560, loss = 0.76 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:42:45.651728: step 42570, loss = 0.91 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:42:46.533598: step 42580, loss = 0.72 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:42:47.407906: step 42590, loss = 0.93 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:42:48.381284: step 42600, loss = 0.78 (1315.0 examples/sec; 0.097 sec/batch)
2017-06-02 03:42:49.165621: step 42610, loss = 0.84 (1631.9 examples/sec; 0.078 sec/batch)
2017-06-02 03:42:50.028719: step 42620, loss = 0.72 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 03:42:50.931031: step 42630, loss = 0.71 (1418.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:42:51.817651: step 42640, loss = 0.81 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:42:52.677832: step 42650, loss = 0.83 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:42:53.558437: step 42660, loss = 0.74 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:42:54.396596: step 42670, loss = 0.92 (1527.2 examples/sec; 0.084 sec/batch)
2017-06-02 03:42:55.281909: step 42680, loss = 0.89 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:42:56.164692: step 42690, loss = 0.71 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:42:57.136508: step 42700, loss = 0.69 (1317.1 examples/sec; 0.097 sec/batch)
2017-06-02 03:42:57.917316: step 42710, loss = 0.82 (1639.3 examples/sec; 0.078 sec/batch)
2017-06-02 03:42:58.793253: step 42720, loss = 0.82 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:42:59.630514: step 42730, loss = 0.72 (1528.8 examples/sec; 0.084 sec/batch)
2017-06-02 03:43:00.499573: step 42740, loss = 0.67 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:43:01.369855: step 42750, loss = 0.64 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:43:02.246337: step 42760, loss = 0.62 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:43:03.148755: step 42770, loss = 0.76 (1418.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:43:04.037594: step 42780, loss = 0.75 (1440.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:43:04.920199: step 42790, loss = 0.76 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:43:05.934588: step 42800, loss = 0.85 (1261.8 examples/sec; 0.101 sec/batch)
2017-06-02 03:43:06.712416: step 42810, loss = 0.73 (1645.6 examples/sec; 0.078 sec/batch)
2017-06-02 03:43:07.605960: step 42820, loss = 0.78 (1432.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:43:08.494024: step 42830, loss = 0.86 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:43:09.363990: step 42840, loss = 0.74 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:43:10.236185: step 42850, loss = 0.95 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:43:11.129329: step 42860, loss = 0.77 (1433.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:43:12.000358: step 42870, loss = 0.73 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:43:12.869181: step 42880, loss = 0.70 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:43:13.725026: step 42890, loss = 0.89 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 03:43:14.729037: step 42900, loss = 0.61 (1274.9 examples/sec; 0.100 sec/batch)
2017-06-02 03:43:15.489474: step 42910, loss = 0.70 (1683.2 examples/sec; 0.076 sec/batch)
2017-06-02 03:43:16.393640: step 42920, loss = 0.90 (1415.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:43:17.240215: step 42930, loss = 0.76 (1512.0 examples/sec; 0.085 sec/batch)
2017-06-02 03:43:18.105169: step 42940, loss = 0.97 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 03:43:18.978278: step 42950, loss = 0.61 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:43:19.835360: step 42960, loss = 0.71 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 03:43:20.725799: step 42970, loss = 0.89 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:43:21.561379: step 42980, loss = 0.76 (1531.9 examples/sec; 0.084 sec/batch)
2017-06-02 03:43:22.424518: step 42990, loss = 0.71 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 03:43:23.413811: step 43000, loss = 0.80 (1293.9 examples/sec; 0.099 sec/batch)
2017-06-02 03:43:24.181031: step 43010, loss = 0.72 (1668.4 examples/sec; 0.077 sec/batch)
2017-06-02 03:43:25.029516: step 43020, loss = 0.72 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 03:43:25.901623: step 43030, loss = 0.75 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:43:26.772255: step 43040, loss = 0.89 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:43:27.633436: step 43050, loss = 0.70 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 03:43:28.493325: step 43060, loss = 0.77 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 03:43:29.356885: step 43070, loss = 0.69 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 03:43:30.204241: step 43080, loss = 0.79 (1510.6 examples/sec; 0.085 sec/batch)
2017-06-02 03:43:31.079773: step 43090, loss = 0.73 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:43:32.041700: step 43100, loss = 0.70 (1330.6 examples/sec; 0.096 sec/batch)
2017-06-02 03:43:32.855227: step 43110, loss = 0.71 (1573.4 examples/sec; 0.081 sec/batch)
2017-06-02 03:43:33.725349: step 43120, loss = 0.78 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:43:34.608886: step 43130, loss = 0.69 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:43:35.496402: step 43140, loss = 0.73 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:43:36.396049: step 43150, loss = 0.83 (1422.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:43:37.282317: step 43160, loss = 0.76 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:43:38.175811: step 43170, loss = 0.86 (1432.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:43:39.064188: step 43180, loss = 0.64 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:43:39.958128: step 43190, loss = 0.76 (1431.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:43:40.911807: step 43200, loss = 0.79 (1342.2 examples/sec; 0.095 sec/batch)
2017-06-02 03:43:41.722633: step 43210, loss = 0.70 (1578.6 examples/sec; 0.081 sec/batch)
2017-06-02 03:43:42.616332: step 43220, loss = 0.71 (1432.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:43:43.490169: step 43230, loss = 0.71 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:43:44.353509: step 43240, loss = 0.80 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 03:43:45.226529: step 43250, loss = 0.79 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:43:46.125600: step 43260, loss = 0.78 (1423.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:43:47.014648: step 43270, loss = 0.77 (1439.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:43:47.886850: step 43280, loss = 0.94 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:43:48.747070: step 43290, loss = 0.77 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 03:43:49.737599: step 43300, loss = 0.65 (1292.2 examples/sec; 0.099 sec/batch)
2017-06-02 03:43:50.491568: step 43310, loss = 0.69 (1697.7 examples/sec; 0.075 sec/batch)
2017-06-02 03:43:51.379063: step 43320, loss = 0.75 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:43:52.246080: step 43330, loss = 0.72 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:43:53.107432: step 43340, loss = 0.59 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:43:53.971918: step 43350, loss = 0.86 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 03:43:54.832605: step 43360, loss = 0.71 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 03:43:55.689470: step 43370, loss = 0.74 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 03:43:56.555310: step 43380, loss = 0.87 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:43:57.424573: step 43390, loss = 0.68 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:43:58.427256: step 43400, loss = 0.73 (1276.6 examples/sec; 0.100 sec/batch)
2017-06-02 03:43:59.216121: step 43410, loss = 0.72 (1622.6 examples/sec; 0.079 sec/batch)
2017-06-02 03:44:00.087943: step 43420, loss = 0.66 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:44:01.000508: step 43430, loss = 0.78 (1402.6 examples/sec; 0.091 sec/batch)
2017-06-02 03:44:01.887882: step 43440, loss = 0.69 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:44:02.768401: step 43450, loss = 0.79 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:44:03.691172: step 43460, loss = 0.73 (1387.1 examples/sec; 0.092 sec/batch)
2017-06-02 03:44:04.597423: step 43470, loss = 0.73 (1412.4 examples/sec; 0.091 sec/batch)
2017-06-02 03:44:05.484370: step 43480, loss = 0.94 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:44:06.372249: step 43490, loss = 0.67 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:44:07.355203: step 43500, loss = 0.70 (1302.2 examples/sec; 0.098 sec/batch)
2017-06-02 03:44:08.151890: step 43510, loss = 0.78 (1606.7 examples/sec; 0.080 sec/batch)
2017-06-02 03:44:09.040641: step 43520, loss = 0.78 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:44:09.937943: step 43530, loss = 0.69 (1426.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:44:10.829673: step 43540, loss = 0.70 (1435.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:44:11.695083: step 43550, loss = 0.71 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:44:12.562616: step 43560, loss = 0.86 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:44:13.407465: step 43570, loss = 0.74 (1515.1 examples/sec; 0.084 sec/batch)
2017-06-02 03:44:14.258656: step 43580, loss = 0.74 (1503.8 examples/sec; 0.085 sec/batch)
2017-06-02 03:44:15.098423: step 43590, loss = 0.55 (1524.2 examples/sec; 0.084 sec/batch)
2017-06-02 03:44:16.060148: step 43600, loss = 0.79 (1330.9 examples/sec; 0.096 sec/batch)
2017-06-02 03:44:16.868592: step 43610, loss = 0.63 (1583.3 examples/sec; 0.081 sec/batch)
2017-06-02 03:44:17.761201: step 43620, loss = 0.66 (1434.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:44:18.620092: step 43630, loss = 0.76 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 03:44:19.470002: step 43640, loss = 0.78 (1506.0 examples/sec; 0.085 sec/batch)
2017-06-02 03:44:20.365303: step 43650, loss = 0.78 (1429.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:44:21.212346: step 43660, loss = 0.73 (1511.1 examples/sec; 0.085 sec/batch)
2017-06-02 03:44:22.084258: step 43670, loss = 0.76 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:44:22.959463: step 43680, loss = 0.89 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:44:23.849607: step 43690, loss = 0.76 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:44:24.813597: step 43700, loss = 0.61 (1327.8 examples/sec; 0.096 sec/batch)
2017-06-02 03:44:25.607162: step 43710, loss = 0.71 (1613.0 examples/sec; 0.079 sec/batch)
2017-06-02 03:44:26.501665: step 43720, loss = 0.90 (1431.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:44:27.330767: step 43730, loss = 0.76 (1543.8 examples/sec; 0.083 sec/batch)
2017-06-02 03:44:28.195659: step 43740, loss = 0.71 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 03:44:29.047175: step 43750, loss = 0.67 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 03:44:29.874187: step 43760, loss = 0.69 (1547.7 examples/sec; 0.083 sec/batch)
2017-06-02 03:44:30.757247: step 43770, loss = 0.71 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:44:31.622502: step 43780, loss = 0.77 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:44:32.476981: step 43790, loss = 0.92 (1498.0 examples/sec; 0.085 sec/batch)
2017-06-02 03:44:33.406454: step 43800, loss = 0.73 (1377.1 examples/sec; 0.093 sec/batch)
2017-06-02 03:44:34.204807: step 43810, loss = 0.63 (1603.3 examples/sec; 0.080 sec/batch)
2017-06-02 03:44:35.065220: step 43820, loss = 0.58 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 03:44:35.916305: step 43830, loss = 0.82 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 03:44:36.792435: step 43840, loss = 0.80 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:44:37.677495: step 43850, loss = 0.73 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:44:38.547362: step 43860, loss = 0.82 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:44:39.413894: step 43870, loss = 0.81 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:44:40.295685: step 43880, loss = 0.82 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:44:41.172580: step 43890, loss = 0.79 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:44:42.125239: step 43900, loss = 0.78 (1343.6 examples/sec; 0.095 sec/batch)
2017-06-02 03:44:42.893707: step 43910, loss = 0.76 (1665.7 examples/sec; 0.077 sec/batch)
2017-06-02 03:44:43.773336: step 43920, loss = 0.66 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:44:44.659931: step 43930, loss = 0.72 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:44:45.544521: step 43940, loss = 0.71 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:44:46.415544: step 43950, loss = 0.67 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:44:47.284442: step 43960, loss = 0.72 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:44:48.153552: step 43970, loss = 0.79 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:44:49.011278: step 43980, loss = 0.88 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 03:44:49.899435: step 43990, loss = 0.70 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:44:50.879085: step 44000, loss = 0.91 (1306.6 examples/sec; 0.098 sec/batch)
2017-06-02 03:44:51.648859: step 44010, loss = 0.83 (1662.8 examples/sec; 0.077 sec/batch)
2017-06-02 03:44:52.531487: step 44020, loss = 0.82 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:44:53.377467: step 44030, loss = 0.66 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 03:44:54.238631: step 44040, loss = 0.87 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 03:44:55.122392: step 44050, loss = 0.66 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:44:55.979306: step 44060, loss = 0.68 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 03:44:56.870935: step 44070, loss = 0.67 (1435.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:44:57.758306: step 44080, loss = 0.85 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:44:58.636012: step 44090, loss = 0.81 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:44:59.598268: step 44100, loss = 0.76 (1330.2 examples/sec; 0.096 sec/batch)
2017-06-02 03:45:00.389850: step 44110, loss = 0.71 (1617.0 examples/sec; 0.079 sec/batch)
2017-06-02 03:45:01.243569: step 44120, loss = 0.83 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 03:45:02.119882: step 44130, loss = 0.84 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:45:03.009492: step 44140, loss = 0.67 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:45:03.893685: step 44150, loss = 0.72 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:45:04.750681: step 44160, loss = 0.73 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 03:45:05.598577: step 44170, loss = 0.72 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 03:45:06.465786: step 44180, loss = 0.81 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:45:07.344534: step 44190, loss = 0.93 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:45:08.324329: step 44200, loss = 0.75 (1306.4 examples/sec; 0.098 sec/batch)
2017-06-02 03:45:09.089174: step 44210, loss = 0.72 (1673.5 examples/sec; 0.076 sec/batch)
2017-06-02 03:45:09.956609: step 44220, loss = 0.78 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:45:10.814939: step 44230, loss = 0.76 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 03:45:11.676765: step 44240, loss = 0.75 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 03:45:12.564975: step 44250, loss = 0.80 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:45:13.434919: step 44260, loss = 0.69 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:45:14.290629: step 44270, loss = 0.73 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 03:45:15.144008: step 44280, loss = 0.87 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 03:45:16.017097: step 44290, loss = 0.63 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:45:17.011154: step 44300, loss = 0.69 (1287.6 examples/sec; 0.099 sec/batch)
2017-06-02 03:45:17.778317: step 44310, loss = 0.77 (1668.5 examples/sec; 0.077 sec/batch)
2017-06-02 03:45:18.651194: step 44320, loss = 0.73 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:45:19.526644: step 44330, loss = 0.73 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:45:20.353240: step 44340, loss = 0.92 (1548.5 examples/sec; 0.083 sec/batch)
2017-06-02 03:45:21.208678: step 44350, loss = 0.94 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 03:45:22.064567: step 44360, loss = 0.81 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 03:45:22.914610: step 44370, loss = 0.69 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 03:45:23.778337: step 44380, loss = 0.76 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 03:45:24.665598: step 44390, loss = 0.72 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:45:25.659363: step 44400, loss = 0.90 (1288.0 examples/sec; 0.099 sec/batch)
2017-06-02 03:45:26.452385: step 44410, loss = 0.87 (1614.1 examples/sec; 0.079 sec/batch)
2017-06-02 03:45:27.339288: step 44420, loss = 1.05 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:45:28.246019: step 44430, loss = 0.70 (1411.7 examples/sec; 0.091 sec/batch)
2017-06-02 03:45:29.148763: step 44440, loss = 0.60 (1417.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:45:30.019251: step 44450, loss = 0.79 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:45:30.895550: step 44460, loss = 0.84 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:45:31.799426: step 44470, loss = 0.66 (1416.1 examples/sec; 0.090 sec/batch)
2017-06-02 03:45:32.678903: step 44480, loss = 0.74 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:45:33.545569: step 44490, loss = 0.68 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:45:34.517035: step 44500, loss = 0.79 (1317.6 examples/sec; 0.097 sec/batch)
2017-06-02 03:45:35.293155: step 44510, loss = 0.77 (1649.2 examples/sec; 0.078 sec/batch)
2017-06-02 03:45:36.158771: step 44520, loss = 0.74 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:45:37.013380: step 44530, loss = 0.89 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 03:45:37.898849: step 44540, loss = 0.74 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:45:38.758004: step 44550, loss = 0.77 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 03:45:39.588617: step 44560, loss = 0.92 (1541.0 examples/sec; 0.083 sec/batch)
2017-06-02 03:45:40.421461: step 44570, loss = 0.70 (1536.9 examples/sec; 0.083 sec/batch)
2017-06-02 03:45:41.302801: step 44580, loss = 0.65 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:45:42.180787: step 44590, loss = 0.71 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:45:43.153993: step 44600, loss = 0.77 (1315.2 examples/sec; 0.097 sec/batch)
2017-06-02 03:45:43.940309: step 44610, loss = 0.66 (1627.8 examples/sec; 0.079 sec/batch)
2017-06-02 03:45:44.813533: step 44620, loss = 0.67 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:45:45.676251: step 44630, loss = 0.82 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 03:45:46.553405: step 44640, loss = 0.59 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:45:47.421590: step 44650, loss = 0.73 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:45:48.305438: step 44660, loss = 0.73 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:45:49.165735: step 44670, loss = 0.75 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 03:45:50.041082: step 44680, loss = 0.70 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:45:50.904932: step 44690, loss = 0.75 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 03:45:51.901607: step 44700, loss = 0.79 (1284.3 examples/sec; 0.100 sec/batch)
2017-06-02 03:45:52.673131: step 44710, loss = 0.89 (1659.1 examples/sec; 0.077 sec/batch)
2017-06-02 03:45:53.534866: step 44720, loss = 0.79 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 03:45:54.399083: step 44730, loss = 0.74 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:45:55.243424: step 44740, loss = 0.81 (1516.0 examples/sec; 0.084 sec/batch)
2017-06-02 03:45:56.117233: step 44750, loss = 0.63 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:45:56.978014: step 44760, loss = 0.82 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 03:45:57.843684: step 44770, loss = 0.64 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:45:58.731748: step 44780, loss = 0.78 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:45:59.585150: step 44790, loss = 0.88 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 03:46:00.551498: step 44800, loss = 0.65 (1324.6 examples/sec; 0.097 sec/batch)
2017-06-02 03:46:01.325930: step 44810, loss = 0.75 (1652.8 examples/sec; 0.077 sec/batch)
2017-06-02 03:46:02.191094: step 44820, loss = 0.75 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:46:03.073998: step 44830, loss = 0.74 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:46:03.929487: step 44840, loss = 0.59 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 03:46:04.766719: step 44850, loss = 0.71 (1528.8 examples/sec; 0.084 sec/batch)
2017-06-02 03:46:05.612985: step 44860, loss = 0.88 (1512.5 examples/sec; 0.085 sec/batch)
2017-06-02 03:46:06.462668: step 44870, loss = 0.90 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 03:46:07.322810: step 44880, loss = 0.76 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:46:08.185035: step 44890, loss = 0.65 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 03:46:09.171689: step 44900, loss = 0.84 (1297.2 examples/sec; 0.099 sec/batch)
2017-06-02 03:46:09.885661: step 44910, loss = 0.64 (1792.8 examples/sec; 0.071 sec/batch)
2017-06-02 03:46:10.728900: step 44920, loss = 0.69 (1518.0 examples/sec; 0.084 sec/batch)
2017-06-02 03:46:11.589241: step 44930, loss = 0.57 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 03:46:12.470679: step 44940, loss = 0.72 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:46:13.326587: step 44950, loss = 0.64 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 03:46:14.204888: step 44960, loss = 0.70 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:46:15.040208: step 44970, loss = 0.79 (1532.4 examples/sec; 0.084 sec/batch)
2017-06-02 03:46:15.890214: step 44980, loss = 0.76 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 03:46:16.748430: step 44990, loss = 0.60 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 03:46:17.728951: step 45000, loss = 0.77 (1305.4 examples/sec; 0.098 sec/batch)
2017-06-02 03:46:18.516044: step 45010, loss = 0.77 (1626.2 examples/sec; 0.079 sec/batch)
2017-06-02 03:46:19.396440: step 45020, loss = 0.76 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:46:20.267042: step 45030, loss = 0.71 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:46:21.143690: step 45040, loss = 0.89 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:46:22.001625: step 45050, loss = 0.88 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 03:46:22.867154: step 45060, loss = 0.97 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:46:23.746113: step 45070, loss = 0.71 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:46:24.606517: step 45080, loss = 0.76 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 03:46:25.474655: step 45090, loss = 0.71 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:46:26.456340: step 45100, loss = 0.70 (1303.9 examples/sec; 0.098 sec/batch)
2017-06-02 03:46:27.230460: step 45110, loss = 0.86 (1653.4 examples/sec; 0.077 sec/batch)
2017-06-02 03:46:28.109514: step 45120, loss = 0.76 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:46:28.985930: step 45130, loss = 0.80 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:46:29.869659: step 45140, loss = 0.70 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:46:30.723519: step 45150, loss = 0.66 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 03:46:31.587919: step 45160, loss = 0.84 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 03:46:32.447669: step 45170, loss = 0.95 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 03:46:33.331395: step 45180, loss = 0.83 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:46:34.214517: step 45190, loss = 0.79 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:46:35.156242: step 45200, loss = 0.82 (1359.2 examples/sec; 0.094 sec/batch)
2017-06-02 03:46:35.919371: step 45210, loss = 0.69 (1677.3 examples/sec; 0.076 sec/batch)
2017-06-02 03:46:36.770222: step 45220, loss = 0.73 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 03:46:37.631881: step 45230, loss = 0.82 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 03:46:38.512982: step 45240, loss = 0.80 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:46:39.385557: step 45250, loss = 0.68 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:46:40.255603: step 45260, loss = 0.70 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:46:41.131018: step 45270, loss = 0.74 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:46:42.006310: step 45280, loss = 0.85 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:46:42.892960: step 45290, loss = 0.69 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:46:43.888683: step 45300, loss = 0.71 (1285.5 examples/sec; 0.100 sec/batch)
2017-06-02 03:46:44.631929: step 45310, loss = 0.85 (1722.3 examples/sec; 0.074 sec/batch)
2017-06-02 03:46:45.508827: step 45320, loss = 0.68 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:46:46.368411: step 45330, loss = 0.81 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:46:47.239564: step 45340, loss = 0.79 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:46:48.104364: step 45350, loss = 0.76 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:46:49.009926: step 45360, loss = 0.69 (1413.6 examples/sec; 0.091 sec/batch)
2017-06-02 03:46:49.895566: step 45370, loss = 0.78 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:46:50.741153: step 45380, loss = 0.78 (1513.7 examples/sec; 0.085 sec/batch)
2017-06-02 03:46:51.624834: step 45390, loss = 0.87 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:46:52.587854: step 45400, loss = 0.87 (1329.1 examples/sec; 0.096 sec/batch)
2017-06-02 03:46:53.394199: step 45410, loss = 0.86 (1587.4 examples/sec; 0.081 sec/batch)
2017-06-02 03:46:54.274353: step 45420, loss = 0.68 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:46:55.165484: step 45430, loss = 0.72 (1436.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:46:56.042306: step 45440, loss = 0.68 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:46:56.912308: step 45450, loss = 0.82 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:46:57.758558: step 45460, loss = 0.66 (1512.5 examples/sec; 0.085 sec/batch)
2017-06-02 03:46:58.607267: step 45470, loss = 0.76 (1508.2 examples/sec; 0.085 sec/batch)
2017-06-02 03:46:59.453995: step 45480, loss = 0.71 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 03:47:00.295581: step 45490, loss = 0.72 (1520.9 examples/sec; 0.084 sec/batch)
2017-06-02 03:47:01.282141: step 45500, loss = 0.82 (1297.4 examples/sec; 0.099 sec/batch)
2017-06-02 03:47:02.062874: step 45510, loss = 0.75 (1639.5 examples/sec; 0.078 sec/batch)
2017-06-02 03:47:02.940391: step 45520, loss = 0.81 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:47:03.802266: step 45530, loss = 0.77 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:47:04.658344: step 45540, loss = 0.77 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 03:47:05.546224: step 45550, loss = 0.77 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:47:06.438135: step 45560, loss = 0.77 (1435.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:47:07.323343: step 45570, loss = 0.84 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:47:08.236353: step 45580, loss = 0.79 (1402.0 examples/sec; 0.091 sec/batch)
2017-06-02 03:47:09.115851: step 45590, loss = 0.74 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:47:10.105120: step 45600, loss = 0.82 (1293.9 examples/sec; 0.099 sec/batch)
2017-06-02 03:47:10.878583: step 45610, loss = 0.76 (1654.9 examples/sec; 0.077 sec/batch)
2017-06-02 03:47:11.753810: step 45620, loss = 0.72 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:47:12.670130: step 45630, loss = 0.84 (1396.9 examples/sec; 0.092 sec/batch)
2017-06-02 03:47:13.555223: step 45640, loss = 0.77 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:47:14.449136: step 45650, loss = 0.71 (1431.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:47:15.303141: step 45660, loss = 0.71 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 03:47:16.163256: step 45670, loss = 0.71 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 03:47:17.007114: step 45680, loss = 0.85 (1516.9 examples/sec; 0.084 sec/batch)
2017-06-02 03:47:17.876092: step 45690, loss = 0.87 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:47:18.844616: step 45700, loss = 0.56 (1321.6 examples/sec; 0.097 sec/batch)
2017-06-02 03:47:19.630519: step 45710, loss = 0.80 (1628.7 examples/sec; 0.079 sec/batch)
2017-06-02 03:47:20.500066: step 45720, loss = 0.78 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:47:21.397817: step 45730, loss = 0.93 (1425.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:47:22.287869: step 45740, loss = 0.60 (1438.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:47:23.132490: step 45750, loss = 0.77 (1515.5 examples/sec; 0.084 sec/batch)
2017-06-02 03:47:24.008719: step 45760, loss = 0.52 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:47:24.859140: step 45770, loss = 0.99 (1505.1 examples/sec; 0.085 sec/batch)
2017-06-02 03:47:25.769105: step 45780, loss = 1.02 (1406.7 examples/sec; 0.091 sec/batch)
2017-06-02 03:47:26.640258: step 45790, loss = 0.57 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:47:27.583017: step 45800, loss = 0.67 (1357.7 examples/sec; 0.094 sec/batch)
2017-06-02 03:47:28.362592: step 45810, loss = 0.87 (1641.9 examples/sec; 0.078 sec/batch)
2017-06-02 03:47:29.226848: step 45820, loss = 0.74 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 03:47:30.100657: step 45830, loss = 0.70 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:47:30.948307: step 45840, loss = 0.82 (1510.1 examples/sec; 0.085 sec/batch)
2017-06-02 03:47:31.829544: step 45850, loss = 0.78 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:47:32.702195: step 45860, loss = 0.68 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:47:33.581393: step 45870, loss = 0.56 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:47:34.448528: step 45880, loss = 0.78 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:47:35.329671: step 45890, loss = 0.62 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:47:36.286043: step 45900, loss = 0.89 (1338.4 examples/sec; 0.096 sec/batch)
2017-06-02 03:47:37.068261: step 45910, loss = 0.64 (1636.4 examples/sec; 0.078 sec/batch)
2017-06-02 03:47:37.950393: step 45920, loss = 0.75 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:47:38.837274: step 45930, loss = 0.91 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:47:39.738093: step 45940, loss = 0.95 (1420.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:47:40.625146: step 45950, loss = 0.75 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:47:41.494273: step 45960, loss = 0.87 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:47:42.380030: step 45970, loss = 0.82 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:47:43.255753: step 45980, loss = 0.56 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:47:44.110656: step 45990, loss = 0.83 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 03:47:45.115695: step 46000, loss = 0.85 (1273.6 examples/sec; 0.101 sec/batch)
2017-06-02 03:47:45.861634: step 46010, loss = 0.78 (1716.0 examples/sec; 0.075 sec/batch)
2017-06-02 03:47:46.753573: step 46020, loss = 0.60 (1435.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:47:47.635360: step 46030, loss = 0.79 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:47:48.505419: step 46040, loss = 0.69 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:47:49.379579: step 46050, loss = 0.77 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:47:50.233756: step 46060, loss = 0.56 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 03:47:51.100277: step 46070, loss = 0.73 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:47:51.989114: step 46080, loss = 0.90 (1440.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:47:52.870654: step 46090, loss = 0.70 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:47:53.871285: step 46100, loss = 0.69 (1279.2 examples/sec; 0.100 sec/batch)
2017-06-02 03:47:54.657434: step 46110, loss = 0.68 (1628.2 examples/sec; 0.079 sec/batch)
2017-06-02 03:47:55.530095: step 46120, loss = 0.77 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:47:56.369503: step 46130, loss = 0.81 (1524.9 examples/sec; 0.084 sec/batch)
2017-06-02 03:47:57.283393: step 46140, loss = 0.98 (1400.6 examples/sec; 0.091 sec/batch)
2017-06-02 03:47:58.164509: step 46150, loss = 0.73 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:47:59.043159: step 46160, loss = 0.76 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:47:59.938148: step 46170, loss = 0.74 (1430.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:48:00.818587: step 46180, loss = 0.80 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:48:01.705394: step 46190, loss = 0.66 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:48:02.674845: step 46200, loss = 0.72 (1320.3 examples/sec; 0.097 sec/batch)
2017-06-02 03:48:03.454298: step 46210, loss = 0.60 (1642.2 examples/sec; 0.078 sec/batch)
2017-06-02 03:48:04.339959: step 46220, loss = 0.84 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:48:05.204165: step 46230, loss = 0.67 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:48:06.060830: step 46240, loss = 0.57 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 03:48:06.896243: step 46250, loss = 0.67 (1532.2 examples/sec; 0.084 sec/batch)
2017-06-02 03:48:07.775620: step 46260, loss = 0.82 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:48:08.628041: step 46270, loss = 0.71 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 03:48:09.491950: step 46280, loss = 0.73 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 03:48:10.381762: step 46290, loss = 0.82 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:48:11.416653: step 46300, loss = 0.63 (1236.8 examples/sec; 0.103 sec/batch)
2017-06-02 03:48:12.161151: step 46310, loss = 0.67 (1719.3 examples/sec; 0.074 sec/batch)
2017-06-02 03:48:13.055177: step 46320, loss = 0.79 (1431.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:48:13.958288: step 46330, loss = 0.74 (1417.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:48:14.850577: step 46340, loss = 0.66 (1434.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:48:15.733706: step 46350, loss = 0.81 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:48:16.606532: step 46360, loss = 0.74 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:48:17.483638: step 46370, loss = 0.77 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:48:18.363986: step 46380, loss = 0.70 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:48:19.255685: step 46390, loss = 0.94 (1435.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:48:20.279746: step 46400, loss = 0.65 (1250.8 examples/sec; 0.102 sec/batch)
2017-06-02 03:48:20.984021: step 46410, loss = 0.59 (1815.6 examples/sec; 0.071 sec/batch)
2017-06-02 03:48:21.857618: step 46420, loss = 0.73 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:48:22.727120: step 46430, loss = 0.74 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:48:23.604755: step 46440, loss = 0.78 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:48:24.482988: step 46450, loss = 0.85 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:48:25.354656: step 46460, loss = 0.67 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:48:26.257678: step 46470, loss = 0.68 (1417.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:48:27.107801: step 46480, loss = 0.96 (1505.7 examples/sec; 0.085 sec/batch)
2017-06-02 03:48:27.962058: step 46490, loss = 1.01 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 03:48:28.910371: step 46500, loss = 0.70 (1349.8 examples/sec; 0.095 sec/batch)
2017-06-02 03:48:29.685613: step 46510, loss = 0.70 (1651.1 examples/sec; 0.078 sec/batch)
2017-06-02 03:48:30.542550: step 46520, loss = 0.77 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 03:48:31.413598: step 46530, loss = 0.67 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:48:32.254870: step 46540, loss = 0.73 (1521.5 examples/sec; 0.084 sec/batch)
2017-06-02 03:48:33.102951: step 46550, loss = 0.68 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 03:48:33.989163: step 46560, loss = 0.66 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:48:34.860484: step 46570, loss = 0.77 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:48:35.762653: step 46580, loss = 0.79 (1418.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:48:36.670543: step 46590, loss = 0.68 (1409.9 examples/sec; 0.091 sec/batch)
2017-06-02 03:48:37.708015: step 46600, loss = 0.64 (1233.8 examples/sec; 0.104 sec/batch)
2017-06-02 03:48:38.483656: step 46610, loss = 0.67 (1650.3 examples/sec; 0.078 sec/batch)
2017-06-02 03:48:39.356344: step 46620, loss = 0.84 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:48:40.245616: step 46630, loss = 0.63 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:48:41.133836: step 46640, loss = 0.90 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:48:42.015763: step 46650, loss = 0.69 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:48:42.887893: step 46660, loss = 0.74 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:48:43.768510: step 46670, loss = 0.77 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:48:44.658150: step 46680, loss = 0.78 (1438.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:48:45.506581: step 46690, loss = 0.58 (1508.7 examples/sec; 0.085 sec/batch)
2017-06-02 03:48:46.479877: step 46700, loss = 0.76 (1315.1 examples/sec; 0.097 sec/batch)
2017-06-02 03:48:47.261470: step 46710, loss = 0.67 (1637.7 examples/sec; 0.078 sec/batch)
2017-06-02 03:48:48.131113: step 46720, loss = 0.71 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:48:48.988568: step 46730, loss = 0.69 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 03:48:49.855156: step 46740, loss = 0.76 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:48:50.693753: step 46750, loss = 0.71 (1526.4 examples/sec; 0.084 sec/batch)
2017-06-02 03:48:51.559124: step 46760, loss = 0.75 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:48:52.399380: step 46770, loss = 0.73 (1523.4 examples/sec; 0.084 sec/batch)
2017-06-02 03:48:53.284715: step 46780, loss = 0.68 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:48:54.137235: step 46790, loss = 0.85 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 03:48:55.090206: step 46800, loss = 0.72 (1343.2 examples/sec; 0.095 sec/batch)
2017-06-02 03:48:55.864214: step 46810, loss = 0.75 (1653.8 examples/sec; 0.077 sec/batch)
2017-06-02 03:48:56.723752: step 46820, loss = 0.66 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 03:48:57.601423: step 46830, loss = 0.81 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:48:58.454539: step 46840, loss = 0.72 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 03:48:59.334640: step 46850, loss = 0.64 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:49:00.193055: step 46860, loss = 0.62 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:49:01.080261: step 46870, loss = 0.66 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:49:01.958803: step 46880, loss = 0.64 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:49:02.838069: step 46890, loss = 0.90 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:49:03.806053: step 46900, loss = 0.75 (1322.3 examples/sec; 0.097 sec/batch)
2017-06-02 03:49:04.557845: step 46910, loss = 0.87 (1702.6 examples/sec; 0.075 sec/batch)
2017-06-02 03:49:05.382028: step 46920, loss = 0.77 (1553.0 examples/sec; 0.082 sec/batch)
2017-06-02 03:49:06.246610: step 46930, loss = 0.73 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 03:49:07.123677: step 46940, loss = 0.82 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:49:07.997256: step 46950, loss = 0.60 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:49:08.869913: step 46960, loss = 0.70 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:49:09.760498: step 46970, loss = 0.78 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:49:10.613648: step 46980, loss = 0.85 (1500.3 examples/sec; 0.085 sec/batch)
2017-06-02 03:49:11.488387: step 46990, loss = 0.66 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:49:12.479567: step 47000, loss = 0.75 (1291.4 examples/sec; 0.099 sec/batch)
2017-06-02 03:49:13.269053: step 47010, loss = 0.77 (1621.3 examples/sec; 0.079 sec/batch)
2017-06-02 03:49:14.150704: step 47020, loss = 0.58 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:49:15.001411: step 47030, loss = 0.79 (1504.6 examples/sec; 0.085 sec/batch)
2017-06-02 03:49:15.888074: step 47040, loss = 0.73 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:49:16.769376: step 47050, loss = 0.80 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:49:17.631367: step 47060, loss = 0.78 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 03:49:18.514012: step 47070, loss = 0.63 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:49:19.383107: step 47080, loss = 0.79 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:49:20.251017: step 47090, loss = 0.77 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:49:21.222673: step 47100, loss = 0.73 (1317.3 examples/sec; 0.097 sec/batch)
2017-06-02 03:49:21.993600: step 47110, loss = 0.92 (1660.3 examples/sec; 0.077 sec/batch)
2017-06-02 03:49:22.875714: step 47120, loss = 0.69 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:49:23.730615: step 47130, loss = 0.85 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 03:49:24.591753: step 47140, loss = 0.81 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 03:49:25.453422: step 47150, loss = 0.94 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 03:49:26.313259: step 47160, loss = 0.70 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 03:49:27.185626: step 47170, loss = 0.87 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:49:28.058872: step 47180, loss = 0.73 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:49:28.938557: step 47190, loss = 0.79 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:49:29.920705: step 47200, loss = 0.64 (1303.3 examples/sec; 0.098 sec/batch)
2017-06-02 03:49:30.691215: step 47210, loss = 0.72 (1661.2 examples/sec; 0.077 sec/batch)
2017-06-02 03:49:31.594609: step 47220, loss = 0.98 (1416.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:49:32.470967: step 47230, loss = 0.71 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:49:33.346430: step 47240, loss = 0.85 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:49:34.223404: step 47250, loss = 0.62 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:49:35.112781: step 47260, loss = 0.74 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:49:35.964556: step 47270, loss = 0.68 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 03:49:36.831865: step 47280, loss = 0.79 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:49:37.697040: step 47290, loss = 0.71 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:49:38.706685: step 47300, loss = 0.86 (1267.8 examples/sec; 0.101 sec/batch)
2017-06-02 03:49:39.440806: step 47310, loss = 0.71 (1743.8 examples/sec; 0.073 sec/batch)
2017-06-02 03:49:40.305090: step 47320, loss = 0.75 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 03:49:41.180212: step 47330, loss = 0.66 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:49:42.070115: step 47340, loss = 0.67 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:49:42.965297: step 47350, loss = 0.68 (1429.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:49:43.837894: step 47360, loss = 0.76 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:49:44.700535: step 47370, loss = 0.76 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 03:49:45.553258: step 47380, loss = 0.74 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 03:49:46.383030: step 47390, loss = 0.73 (1542.6 examples/sec; 0.083 sec/batch)
2017-06-02 03:49:47.364084: step 47400, loss = 0.92 (1304.7 examples/sec; 0.098 sec/batch)
2017-06-02 03:49:48.142389: step 47410, loss = 0.60 (1644.6 examples/sec; 0.078 sec/batch)
2017-06-02 03:49:49.026915: step 47420, loss = 0.73 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:49:49.898754: step 47430, loss = 0.78 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:49:50.771916: step 47440, loss = 0.84 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:49:51.750600: step 47450, loss = 0.82 (1307.9 examples/sec; 0.098 sec/batch)
2017-06-02 03:49:52.649084: step 47460, loss = 0.62 (1424.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:49:53.550548: step 47470, loss = 0.82 (1419.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:49:54.443147: step 47480, loss = 0.73 (1434.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:49:55.316042: step 47490, loss = 0.65 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:49:56.288323: step 47500, loss = 0.76 (1316.5 examples/sec; 0.097 sec/batch)
2017-06-02 03:49:57.089728: step 47510, loss = 0.83 (1597.2 examples/sec; 0.080 sec/batch)
2017-06-02 03:49:57.974042: step 47520, loss = 0.67 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:49:58.852339: step 47530, loss = 0.77 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:49:59.753605: step 47540, loss = 0.74 (1420.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:50:00.649868: step 47550, loss = 0.73 (1428.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:50:01.549137: step 47560, loss = 0.64 (1423.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:50:02.412937: step 47570, loss = 0.80 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 03:50:03.310220: step 47580, loss = 0.73 (1426.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:50:04.196622: step 47590, loss = 0.69 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:50:05.174569: step 47600, loss = 0.79 (1308.8 examples/sec; 0.098 sec/batch)
2017-06-02 03:50:05.986594: step 47610, loss = 0.68 (1576.3 examples/sec; 0.081 sec/batch)
2017-06-02 03:50:06.862011: step 47620, loss = 0.63 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:50:07.728251: step 47630, loss = 0.89 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:50:08.620055: step 47640, loss = 0.74 (1435.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:50:09.497648: step 47650, loss = 0.73 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:50:10.374264: step 47660, loss = 0.66 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:50:11.268192: step 47670, loss = 0.87 (1431.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:50:12.139619: step 47680, loss = 0.65 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:50:13.004417: step 47690, loss = 0.72 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:50:13.994625: step 47700, loss = 0.93 (1292.7 examples/sec; 0.099 sec/batch)
2017-06-02 03:50:14.769376: step 47710, loss = 0.70 (1652.1 examples/sec; 0.077 sec/batch)
2017-06-02 03:50:15.664535: step 47720, loss = 0.67 (1429.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:50:16.545061: step 47730, loss = 0.67 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:50:17.424445: step 47740, loss = 0.71 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:50:18.324375: step 47750, loss = 0.84 (1422.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:50:19.216749: step 47760, loss = 0.76 (1434.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:50:20.112948: step 47770, loss = 0.80 (1428.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:50:20.960637: step 47780, loss = 0.66 (1510.0 examples/sec; 0.085 sec/batch)
2017-06-02 03:50:21.846226: step 47790, loss = 0.79 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:50:22.807998: step 47800, loss = 0.71 (1330.9 examples/sec; 0.096 sec/batch)
2017-06-02 03:50:23.613234: step 47810, loss = 0.75 (1589.6 examples/sec; 0.081 sec/batch)
2017-06-02 03:50:24.489614: step 47820, loss = 0.84 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:50:25.357038: step 47830, loss = 0.71 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:50:26.255346: step 47840, loss = 0.65 (1424.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:50:27.150286: step 47850, loss = 0.60 (1430.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:50:28.042491: step 47860, loss = 0.74 (1434.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:50:28.937795: step 47870, loss = 0.64 (1429.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:50:29.821349: step 47880, loss = 0.67 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:50:30.695678: step 47890, loss = 0.77 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:50:31.665867: step 47900, loss = 0.87 (1319.3 examples/sec; 0.097 sec/batch)
2017-06-02 03:50:32.420191: step 47910, loss = 0.58 (1696.9 examples/sec; 0.075 sec/batch)
2017-06-02 03:50:33.291296: step 47920, loss = 0.77 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:50:34.150031: step 47930, loss = 0.88 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 03:50:35.032041: step 47940, loss = 0.71 (1451.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:50:35.897280: step 47950, loss = 0.71 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:50:36.769073: step 47960, loss = 0.78 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:50:37.641792: step 47970, loss = 0.77 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:50:38.493221: step 47980, loss = 0.68 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 03:50:39.326870: step 47990, loss = 0.55 (1535.4 examples/sec; 0.083 sec/batch)
2017-06-02 03:50:40.341554: step 48000, loss = 0.67 (1261.5 examples/sec; 0.101 sec/batch)
2017-06-02 03:50:41.072289: step 48010, loss = 0.90 (1751.7 examples/sec; 0.073 sec/batch)
2017-06-02 03:50:41.928646: step 48020, loss = 0.76 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 03:50:42.795842: step 48030, loss = 0.58 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:50:43.650683: step 48040, loss = 0.81 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 03:50:44.509285: step 48050, loss = 0.73 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 03:50:45.416942: step 48060, loss = 0.78 (1410.2 examples/sec; 0.091 sec/batch)
2017-06-02 03:50:46.260983: step 48070, loss = 0.66 (1516.5 examples/sec; 0.084 sec/batch)
2017-06-02 03:50:47.130206: step 48080, loss = 0.60 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:50:48.001622: step 48090, loss = 0.68 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:50:48.975269: step 48100, loss = 0.65 (1314.6 examples/sec; 0.097 sec/batch)
2017-06-02 03:50:49.733093: step 48110, loss = 0.89 (1689.1 examples/sec; 0.076 sec/batch)
2017-06-02 03:50:50.617149: step 48120, loss = 0.71 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:50:51.458921: step 48130, loss = 0.69 (1520.6 examples/sec; 0.084 sec/batch)
2017-06-02 03:50:52.314484: step 48140, loss = 0.78 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:50:53.192533: step 48150, loss = 0.78 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:50:54.081097: step 48160, loss = 0.74 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:50:54.937021: step 48170, loss = 0.95 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 03:50:55.824788: step 48180, loss = 0.63 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:50:56.704625: step 48190, loss = 0.84 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:50:57.662326: step 48200, loss = 0.79 (1336.5 examples/sec; 0.096 sec/batch)
2017-06-02 03:50:58.414370: step 48210, loss = 0.87 (1702.1 examples/sec; 0.075 sec/batch)
2017-06-02 03:50:59.281108: step 48220, loss = 0.78 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:51:00.152186: step 48230, loss = 0.71 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:51:01.004936: step 48240, loss = 0.85 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 03:51:01.883300: step 48250, loss = 0.78 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:51:02.769405: step 48260, loss = 0.77 (1444.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:51:03.654208: step 48270, loss = 0.70 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:51:04.546751: step 48280, loss = 0.86 (1434.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:51:05.433625: step 48290, loss = 0.67 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:51:06.390524: step 48300, loss = 0.81 (1337.7 examples/sec; 0.096 sec/batch)
2017-06-02 03:51:07.171793: step 48310, loss = 0.73 (1638.4 examples/sec; 0.078 sec/batch)
2017-06-02 03:51:08.065502: step 48320, loss = 0.71 (1432.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:51:08.967967: step 48330, loss = 0.65 (1418.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:51:09.868341: step 48340, loss = 0.64 (1421.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:51:10.765126: step 48350, loss = 0.72 (1427.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:51:11.653819: step 48360, loss = 0.82 (1440.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:51:12.528635: step 48370, loss = 0.73 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:51:13.420188: step 48380, loss = 0.83 (1435.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:51:14.281488: step 48390, loss = 0.84 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:51:15.232503: step 48400, loss = 0.67 (1345.9 examples/sec; 0.095 sec/batch)
2017-06-02 03:51:15.995240: step 48410, loss = 0.82 (1678.2 examples/sec; 0.076 sec/batch)
2017-06-02 03:51:16.879226: step 48420, loss = 0.72 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:51:17.759280: step 48430, loss = 0.79 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:51:18.621346: step 48440, loss = 0.57 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 03:51:19.486001: step 48450, loss = 0.74 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 03:51:20.354285: step 48460, loss = 0.79 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:51:21.206751: step 48470, loss = 0.74 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 03:51:22.066138: step 48480, loss = 0.76 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 03:51:22.935715: step 48490, loss = 0.77 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:51:23.905708: step 48500, loss = 0.73 (1319.6 examples/sec; 0.097 sec/batch)
2017-06-02 03:51:24.667086: step 48510, loss = 0.71 (1681.2 examples/sec; 0.076 sec/batch)
2017-06-02 03:51:25.564539: step 48520, loss = 0.81 (1426.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:51:26.404756: step 48530, loss = 0.78 (1523.4 examples/sec; 0.084 sec/batch)
2017-06-02 03:51:27.280084: step 48540, loss = 0.69 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:51:28.178879: step 48550, loss = 0.77 (1424.1 examples/sec; 0.090 sec/batch)
2017-06-02 03:51:29.069322: step 48560, loss = 0.77 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:51:29.953147: step 48570, loss = 0.56 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:51:30.812889: step 48580, loss = 0.88 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 03:51:31.696429: step 48590, loss = 0.67 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:51:32.666825: step 48600, loss = 0.85 (1319.0 examples/sec; 0.097 sec/batch)
2017-06-02 03:51:33.454931: step 48610, loss = 0.91 (1624.1 examples/sec; 0.079 sec/batch)
2017-06-02 03:51:34.350700: step 48620, loss = 0.85 (1428.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:51:35.249056: step 48630, loss = 0.74 (1424.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:51:36.125396: step 48640, loss = 0.81 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:51:37.018628: step 48650, loss = 0.69 (1433.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:51:37.897524: step 48660, loss = 0.88 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:51:38.758835: step 48670, loss = 0.72 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:51:39.632448: step 48680, loss = 0.81 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:51:40.523783: step 48690, loss = 0.77 (1436.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:51:41.488646: step 48700, loss = 0.64 (1326.6 examples/sec; 0.096 sec/batch)
2017-06-02 03:51:42.273512: step 48710, loss = 0.70 (1630.9 examples/sec; 0.078 sec/batch)
2017-06-02 03:51:43.132740: step 48720, loss = 0.69 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 03:51:44.010449: step 48730, loss = 0.71 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:51:44.889805: step 48740, loss = 0.84 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:51:45.728147: step 48750, loss = 0.70 (1526.8 examples/sec; 0.084 sec/batch)
2017-06-02 03:51:46.515105: step 48760, loss = 0.77 (1626.5 examples/sec; 0.079 sec/batch)
2017-06-02 03:51:47.323222: step 48770, loss = 0.70 (1583.9 examples/sec; 0.081 sec/batch)
2017-06-02 03:51:48.123393: step 48780, loss = 0.69 (1599.6 examples/sec; 0.080 sec/batch)
2017-06-02 03:51:48.964510: step 48790, loss = 0.86 (1521.8 examples/sec; 0.084 sec/batch)
2017-06-02 03:51:49.918888: step 48800, loss = 0.73 (1341.2 examples/sec; 0.095 sec/batch)
2017-06-02 03:51:50.694247: step 48810, loss = 0.78 (1650.9 examples/sec; 0.078 sec/batch)
2017-06-02 03:51:51.546736: step 48820, loss = 0.56 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 03:51:52.429597: step 48830, loss = 0.77 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:51:53.305054: step 48840, loss = 0.84 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:51:54.174247: step 48850, loss = 0.76 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:51:55.066801: step 48860, loss = 0.87 (1434.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:51:55.967914: step 48870, loss = 0.79 (1420.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:51:56.850827: step 48880, loss = 0.80 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:51:57.713469: step 48890, loss = 0.79 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 03:51:58.682264: step 48900, loss = 0.69 (1321.3 examples/sec; 0.097 sec/batch)
2017-06-02 03:51:59.470401: step 48910, loss = 0.86 (1624.0 examples/sec; 0.079 sec/batch)
2017-06-02 03:52:00.356617: step 48920, loss = 0.66 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:52:01.239969: step 48930, loss = 0.77 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:52:02.121830: step 48940, loss = 0.92 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:52:03.010504: step 48950, loss = 0.58 (1440.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:52:03.901961: step 48960, loss = 0.89 (1435.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:52:04.762376: step 48970, loss = 0.90 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 03:52:05.625319: step 48980, loss = 0.76 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 03:52:06.517523: step 48990, loss = 0.82 (1434.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:52:07.489044: step 49000, loss = 0.74 (1317.5 examples/sec; 0.097 sec/batch)
2017-06-02 03:52:08.253019: step 49010, loss = 0.71 (1675.5 examples/sec; 0.076 sec/batch)
2017-06-02 03:52:09.112444: step 49020, loss = 0.75 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 03:52:09.993359: step 49030, loss = 0.66 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:52:10.857478: step 49040, loss = 0.72 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 03:52:11.720050: step 49050, loss = 0.69 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 03:52:12.611423: step 49060, loss = 0.88 (1436.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:52:13.481822: step 49070, loss = 0.75 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:52:14.331048: step 49080, loss = 0.85 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 03:52:15.207592: step 49090, loss = 0.61 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:52:16.191667: step 49100, loss = 1.01 (1300.7 examples/sec; 0.098 sec/batch)
2017-06-02 03:52:16.974054: step 49110, loss = 0.88 (1636.0 examples/sec; 0.078 sec/batch)
2017-06-02 03:52:17.826792: step 49120, loss = 0.76 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 03:52:18.725662: step 49130, loss = 0.75 (1424.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:52:19.597125: step 49140, loss = 0.76 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:52:20.452765: step 49150, loss = 0.65 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 03:52:21.343721: step 49160, loss = 0.65 (1436.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:52:22.187204: step 49170, loss = 0.75 (1517.5 examples/sec; 0.084 sec/batch)
2017-06-02 03:52:23.058156: step 49180, loss = 0.64 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:52:23.904996: step 49190, loss = 0.68 (1511.5 examples/sec; 0.085 sec/batch)
2017-06-02 03:52:24.849881: step 49200, loss = 0.68 (1354.7 examples/sec; 0.094 sec/batch)
2017-06-02 03:52:25.625961: step 49210, loss = 0.80 (1649.3 examples/sec; 0.078 sec/batch)
2017-06-02 03:52:26.518191: step 49220, loss = 0.78 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:52:27.414549: step 49230, loss = 0.72 (1427.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:52:28.264831: step 49240, loss = 0.72 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 03:52:29.159563: step 49250, loss = 0.82 (1430.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:52:30.047345: step 49260, loss = 0.93 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:52:30.897154: step 49270, loss = 0.83 (1506.2 examples/sec; 0.085 sec/batch)
2017-06-02 03:52:31.756120: step 49280, loss = 0.70 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:52:32.627569: step 49290, loss = 0.64 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:52:33.610787: step 49300, loss = 0.78 (1301.8 examples/sec; 0.098 sec/batch)
2017-06-02 03:52:34.379030: step 49310, loss = 0.64 (1666.1 examples/sec; 0.077 sec/batch)
2017-06-02 03:52:35.256424: step 49320, loss = 0.70 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:52:36.143510: step 49330, loss = 0.82 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:52:37.000851: step 49340, loss = 0.69 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 03:52:37.876741: step 49350, loss = 0.77 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:52:38.755616: step 49360, loss = 0.63 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:52:39.643122: step 49370, loss = 0.65 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:52:40.530376: step 49380, loss = 0.60 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:52:41.393199: step 49390, loss = 0.78 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 03:52:42.358744: step 49400, loss = 0.84 (1325.7 examples/sec; 0.097 sec/batch)
2017-06-02 03:52:43.129101: step 49410, loss = 0.61 (1661.5 examples/sec; 0.077 sec/batch)
2017-06-02 03:52:44.007330: step 49420, loss = 0.71 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:52:44.862612: step 49430, loss = 0.80 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 03:52:45.730894: step 49440, loss = 0.63 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:52:46.583395: step 49450, loss = 0.86 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 03:52:47.438907: step 49460, loss = 0.70 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 03:52:48.318974: step 49470, loss = 0.83 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:52:49.184916: step 49480, loss = 0.59 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:52:50.048643: step 49490, loss = 0.64 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 03:52:51.001718: step 49500, loss = 0.69 (1343.0 examples/sec; 0.095 sec/batch)
2017-06-02 03:52:51.782154: step 49510, loss = 0.71 (1640.1 examples/sec; 0.078 sec/batch)
2017-06-02 03:52:52.684877: step 49520, loss = 0.62 (1417.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:52:53.539613: step 49530, loss = 0.74 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 03:52:54.382360: step 49540, loss = 0.67 (1518.8 examples/sec; 0.084 sec/batch)
2017-06-02 03:52:55.243048: step 49550, loss = 0.76 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 03:52:56.078428: step 49560, loss = 0.80 (1532.2 examples/sec; 0.084 sec/batch)
2017-06-02 03:52:56.966879: step 49570, loss = 0.72 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:52:57.863366: step 49580, loss = 0.76 (1427.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:52:58.715948: step 49590, loss = 0.76 (1501.3 examples/sec; 0.085 sec/batch)
2017-06-02 03:52:59.698073: step 49600, loss = 0.77 (1303.3 examples/sec; 0.098 sec/batch)
2017-06-02 03:53:00.451118: step 49610, loss = 0.66 (1699.8 examples/sec; 0.075 sec/batch)
2017-06-02 03:53:01.314410: step 49620, loss = 0.74 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 03:53:02.199639: step 49630, loss = 0.73 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:53:03.073022: step 49640, loss = 0.70 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:53:03.960565: step 49650, loss = 0.84 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:53:04.841082: step 49660, loss = 0.79 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:53:05.701823: step 49670, loss = 0.78 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:53:06.600371: step 49680, loss = 0.82 (1424.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:53:07.470448: step 49690, loss = 0.72 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:53:08.453799: step 49700, loss = 0.80 (1301.7 examples/sec; 0.098 sec/batch)
2017-06-02 03:53:09.228205: step 49710, loss = 0.63 (1652.9 examples/sec; 0.077 sec/batch)
2017-06-02 03:53:10.120719: step 49720, loss = 0.68 (1434.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:53:11.008710: step 49730, loss = 0.88 (1441.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:53:11.902622: step 49740, loss = 0.91 (1431.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:53:12.796206: step 49750, loss = 0.73 (1432.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:53:13.681924: step 49760, loss = 0.84 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:53:14.544783: step 49770, loss = 0.69 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 03:53:15.438922: step 49780, loss = 0.84 (1431.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:53:16.313596: step 49790, loss = 0.65 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:53:17.268257: step 49800, loss = 0.98 (1340.8 examples/sec; 0.095 sec/batch)
2017-06-02 03:53:18.047030: step 49810, loss = 0.73 (1643.7 examples/sec; 0.078 sec/batch)
2017-06-02 03:53:18.924379: step 49820, loss = 0.65 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:53:19.792448: step 49830, loss = 0.70 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:53:20.678791: step 49840, loss = 0.89 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:53:21.545073: step 49850, loss = 0.68 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:53:22.384597: step 49860, loss = 0.62 (1524.7 examples/sec; 0.084 sec/batch)
2017-06-02 03:53:23.266009: step 49870, loss = 0.77 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:53:24.121782: step 49880, loss = 0.64 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 03:53:25.004470: step 49890, loss = 0.80 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:53:26.013744: step 49900, loss = 0.80 (1268.3 examples/sec; 0.101 sec/batch)
2017-06-02 03:53:26.756415: step 49910, loss = 0.90 (1723.5 examples/sec; 0.074 sec/batch)
2017-06-02 03:53:27.642661: step 49920, loss = 0.70 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:53:28.544508: step 49930, loss = 0.67 (1419.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:53:29.417812: step 49940, loss = 0.72 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:53:30.290272: step 49950, loss = 0.86 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:53:31.163969: step 49960, loss = 0.69 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:53:32.063886: step 49970, loss = 0.71 (1422.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:53:32.957439: step 49980, loss = 0.86 (1432.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:53:33.824192: step 49990, loss = 0.67 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:53:34.805759: step 50000, loss = 0.79 (1304.0 examples/sec; 0.098 sec/batch)
2017-06-02 03:53:35.589582: step 50010, loss = 0.79 (1633.0 examples/sec; 0.078 sec/batch)
2017-06-02 03:53:36.444304: step 50020, loss = 0.78 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 03:53:37.301096: step 50030, loss = 0.68 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 03:53:38.186765: step 50040, loss = 0.64 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:53:39.064986: step 50050, loss = 0.84 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:53:39.927817: step 50060, loss = 0.68 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 03:53:40.806722: step 50070, loss = 0.80 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:53:41.706708: step 50080, loss = 0.63 (1422.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:53:42.595543: step 50090, loss = 0.97 (1440.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:53:43.568790: step 50100, loss = 0.87 (1315.2 examples/sec; 0.097 sec/batch)
2017-06-02 03:53:44.333656: step 50110, loss = 0.56 (1673.5 examples/sec; 0.076 sec/batch)
2017-06-02 03:53:45.225272: step 50120, loss = 0.73 (1435.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:53:46.108781: step 50130, loss = 0.78 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:53:46.995804: step 50140, loss = 0.71 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:53:47.902414: step 50150, loss = 0.69 (1411.9 examples/sec; 0.091 sec/batch)
2017-06-02 03:53:48.793084: step 50160, loss = 0.82 (1437.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:53:49.666949: step 50170, loss = 0.78 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:53:50.490848: step 50180, loss = 0.83 (1553.6 examples/sec; 0.082 sec/batch)
2017-06-02 03:53:51.380477: step 50190, loss = 0.60 (1438.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:53:52.360456: step 50200, loss = 0.75 (1306.1 examples/sec; 0.098 sec/batch)
2017-06-02 03:53:53.149060: step 50210, loss = 0.83 (1623.1 examples/sec; 0.079 sec/batch)
2017-06-02 03:53:54.021444: step 50220, loss = 0.62 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:53:54.889975: step 50230, loss = 0.73 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:53:55.750591: step 50240, loss = 0.73 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 03:53:56.620603: step 50250, loss = 0.64 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:53:57.503461: step 50260, loss = 0.65 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:53:58.380707: step 50270, loss = 0.63 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:53:59.238915: step 50280, loss = 0.82 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 03:54:00.113968: step 50290, loss = 0.76 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:54:01.155949: step 50300, loss = 0.80 (1228.4 examples/sec; 0.104 sec/batch)
2017-06-02 03:54:01.848735: step 50310, loss = 0.88 (1847.6 examples/sec; 0.069 sec/batch)
2017-06-02 03:54:02.675769: step 50320, loss = 0.62 (1547.7 examples/sec; 0.083 sec/batch)
2017-06-02 03:54:03.528552: step 50330, loss = 0.86 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 03:54:04.402537: step 50340, loss = 0.78 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:54:05.283483: step 50350, loss = 0.57 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:54:06.155914: step 50360, loss = 0.74 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:54:07.019586: step 50370, loss = 0.82 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:54:07.885271: step 50380, loss = 0.63 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:54:08.770064: step 50390, loss = 0.76 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:54:09.736643: step 50400, loss = 0.70 (1324.2 examples/sec; 0.097 sec/batch)
2017-06-02 03:54:10.499328: step 50410, loss = 0.73 (1678.3 examples/sec; 0.076 sec/batch)
2017-06-02 03:54:11.379565: step 50420, loss = 0.69 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:54:12.265076: step 50430, loss = 0.61 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:54:13.168994: step 50440, loss = 0.82 (1416.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:54:14.049575: step 50450, loss = 0.68 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:54:14.933341: step 50460, loss = 0.68 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:54:15.833888: step 50470, loss = 0.69 (1421.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:54:16.714110: step 50480, loss = 0.75 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:54:17.594887: step 50490, loss = 0.74 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:54:18.609281: step 50500, loss = 0.54 (1261.8 examples/sec; 0.101 sec/batch)
2017-06-02 03:54:19.364304: step 50510, loss = 0.83 (1695.3 examples/sec; 0.076 sec/batch)
2017-06-02 03:54:20.231127: step 50520, loss = 0.80 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:54:21.085006: step 50530, loss = 0.62 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 03:54:21.974858: step 50540, loss = 0.65 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:54:22.865380: step 50550, loss = 0.73 (1437.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:54:23.750615: step 50560, loss = 0.64 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:54:24.633238: step 50570, loss = 0.71 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:54:25.480844: step 50580, loss = 0.69 (1510.1 examples/sec; 0.085 sec/batch)
2017-06-02 03:54:26.350717: step 50590, loss = 0.70 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:54:27.338438: step 50600, loss = 0.83 (1295.9 examples/sec; 0.099 sec/batch)
2017-06-02 03:54:28.129091: step 50610, loss = 0.76 (1618.9 examples/sec; 0.079 sec/batch)
2017-06-02 03:54:29.027344: step 50620, loss = 0.72 (1425.0 examples/sec; 0.090 sec/batch)
2017-06-02 03:54:29.889365: step 50630, loss = 0.71 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 03:54:30.751136: step 50640, loss = 0.78 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 03:54:31.604019: step 50650, loss = 0.67 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 03:54:32.484349: step 50660, loss = 0.73 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:54:33.347621: step 50670, loss = 0.74 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 03:54:34.210023: step 50680, loss = 0.73 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 03:54:35.086490: step 50690, loss = 0.65 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:54:36.050479: step 50700, loss = 0.84 (1327.8 examples/sec; 0.096 sec/batch)
2017-06-02 03:54:36.837319: step 50710, loss = 0.90 (1626.8 examples/sec; 0.079 sec/batch)
2017-06-02 03:54:37.708983: step 50720, loss = 0.67 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:54:38.593852: step 50730, loss = 0.71 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:54:39.455118: step 50740, loss = 0.77 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 03:54:40.316255: step 50750, loss = 0.75 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 03:54:41.192613: step 50760, loss = 0.79 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:54:42.066279: step 50770, loss = 0.78 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:54:42.929602: step 50780, loss = 0.86 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 03:54:43.798551: step 50790, loss = 0.76 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:54:44.787805: step 50800, loss = 0.69 (1293.9 examples/sec; 0.099 sec/batch)
2017-06-02 03:54:45.570839: step 50810, loss = 0.76 (1634.6 examples/sec; 0.078 sec/batch)
2017-06-02 03:54:46.448534: step 50820, loss = 0.63 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:54:47.324857: step 50830, loss = 0.71 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:54:48.174190: step 50840, loss = 0.72 (1507.1 examples/sec; 0.085 sec/batch)
2017-06-02 03:54:49.024038: step 50850, loss = 0.77 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 03:54:49.872628: step 50860, loss = 0.65 (1508.4 examples/sec; 0.085 sec/batch)
2017-06-02 03:54:50.747066: step 50870, loss = 0.83 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:54:51.615909: step 50880, loss = 0.75 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:54:52.515570: step 50890, loss = 0.81 (1422.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:54:53.511927: step 50900, loss = 0.75 (1284.7 examples/sec; 0.100 sec/batch)
2017-06-02 03:54:54.258407: step 50910, loss = 0.88 (1714.7 examples/sec; 0.075 sec/batch)
2017-06-02 03:54:55.114412: step 50920, loss = 0.66 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 03:54:55.992647: step 50930, loss = 0.74 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:54:56.851882: step 50940, loss = 1.07 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 03:54:57.694603: step 50950, loss = 0.70 (1518.9 examples/sec; 0.084 sec/batch)
2017-06-02 03:54:58.537578: step 50960, loss = 0.92 (1518.4 examples/sec; 0.084 sec/batch)
2017-06-02 03:54:59.393448: step 50970, loss = 0.88 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 03:55:00.265548: step 50980, loss = 0.82 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:55:01.156996: step 50990, loss = 0.59 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:55:02.129679: step 51000, loss = 0.72 (1315.9 examples/sec; 0.097 sec/batch)
2017-06-02 03:55:02.922975: step 51010, loss = 0.94 (1613.5 examples/sec; 0.079 sec/batch)
2017-06-02 03:55:03.803053: step 51020, loss = 0.74 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:55:04.683209: step 51030, loss = 0.82 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:55:05.538242: step 51040, loss = 0.75 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 03:55:06.400710: step 51050, loss = 0.77 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:55:07.286737: step 51060, loss = 0.73 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:55:08.146782: step 51070, loss = 0.75 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 03:55:09.034816: step 51080, loss = 0.84 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:55:09.916064: step 51090, loss = 0.86 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:55:10.890188: step 51100, loss = 0.75 (1314.0 examples/sec; 0.097 sec/batch)
2017-06-02 03:55:11.668631: step 51110, loss = 0.75 (1644.3 examples/sec; 0.078 sec/batch)
2017-06-02 03:55:12.550252: step 51120, loss = 0.92 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:55:13.428614: step 51130, loss = 0.59 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:55:14.282328: step 51140, loss = 0.72 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 03:55:15.147564: step 51150, loss = 0.79 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:55:15.993888: step 51160, loss = 0.67 (1512.4 examples/sec; 0.085 sec/batch)
2017-06-02 03:55:16.854245: step 51170, loss = 0.79 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 03:55:17.726491: step 51180, loss = 0.78 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:55:18.601991: step 51190, loss = 0.71 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:55:19.545451: step 51200, loss = 0.85 (1356.7 examples/sec; 0.094 sec/batch)
2017-06-02 03:55:20.314488: step 51210, loss = 0.53 (1664.4 examples/sec; 0.077 sec/batch)
2017-06-02 03:55:21.184213: step 51220, loss = 0.63 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:55:22.047245: step 51230, loss = 0.78 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:55:22.918232: step 51240, loss = 0.68 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:55:23.793592: step 51250, loss = 0.58 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:55:24.687367: step 51260, loss = 0.81 (1432.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:55:25.575586: step 51270, loss = 0.59 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:55:26.430387: step 51280, loss = 0.78 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 03:55:27.280087: step 51290, loss = 0.74 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 03:55:28.256940: step 51300, loss = 0.65 (1310.3 examples/sec; 0.098 sec/batch)
2017-06-02 03:55:28.990441: step 51310, loss = 0.71 (1745.1 examples/sec; 0.073 sec/batch)
2017-06-02 03:55:29.845697: step 51320, loss = 0.82 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 03:55:30.705592: step 51330, loss = 0.66 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 03:55:31.586175: step 51340, loss = 0.82 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:55:32.468496: step 51350, loss = 0.77 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:55:33.373160: step 51360, loss = 0.63 (1414.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:55:34.212191: step 51370, loss = 0.63 (1525.6 examples/sec; 0.084 sec/batch)
2017-06-02 03:55:35.102849: step 51380, loss = 0.71 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:55:35.989316: step 51390, loss = 0.67 (1443.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:55:36.969545: step 51400, loss = 0.78 (1305.8 examples/sec; 0.098 sec/batch)
2017-06-02 03:55:37.760561: step 51410, loss = 0.78 (1618.2 examples/sec; 0.079 sec/batch)
2017-06-02 03:55:38.668035: step 51420, loss = 0.69 (1410.5 examples/sec; 0.091 sec/batch)
2017-06-02 03:55:39.527415: step 51430, loss = 0.64 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 03:55:40.395729: step 51440, loss = 0.78 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:55:41.273318: step 51450, loss = 0.68 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:55:42.173750: step 51460, loss = 0.91 (1421.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:55:43.035043: step 51470, loss = 0.74 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:55:43.917218: step 51480, loss = 0.72 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:55:44.781186: step 51490, loss = 0.63 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 03:55:45.800428: step 51500, loss = 0.76 (1255.8 examples/sec; 0.102 sec/batch)
2017-06-02 03:55:46.507253: step 51510, loss = 0.88 (1810.9 examples/sec; 0.071 sec/batch)
2017-06-02 03:55:47.396837: step 51520, loss = 0.73 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:55:48.248781: step 51530, loss = 0.74 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 03:55:49.106487: step 51540, loss = 0.64 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 03:55:49.946944: step 51550, loss = 0.83 (1523.0 examples/sec; 0.084 sec/batch)
2017-06-02 03:55:50.815488: step 51560, loss = 0.82 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:55:51.721138: step 51570, loss = 0.73 (1413.3 examples/sec; 0.091 sec/batch)
2017-06-02 03:55:52.616683: step 51580, loss = 0.78 (1429.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:55:53.520158: step 51590, loss = 0.71 (1416.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:55:54.502562: step 51600, loss = 0.73 (1302.9 examples/sec; 0.098 sec/batch)
2017-06-02 03:55:55.293933: step 51610, loss = 0.70 (1617.4 examples/sec; 0.079 sec/batch)
2017-06-02 03:55:56.171115: step 51620, loss = 0.75 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:55:57.073578: step 51630, loss = 0.71 (1418.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:55:57.973835: step 51640, loss = 0.68 (1421.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:55:58.873231: step 51650, loss = 0.71 (1423.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:55:59.746970: step 51660, loss = 0.65 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:56:00.639164: step 51670, loss = 0.85 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 03:56:01.522631: step 51680, loss = 0.78 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:56:02.403828: step 51690, loss = 0.76 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:56:03.439908: step 51700, loss = 0.71 (1235.4 examples/sec; 0.104 sec/batch)
2017-06-02 03:56:04.172111: step 51710, loss = 0.73 (1748.2 examples/sec; 0.073 sec/batch)
2017-06-02 03:56:05.052667: step 51720, loss = 0.66 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:56:05.926394: step 51730, loss = 0.84 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:56:06.822641: step 51740, loss = 0.64 (1428.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:56:07.705649: step 51750, loss = 0.66 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:56:08.583928: step 51760, loss = 0.92 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:56:09.484774: step 51770, loss = 1.01 (1420.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:56:10.355861: step 51780, loss = 0.76 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:56:11.237694: step 51790, loss = 0.75 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:56:12.313938: step 51800, loss = 0.68 (1189.3 examples/sec; 0.108 sec/batch)
2017-06-02 03:56:13.010453: step 51810, loss = 0.66 (1837.7 examples/sec; 0.070 sec/batch)
2017-06-02 03:56:13.899799: step 51820, loss = 0.75 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:56:14.794424: step 51830, loss = 0.66 (1430.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:56:15.671117: step 51840, loss = 0.84 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:56:16.568169: step 51850, loss = 0.71 (1426.9 examples/sec; 0.090 sec/batch)
2017-06-02 03:56:17.457486: step 51860, loss = 0.73 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:56:18.346658: step 51870, loss = 0.74 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:56:19.212339: step 51880, loss = 0.86 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:56:20.095831: step 51890, loss = 0.72 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:56:21.073599: step 51900, loss = 0.62 (1309.1 examples/sec; 0.098 sec/batch)
2017-06-02 03:56:21.852066: step 51910, loss = 0.77 (1644.3 examples/sec; 0.078 sec/batch)
2017-06-02 03:56:22.706412: step 51920, loss = 0.85 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 03:56:23.591953: step 51930, loss = 0.67 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:56:24.475908: step 51940, loss = 0.76 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:56:25.350876: step 51950, loss = 0.83 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:56:26.251390: step 51960, loss = 0.60 (1421.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:56:27.150417: step 51970, loss = 0.69 (1423.8 examples/sec; 0.090 sec/batch)
2017-06-02 03:56:27.995074: step 51980, loss = 0.60 (1515.4 examples/sec; 0.084 sec/batch)
2017-06-02 03:56:28.889679: step 51990, loss = 0.73 (1430.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:56:29.863595: step 52000, loss = 0.74 (1314.3 examples/sec; 0.097 sec/batch)
2017-06-02 03:56:30.644592: step 52010, loss = 0.71 (1638.9 examples/sec; 0.078 sec/batch)
2017-06-02 03:56:31.511237: step 52020, loss = 0.80 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:56:32.376368: step 52030, loss = 0.66 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:56:33.255519: step 52040, loss = 0.80 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:56:34.096964: step 52050, loss = 0.85 (1521.2 examples/sec; 0.084 sec/batch)
2017-06-02 03:56:34.969340: step 52060, loss = 0.75 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:56:35.868536: step 52070, loss = 0.83 (1423.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:56:36.740855: step 52080, loss = 0.74 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:56:37.613919: step 52090, loss = 0.80 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:56:38.572970: step 52100, loss = 0.72 (1334.6 examples/sec; 0.096 sec/batch)
2017-06-02 03:56:39.364990: step 52110, loss = 0.78 (1616.1 examples/sec; 0.079 sec/batch)
2017-06-02 03:56:40.253924: step 52120, loss = 0.86 (1439.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:56:41.145631: step 52130, loss = 0.71 (1435.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:56:41.993573: step 52140, loss = 0.74 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 03:56:42.861648: step 52150, loss = 0.75 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:56:43.757220: step 52160, loss = 0.81 (1429.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:56:44.609435: step 52170, loss = 0.70 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 03:56:45.487474: step 52180, loss = 0.70 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:56:46.344984: step 52190, loss = 0.87 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 03:56:47.280804: step 52200, loss = 0.81 (1367.8 examples/sec; 0.094 sec/batch)
2017-06-02 03:56:48.013473: step 52210, loss = 0.80 (1747.0 examples/sec; 0.073 sec/batch)
2017-06-02 03:56:48.833569: step 52220, loss = 0.71 (1560.8 examples/sec; 0.082 sec/batch)
2017-06-02 03:56:49.671771: step 52230, loss = 0.76 (1527.1 examples/sec; 0.084 sec/batch)
2017-06-02 03:56:50.567931: step 52240, loss = 0.64 (1428.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:56:51.464014: step 52250, loss = 0.82 (1428.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:56:52.325996: step 52260, loss = 0.90 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 03:56:53.194372: step 52270, loss = 0.80 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:56:54.072377: step 52280, loss = 0.64 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:56:54.946230: step 52290, loss = 0.75 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:56:55.931866: step 52300, loss = 0.76 (1298.7 examples/sec; 0.099 sec/batch)
2017-06-02 03:56:56.719763: step 52310, loss = 0.79 (1624.6 examples/sec; 0.079 sec/batch)
2017-06-02 03:56:57.614647: step 52320, loss = 0.71 (1430.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:56:58.470678: step 52330, loss = 0.70 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 03:56:59.341165: step 52340, loss = 0.70 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:57:00.223266: step 52350, loss = 0.93 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:57:01.098419: step 52360, loss = 0.74 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:57:01.974784: step 52370, loss = 0.86 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:57:02.837544: step 52380, loss = 0.83 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 03:57:03.702893: step 52390, loss = 0.60 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:57:04.674023: step 52400, loss = 0.66 (1318.1 examples/sec; 0.097 sec/batch)
2017-06-02 03:57:05.459441: step 52410, loss = 0.72 (1629.7 examples/sec; 0.079 sec/batch)
2017-06-02 03:57:06.339783: step 52420, loss = 0.82 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:57:07.222906: step 52430, loss = 0.80 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:57:08.088379: step 52440, loss = 0.70 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:57:08.949576: step 52450, loss = 0.66 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 03:57:09.813398: step 52460, loss = 0.85 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 03:57:10.679417: step 52470, loss = 0.85 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:57:11.531743: step 52480, loss = 0.64 (1501.8 examples/sec; 0.085 sec/batch)
2017-06-02 03:57:12.386913: step 52490, loss = 0.82 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 03:57:13.352128: step 52500, loss = 0.73 (1326.1 examples/sec; 0.097 sec/batch)
2017-06-02 03:57:14.136014: step 52510, loss = 0.71 (1632.9 examples/sec; 0.078 sec/batch)
2017-06-02 03:57:14.993791: step 52520, loss = 0.63 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 03:57:15.852215: step 52530, loss = 0.73 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:57:16.723874: step 52540, loss = 0.70 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:57:17.596343: step 52550, loss = 0.95 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:57:18.458205: step 52560, loss = 0.73 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:57:19.334839: step 52570, loss = 0.72 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:57:20.204571: step 52580, loss = 0.75 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:57:21.070157: step 52590, loss = 0.76 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:57:22.033951: step 52600, loss = 0.68 (1328.1 examples/sec; 0.096 sec/batch)
2017-06-02 03:57:22.814427: step 52610, loss = 0.70 (1640.0 examples/sec; 0.078 sec/batch)
2017-06-02 03:57:23.678795: step 52620, loss = 0.80 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 03:57:24.519121: step 52630, loss = 0.68 (1523.2 examples/sec; 0.084 sec/batch)
2017-06-02 03:57:25.362533: step 52640, loss = 0.81 (1517.7 examples/sec; 0.084 sec/batch)
2017-06-02 03:57:26.258029: step 52650, loss = 0.77 (1429.3 examples/sec; 0.090 sec/batch)
2017-06-02 03:57:27.135781: step 52660, loss = 0.77 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:57:27.993341: step 52670, loss = 0.65 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 03:57:28.873365: step 52680, loss = 0.82 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:57:29.749381: step 52690, loss = 0.66 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:57:30.716822: step 52700, loss = 0.62 (1323.1 examples/sec; 0.097 sec/batch)
2017-06-02 03:57:31.474821: step 52710, loss = 0.72 (1688.7 examples/sec; 0.076 sec/batch)
2017-06-02 03:57:32.342587: step 52720, loss = 0.86 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:57:33.181997: step 52730, loss = 0.74 (1524.9 examples/sec; 0.084 sec/batch)
2017-06-02 03:57:34.045923: step 52740, loss = 0.63 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 03:57:34.923094: step 52750, loss = 0.73 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:57:35.801476: step 52760, loss = 0.73 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:57:36.686670: step 52770, loss = 0.82 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:57:37.567790: step 52780, loss = 0.72 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:57:38.442713: step 52790, loss = 0.74 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:57:39.422524: step 52800, loss = 0.78 (1306.4 examples/sec; 0.098 sec/batch)
2017-06-02 03:57:40.201782: step 52810, loss = 0.78 (1642.6 examples/sec; 0.078 sec/batch)
2017-06-02 03:57:41.058062: step 52820, loss = 0.80 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 03:57:41.942534: step 52830, loss = 0.75 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 03:57:42.821619: step 52840, loss = 0.85 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:57:43.701883: step 52850, loss = 0.81 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:57:44.573806: step 52860, loss = 0.65 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:57:45.428227: step 52870, loss = 0.71 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 03:57:46.295116: step 52880, loss = 0.80 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:57:47.152097: step 52890, loss = 0.71 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 03:57:48.100454: step 52900, loss = 0.70 (1349.7 examples/sec; 0.095 sec/batch)
2017-06-02 03:57:48.882293: step 52910, loss = 0.73 (1637.2 examples/sec; 0.078 sec/batch)
2017-06-02 03:57:49.759966: step 52920, loss = 0.66 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:57:50.634172: step 52930, loss = 0.86 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:57:51.524887: step 52940, loss = 0.75 (1437.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:57:52.416804: step 52950, loss = 0.73 (1435.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:57:53.267678: step 52960, loss = 0.76 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 03:57:54.147342: step 52970, loss = 0.69 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:57:55.017335: step 52980, loss = 0.70 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:57:55.883292: step 52990, loss = 0.81 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:57:56.845736: step 53000, loss = 0.82 (1330.0 examples/sec; 0.096 sec/batch)
2017-06-02 03:57:57.620545: step 53010, loss = 0.70 (1652.1 examples/sec; 0.077 sec/batch)
2017-06-02 03:57:58.535447: step 53020, loss = 0.91 (1399.0 examples/sec; 0.091 sec/batch)
2017-06-02 03:57:59.411011: step 53030, loss = 0.85 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:58:00.281682: step 53040, loss = 0.77 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:58:01.162021: step 53050, loss = 0.77 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:58:02.028365: step 53060, loss = 0.93 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 03:58:02.881000: step 53070, loss = 0.68 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 03:58:03.750580: step 53080, loss = 0.61 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:58:04.605678: step 53090, loss = 0.78 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 03:58:05.636501: step 53100, loss = 0.83 (1241.7 examples/sec; 0.103 sec/batch)
2017-06-02 03:58:06.357816: step 53110, loss = 0.77 (1774.5 examples/sec; 0.072 sec/batch)
2017-06-02 03:58:07.239339: step 53120, loss = 0.84 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:58:08.130385: step 53130, loss = 0.76 (1436.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:58:09.017883: step 53140, loss = 0.67 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:58:09.923268: step 53150, loss = 0.76 (1413.8 examples/sec; 0.091 sec/batch)
2017-06-02 03:58:10.807051: step 53160, loss = 0.75 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:58:11.699383: step 53170, loss = 0.72 (1434.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:58:12.585651: step 53180, loss = 0.86 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:58:13.441036: step 53190, loss = 0.74 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 03:58:14.418401: step 53200, loss = 0.85 (1309.7 examples/sec; 0.098 sec/batch)
2017-06-02 03:58:15.215180: step 53210, loss = 0.83 (1606.5 examples/sec; 0.080 sec/batch)
2017-06-02 03:58:16.060595: step 53220, loss = 0.74 (1514.0 examples/sec; 0.085 sec/batch)
2017-06-02 03:58:16.908729: step 53230, loss = 0.71 (1509.2 examples/sec; 0.085 sec/batch)
2017-06-02 03:58:17.783026: step 53240, loss = 0.69 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:58:18.663278: step 53250, loss = 0.92 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:58:19.547440: step 53260, loss = 0.79 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:58:20.411753: step 53270, loss = 0.69 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 03:58:21.263026: step 53280, loss = 0.67 (1503.6 examples/sec; 0.085 sec/batch)
2017-06-02 03:58:22.127222: step 53290, loss = 0.61 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 03:58:23.083748: step 53300, loss = 0.76 (1338.2 examples/sec; 0.096 sec/batch)
2017-06-02 03:58:23.863581: step 53310, loss = 0.58 (1641.4 examples/sec; 0.078 sec/batch)
2017-06-02 03:58:24.728984: step 53320, loss = 0.67 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:58:25.601389: step 53330, loss = 0.85 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 03:58:26.483192: step 53340, loss = 0.70 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:58:27.331525: step 53350, loss = 0.63 (1508.9 examples/sec; 0.085 sec/batch)
2017-06-02 03:58:28.190411: step 53360, loss = 0.66 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 03:58:29.086531: step 53370, loss = 0.74 (1428.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:58:29.965897: step 53380, loss = 0.74 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:58:30.824643: step 53390, loss = 0.85 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 03:58:31.813253: step 53400, loss = 0.90 (1294.7 examples/sec; 0.099 sec/batch)
2017-06-02 03:58:32.582569: step 53410, loss = 0.77 (1663.8 examples/sec; 0.077 sec/batch)
2017-06-02 03:58:33.484998: step 53420, loss = 0.78 (1418.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:58:34.368389: step 53430, loss = 0.92 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 03:58:35.247346: step 53440, loss = 0.64 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:58:36.149004: step 53450, loss = 0.78 (1419.6 examples/sec; 0.090 sec/batch)
2017-06-02 03:58:37.041955: step 53460, loss = 0.88 (1433.4 examples/sec; 0.089 sec/batch)
2017-06-02 03:58:37.916368: step 53470, loss = 0.68 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:58:38.806173: step 53480, loss = 0.75 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 03:58:39.661997: step 53490, loss = 0.67 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 03:58:40.638098: step 53500, loss = 0.70 (1311.3 examples/sec; 0.098 sec/batch)
2017-06-02 03:58:41.434360: step 53510, loss = 0.64 (1607.5 examples/sec; 0.080 sec/batch)
2017-06-02 03:58:42.329780: step 53520, loss = 0.82 (1429.5 examples/sec; 0.090 sec/batch)
2017-06-02 03:58:43.203845: step 53530, loss = 0.74 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:58:44.104369: step 53540, loss = 0.96 (1421.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:58:44.982242: step 53550, loss = 0.75 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 03:58:45.847337: step 53560, loss = 0.64 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:58:46.714358: step 53570, loss = 0.82 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:58:47.609955: step 53580, loss = 0.65 (1429.2 examples/sec; 0.090 sec/batch)
2017-06-02 03:58:48.501866: step 53590, loss = 0.73 (1435.1 examples/sec; 0.089 sec/batch)
2017-06-02 03:58:49.521615: step 53600, loss = 0.69 (1255.2 examples/sec; 0.102 sec/batch)
2017-06-02 03:58:50.256266: step 53610, loss = 0.75 (1742.3 examples/sec; 0.073 sec/batch)
2017-06-02 03:58:51.150015: step 53620, loss = 0.76 (1432.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:58:52.012042: step 53630, loss = 0.80 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 03:58:52.879885: step 53640, loss = 0.75 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:58:53.759162: step 53650, loss = 0.69 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:58:54.633815: step 53660, loss = 0.67 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:58:55.510785: step 53670, loss = 0.81 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:58:56.378629: step 53680, loss = 0.82 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 03:58:57.268288: step 53690, loss = 0.87 (1438.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:58:58.230236: step 53700, loss = 0.86 (1330.6 examples/sec; 0.096 sec/batch)
2017-06-02 03:58:59.015975: step 53710, loss = 0.90 (1629.0 examples/sec; 0.079 sec/batch)
2017-06-02 03:58:59.859060: step 53720, loss = 0.71 (1518.2 examples/sec; 0.084 sec/batch)
2017-06-02 03:59:00.707559: step 53730, loss = 0.71 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 03:59:01.564033: step 53740, loss = 0.74 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 03:59:02.425358: step 53750, loss = 0.78 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:59:03.258703: step 53760, loss = 0.75 (1536.0 examples/sec; 0.083 sec/batch)
2017-06-02 03:59:04.093511: step 53770, loss = 0.75 (1533.3 examples/sec; 0.083 sec/batch)
2017-06-02 03:59:04.957997: step 53780, loss = 0.58 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 03:59:05.814140: step 53790, loss = 0.70 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:59:06.789364: step 53800, loss = 0.67 (1312.5 examples/sec; 0.098 sec/batch)
2017-06-02 03:59:07.580116: step 53810, loss = 0.72 (1618.7 examples/sec; 0.079 sec/batch)
2017-06-02 03:59:08.475083: step 53820, loss = 0.68 (1430.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:59:09.343244: step 53830, loss = 0.73 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:59:10.221514: step 53840, loss = 0.78 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:59:11.091639: step 53850, loss = 0.75 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 03:59:11.975184: step 53860, loss = 0.85 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:59:12.881007: step 53870, loss = 0.76 (1413.1 examples/sec; 0.091 sec/batch)
2017-06-02 03:59:13.785974: step 53880, loss = 0.81 (1414.4 examples/sec; 0.090 sec/batch)
2017-06-02 03:59:14.704234: step 53890, loss = 0.82 (1393.9 examples/sec; 0.092 sec/batch)
2017-06-02 03:59:15.703526: step 53900, loss = 0.93 (1280.9 examples/sec; 0.100 sec/batch)
2017-06-02 03:59:16.500950: step 53910, loss = 0.85 (1605.2 examples/sec; 0.080 sec/batch)
2017-06-02 03:59:17.369075: step 53920, loss = 0.71 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:59:18.259178: step 53930, loss = 0.72 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 03:59:19.123978: step 53940, loss = 0.75 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 03:59:20.007107: step 53950, loss = 0.80 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:59:20.884209: step 53960, loss = 0.69 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:59:21.728784: step 53970, loss = 0.70 (1515.6 examples/sec; 0.084 sec/batch)
2017-06-02 03:59:22.598151: step 53980, loss = 0.85 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:59:23.476200: step 53990, loss = 0.76 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 03:59:24.510589: step 54000, loss = 0.76 (1237.4 examples/sec; 0.103 sec/batch)
2017-06-02 03:59:25.243261: step 54010, loss = 0.77 (1747.1 examples/sec; 0.073 sec/batch)
2017-06-02 03:59:26.121781: step 54020, loss = 0.84 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 03:59:27.013812: step 54030, loss = 0.65 (1434.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:59:27.898694: step 54040, loss = 0.90 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:59:28.743595: step 54050, loss = 0.73 (1515.0 examples/sec; 0.084 sec/batch)
2017-06-02 03:59:29.610994: step 54060, loss = 0.79 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:59:30.476032: step 54070, loss = 0.77 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 03:59:31.343981: step 54080, loss = 0.73 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 03:59:32.215289: step 54090, loss = 0.70 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 03:59:33.183919: step 54100, loss = 0.76 (1321.5 examples/sec; 0.097 sec/batch)
2017-06-02 03:59:33.949817: step 54110, loss = 0.72 (1671.2 examples/sec; 0.077 sec/batch)
2017-06-02 03:59:34.854920: step 54120, loss = 0.77 (1414.2 examples/sec; 0.091 sec/batch)
2017-06-02 03:59:35.752485: step 54130, loss = 0.82 (1426.1 examples/sec; 0.090 sec/batch)
2017-06-02 03:59:36.644352: step 54140, loss = 0.73 (1435.2 examples/sec; 0.089 sec/batch)
2017-06-02 03:59:37.527444: step 54150, loss = 0.68 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:59:38.408710: step 54160, loss = 0.55 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:59:39.294140: step 54170, loss = 0.90 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 03:59:40.170699: step 54180, loss = 0.76 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 03:59:41.058489: step 54190, loss = 0.74 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 03:59:42.046153: step 54200, loss = 0.86 (1296.0 examples/sec; 0.099 sec/batch)
2017-06-02 03:59:42.821582: step 54210, loss = 0.83 (1650.8 examples/sec; 0.078 sec/batch)
2017-06-02 03:59:43.714601: step 54220, loss = 0.65 (1433.3 examples/sec; 0.089 sec/batch)
2017-06-02 03:59:44.589854: step 54230, loss = 0.70 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:59:45.464024: step 54240, loss = 0.74 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 03:59:46.304669: step 54250, loss = 0.79 (1522.6 examples/sec; 0.084 sec/batch)
2017-06-02 03:59:47.171650: step 54260, loss = 0.75 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 03:59:48.030176: step 54270, loss = 0.66 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 03:59:48.905859: step 54280, loss = 0.62 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 03:59:49.802440: step 54290, loss = 0.80 (1427.7 examples/sec; 0.090 sec/batch)
2017-06-02 03:59:50.797492: step 54300, loss = 0.56 (1286.4 examples/sec; 0.100 sec/batch)
2017-06-02 03:59:51.629809: step 54310, loss = 0.64 (1537.9 examples/sec; 0.083 sec/batch)
2017-06-02 03:59:52.512795: step 54320, loss = 0.85 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 03:59:53.375415: step 54330, loss = 0.58 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 03:59:54.235151: step 54340, loss = 0.72 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 03:59:55.112844: step 54350, loss = 0.63 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 03:59:55.989870: step 54360, loss = 0.73 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 03:59:56.857934: step 54370, loss = 0.57 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 03:59:57.743819: step 54380, loss = 0.68 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 03:59:58.602427: step 54390, loss = 0.69 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 03:59:59.555828: step 54400, loss = 0.75 (1342.6 examples/sec; 0.095 sec/batch)
2017-06-02 04:00:00.344331: step 54410, loss = 0.88 (1623.3 examples/sec; 0.079 sec/batch)
2017-06-02 04:00:01.246814: step 54420, loss = 0.72 (1418.3 examples/sec; 0.090 sec/batch)
2017-06-02 04:00:02.137745: step 54430, loss = 0.70 (1436.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:00:03.038634: step 54440, loss = 0.72 (1420.8 examples/sec; 0.090 sec/batch)
2017-06-02 04:00:03.938518: step 54450, loss = 0.81 (1422.4 examples/sec; 0.090 sec/batch)
2017-06-02 04:00:04.834998: step 54460, loss = 0.63 (1427.8 examples/sec; 0.090 sec/batch)
2017-06-02 04:00:05.717849: step 54470, loss = 0.63 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:00:06.604000: step 54480, loss = 0.88 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 04:00:07.462837: step 54490, loss = 0.67 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:00:08.467806: step 54500, loss = 0.74 (1273.7 examples/sec; 0.100 sec/batch)
2017-06-02 04:00:09.268240: step 54510, loss = 0.71 (1599.1 examples/sec; 0.080 sec/batch)
2017-06-02 04:00:10.143266: step 54520, loss = 0.80 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:00:11.020046: step 54530, loss = 0.71 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:00:11.899862: step 54540, loss = 0.76 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:00:12.767697: step 54550, loss = 0.68 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:00:13.627977: step 54560, loss = 0.75 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:00:14.510965: step 54570, loss = 0.74 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:00:15.392365: step 54580, loss = 0.83 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:00:16.291540: step 54590, loss = 0.69 (1423.5 examples/sec; 0.090 sec/batch)
2017-06-02 04:00:17.274414: step 54600, loss = 0.72 (1302.3 examples/sec; 0.098 sec/batch)
2017-06-02 04:00:18.063570: step 54610, loss = 0.84 (1622.0 examples/sec; 0.079 sec/batch)
2017-06-02 04:00:18.944451: step 54620, loss = 0.73 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:00:19.814820: step 54630, loss = 0.77 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:00:20.728149: step 54640, loss = 0.74 (1401.5 examples/sec; 0.091 sec/batch)
2017-06-02 04:00:21.600276: step 54650, loss = 0.72 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:00:22.528153: step 54660, loss = 0.72 (1379.5 examples/sec; 0.093 sec/batch)
2017-06-02 04:00:23.414835: step 54670, loss = 0.79 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:00:24.300119: step 54680, loss = 0.69 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:00:25.199604: step 54690, loss = 0.67 (1423.0 examples/sec; 0.090 sec/batch)
2017-06-02 04:00:26.199092: step 54700, loss = 0.92 (1280.7 examples/sec; 0.100 sec/batch)
2017-06-02 04:00:26.984171: step 54710, loss = 0.63 (1630.4 examples/sec; 0.079 sec/batch)
2017-06-02 04:00:27.871792: step 54720, loss = 0.71 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:00:28.774288: step 54730, loss = 0.81 (1418.3 examples/sec; 0.090 sec/batch)
2017-06-02 04:00:29.630073: step 54740, loss = 0.77 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:00:30.503251: step 54750, loss = 0.75 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:00:31.373868: step 54760, loss = 0.70 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:00:32.247688: step 54770, loss = 0.77 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:00:33.096099: step 54780, loss = 0.80 (1508.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:00:33.945837: step 54790, loss = 0.74 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:00:34.892083: step 54800, loss = 0.63 (1352.7 examples/sec; 0.095 sec/batch)
2017-06-02 04:00:35.684223: step 54810, loss = 0.67 (1615.9 examples/sec; 0.079 sec/batch)
2017-06-02 04:00:36.546820: step 54820, loss = 0.68 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:00:37.396681: step 54830, loss = 0.74 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:00:38.256225: step 54840, loss = 0.66 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:00:39.131527: step 54850, loss = 0.70 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:00:40.014132: step 54860, loss = 0.67 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:00:40.893027: step 54870, loss = 0.79 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:00:41.748095: step 54880, loss = 0.66 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:00:42.619324: step 54890, loss = 0.74 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:00:43.638872: step 54900, loss = 0.68 (1255.5 examples/sec; 0.102 sec/batch)
2017-06-02 04:00:44.407014: step 54910, loss = 0.64 (1666.4 examples/sec; 0.077 sec/batch)
2017-06-02 04:00:45.304728: step 54920, loss = 0.80 (1425.8 examples/sec; 0.090 sec/batch)
2017-06-02 04:00:46.172250: step 54930, loss = 0.70 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:00:47.070267: step 54940, loss = 0.74 (1425.4 examples/sec; 0.090 sec/batch)
2017-06-02 04:00:47.942711: step 54950, loss = 0.68 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:00:48.823292: step 54960, loss = 0.76 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:00:49.711973: step 54970, loss = 0.65 (1440.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:00:50.579657: step 54980, loss = 0.67 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:00:51.473488: step 54990, loss = 0.82 (1432.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:00:52.454222: step 55000, loss = 0.73 (1305.1 examples/sec; 0.098 sec/batch)
2017-06-02 04:00:53.223959: step 55010, loss = 0.63 (1662.9 examples/sec; 0.077 sec/batch)
2017-06-02 04:00:54.113175: step 55020, loss = 0.80 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:00:54.992689: step 55030, loss = 0.74 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:00:55.862071: step 55040, loss = 0.83 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:00:56.738449: step 55050, loss = 0.78 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:00:57.647124: step 55060, loss = 0.74 (1408.7 examples/sec; 0.091 sec/batch)
2017-06-02 04:00:58.529260: step 55070, loss = 0.67 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:00:59.417865: step 55080, loss = 0.84 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 04:01:00.289551: step 55090, loss = 0.81 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:01:01.280409: step 55100, loss = 0.75 (1291.8 examples/sec; 0.099 sec/batch)
2017-06-02 04:01:02.039316: step 55110, loss = 0.70 (1686.7 examples/sec; 0.076 sec/batch)
2017-06-02 04:01:02.908186: step 55120, loss = 0.65 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:01:03.766391: step 55130, loss = 0.70 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:01:04.619487: step 55140, loss = 0.63 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:01:05.433927: step 55150, loss = 0.76 (1571.6 examples/sec; 0.081 sec/batch)
2017-06-02 04:01:06.290376: step 55160, loss = 0.70 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:01:07.182913: step 55170, loss = 0.83 (1434.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:01:08.068491: step 55180, loss = 0.91 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 04:01:08.957563: step 55190, loss = 0.73 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:01:09.940726: step 55200, loss = 0.76 (1301.9 examples/sec; 0.098 sec/batch)
2017-06-02 04:01:10.748693: step 55210, loss = 0.72 (1584.2 examples/sec; 0.081 sec/batch)
2017-06-02 04:01:11.639019: step 55220, loss = 0.70 (1437.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:01:12.521451: step 55230, loss = 0.78 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:01:13.418720: step 55240, loss = 0.65 (1426.6 examples/sec; 0.090 sec/batch)
2017-06-02 04:01:14.290651: step 55250, loss = 0.74 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:01:15.186152: step 55260, loss = 0.69 (1429.4 examples/sec; 0.090 sec/batch)
2017-06-02 04:01:16.052261: step 55270, loss = 0.68 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:01:16.942707: step 55280, loss = 0.95 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:01:17.806462: step 55290, loss = 0.66 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:01:18.818706: step 55300, loss = 0.66 (1264.5 examples/sec; 0.101 sec/batch)
2017-06-02 04:01:19.605993: step 55310, loss = 0.60 (1625.8 examples/sec; 0.079 sec/batch)
2017-06-02 04:01:20.487807: step 55320, loss = 0.88 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:01:21.347704: step 55330, loss = 0.73 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:01:22.204647: step 55340, loss = 0.70 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:01:23.072141: step 55350, loss = 0.79 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:01:23.968307: step 55360, loss = 0.87 (1428.3 examples/sec; 0.090 sec/batch)
2017-06-02 04:01:24.860942: step 55370, loss = 0.76 (1434.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:01:25.752472: step 55380, loss = 0.73 (1435.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:01:26.637830: step 55390, loss = 0.62 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:01:27.613440: step 55400, loss = 0.64 (1312.0 examples/sec; 0.098 sec/batch)
2017-06-02 04:01:28.406042: step 55410, loss = 0.74 (1614.9 examples/sec; 0.079 sec/batch)
2017-06-02 04:01:29.290796: step 55420, loss = 0.94 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:01:30.162419: step 55430, loss = 0.69 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:01:31.056517: step 55440, loss = 0.62 (1431.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:01:31.908262: step 55450, loss = 0.80 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:01:32.781402: step 55460, loss = 0.73 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:01:33.650354: step 55470, loss = 0.80 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:01:34.523557: step 55480, loss = 0.79 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:01:35.396932: step 55490, loss = 0.71 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:01:36.340772: step 55500, loss = 0.73 (1356.2 examples/sec; 0.094 sec/batch)
2017-06-02 04:01:37.152918: step 55510, loss = 0.71 (1576.1 examples/sec; 0.081 sec/batch)
2017-06-02 04:01:38.020219: step 55520, loss = 0.72 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:01:38.910323: step 55530, loss = 0.70 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:01:39.787556: step 55540, loss = 0.93 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:01:40.623349: step 55550, loss = 0.82 (1531.5 examples/sec; 0.084 sec/batch)
2017-06-02 04:01:41.528635: step 55560, loss = 0.85 (1413.9 examples/sec; 0.091 sec/batch)
2017-06-02 04:01:42.392875: step 55570, loss = 0.86 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:01:43.276859: step 55580, loss = 0.62 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:01:44.141235: step 55590, loss = 0.77 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:01:45.131875: step 55600, loss = 0.83 (1292.1 examples/sec; 0.099 sec/batch)
2017-06-02 04:01:45.921750: step 55610, loss = 0.61 (1620.5 examples/sec; 0.079 sec/batch)
2017-06-02 04:01:46.826612: step 55620, loss = 0.60 (1414.6 examples/sec; 0.090 sec/batch)
2017-06-02 04:01:47.691046: step 55630, loss = 0.98 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:01:48.551901: step 55640, loss = 0.73 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:01:49.430358: step 55650, loss = 0.72 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:01:50.315298: step 55660, loss = 0.87 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:01:51.185393: step 55670, loss = 0.75 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:01:52.034553: step 55680, loss = 0.92 (1507.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:01:52.893351: step 55690, loss = 0.93 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:01:53.896908: step 55700, loss = 0.75 (1275.4 examples/sec; 0.100 sec/batch)
2017-06-02 04:01:54.618587: step 55710, loss = 0.90 (1773.6 examples/sec; 0.072 sec/batch)
2017-06-02 04:01:55.521362: step 55720, loss = 0.84 (1417.8 examples/sec; 0.090 sec/batch)
2017-06-02 04:01:56.405586: step 55730, loss = 0.79 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:01:57.285947: step 55740, loss = 0.82 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:01:58.170041: step 55750, loss = 0.85 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:01:59.024752: step 55760, loss = 0.78 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:01:59.906839: step 55770, loss = 0.74 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:02:00.780295: step 55780, loss = 0.74 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:02:01.666033: step 55790, loss = 0.75 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:02:02.639339: step 55800, loss = 0.75 (1315.1 examples/sec; 0.097 sec/batch)
2017-06-02 04:02:03.436310: step 55810, loss = 0.59 (1606.1 examples/sec; 0.080 sec/batch)
2017-06-02 04:02:04.350775: step 55820, loss = 0.94 (1399.7 examples/sec; 0.091 sec/batch)
2017-06-02 04:02:05.245241: step 55830, loss = 0.71 (1431.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:02:06.131589: step 55840, loss = 0.64 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:02:07.015834: step 55850, loss = 0.88 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:02:07.877801: step 55860, loss = 0.62 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:02:08.749924: step 55870, loss = 0.81 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:02:09.629409: step 55880, loss = 0.69 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:02:10.510806: step 55890, loss = 0.79 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:02:11.495129: step 55900, loss = 0.65 (1300.4 examples/sec; 0.098 sec/batch)
2017-06-02 04:02:12.279565: step 55910, loss = 0.67 (1631.8 examples/sec; 0.078 sec/batch)
2017-06-02 04:02:13.168667: step 55920, loss = 0.78 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:02:14.009392: step 55930, loss = 0.61 (1522.5 examples/sec; 0.084 sec/batch)
2017-06-02 04:02:14.895431: step 55940, loss = 0.54 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:02:15.785782: step 55950, loss = 0.81 (1437.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:02:16.666952: step 55960, loss = 0.72 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:02:17.524951: step 55970, loss = 0.76 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:02:18.394792: step 55980, loss = 0.82 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:02:19.248026: step 55990, loss = 1.03 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:02:20.227097: step 56000, loss = 0.62 (1307.4 examples/sec; 0.098 sec/batch)
2017-06-02 04:02:20.989309: step 56010, loss = 0.66 (1679.3 examples/sec; 0.076 sec/batch)
2017-06-02 04:02:21.854782: step 56020, loss = 0.63 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:02:22.710519: step 56030, loss = 0.79 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:02:23.564524: step 56040, loss = 0.76 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:02:24.443112: step 56050, loss = 0.83 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:02:25.329293: step 56060, loss = 0.74 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 04:02:26.196835: step 56070, loss = 0.78 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:02:27.092015: step 56080, loss = 0.70 (1429.9 examples/sec; 0.090 sec/batch)
2017-06-02 04:02:27.958822: step 56090, loss = 0.67 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:02:28.944133: step 56100, loss = 0.82 (1299.1 examples/sec; 0.099 sec/batch)
2017-06-02 04:02:29.723451: step 56110, loss = 0.65 (1642.5 examples/sec; 0.078 sec/batch)
2017-06-02 04:02:30.568343: step 56120, loss = 0.80 (1515.0 examples/sec; 0.084 sec/batch)
2017-06-02 04:02:31.430529: step 56130, loss = 0.65 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:02:32.317493: step 56140, loss = 0.81 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:02:33.186086: step 56150, loss = 0.79 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:02:34.052985: step 56160, loss = 0.74 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:02:34.926474: step 56170, loss = 0.63 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:02:35.787322: step 56180, loss = 0.81 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:02:36.630942: step 56190, loss = 0.81 (1517.3 examples/sec; 0.084 sec/batch)
2017-06-02 04:02:37.618110: step 56200, loss = 0.71 (1296.6 examples/sec; 0.099 sec/batch)
2017-06-02 04:02:38.385448: step 56210, loss = 0.79 (1668.1 examples/sec; 0.077 sec/batch)
2017-06-02 04:02:39.241903: step 56220, loss = 0.76 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:02:40.133928: step 56230, loss = 0.62 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:02:40.978811: step 56240, loss = 0.74 (1515.0 examples/sec; 0.084 sec/batch)
2017-06-02 04:02:41.857462: step 56250, loss = 0.75 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:02:42.705564: step 56260, loss = 0.78 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:02:43.558982: step 56270, loss = 0.75 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:02:44.438292: step 56280, loss = 0.77 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:02:45.267889: step 56290, loss = 0.72 (1542.9 examples/sec; 0.083 sec/batch)
2017-06-02 04:02:46.263996: step 56300, loss = 0.76 (1286.6 examples/sec; 0.099 sec/batch)
2017-06-02 04:02:47.059769: step 56310, loss = 0.66 (1606.0 examples/sec; 0.080 sec/batch)
2017-06-02 04:02:47.930758: step 56320, loss = 0.86 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:02:48.819173: step 56330, loss = 0.67 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:02:49.702473: step 56340, loss = 1.00 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:02:50.582334: step 56350, loss = 0.73 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:02:51.440013: step 56360, loss = 0.68 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:02:52.308282: step 56370, loss = 0.78 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:02:53.158492: step 56380, loss = 0.78 (1505.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:02:54.016900: step 56390, loss = 0.67 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:02:54.976991: step 56400, loss = 0.73 (1333.2 examples/sec; 0.096 sec/batch)
2017-06-02 04:02:55.769088: step 56410, loss = 0.72 (1616.0 examples/sec; 0.079 sec/batch)
2017-06-02 04:02:56.636199: step 56420, loss = 0.95 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:02:57.532542: step 56430, loss = 0.78 (1428.0 examples/sec; 0.090 sec/batch)
2017-06-02 04:02:58.397425: step 56440, loss = 0.67 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:02:59.283841: step 56450, loss = 0.71 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:03:00.131456: step 56460, loss = 0.69 (1510.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:03:00.997098: step 56470, loss = 0.75 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:03:01.876220: step 56480, loss = 0.61 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:03:02.737045: step 56490, loss = 0.84 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:03:03.677035: step 56500, loss = 0.80 (1361.7 examples/sec; 0.094 sec/batch)
2017-06-02 04:03:04.493180: step 56510, loss = 0.87 (1568.4 examples/sec; 0.082 sec/batch)
2017-06-02 04:03:05.341890: step 56520, loss = 0.63 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:03:06.212812: step 56530, loss = 0.72 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:03:07.076652: step 56540, loss = 0.85 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:03:07.976827: step 56550, loss = 0.74 (1421.9 examples/sec; 0.090 sec/batch)
2017-06-02 04:03:08.839965: step 56560, loss = 0.82 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:03:09.707833: step 56570, loss = 0.67 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:03:10.558707: step 56580, loss = 0.70 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:03:11.452543: step 56590, loss = 0.68 (1432.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:03:12.417767: step 56600, loss = 0.67 (1326.1 examples/sec; 0.097 sec/batch)
2017-06-02 04:03:13.216588: step 56610, loss = 0.98 (1602.4 examples/sec; 0.080 sec/batch)
2017-06-02 04:03:14.093912: step 56620, loss = 0.81 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:03:14.952512: step 56630, loss = 0.69 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:03:15.849460: step 56640, loss = 0.68 (1427.1 examples/sec; 0.090 sec/batch)
2017-06-02 04:03:16.733070: step 56650, loss = 0.64 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:03:17.584857: step 56660, loss = 0.71 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:03:18.474021: step 56670, loss = 0.78 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:03:19.354587: step 56680, loss = 0.71 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:03:20.220783: step 56690, loss = 0.64 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:03:21.199619: step 56700, loss = 0.69 (1307.7 examples/sec; 0.098 sec/batch)
2017-06-02 04:03:21.972264: step 56710, loss = 0.81 (1656.6 examples/sec; 0.077 sec/batch)
2017-06-02 04:03:22.854364: step 56720, loss = 0.87 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:03:23.722684: step 56730, loss = 0.86 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:03:24.590326: step 56740, loss = 0.73 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:03:25.462189: step 56750, loss = 0.77 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:03:26.340395: step 56760, loss = 0.67 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:03:27.239091: step 56770, loss = 0.77 (1424.3 examples/sec; 0.090 sec/batch)
2017-06-02 04:03:28.123963: step 56780, loss = 0.82 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:03:29.007851: step 56790, loss = 0.71 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:03:30.003532: step 56800, loss = 0.93 (1285.5 examples/sec; 0.100 sec/batch)
2017-06-02 04:03:30.796453: step 56810, loss = 0.61 (1614.3 examples/sec; 0.079 sec/batch)
2017-06-02 04:03:31.672434: step 56820, loss = 0.78 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:03:32.554427: step 56830, loss = 0.65 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:03:33.408973: step 56840, loss = 0.67 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:03:34.295430: step 56850, loss = 0.83 (1443.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:03:35.175544: step 56860, loss = 0.71 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:03:36.050805: step 56870, loss = 0.76 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:03:36.924711: step 56880, loss = 0.82 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:03:37.788863: step 56890, loss = 0.69 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:03:38.785533: step 56900, loss = 0.83 (1284.3 examples/sec; 0.100 sec/batch)
2017-06-02 04:03:39.559765: step 56910, loss = 0.89 (1653.2 examples/sec; 0.077 sec/batch)
2017-06-02 04:03:40.453111: step 56920, loss = 0.88 (1432.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:03:41.317621: step 56930, loss = 0.67 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:03:42.178895: step 56940, loss = 0.76 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:03:43.083678: step 56950, loss = 0.93 (1414.7 examples/sec; 0.090 sec/batch)
2017-06-02 04:03:43.972418: step 56960, loss = 0.71 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 04:03:44.859626: step 56970, loss = 0.89 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:03:45.737727: step 56980, loss = 0.72 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:03:46.607485: step 56990, loss = 0.67 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:03:47.619864: step 57000, loss = 0.91 (1264.3 examples/sec; 0.101 sec/batch)
2017-06-02 04:03:48.431293: step 57010, loss = 0.63 (1577.5 examples/sec; 0.081 sec/batch)
2017-06-02 04:03:49.313623: step 57020, loss = 0.94 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:03:50.187848: step 57030, loss = 0.75 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:03:51.096435: step 57040, loss = 0.60 (1408.8 examples/sec; 0.091 sec/batch)
2017-06-02 04:03:51.987804: step 57050, loss = 0.90 (1436.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:03:52.860512: step 57060, loss = 0.71 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:03:53.746126: step 57070, loss = 0.74 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:03:54.617177: step 57080, loss = 0.70 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:03:55.505131: step 57090, loss = 0.75 (1441.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:03:56.498130: step 57100, loss = 0.64 (1289.0 examples/sec; 0.099 sec/batch)
2017-06-02 04:03:57.282622: step 57110, loss = 0.77 (1631.6 examples/sec; 0.078 sec/batch)
2017-06-02 04:03:58.150995: step 57120, loss = 0.61 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:03:59.037245: step 57130, loss = 1.02 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:03:59.887606: step 57140, loss = 0.80 (1505.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:04:00.773426: step 57150, loss = 0.57 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:04:01.644179: step 57160, loss = 0.72 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:04:02.529788: step 57170, loss = 0.66 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:04:03.416941: step 57180, loss = 0.61 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:04:04.294472: step 57190, loss = 0.67 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:04:05.268718: step 57200, loss = 0.78 (1313.8 examples/sec; 0.097 sec/batch)
2017-06-02 04:04:06.058302: step 57210, loss = 0.63 (1621.1 examples/sec; 0.079 sec/batch)
2017-06-02 04:04:06.938193: step 57220, loss = 0.87 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:04:07.830383: step 57230, loss = 0.78 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:04:08.733572: step 57240, loss = 0.71 (1417.2 examples/sec; 0.090 sec/batch)
2017-06-02 04:04:09.604370: step 57250, loss = 0.78 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:04:10.509849: step 57260, loss = 0.88 (1413.6 examples/sec; 0.091 sec/batch)
2017-06-02 04:04:11.407631: step 57270, loss = 0.71 (1425.7 examples/sec; 0.090 sec/batch)
2017-06-02 04:04:12.295248: step 57280, loss = 0.65 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:04:13.178390: step 57290, loss = 0.74 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:04:14.173273: step 57300, loss = 0.75 (1286.6 examples/sec; 0.099 sec/batch)
2017-06-02 04:04:14.945412: step 57310, loss = 0.83 (1657.7 examples/sec; 0.077 sec/batch)
2017-06-02 04:04:15.792586: step 57320, loss = 0.79 (1510.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:04:16.671980: step 57330, loss = 0.76 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:04:17.548480: step 57340, loss = 0.71 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:04:18.441225: step 57350, loss = 0.71 (1433.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:04:19.333534: step 57360, loss = 0.79 (1434.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:04:20.242830: step 57370, loss = 0.57 (1407.7 examples/sec; 0.091 sec/batch)
2017-06-02 04:04:21.150518: step 57380, loss = 0.81 (1410.2 examples/sec; 0.091 sec/batch)
2017-06-02 04:04:22.035294: step 57390, loss = 0.80 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:04:23.053571: step 57400, loss = 0.72 (1257.0 examples/sec; 0.102 sec/batch)
2017-06-02 04:04:23.825047: step 57410, loss = 0.65 (1659.2 examples/sec; 0.077 sec/batch)
2017-06-02 04:04:24.720259: step 57420, loss = 0.74 (1429.8 examples/sec; 0.090 sec/batch)
2017-06-02 04:04:25.584895: step 57430, loss = 0.76 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:04:26.464454: step 57440, loss = 0.68 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:04:27.351238: step 57450, loss = 0.87 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 04:04:28.240848: step 57460, loss = 0.64 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:04:29.131009: step 57470, loss = 0.82 (1437.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:04:30.006002: step 57480, loss = 0.82 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:04:30.860719: step 57490, loss = 0.70 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:04:31.837094: step 57500, loss = 0.81 (1311.0 examples/sec; 0.098 sec/batch)
2017-06-02 04:04:32.601897: step 57510, loss = 0.77 (1673.7 examples/sec; 0.076 sec/batch)
2017-06-02 04:04:33.466423: step 57520, loss = 0.90 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:04:34.338044: step 57530, loss = 0.86 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:04:35.194028: step 57540, loss = 0.77 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:04:36.065113: step 57550, loss = 0.66 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:04:36.945618: step 57560, loss = 0.91 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:04:37.827611: step 57570, loss = 0.70 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:04:38.713353: step 57580, loss = 0.66 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:04:39.583069: step 57590, loss = 0.81 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:04:40.548503: step 57600, loss = 0.81 (1325.8 examples/sec; 0.097 sec/batch)
2017-06-02 04:04:41.358620: step 57610, loss = 0.66 (1580.0 examples/sec; 0.081 sec/batch)
2017-06-02 04:04:42.236008: step 57620, loss = 0.77 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:04:43.150487: step 57630, loss = 0.66 (1399.7 examples/sec; 0.091 sec/batch)
2017-06-02 04:04:44.023568: step 57640, loss = 0.80 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:04:44.917612: step 57650, loss = 0.73 (1431.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:04:45.825392: step 57660, loss = 0.86 (1410.0 examples/sec; 0.091 sec/batch)
2017-06-02 04:04:46.711077: step 57670, loss = 0.71 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 04:04:47.602901: step 57680, loss = 0.74 (1435.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:04:48.472035: step 57690, loss = 0.82 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:04:49.462503: step 57700, loss = 0.78 (1292.3 examples/sec; 0.099 sec/batch)
2017-06-02 04:04:50.249639: step 57710, loss = 0.76 (1626.1 examples/sec; 0.079 sec/batch)
2017-06-02 04:04:51.145044: step 57720, loss = 0.60 (1429.5 examples/sec; 0.090 sec/batch)
2017-06-02 04:04:52.018398: step 57730, loss = 0.80 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:04:52.887499: step 57740, loss = 0.84 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:04:53.760359: step 57750, loss = 0.84 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:04:54.657553: step 57760, loss = 0.84 (1426.6 examples/sec; 0.090 sec/batch)
2017-06-02 04:04:55.524596: step 57770, loss = 0.67 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:04:56.397810: step 57780, loss = 1.01 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:04:57.251255: step 57790, loss = 0.71 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:04:58.225134: step 57800, loss = 0.70 (1314.3 examples/sec; 0.097 sec/batch)
2017-06-02 04:04:59.028542: step 57810, loss = 0.76 (1593.3 examples/sec; 0.080 sec/batch)
2017-06-02 04:04:59.916854: step 57820, loss = 0.71 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:05:00.796259: step 57830, loss = 0.78 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:05:01.686482: step 57840, loss = 0.52 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:05:02.555391: step 57850, loss = 0.89 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:05:03.424020: step 57860, loss = 0.72 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:05:04.290986: step 57870, loss = 0.74 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:05:05.161196: step 57880, loss = 0.78 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:05:06.036207: step 57890, loss = 0.81 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:05:07.008208: step 57900, loss = 0.96 (1316.9 examples/sec; 0.097 sec/batch)
2017-06-02 04:05:07.769874: step 57910, loss = 0.70 (1680.5 examples/sec; 0.076 sec/batch)
2017-06-02 04:05:08.647627: step 57920, loss = 0.81 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:05:09.522855: step 57930, loss = 0.63 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:05:10.407927: step 57940, loss = 0.67 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 04:05:11.293026: step 57950, loss = 0.77 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:05:12.157030: step 57960, loss = 0.78 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:05:13.034367: step 57970, loss = 0.82 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:05:13.864168: step 57980, loss = 0.56 (1542.6 examples/sec; 0.083 sec/batch)
2017-06-02 04:05:14.695089: step 57990, loss = 0.86 (1540.4 examples/sec; 0.083 sec/batch)
2017-06-02 04:05:15.645803: step 58000, loss = 0.68 (1346.4 examples/sec; 0.095 sec/batch)
2017-06-02 04:05:16.418247: step 58010, loss = 0.75 (1657.1 examples/sec; 0.077 sec/batch)
2017-06-02 04:05:17.269913: step 58020, loss = 0.76 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:05:18.118421: step 58030, loss = 0.79 (1508.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:05:18.997581: step 58040, loss = 0.72 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:05:19.885826: step 58050, loss = 0.65 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:05:20.761480: step 58060, loss = 0.79 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:05:21.650646: step 58070, loss = 0.86 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:05:22.514165: step 58080, loss = 0.80 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:05:23.401906: step 58090, loss = 0.96 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:05:24.359170: step 58100, loss = 0.78 (1337.1 examples/sec; 0.096 sec/batch)
2017-06-02 04:05:25.149210: step 58110, loss = 0.70 (1620.2 examples/sec; 0.079 sec/batch)
2017-06-02 04:05:26.017353: step 58120, loss = 0.64 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:05:26.894368: step 58130, loss = 0.70 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:05:27.787814: step 58140, loss = 0.66 (1432.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:05:28.646451: step 58150, loss = 0.66 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:05:29.538490: step 58160, loss = 0.61 (1434.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:05:30.426356: step 58170, loss = 0.82 (1441.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:05:31.303613: step 58180, loss = 0.66 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:05:32.191646: step 58190, loss = 0.77 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 04:05:33.135373: step 58200, loss = 0.68 (1356.3 examples/sec; 0.094 sec/batch)
2017-06-02 04:05:33.901977: step 58210, loss = 0.68 (1669.7 examples/sec; 0.077 sec/batch)
2017-06-02 04:05:34.738543: step 58220, loss = 0.85 (1530.1 examples/sec; 0.084 sec/batch)
2017-06-02 04:05:35.616151: step 58230, loss = 0.81 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:05:36.485821: step 58240, loss = 0.77 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:05:37.347942: step 58250, loss = 0.57 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:05:38.236561: step 58260, loss = 0.69 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:05:39.099609: step 58270, loss = 0.76 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:05:39.962328: step 58280, loss = 0.75 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:05:40.837959: step 58290, loss = 0.78 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:05:41.879721: step 58300, loss = 0.64 (1228.7 examples/sec; 0.104 sec/batch)
2017-06-02 04:05:42.599378: step 58310, loss = 0.78 (1778.6 examples/sec; 0.072 sec/batch)
2017-06-02 04:05:43.442007: step 58320, loss = 0.79 (1519.1 examples/sec; 0.084 sec/batch)
2017-06-02 04:05:44.307898: step 58330, loss = 0.71 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:05:45.181076: step 58340, loss = 0.67 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:05:46.046547: step 58350, loss = 0.75 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:05:46.884483: step 58360, loss = 0.70 (1527.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:05:47.737267: step 58370, loss = 0.74 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:05:48.611059: step 58380, loss = 0.73 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:05:49.477865: step 58390, loss = 0.74 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:05:50.427985: step 58400, loss = 0.78 (1347.2 examples/sec; 0.095 sec/batch)
2017-06-02 04:05:51.234952: step 58410, loss = 0.63 (1586.2 examples/sec; 0.081 sec/batch)
2017-06-02 04:05:52.110281: step 58420, loss = 0.78 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:05:52.955746: step 58430, loss = 0.71 (1514.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:05:53.823658: step 58440, loss = 0.64 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:05:54.677265: step 58450, loss = 0.58 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:05:55.561513: step 58460, loss = 0.57 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:05:56.428420: step 58470, loss = 0.73 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:05:57.321609: step 58480, loss = 0.79 (1433.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:05:58.197088: step 58490, loss = 0.84 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:05:59.164200: step 58500, loss = 0.53 (1323.5 examples/sec; 0.097 sec/batch)
2017-06-02 04:05:59.922019: step 58510, loss = 0.81 (1689.1 examples/sec; 0.076 sec/batch)
2017-06-02 04:06:00.818707: step 58520, loss = 0.70 (1427.5 examples/sec; 0.090 sec/batch)
2017-06-02 04:06:01.728268: step 58530, loss = 0.85 (1407.3 examples/sec; 0.091 sec/batch)
2017-06-02 04:06:02.570814: step 58540, loss = 0.80 (1519.2 examples/sec; 0.084 sec/batch)
2017-06-02 04:06:03.414214: step 58550, loss = 0.81 (1517.7 examples/sec; 0.084 sec/batch)
2017-06-02 04:06:04.289201: step 58560, loss = 0.72 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:06:05.159523: step 58570, loss = 0.64 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:06:06.024863: step 58580, loss = 0.80 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:06:06.892841: step 58590, loss = 0.84 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:06:07.889070: step 58600, loss = 0.69 (1284.8 examples/sec; 0.100 sec/batch)
2017-06-02 04:06:08.674803: step 58610, loss = 0.72 (1629.1 examples/sec; 0.079 sec/batch)
2017-06-02 04:06:09.526450: step 58620, loss = 0.68 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:06:10.374785: step 58630, loss = 0.68 (1508.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:06:11.262427: step 58640, loss = 0.79 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:06:12.150691: step 58650, loss = 0.79 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:06:13.052657: step 58660, loss = 0.71 (1419.1 examples/sec; 0.090 sec/batch)
2017-06-02 04:06:13.937358: step 58670, loss = 0.71 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:06:14.792947: step 58680, loss = 0.71 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:06:15.688394: step 58690, loss = 0.89 (1429.5 examples/sec; 0.090 sec/batch)
2017-06-02 04:06:16.650971: step 58700, loss = 0.76 (1329.7 examples/sec; 0.096 sec/batch)
2017-06-02 04:06:17.416087: step 58710, loss = 0.75 (1673.0 examples/sec; 0.077 sec/batch)
2017-06-02 04:06:18.267465: step 58720, loss = 0.88 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:06:19.110333: step 58730, loss = 0.77 (1518.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:06:19.983797: step 58740, loss = 0.82 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:06:20.840791: step 58750, loss = 0.65 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:06:21.692854: step 58760, loss = 0.90 (1502.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:06:22.571511: step 58770, loss = 0.77 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:06:23.466154: step 58780, loss = 0.73 (1430.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:06:24.342273: step 58790, loss = 0.70 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:06:25.305048: step 58800, loss = 0.89 (1329.5 examples/sec; 0.096 sec/batch)
2017-06-02 04:06:26.091728: step 58810, loss = 0.78 (1627.1 examples/sec; 0.079 sec/batch)
2017-06-02 04:06:26.983433: step 58820, loss = 0.86 (1435.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:06:27.857653: step 58830, loss = 0.80 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:06:28.736166: step 58840, loss = 0.74 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:06:29.575357: step 58850, loss = 0.81 (1525.3 examples/sec; 0.084 sec/batch)
2017-06-02 04:06:30.432265: step 58860, loss = 0.74 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:06:31.292973: step 58870, loss = 0.65 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:06:32.161291: step 58880, loss = 0.78 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:06:33.043921: step 58890, loss = 0.62 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:06:34.010523: step 58900, loss = 0.63 (1324.2 examples/sec; 0.097 sec/batch)
2017-06-02 04:06:34.784134: step 58910, loss = 0.68 (1654.6 examples/sec; 0.077 sec/batch)
2017-06-02 04:06:35.684707: step 58920, loss = 0.73 (1421.3 examples/sec; 0.090 sec/batch)
2017-06-02 04:06:36.543746: step 58930, loss = 0.67 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:06:37.423462: step 58940, loss = 0.71 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:06:38.264389: step 58950, loss = 0.77 (1522.1 examples/sec; 0.084 sec/batch)
2017-06-02 04:06:39.146462: step 58960, loss = 0.64 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:06:40.014360: step 58970, loss = 0.84 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:06:40.873285: step 58980, loss = 0.56 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:06:41.750923: step 58990, loss = 0.64 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:06:42.782211: step 59000, loss = 0.75 (1241.1 examples/sec; 0.103 sec/batch)
2017-06-02 04:06:43.531824: step 59010, loss = 0.78 (1707.5 examples/sec; 0.075 sec/batch)
2017-06-02 04:06:44.399043: step 59020, loss = 0.76 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:06:45.265665: step 59030, loss = 0.75 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:06:46.106672: step 59040, loss = 0.78 (1522.0 examples/sec; 0.084 sec/batch)
2017-06-02 04:06:46.976057: step 59050, loss = 0.74 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:06:47.842556: step 59060, loss = 0.75 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:06:48.710970: step 59070, loss = 0.70 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:06:49.598716: step 59080, loss = 0.70 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:06:50.441919: step 59090, loss = 0.62 (1518.0 examples/sec; 0.084 sec/batch)
2017-06-02 04:06:51.404703: step 59100, loss = 0.69 (1329.5 examples/sec; 0.096 sec/batch)
2017-06-02 04:06:52.180986: step 59110, loss = 0.74 (1648.9 examples/sec; 0.078 sec/batch)
2017-06-02 04:06:53.032940: step 59120, loss = 0.58 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:06:53.884360: step 59130, loss = 0.73 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:06:54.737467: step 59140, loss = 0.92 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:06:55.614194: step 59150, loss = 0.82 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:06:56.480813: step 59160, loss = 0.75 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:06:57.360720: step 59170, loss = 0.85 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:06:58.221124: step 59180, loss = 0.77 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:06:59.104049: step 59190, loss = 0.67 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:07:00.093454: step 59200, loss = 0.85 (1293.7 examples/sec; 0.099 sec/batch)
2017-06-02 04:07:00.885449: step 59210, loss = 0.79 (1616.2 examples/sec; 0.079 sec/batch)
2017-06-02 04:07:01.750293: step 59220, loss = 0.80 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:07:02.644084: step 59230, loss = 0.83 (1432.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:07:03.534759: step 59240, loss = 0.79 (1437.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:07:04.390311: step 59250, loss = 0.68 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:07:05.272765: step 59260, loss = 0.86 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:07:06.162750: step 59270, loss = 0.83 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 04:07:07.061921: step 59280, loss = 0.78 (1423.5 examples/sec; 0.090 sec/batch)
2017-06-02 04:07:07.927622: step 59290, loss = 0.66 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:07:08.893986: step 59300, loss = 0.86 (1324.6 examples/sec; 0.097 sec/batch)
2017-06-02 04:07:09.698421: step 59310, loss = 0.75 (1591.2 examples/sec; 0.080 sec/batch)
2017-06-02 04:07:10.579379: step 59320, loss = 0.73 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:07:11.462004: step 59330, loss = 0.79 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:07:12.329758: step 59340, loss = 0.67 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:07:13.224549: step 59350, loss = 0.84 (1430.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:07:14.102172: step 59360, loss = 0.70 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:07:14.990290: step 59370, loss = 0.75 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:07:15.864746: step 59380, loss = 0.73 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:07:16.755964: step 59390, loss = 0.86 (1436.2 examples/sec; 0.089 sec/batch)
2017-06-02 04:07:17.729301: step 59400, loss = 0.73 (1315.1 examples/sec; 0.097 sec/batch)
2017-06-02 04:07:18.510089: step 59410, loss = 0.70 (1639.4 examples/sec; 0.078 sec/batch)
2017-06-02 04:07:19.358624: step 59420, loss = 0.73 (1508.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:07:20.214185: step 59430, loss = 0.72 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:07:21.064423: step 59440, loss = 0.69 (1505.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:07:21.930292: step 59450, loss = 0.72 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:07:22.815366: step 59460, loss = 0.77 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 04:07:23.686815: step 59470, loss = 0.75 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:07:24.549361: step 59480, loss = 0.71 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:07:25.428159: step 59490, loss = 0.70 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:07:26.447016: step 59500, loss = 0.71 (1256.3 examples/sec; 0.102 sec/batch)
2017-06-02 04:07:27.224600: step 59510, loss = 0.75 (1646.2 examples/sec; 0.078 sec/batch)
2017-06-02 04:07:28.084758: step 59520, loss = 0.69 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:07:28.943704: step 59530, loss = 0.76 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:07:29.792048: step 59540, loss = 0.78 (1508.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:07:30.665571: step 59550, loss = 0.75 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:07:31.530134: step 59560, loss = 0.78 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:07:32.394618: step 59570, loss = 0.66 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:07:33.263223: step 59580, loss = 0.80 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:07:34.100313: step 59590, loss = 0.82 (1529.1 examples/sec; 0.084 sec/batch)
2017-06-02 04:07:35.067348: step 59600, loss = 0.62 (1323.6 examples/sec; 0.097 sec/batch)
2017-06-02 04:07:35.868869: step 59610, loss = 0.77 (1597.0 examples/sec; 0.080 sec/batch)
2017-06-02 04:07:36.714366: step 59620, loss = 0.80 (1513.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:07:37.581635: step 59630, loss = 0.86 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:07:38.460782: step 59640, loss = 0.73 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:07:39.341842: step 59650, loss = 0.76 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:07:40.217884: step 59660, loss = 0.73 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:07:41.100936: step 59670, loss = 0.70 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:07:41.995511: step 59680, loss = 0.64 (1430.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:07:42.858399: step 59690, loss = 0.83 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:07:43.850744: step 59700, loss = 0.80 (1289.9 examples/sec; 0.099 sec/batch)
2017-06-02 04:07:44.638132: step 59710, loss = 0.66 (1625.6 examples/sec; 0.079 sec/batch)
2017-06-02 04:07:45.518030: step 59720, loss = 0.70 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:07:46.386118: step 59730, loss = 0.71 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:07:47.261862: step 59740, loss = 0.80 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:07:48.143140: step 59750, loss = 0.69 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:07:49.001656: step 59760, loss = 0.83 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:07:49.873839: step 59770, loss = 0.62 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:07:50.747609: step 59780, loss = 0.65 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:07:51.618815: step 59790, loss = 0.68 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:07:52.569707: step 59800, loss = 0.76 (1346.1 examples/sec; 0.095 sec/batch)
2017-06-02 04:07:53.352811: step 59810, loss = 0.65 (1634.5 examples/sec; 0.078 sec/batch)
2017-06-02 04:07:54.215212: step 59820, loss = 0.61 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:07:55.122176: step 59830, loss = 0.72 (1411.3 examples/sec; 0.091 sec/batch)
2017-06-02 04:07:56.019927: step 59840, loss = 0.64 (1425.8 examples/sec; 0.090 sec/batch)
2017-06-02 04:07:56.895781: step 59850, loss = 0.78 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:07:57.811602: step 59860, loss = 0.71 (1397.6 examples/sec; 0.092 sec/batch)
2017-06-02 04:07:58.686260: step 59870, loss = 0.71 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:07:59.621715: step 59880, loss = 0.74 (1368.3 examples/sec; 0.094 sec/batch)
2017-06-02 04:08:00.528222: step 59890, loss = 0.93 (1412.0 examples/sec; 0.091 sec/batch)
2017-06-02 04:08:01.504781: step 59900, loss = 0.88 (1310.7 examples/sec; 0.098 sec/batch)
2017-06-02 04:08:02.288217: step 59910, loss = 0.72 (1633.8 examples/sec; 0.078 sec/batch)
2017-06-02 04:08:03.176488: step 59920, loss = 0.67 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:08:04.071584: step 59930, loss = 0.84 (1430.0 examples/sec; 0.090 sec/batch)
2017-06-02 04:08:04.950081: step 59940, loss = 0.77 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:08:05.836298: step 59950, loss = 0.73 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:08:06.705050: step 59960, loss = 0.85 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:08:07.586309: step 59970, loss = 0.62 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:08:08.470766: step 59980, loss = 0.78 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:08:09.310922: step 59990, loss = 0.82 (1523.5 examples/sec; 0.084 sec/batch)
2017-06-02 04:08:10.261519: step 60000, loss = 0.84 (1346.5 examples/sec; 0.095 sec/batch)
2017-06-02 04:08:11.044310: step 60010, loss = 0.79 (1635.2 examples/sec; 0.078 sec/batch)
2017-06-02 04:08:11.930704: step 60020, loss = 0.78 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:08:12.816115: step 60030, loss = 0.84 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:08:13.691462: step 60040, loss = 0.87 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:08:14.544270: step 60050, loss = 0.72 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:08:15.408196: step 60060, loss = 0.74 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:08:16.291943: step 60070, loss = 0.69 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:08:17.140789: step 60080, loss = 0.74 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:08:17.982867: step 60090, loss = 0.68 (1520.1 examples/sec; 0.084 sec/batch)
2017-06-02 04:08:18.938843: step 60100, loss = 0.77 (1338.9 examples/sec; 0.096 sec/batch)
2017-06-02 04:08:19.719926: step 60110, loss = 0.76 (1638.8 examples/sec; 0.078 sec/batch)
2017-06-02 04:08:20.594252: step 60120, loss = 0.78 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:08:21.486493: step 60130, loss = 0.77 (1434.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:08:22.372856: step 60140, loss = 0.85 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:08:23.257253: step 60150, loss = 0.68 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:08:24.124929: step 60160, loss = 0.64 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:08:24.994077: step 60170, loss = 0.58 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:08:25.876070: step 60180, loss = 0.86 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:08:26.754063: step 60190, loss = 0.67 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:08:27.752166: step 60200, loss = 0.63 (1282.4 examples/sec; 0.100 sec/batch)
2017-06-02 04:08:28.506982: step 60210, loss = 0.72 (1695.8 examples/sec; 0.075 sec/batch)
2017-06-02 04:08:29.395242: step 60220, loss = 0.71 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:08:30.275434: step 60230, loss = 0.66 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:08:31.129502: step 60240, loss = 0.80 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:08:32.031407: step 60250, loss = 0.72 (1419.2 examples/sec; 0.090 sec/batch)
2017-06-02 04:08:32.909648: step 60260, loss = 0.79 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:08:33.794788: step 60270, loss = 0.92 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:08:34.666611: step 60280, loss = 0.78 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:08:35.520071: step 60290, loss = 0.90 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:08:36.486049: step 60300, loss = 0.76 (1325.1 examples/sec; 0.097 sec/batch)
2017-06-02 04:08:37.271741: step 60310, loss = 0.69 (1629.1 examples/sec; 0.079 sec/batch)
2017-06-02 04:08:38.144748: step 60320, loss = 0.68 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:08:39.018240: step 60330, loss = 0.66 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:08:39.890670: step 60340, loss = 0.73 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:08:40.753889: step 60350, loss = 0.67 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:08:41.609509: step 60360, loss = 0.90 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:08:42.485059: step 60370, loss = 0.86 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:08:43.375876: step 60380, loss = 0.96 (1436.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:08:44.274837: step 60390, loss = 0.82 (1423.9 examples/sec; 0.090 sec/batch)
2017-06-02 04:08:45.270884: step 60400, loss = 0.79 (1285.1 examples/sec; 0.100 sec/batch)
2017-06-02 04:08:46.054316: step 60410, loss = 0.67 (1633.8 examples/sec; 0.078 sec/batch)
2017-06-02 04:08:46.947331: step 60420, loss = 0.92 (1433.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:08:47.844788: step 60430, loss = 0.71 (1426.3 examples/sec; 0.090 sec/batch)
2017-06-02 04:08:48.738451: step 60440, loss = 0.80 (1432.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:08:49.615664: step 60450, loss = 0.61 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:08:50.477111: step 60460, loss = 0.70 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:08:51.339687: step 60470, loss = 0.71 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:08:52.237344: step 60480, loss = 0.76 (1425.9 examples/sec; 0.090 sec/batch)
2017-06-02 04:08:53.130151: step 60490, loss = 0.83 (1433.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:08:54.127257: step 60500, loss = 0.72 (1283.7 examples/sec; 0.100 sec/batch)
2017-06-02 04:08:54.906706: step 60510, loss = 0.64 (1642.2 examples/sec; 0.078 sec/batch)
2017-06-02 04:08:55.783335: step 60520, loss = 0.72 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:08:56.659481: step 60530, loss = 0.59 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:08:57.547835: step 60540, loss = 0.94 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:08:58.404405: step 60550, loss = 0.55 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:08:59.293520: step 60560, loss = 0.67 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:09:00.168085: step 60570, loss = 0.70 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:09:01.044111: step 60580, loss = 0.77 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:09:01.932313: step 60590, loss = 0.63 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:09:02.930825: step 60600, loss = 0.76 (1281.9 examples/sec; 0.100 sec/batch)
2017-06-02 04:09:03.711867: step 60610, loss = 0.89 (1638.9 examples/sec; 0.078 sec/batch)
2017-06-02 04:09:04.567388: step 60620, loss = 0.78 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:09:05.432624: step 60630, loss = 0.71 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:09:06.353062: step 60640, loss = 0.73 (1390.6 examples/sec; 0.092 sec/batch)
2017-06-02 04:09:07.241180: step 60650, loss = 0.71 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 04:09:08.149527: step 60660, loss = 0.74 (1409.2 examples/sec; 0.091 sec/batch)
2017-06-02 04:09:08.998520: step 60670, loss = 0.83 (1507.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:09:09.853690: step 60680, loss = 0.68 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:09:10.732852: step 60690, loss = 0.75 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:09:11.674672: step 60700, loss = 0.63 (1359.1 examples/sec; 0.094 sec/batch)
2017-06-02 04:09:12.452971: step 60710, loss = 0.57 (1644.6 examples/sec; 0.078 sec/batch)
2017-06-02 04:09:13.323215: step 60720, loss = 0.73 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:09:14.168669: step 60730, loss = 0.88 (1514.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:09:15.057749: step 60740, loss = 0.72 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:09:15.942880: step 60750, loss = 0.71 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:09:16.826579: step 60760, loss = 0.79 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:09:17.716071: step 60770, loss = 0.76 (1439.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:09:18.567216: step 60780, loss = 0.79 (1503.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:09:19.434152: step 60790, loss = 0.76 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:09:20.424024: step 60800, loss = 0.70 (1293.1 examples/sec; 0.099 sec/batch)
2017-06-02 04:09:21.219221: step 60810, loss = 0.68 (1609.7 examples/sec; 0.080 sec/batch)
2017-06-02 04:09:22.093584: step 60820, loss = 0.91 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:09:22.972143: step 60830, loss = 0.70 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:09:23.857615: step 60840, loss = 0.82 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:09:24.750950: step 60850, loss = 0.78 (1432.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:09:25.622085: step 60860, loss = 0.73 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:09:26.509373: step 60870, loss = 0.77 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:09:27.404700: step 60880, loss = 0.59 (1429.7 examples/sec; 0.090 sec/batch)
2017-06-02 04:09:28.288156: step 60890, loss = 0.99 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:09:29.286813: step 60900, loss = 0.78 (1281.7 examples/sec; 0.100 sec/batch)
2017-06-02 04:09:30.076621: step 60910, loss = 0.75 (1620.6 examples/sec; 0.079 sec/batch)
2017-06-02 04:09:30.934964: step 60920, loss = 0.72 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:09:31.797950: step 60930, loss = 0.81 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:09:32.689609: step 60940, loss = 0.68 (1435.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:09:33.565556: step 60950, loss = 0.57 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:09:34.412242: step 60960, loss = 0.83 (1511.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:09:35.327127: step 60970, loss = 0.81 (1399.1 examples/sec; 0.091 sec/batch)
2017-06-02 04:09:36.213195: step 60980, loss = 0.87 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:09:37.084335: step 60990, loss = 0.71 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:09:38.122155: step 61000, loss = 0.84 (1233.3 examples/sec; 0.104 sec/batch)
2017-06-02 04:09:38.850447: step 61010, loss = 0.73 (1757.5 examples/sec; 0.073 sec/batch)
2017-06-02 04:09:39.749569: step 61020, loss = 0.57 (1423.6 examples/sec; 0.090 sec/batch)
2017-06-02 04:09:40.641578: step 61030, loss = 0.75 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:09:41.487984: step 61040, loss = 0.87 (1512.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:09:42.372157: step 61050, loss = 0.75 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:09:43.247859: step 61060, loss = 0.73 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:09:44.113509: step 61070, loss = 0.60 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:09:44.980573: step 61080, loss = 0.70 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:09:45.868478: step 61090, loss = 0.73 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:09:46.865061: step 61100, loss = 0.80 (1284.4 examples/sec; 0.100 sec/batch)
2017-06-02 04:09:47.584278: step 61110, loss = 0.66 (1779.7 examples/sec; 0.072 sec/batch)
2017-06-02 04:09:48.461425: step 61120, loss = 0.74 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:09:49.352083: step 61130, loss = 0.92 (1437.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:09:50.225259: step 61140, loss = 0.86 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:09:51.197377: step 61150, loss = 0.71 (1316.7 examples/sec; 0.097 sec/batch)
2017-06-02 04:09:52.089511: step 61160, loss = 0.71 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:09:52.950147: step 61170, loss = 0.86 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:09:53.811128: step 61180, loss = 0.81 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:09:54.694486: step 61190, loss = 0.73 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:09:55.671471: step 61200, loss = 0.74 (1310.2 examples/sec; 0.098 sec/batch)
2017-06-02 04:09:56.437637: step 61210, loss = 0.58 (1670.6 examples/sec; 0.077 sec/batch)
2017-06-02 04:09:57.314959: step 61220, loss = 0.70 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:09:58.183966: step 61230, loss = 0.81 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:09:59.047837: step 61240, loss = 0.63 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:09:59.929422: step 61250, loss = 0.78 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:10:00.816118: step 61260, loss = 0.74 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:10:01.692311: step 61270, loss = 0.73 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:10:02.562752: step 61280, loss = 0.73 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:10:03.425433: step 61290, loss = 0.75 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:10:04.424151: step 61300, loss = 0.70 (1281.6 examples/sec; 0.100 sec/batch)
2017-06-02 04:10:05.195727: step 61310, loss = 0.74 (1658.9 examples/sec; 0.077 sec/batch)
2017-06-02 04:10:06.090952: step 61320, loss = 0.92 (1429.8 examples/sec; 0.090 sec/batch)
2017-06-02 04:10:06.985215: step 61330, loss = 0.71 (1431.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:10:07.857808: step 61340, loss = 0.70 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:10:08.730687: step 61350, loss = 0.67 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:10:09.607466: step 61360, loss = 0.90 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:10:10.493237: step 61370, loss = 0.69 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:10:11.364031: step 61380, loss = 0.71 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:10:12.254076: step 61390, loss = 0.73 (1438.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:10:13.290599: step 61400, loss = 0.73 (1234.9 examples/sec; 0.104 sec/batch)
2017-06-02 04:10:14.059388: step 61410, loss = 0.77 (1665.0 examples/sec; 0.077 sec/batch)
2017-06-02 04:10:14.936485: step 61420, loss = 0.74 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:10:15.826119: step 61430, loss = 0.83 (1438.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:10:16.704694: step 61440, loss = 0.61 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:10:17.573651: step 61450, loss = 0.83 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:10:18.452131: step 61460, loss = 0.76 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:10:19.282434: step 61470, loss = 0.68 (1541.6 examples/sec; 0.083 sec/batch)
2017-06-02 04:10:20.137275: step 61480, loss = 0.79 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:10:21.008034: step 61490, loss = 0.69 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:10:21.964651: step 61500, loss = 0.66 (1338.0 examples/sec; 0.096 sec/batch)
2017-06-02 04:10:22.724340: step 61510, loss = 0.81 (1684.9 examples/sec; 0.076 sec/batch)
2017-06-02 04:10:23.588279: step 61520, loss = 0.60 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:10:24.451095: step 61530, loss = 0.67 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:10:25.336135: step 61540, loss = 0.70 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:10:26.228805: step 61550, loss = 0.77 (1433.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:10:27.128190: step 61560, loss = 0.57 (1423.2 examples/sec; 0.090 sec/batch)
2017-06-02 04:10:28.025301: step 61570, loss = 0.70 (1426.8 examples/sec; 0.090 sec/batch)
2017-06-02 04:10:28.942676: step 61580, loss = 0.75 (1395.3 examples/sec; 0.092 sec/batch)
2017-06-02 04:10:29.838316: step 61590, loss = 0.70 (1429.1 examples/sec; 0.090 sec/batch)
2017-06-02 04:10:30.834612: step 61600, loss = 0.67 (1284.8 examples/sec; 0.100 sec/batch)
2017-06-02 04:10:31.629173: step 61610, loss = 0.75 (1611.0 examples/sec; 0.079 sec/batch)
2017-06-02 04:10:32.521357: step 61620, loss = 0.78 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:10:33.396925: step 61630, loss = 0.68 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:10:34.237618: step 61640, loss = 0.73 (1522.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:10:35.125926: step 61650, loss = 0.85 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:10:35.990912: step 61660, loss = 0.68 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:10:36.865438: step 61670, loss = 0.87 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:10:37.746802: step 61680, loss = 0.74 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:10:38.628516: step 61690, loss = 0.80 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:10:39.608666: step 61700, loss = 0.65 (1305.9 examples/sec; 0.098 sec/batch)
2017-06-02 04:10:40.391694: step 61710, loss = 0.83 (1634.7 examples/sec; 0.078 sec/batch)
2017-06-02 04:10:41.251917: step 61720, loss = 0.65 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:10:42.112636: step 61730, loss = 0.57 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:10:43.008696: step 61740, loss = 0.74 (1428.5 examples/sec; 0.090 sec/batch)
2017-06-02 04:10:43.900924: step 61750, loss = 0.81 (1434.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:10:44.824641: step 61760, loss = 0.68 (1385.7 examples/sec; 0.092 sec/batch)
2017-06-02 04:10:45.692351: step 61770, loss = 0.66 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:10:46.620780: step 61780, loss = 0.84 (1378.7 examples/sec; 0.093 sec/batch)
2017-06-02 04:10:47.508495: step 61790, loss = 0.92 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:10:48.488008: step 61800, loss = 0.60 (1306.8 examples/sec; 0.098 sec/batch)
2017-06-02 04:10:49.261942: step 61810, loss = 0.83 (1653.9 examples/sec; 0.077 sec/batch)
2017-06-02 04:10:50.148408: step 61820, loss = 0.69 (1443.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:10:51.018881: step 61830, loss = 0.66 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:10:51.876116: step 61840, loss = 0.71 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:10:52.724156: step 61850, loss = 0.64 (1509.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:10:53.597470: step 61860, loss = 0.63 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:10:54.456853: step 61870, loss = 0.96 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:10:55.315370: step 61880, loss = 0.84 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:10:56.193157: step 61890, loss = 0.72 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:10:57.154474: step 61900, loss = 0.95 (1331.5 examples/sec; 0.096 sec/batch)
2017-06-02 04:10:57.938407: step 61910, loss = 0.74 (1632.8 examples/sec; 0.078 sec/batch)
2017-06-02 04:10:58.815397: step 61920, loss = 0.79 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:10:59.708643: step 61930, loss = 0.76 (1433.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:11:00.597956: step 61940, loss = 0.64 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:11:01.473715: step 61950, loss = 0.92 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:11:02.335091: step 61960, loss = 0.69 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:11:03.205995: step 61970, loss = 0.66 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:11:04.100797: step 61980, loss = 0.77 (1430.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:11:04.969711: step 61990, loss = 0.72 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:11:05.961876: step 62000, loss = 0.65 (1290.1 examples/sec; 0.099 sec/batch)
2017-06-02 04:11:06.758883: step 62010, loss = 0.60 (1606.0 examples/sec; 0.080 sec/batch)
2017-06-02 04:11:07.628952: step 62020, loss = 0.63 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:11:08.521038: step 62030, loss = 0.82 (1434.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:11:09.409656: step 62040, loss = 0.73 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 04:11:10.283115: step 62050, loss = 0.73 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:11:11.120865: step 62060, loss = 0.65 (1527.9 examples/sec; 0.084 sec/batch)
2017-06-02 04:11:11.990890: step 62070, loss = 0.60 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:11:12.857992: step 62080, loss = 0.76 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:11:13.746382: step 62090, loss = 0.72 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:11:14.718960: step 62100, loss = 0.77 (1316.1 examples/sec; 0.097 sec/batch)
2017-06-02 04:11:15.493967: step 62110, loss = 0.56 (1651.6 examples/sec; 0.078 sec/batch)
2017-06-02 04:11:16.368544: step 62120, loss = 0.88 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:11:17.248863: step 62130, loss = 0.65 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:11:18.127291: step 62140, loss = 0.95 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:11:18.998039: step 62150, loss = 0.57 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:11:19.874174: step 62160, loss = 0.87 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:11:20.737672: step 62170, loss = 0.73 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:11:21.594253: step 62180, loss = 0.68 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:11:22.445458: step 62190, loss = 0.76 (1503.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:11:23.394271: step 62200, loss = 0.58 (1349.0 examples/sec; 0.095 sec/batch)
2017-06-02 04:11:24.160456: step 62210, loss = 0.84 (1670.6 examples/sec; 0.077 sec/batch)
2017-06-02 04:11:25.027126: step 62220, loss = 0.96 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:11:25.917320: step 62230, loss = 0.72 (1437.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:11:26.792316: step 62240, loss = 0.62 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:11:27.664078: step 62250, loss = 0.74 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:11:28.516794: step 62260, loss = 0.62 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:11:29.399390: step 62270, loss = 0.61 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:11:30.283719: step 62280, loss = 0.88 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:11:31.154024: step 62290, loss = 0.77 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:11:32.104411: step 62300, loss = 0.80 (1346.8 examples/sec; 0.095 sec/batch)
2017-06-02 04:11:32.892373: step 62310, loss = 0.63 (1624.4 examples/sec; 0.079 sec/batch)
2017-06-02 04:11:33.770823: step 62320, loss = 0.58 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:11:34.660972: step 62330, loss = 0.71 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:11:35.520671: step 62340, loss = 0.87 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:11:36.379405: step 62350, loss = 0.69 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:11:37.269405: step 62360, loss = 0.66 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 04:11:38.157239: step 62370, loss = 0.61 (1441.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:11:39.042555: step 62380, loss = 0.73 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:11:39.905537: step 62390, loss = 0.72 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:11:40.891881: step 62400, loss = 0.71 (1297.7 examples/sec; 0.099 sec/batch)
2017-06-02 04:11:41.651004: step 62410, loss = 0.71 (1686.2 examples/sec; 0.076 sec/batch)
2017-06-02 04:11:42.524631: step 62420, loss = 0.76 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:11:43.386518: step 62430, loss = 0.88 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:11:44.287073: step 62440, loss = 0.70 (1421.3 examples/sec; 0.090 sec/batch)
2017-06-02 04:11:45.171874: step 62450, loss = 0.92 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:11:46.065222: step 62460, loss = 0.88 (1432.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:11:46.933491: step 62470, loss = 0.70 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:11:47.811129: step 62480, loss = 0.72 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:11:48.693574: step 62490, loss = 0.61 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:11:49.673007: step 62500, loss = 0.62 (1306.9 examples/sec; 0.098 sec/batch)
2017-06-02 04:11:50.450280: step 62510, loss = 0.79 (1646.8 examples/sec; 0.078 sec/batch)
2017-06-02 04:11:51.352968: step 62520, loss = 0.81 (1418.0 examples/sec; 0.090 sec/batch)
2017-06-02 04:11:52.212650: step 62530, loss = 0.53 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:11:53.097965: step 62540, loss = 0.64 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:11:53.946338: step 62550, loss = 0.87 (1508.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:11:54.819228: step 62560, loss = 0.83 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:11:55.695707: step 62570, loss = 0.88 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:11:56.590725: step 62580, loss = 0.64 (1430.1 examples/sec; 0.090 sec/batch)
2017-06-02 04:11:57.469097: step 62590, loss = 0.76 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:11:58.453705: step 62600, loss = 0.84 (1300.0 examples/sec; 0.098 sec/batch)
2017-06-02 04:11:59.230413: step 62610, loss = 0.79 (1648.0 examples/sec; 0.078 sec/batch)
2017-06-02 04:12:00.089409: step 62620, loss = 0.70 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:12:00.971669: step 62630, loss = 0.76 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:12:01.833044: step 62640, loss = 0.75 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:12:02.676410: step 62650, loss = 0.64 (1517.7 examples/sec; 0.084 sec/batch)
2017-06-02 04:12:03.546352: step 62660, loss = 0.85 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:12:04.405238: step 62670, loss = 0.65 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:12:05.279972: step 62680, loss = 0.75 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:12:06.161304: step 62690, loss = 0.81 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:12:07.186146: step 62700, loss = 0.67 (1249.0 examples/sec; 0.102 sec/batch)
2017-06-02 04:12:07.923867: step 62710, loss = 0.63 (1735.1 examples/sec; 0.074 sec/batch)
2017-06-02 04:12:08.799057: step 62720, loss = 0.79 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:12:09.632603: step 62730, loss = 0.73 (1535.6 examples/sec; 0.083 sec/batch)
2017-06-02 04:12:10.503889: step 62740, loss = 0.76 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:12:11.380294: step 62750, loss = 0.73 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:12:12.245084: step 62760, loss = 0.66 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:12:13.103100: step 62770, loss = 0.80 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:12:13.976344: step 62780, loss = 0.83 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:12:14.859151: step 62790, loss = 0.74 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:12:15.819794: step 62800, loss = 0.69 (1332.4 examples/sec; 0.096 sec/batch)
2017-06-02 04:12:16.597720: step 62810, loss = 0.75 (1645.4 examples/sec; 0.078 sec/batch)
2017-06-02 04:12:17.475422: step 62820, loss = 0.76 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:12:18.322087: step 62830, loss = 0.81 (1511.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:12:19.164593: step 62840, loss = 0.77 (1519.3 examples/sec; 0.084 sec/batch)
2017-06-02 04:12:20.031368: step 62850, loss = 0.70 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:12:20.897498: step 62860, loss = 0.79 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:12:21.791698: step 62870, loss = 0.73 (1431.4 examples/sec; 0.089 sec/batch)
2017-06-02 04:12:22.636660: step 62880, loss = 0.76 (1514.9 examples/sec; 0.084 sec/batch)
2017-06-02 04:12:23.534397: step 62890, loss = 0.69 (1425.8 examples/sec; 0.090 sec/batch)
2017-06-02 04:12:24.545556: step 62900, loss = 0.69 (1265.8 examples/sec; 0.101 sec/batch)
2017-06-02 04:12:25.235078: step 62910, loss = 0.72 (1856.4 examples/sec; 0.069 sec/batch)
2017-06-02 04:12:26.075907: step 62920, loss = 0.64 (1522.3 examples/sec; 0.084 sec/batch)
2017-06-02 04:12:26.942301: step 62930, loss = 0.81 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:12:27.770217: step 62940, loss = 0.79 (1546.1 examples/sec; 0.083 sec/batch)
2017-06-02 04:12:28.633632: step 62950, loss = 0.59 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:12:29.489755: step 62960, loss = 0.70 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:12:30.361896: step 62970, loss = 0.74 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:12:31.198249: step 62980, loss = 0.75 (1530.4 examples/sec; 0.084 sec/batch)
2017-06-02 04:12:32.102197: step 62990, loss = 0.73 (1416.0 examples/sec; 0.090 sec/batch)
2017-06-02 04:12:33.073844: step 63000, loss = 0.82 (1317.4 examples/sec; 0.097 sec/batch)
2017-06-02 04:12:33.860235: step 63010, loss = 0.72 (1627.7 examples/sec; 0.079 sec/batch)
2017-06-02 04:12:34.731603: step 63020, loss = 0.66 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:12:35.581561: step 63030, loss = 0.70 (1506.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:12:36.453059: step 63040, loss = 0.82 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:12:37.297724: step 63050, loss = 0.75 (1515.4 examples/sec; 0.084 sec/batch)
2017-06-02 04:12:38.154796: step 63060, loss = 0.77 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:12:39.018145: step 63070, loss = 0.78 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:12:39.865189: step 63080, loss = 0.71 (1511.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:12:40.757011: step 63090, loss = 0.73 (1435.2 examples/sec; 0.089 sec/batch)
2017-06-02 04:12:41.752861: step 63100, loss = 0.74 (1285.3 examples/sec; 0.100 sec/batch)
2017-06-02 04:12:42.548796: step 63110, loss = 0.75 (1608.2 examples/sec; 0.080 sec/batch)
2017-06-02 04:12:43.454303: step 63120, loss = 0.78 (1413.6 examples/sec; 0.091 sec/batch)
2017-06-02 04:12:44.353762: step 63130, loss = 0.69 (1423.1 examples/sec; 0.090 sec/batch)
2017-06-02 04:12:45.224289: step 63140, loss = 0.75 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:12:46.113349: step 63150, loss = 0.70 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:12:46.984005: step 63160, loss = 0.77 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:12:47.852927: step 63170, loss = 0.70 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:12:48.728097: step 63180, loss = 0.68 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:12:49.614421: step 63190, loss = 0.74 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 04:12:50.593389: step 63200, loss = 0.85 (1307.5 examples/sec; 0.098 sec/batch)
2017-06-02 04:12:51.379830: step 63210, loss = 0.70 (1627.6 examples/sec; 0.079 sec/batch)
2017-06-02 04:12:52.254084: step 63220, loss = 0.78 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:12:53.116986: step 63230, loss = 0.63 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:12:54.019525: step 63240, loss = 0.78 (1418.2 examples/sec; 0.090 sec/batch)
2017-06-02 04:12:54.920232: step 63250, loss = 0.70 (1421.1 examples/sec; 0.090 sec/batch)
2017-06-02 04:12:55.786034: step 63260, loss = 0.75 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:12:56.653922: step 63270, loss = 0.68 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:12:57.528698: step 63280, loss = 0.82 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:12:58.380634: step 63290, loss = 0.73 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:12:59.333278: step 63300, loss = 0.69 (1343.6 examples/sec; 0.095 sec/batch)
2017-06-02 04:13:00.122847: step 63310, loss = 0.72 (1621.2 examples/sec; 0.079 sec/batch)
2017-06-02 04:13:01.003444: step 63320, loss = 0.71 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:13:01.880846: step 63330, loss = 0.74 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:13:02.784168: step 63340, loss = 0.73 (1417.0 examples/sec; 0.090 sec/batch)
2017-06-02 04:13:03.649553: step 63350, loss = 0.79 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:13:04.509927: step 63360, loss = 0.80 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:13:05.401329: step 63370, loss = 0.69 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:13:06.268018: step 63380, loss = 0.64 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:13:07.150037: step 63390, loss = 0.69 (1451.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:13:08.131451: step 63400, loss = 0.85 (1304.2 examples/sec; 0.098 sec/batch)
2017-06-02 04:13:08.929724: step 63410, loss = 0.67 (1603.5 examples/sec; 0.080 sec/batch)
2017-06-02 04:13:09.803277: step 63420, loss = 0.58 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:13:10.697159: step 63430, loss = 0.68 (1431.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:13:11.583830: step 63440, loss = 0.59 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:13:12.468971: step 63450, loss = 0.57 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:13:13.351520: step 63460, loss = 0.81 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:13:14.217063: step 63470, loss = 0.60 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:13:15.088454: step 63480, loss = 0.68 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:13:15.966627: step 63490, loss = 0.82 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:13:16.942738: step 63500, loss = 0.82 (1311.3 examples/sec; 0.098 sec/batch)
2017-06-02 04:13:17.744158: step 63510, loss = 0.54 (1597.1 examples/sec; 0.080 sec/batch)
2017-06-02 04:13:18.604947: step 63520, loss = 0.71 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:13:19.491104: step 63530, loss = 0.73 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 04:13:20.386643: step 63540, loss = 0.71 (1429.3 examples/sec; 0.090 sec/batch)
2017-06-02 04:13:21.255507: step 63550, loss = 0.82 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:13:22.140889: step 63560, loss = 0.64 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:13:23.015195: step 63570, loss = 0.77 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:13:23.870815: step 63580, loss = 0.74 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:13:24.746957: step 63590, loss = 0.75 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:13:25.734354: step 63600, loss = 0.77 (1296.3 examples/sec; 0.099 sec/batch)
2017-06-02 04:13:26.535653: step 63610, loss = 0.87 (1597.4 examples/sec; 0.080 sec/batch)
2017-06-02 04:13:27.449129: step 63620, loss = 0.71 (1401.2 examples/sec; 0.091 sec/batch)
2017-06-02 04:13:28.359762: step 63630, loss = 0.69 (1405.6 examples/sec; 0.091 sec/batch)
2017-06-02 04:13:29.245575: step 63640, loss = 0.86 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:13:30.113716: step 63650, loss = 0.75 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:13:31.007155: step 63660, loss = 0.62 (1432.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:13:31.888742: step 63670, loss = 0.72 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:13:32.762098: step 63680, loss = 0.69 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:13:33.633384: step 63690, loss = 0.76 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:13:34.650877: step 63700, loss = 0.72 (1258.0 examples/sec; 0.102 sec/batch)
2017-06-02 04:13:35.371144: step 63710, loss = 0.76 (1777.1 examples/sec; 0.072 sec/batch)
2017-06-02 04:13:36.240353: step 63720, loss = 0.88 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:13:37.106529: step 63730, loss = 0.64 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:13:37.979041: step 63740, loss = 0.66 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:13:38.885882: step 63750, loss = 0.89 (1411.5 examples/sec; 0.091 sec/batch)
2017-06-02 04:13:39.785171: step 63760, loss = 0.65 (1423.4 examples/sec; 0.090 sec/batch)
2017-06-02 04:13:40.658571: step 63770, loss = 0.62 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:13:41.534434: step 63780, loss = 0.77 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:13:42.403245: step 63790, loss = 0.86 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:13:43.382310: step 63800, loss = 0.76 (1307.4 examples/sec; 0.098 sec/batch)
2017-06-02 04:13:44.188095: step 63810, loss = 0.64 (1588.5 examples/sec; 0.081 sec/batch)
2017-06-02 04:13:45.070577: step 63820, loss = 0.68 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:13:45.979452: step 63830, loss = 0.88 (1408.3 examples/sec; 0.091 sec/batch)
2017-06-02 04:13:46.853058: step 63840, loss = 0.71 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:13:47.741174: step 63850, loss = 0.70 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:13:48.600829: step 63860, loss = 0.60 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:13:49.501120: step 63870, loss = 0.68 (1421.8 examples/sec; 0.090 sec/batch)
2017-06-02 04:13:50.382560: step 63880, loss = 0.66 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:13:51.294117: step 63890, loss = 0.72 (1404.2 examples/sec; 0.091 sec/batch)
2017-06-02 04:13:52.274055: step 63900, loss = 0.74 (1306.2 examples/sec; 0.098 sec/batch)
2017-06-02 04:13:53.022381: step 63910, loss = 0.78 (1710.5 examples/sec; 0.075 sec/batch)
2017-06-02 04:13:53.882705: step 63920, loss = 0.82 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:13:54.760232: step 63930, loss = 0.78 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:13:55.654142: step 63940, loss = 0.70 (1431.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:13:56.540601: step 63950, loss = 0.74 (1443.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:13:57.416572: step 63960, loss = 0.74 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:13:58.300776: step 63970, loss = 0.69 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:13:59.182114: step 63980, loss = 0.59 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:14:00.062865: step 63990, loss = 0.71 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:14:01.082580: step 64000, loss = 0.74 (1255.3 examples/sec; 0.102 sec/batch)
2017-06-02 04:14:01.860882: step 64010, loss = 0.81 (1644.6 examples/sec; 0.078 sec/batch)
2017-06-02 04:14:02.760620: step 64020, loss = 0.79 (1422.6 examples/sec; 0.090 sec/batch)
2017-06-02 04:14:03.652420: step 64030, loss = 0.67 (1435.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:14:04.552230: step 64040, loss = 0.75 (1422.5 examples/sec; 0.090 sec/batch)
2017-06-02 04:14:05.447270: step 64050, loss = 0.65 (1430.1 examples/sec; 0.090 sec/batch)
2017-06-02 04:14:06.324088: step 64060, loss = 0.78 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:14:07.195209: step 64070, loss = 0.79 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:14:08.074674: step 64080, loss = 0.67 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:14:08.936974: step 64090, loss = 0.88 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:14:09.966272: step 64100, loss = 0.92 (1243.6 examples/sec; 0.103 sec/batch)
2017-06-02 04:14:10.710141: step 64110, loss = 0.84 (1720.7 examples/sec; 0.074 sec/batch)
2017-06-02 04:14:11.593553: step 64120, loss = 0.75 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:14:12.469393: step 64130, loss = 0.75 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:14:13.355479: step 64140, loss = 0.88 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:14:14.257252: step 64150, loss = 0.71 (1419.4 examples/sec; 0.090 sec/batch)
2017-06-02 04:14:15.146464: step 64160, loss = 0.87 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:14:16.025101: step 64170, loss = 0.72 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:14:16.880648: step 64180, loss = 0.65 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:14:17.759004: step 64190, loss = 0.76 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:14:18.717919: step 64200, loss = 0.67 (1334.8 examples/sec; 0.096 sec/batch)
2017-06-02 04:14:19.487343: step 64210, loss = 0.63 (1663.6 examples/sec; 0.077 sec/batch)
2017-06-02 04:14:20.344305: step 64220, loss = 0.69 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:14:21.207917: step 64230, loss = 0.70 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:14:22.092934: step 64240, loss = 0.65 (1446.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:14:22.977683: step 64250, loss = 0.73 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:14:23.874913: step 64260, loss = 0.70 (1426.6 examples/sec; 0.090 sec/batch)
2017-06-02 04:14:24.755635: step 64270, loss = 0.87 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:14:25.646172: step 64280, loss = 0.60 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:14:26.494097: step 64290, loss = 0.84 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:14:27.516759: step 64300, loss = 0.72 (1251.6 examples/sec; 0.102 sec/batch)
2017-06-02 04:14:28.265302: step 64310, loss = 0.83 (1710.0 examples/sec; 0.075 sec/batch)
2017-06-02 04:14:29.141319: step 64320, loss = 0.66 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:14:29.992847: step 64330, loss = 0.78 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:14:30.878713: step 64340, loss = 0.65 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:14:31.729923: step 64350, loss = 0.70 (1503.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:14:32.579655: step 64360, loss = 0.77 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:14:33.450292: step 64370, loss = 0.61 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:14:34.310828: step 64380, loss = 0.77 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:14:35.196318: step 64390, loss = 0.79 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:14:36.149979: step 64400, loss = 0.79 (1342.2 examples/sec; 0.095 sec/batch)
2017-06-02 04:14:36.897687: step 64410, loss = 0.58 (1711.9 examples/sec; 0.075 sec/batch)
2017-06-02 04:14:37.746431: step 64420, loss = 0.79 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:14:38.625805: step 64430, loss = 0.66 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:14:39.533832: step 64440, loss = 0.73 (1409.7 examples/sec; 0.091 sec/batch)
2017-06-02 04:14:40.425359: step 64450, loss = 0.75 (1435.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:14:41.309306: step 64460, loss = 0.72 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:14:42.210835: step 64470, loss = 0.81 (1419.8 examples/sec; 0.090 sec/batch)
2017-06-02 04:14:43.080451: step 64480, loss = 0.75 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:14:43.973371: step 64490, loss = 0.81 (1433.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:14:44.959310: step 64500, loss = 0.76 (1298.2 examples/sec; 0.099 sec/batch)
2017-06-02 04:14:45.722285: step 64510, loss = 0.74 (1677.7 examples/sec; 0.076 sec/batch)
2017-06-02 04:14:46.559865: step 64520, loss = 0.76 (1528.2 examples/sec; 0.084 sec/batch)
2017-06-02 04:14:47.456640: step 64530, loss = 0.82 (1427.4 examples/sec; 0.090 sec/batch)
2017-06-02 04:14:48.321094: step 64540, loss = 0.78 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:14:49.192836: step 64550, loss = 0.61 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:14:50.064653: step 64560, loss = 0.60 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:14:50.896491: step 64570, loss = 0.73 (1538.8 examples/sec; 0.083 sec/batch)
2017-06-02 04:14:51.775560: step 64580, loss = 0.78 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:14:52.597813: step 64590, loss = 0.81 (1556.7 examples/sec; 0.082 sec/batch)
2017-06-02 04:14:53.553108: step 64600, loss = 0.63 (1339.9 examples/sec; 0.096 sec/batch)
2017-06-02 04:14:54.330205: step 64610, loss = 0.67 (1647.2 examples/sec; 0.078 sec/batch)
2017-06-02 04:14:55.210671: step 64620, loss = 0.69 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:14:56.083110: step 64630, loss = 0.87 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:14:56.946967: step 64640, loss = 0.76 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:14:57.844110: step 64650, loss = 0.65 (1426.7 examples/sec; 0.090 sec/batch)
2017-06-02 04:14:58.724524: step 64660, loss = 0.68 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:14:59.604621: step 64670, loss = 0.97 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:15:00.454943: step 64680, loss = 0.69 (1505.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:15:01.311043: step 64690, loss = 0.69 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:15:02.262682: step 64700, loss = 0.74 (1345.0 examples/sec; 0.095 sec/batch)
2017-06-02 04:15:03.048473: step 64710, loss = 0.85 (1628.9 examples/sec; 0.079 sec/batch)
2017-06-02 04:15:03.883843: step 64720, loss = 0.68 (1532.3 examples/sec; 0.084 sec/batch)
2017-06-02 04:15:04.755721: step 64730, loss = 0.69 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:15:05.640512: step 64740, loss = 0.78 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:15:06.508979: step 64750, loss = 0.80 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:15:07.389013: step 64760, loss = 0.73 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:15:08.274864: step 64770, loss = 0.73 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:15:09.143973: step 64780, loss = 0.70 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:15:10.027713: step 64790, loss = 0.80 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:15:11.000978: step 64800, loss = 0.77 (1315.2 examples/sec; 0.097 sec/batch)
2017-06-02 04:15:11.774529: step 64810, loss = 0.80 (1654.7 examples/sec; 0.077 sec/batch)
2017-06-02 04:15:12.628472: step 64820, loss = 0.67 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:15:13.458395: step 64830, loss = 0.63 (1542.3 examples/sec; 0.083 sec/batch)
2017-06-02 04:15:14.303790: step 64840, loss = 0.74 (1514.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:15:15.155686: step 64850, loss = 0.73 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:15:16.040107: step 64860, loss = 0.71 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:15:16.929860: step 64870, loss = 0.74 (1438.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:15:17.786410: step 64880, loss = 0.70 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:15:18.641883: step 64890, loss = 0.70 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:15:19.603180: step 64900, loss = 0.78 (1331.5 examples/sec; 0.096 sec/batch)
2017-06-02 04:15:20.403992: step 64910, loss = 0.66 (1598.4 examples/sec; 0.080 sec/batch)
2017-06-02 04:15:21.299856: step 64920, loss = 0.66 (1428.8 examples/sec; 0.090 sec/batch)
2017-06-02 04:15:22.173320: step 64930, loss = 0.67 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:15:23.063878: step 64940, loss = 0.87 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:15:23.955190: step 64950, loss = 0.74 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:15:24.855539: step 64960, loss = 0.68 (1421.7 examples/sec; 0.090 sec/batch)
2017-06-02 04:15:25.744994: step 64970, loss = 0.73 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:15:26.605191: step 64980, loss = 0.86 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:15:27.478266: step 64990, loss = 0.71 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:15:28.483933: step 65000, loss = 0.55 (1272.8 examples/sec; 0.101 sec/batch)
2017-06-02 04:15:29.282878: step 65010, loss = 0.89 (1602.2 examples/sec; 0.080 sec/batch)
2017-06-02 04:15:30.164091: step 65020, loss = 0.79 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:15:30.971920: step 65030, loss = 0.83 (1584.5 examples/sec; 0.081 sec/batch)
2017-06-02 04:15:31.774388: step 65040, loss = 0.60 (1595.1 examples/sec; 0.080 sec/batch)
2017-06-02 04:15:32.664311: step 65050, loss = 0.61 (1438.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:15:33.542802: step 65060, loss = 0.68 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:15:34.409707: step 65070, loss = 0.68 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:15:35.278716: step 65080, loss = 0.62 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:15:36.159062: step 65090, loss = 0.68 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:15:37.141370: step 65100, loss = 0.69 (1303.1 examples/sec; 0.098 sec/batch)
2017-06-02 04:15:37.935525: step 65110, loss = 0.80 (1611.8 examples/sec; 0.079 sec/batch)
2017-06-02 04:15:38.779614: step 65120, loss = 0.84 (1516.4 examples/sec; 0.084 sec/batch)
2017-06-02 04:15:39.683694: step 65130, loss = 0.63 (1415.8 examples/sec; 0.090 sec/batch)
2017-06-02 04:15:40.566715: step 65140, loss = 0.68 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:15:41.406644: step 65150, loss = 0.75 (1523.9 examples/sec; 0.084 sec/batch)
2017-06-02 04:15:42.284234: step 65160, loss = 0.86 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:15:43.182895: step 65170, loss = 0.90 (1424.3 examples/sec; 0.090 sec/batch)
2017-06-02 04:15:44.031636: step 65180, loss = 0.66 (1508.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:15:44.927325: step 65190, loss = 0.66 (1429.1 examples/sec; 0.090 sec/batch)
2017-06-02 04:15:45.924933: step 65200, loss = 0.77 (1283.0 examples/sec; 0.100 sec/batch)
2017-06-02 04:15:46.722241: step 65210, loss = 0.61 (1605.4 examples/sec; 0.080 sec/batch)
2017-06-02 04:15:47.616421: step 65220, loss = 0.68 (1431.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:15:48.521583: step 65230, loss = 0.56 (1414.1 examples/sec; 0.091 sec/batch)
2017-06-02 04:15:49.393874: step 65240, loss = 0.73 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:15:50.248181: step 65250, loss = 0.61 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:15:51.078112: step 65260, loss = 0.76 (1542.3 examples/sec; 0.083 sec/batch)
2017-06-02 04:15:51.919526: step 65270, loss = 0.82 (1521.2 examples/sec; 0.084 sec/batch)
2017-06-02 04:15:52.788026: step 65280, loss = 0.71 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:15:53.660687: step 65290, loss = 0.85 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:15:54.627395: step 65300, loss = 0.78 (1324.1 examples/sec; 0.097 sec/batch)
2017-06-02 04:15:55.427104: step 65310, loss = 0.83 (1600.6 examples/sec; 0.080 sec/batch)
2017-06-02 04:15:56.322389: step 65320, loss = 0.66 (1429.7 examples/sec; 0.090 sec/batch)
2017-06-02 04:15:57.190918: step 65330, loss = 0.71 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:15:58.074949: step 65340, loss = 0.88 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:15:58.942424: step 65350, loss = 0.81 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:15:59.817489: step 65360, loss = 0.71 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:16:00.699186: step 65370, loss = 0.76 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:16:01.574932: step 65380, loss = 0.74 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:16:02.454120: step 65390, loss = 0.76 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:16:03.484728: step 65400, loss = 0.72 (1242.0 examples/sec; 0.103 sec/batch)
2017-06-02 04:16:04.218760: step 65410, loss = 0.69 (1743.9 examples/sec; 0.073 sec/batch)
2017-06-02 04:16:05.124380: step 65420, loss = 0.74 (1413.4 examples/sec; 0.091 sec/batch)
2017-06-02 04:16:06.017443: step 65430, loss = 0.75 (1433.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:16:06.900195: step 65440, loss = 0.63 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:16:07.785989: step 65450, loss = 0.91 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:16:08.674348: step 65460, loss = 0.89 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:16:09.557706: step 65470, loss = 0.90 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:16:10.457108: step 65480, loss = 0.66 (1423.1 examples/sec; 0.090 sec/batch)
2017-06-02 04:16:11.349025: step 65490, loss = 0.79 (1435.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:16:12.338279: step 65500, loss = 0.70 (1293.9 examples/sec; 0.099 sec/batch)
2017-06-02 04:16:13.100708: step 65510, loss = 0.68 (1678.8 examples/sec; 0.076 sec/batch)
2017-06-02 04:16:13.968433: step 65520, loss = 0.81 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:16:14.824315: step 65530, loss = 0.75 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:16:15.695389: step 65540, loss = 0.59 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:16:16.580374: step 65550, loss = 0.70 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:16:17.442915: step 65560, loss = 0.74 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:16:18.320106: step 65570, loss = 0.72 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:16:19.197860: step 65580, loss = 0.73 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:16:20.074750: step 65590, loss = 0.83 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:16:21.065897: step 65600, loss = 0.75 (1291.4 examples/sec; 0.099 sec/batch)
2017-06-02 04:16:21.879071: step 65610, loss = 0.67 (1574.1 examples/sec; 0.081 sec/batch)
2017-06-02 04:16:22.764311: step 65620, loss = 0.78 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:16:23.641867: step 65630, loss = 0.60 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:16:24.530650: step 65640, loss = 0.71 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 04:16:25.431059: step 65650, loss = 0.90 (1421.6 examples/sec; 0.090 sec/batch)
2017-06-02 04:16:26.327408: step 65660, loss = 0.68 (1428.0 examples/sec; 0.090 sec/batch)
2017-06-02 04:16:27.217120: step 65670, loss = 0.65 (1438.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:16:28.080715: step 65680, loss = 0.73 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:16:28.957660: step 65690, loss = 0.64 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:16:29.948914: step 65700, loss = 0.79 (1291.3 examples/sec; 0.099 sec/batch)
2017-06-02 04:16:30.710410: step 65710, loss = 0.87 (1680.9 examples/sec; 0.076 sec/batch)
2017-06-02 04:16:31.565226: step 65720, loss = 0.70 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:16:32.434991: step 65730, loss = 0.87 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:16:33.310889: step 65740, loss = 0.90 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:16:34.173369: step 65750, loss = 0.79 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:16:35.051724: step 65760, loss = 0.73 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:16:35.913408: step 65770, loss = 0.66 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:16:36.761350: step 65780, loss = 0.78 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:16:37.609842: step 65790, loss = 0.72 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:16:38.575456: step 65800, loss = 0.82 (1325.6 examples/sec; 0.097 sec/batch)
2017-06-02 04:16:39.331897: step 65810, loss = 0.75 (1692.1 examples/sec; 0.076 sec/batch)
2017-06-02 04:16:40.195176: step 65820, loss = 0.69 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:16:41.063984: step 65830, loss = 0.78 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:16:41.924268: step 65840, loss = 0.68 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:16:42.811493: step 65850, loss = 0.76 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:16:43.692928: step 65860, loss = 0.81 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:16:44.578410: step 65870, loss = 0.62 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:16:45.439826: step 65880, loss = 0.73 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:16:46.302689: step 65890, loss = 0.73 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:16:47.259027: step 65900, loss = 0.79 (1338.4 examples/sec; 0.096 sec/batch)
2017-06-02 04:16:48.059456: step 65910, loss = 0.79 (1599.1 examples/sec; 0.080 sec/batch)
2017-06-02 04:16:48.923449: step 65920, loss = 0.66 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:16:49.795478: step 65930, loss = 0.68 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:16:50.661942: step 65940, loss = 0.74 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:16:51.516305: step 65950, loss = 0.84 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:16:52.401380: step 65960, loss = 0.70 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 04:16:53.293128: step 65970, loss = 0.62 (1435.4 examples/sec; 0.089 sec/batch)
2017-06-02 04:16:54.168110: step 65980, loss = 0.82 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:16:55.031328: step 65990, loss = 0.75 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:16:56.004525: step 66000, loss = 0.79 (1315.2 examples/sec; 0.097 sec/batch)
2017-06-02 04:16:56.814798: step 66010, loss = 0.66 (1579.7 examples/sec; 0.081 sec/batch)
2017-06-02 04:16:57.690834: step 66020, loss = 0.65 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:16:58.560616: step 66030, loss = 0.58 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:16:59.457288: step 66040, loss = 0.68 (1427.5 examples/sec; 0.090 sec/batch)
2017-06-02 04:17:00.334058: step 66050, loss = 0.65 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:17:01.210930: step 66060, loss = 0.87 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:17:02.102369: step 66070, loss = 0.77 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:17:02.997352: step 66080, loss = 0.75 (1430.2 examples/sec; 0.089 sec/batch)
2017-06-02 04:17:03.911397: step 66090, loss = 0.71 (1400.4 examples/sec; 0.091 sec/batch)
2017-06-02 04:17:04.890160: step 66100, loss = 0.76 (1307.7 examples/sec; 0.098 sec/batch)
2017-06-02 04:17:05.674416: step 66110, loss = 0.65 (1632.1 examples/sec; 0.078 sec/batch)
2017-06-02 04:17:06.546029: step 66120, loss = 0.74 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:17:07.409963: step 66130, loss = 0.49 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:17:08.262409: step 66140, loss = 0.66 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:17:09.095212: step 66150, loss = 0.80 (1537.0 examples/sec; 0.083 sec/batch)
2017-06-02 04:17:09.941960: step 66160, loss = 0.61 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:17:10.813638: step 66170, loss = 0.57 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:17:11.691527: step 66180, loss = 0.68 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:17:12.568840: step 66190, loss = 0.72 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:17:13.539434: step 66200, loss = 0.77 (1318.8 examples/sec; 0.097 sec/batch)
2017-06-02 04:17:14.336250: step 66210, loss = 0.83 (1606.4 examples/sec; 0.080 sec/batch)
2017-06-02 04:17:15.194026: step 66220, loss = 0.87 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:17:16.073770: step 66230, loss = 0.68 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:17:16.952087: step 66240, loss = 0.67 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:17:17.816952: step 66250, loss = 0.75 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:17:18.685620: step 66260, loss = 0.79 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:17:19.574621: step 66270, loss = 0.62 (1439.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:17:20.455118: step 66280, loss = 0.79 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:17:21.326667: step 66290, loss = 0.68 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:17:22.275852: step 66300, loss = 0.74 (1348.5 examples/sec; 0.095 sec/batch)
2017-06-02 04:17:23.050155: step 66310, loss = 0.84 (1653.1 examples/sec; 0.077 sec/batch)
2017-06-02 04:17:23.949279: step 66320, loss = 0.64 (1423.6 examples/sec; 0.090 sec/batch)
2017-06-02 04:17:24.834248: step 66330, loss = 0.74 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:17:25.700105: step 66340, loss = 0.75 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:17:26.551459: step 66350, loss = 0.67 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:17:27.452968: step 66360, loss = 0.83 (1419.8 examples/sec; 0.090 sec/batch)
2017-06-02 04:17:28.306645: step 66370, loss = 0.60 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:17:29.170211: step 66380, loss = 0.70 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:17:30.024719: step 66390, loss = 0.75 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:17:30.970645: step 66400, loss = 0.76 (1353.2 examples/sec; 0.095 sec/batch)
2017-06-02 04:17:31.751013: step 66410, loss = 0.73 (1640.2 examples/sec; 0.078 sec/batch)
2017-06-02 04:17:32.626906: step 66420, loss = 0.74 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:17:33.525042: step 66430, loss = 0.76 (1425.2 examples/sec; 0.090 sec/batch)
2017-06-02 04:17:34.419775: step 66440, loss = 0.60 (1430.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:17:35.265443: step 66450, loss = 0.72 (1513.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:17:36.159673: step 66460, loss = 0.58 (1431.4 examples/sec; 0.089 sec/batch)
2017-06-02 04:17:37.053801: step 66470, loss = 0.73 (1431.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:17:37.943074: step 66480, loss = 0.82 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 04:17:38.824516: step 66490, loss = 0.61 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:17:39.805906: step 66500, loss = 0.62 (1304.3 examples/sec; 0.098 sec/batch)
2017-06-02 04:17:40.611383: step 66510, loss = 0.55 (1589.1 examples/sec; 0.081 sec/batch)
2017-06-02 04:17:41.481173: step 66520, loss = 0.76 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:17:42.372515: step 66530, loss = 0.59 (1436.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:17:43.247459: step 66540, loss = 0.64 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:17:44.137966: step 66550, loss = 0.81 (1437.4 examples/sec; 0.089 sec/batch)
2017-06-02 04:17:45.037146: step 66560, loss = 0.62 (1423.5 examples/sec; 0.090 sec/batch)
2017-06-02 04:17:45.944146: step 66570, loss = 0.60 (1411.2 examples/sec; 0.091 sec/batch)
2017-06-02 04:17:46.845085: step 66580, loss = 0.66 (1420.7 examples/sec; 0.090 sec/batch)
2017-06-02 04:17:47.746617: step 66590, loss = 0.82 (1419.8 examples/sec; 0.090 sec/batch)
2017-06-02 04:17:48.724348: step 66600, loss = 0.82 (1309.1 examples/sec; 0.098 sec/batch)
2017-06-02 04:17:49.500500: step 66610, loss = 0.66 (1649.2 examples/sec; 0.078 sec/batch)
2017-06-02 04:17:50.372336: step 66620, loss = 0.89 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:17:51.263686: step 66630, loss = 0.56 (1436.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:17:52.125386: step 66640, loss = 0.64 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:17:52.989221: step 66650, loss = 0.94 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:17:53.848333: step 66660, loss = 0.53 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:17:54.694197: step 66670, loss = 0.57 (1513.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:17:55.562876: step 66680, loss = 0.70 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:17:56.451967: step 66690, loss = 0.67 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:17:57.422773: step 66700, loss = 0.75 (1318.5 examples/sec; 0.097 sec/batch)
2017-06-02 04:17:58.208107: step 66710, loss = 0.77 (1629.9 examples/sec; 0.079 sec/batch)
2017-06-02 04:17:59.096431: step 66720, loss = 0.68 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:17:59.966870: step 66730, loss = 0.87 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:18:00.840154: step 66740, loss = 0.78 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:18:01.712943: step 66750, loss = 0.85 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:18:02.553345: step 66760, loss = 0.67 (1523.1 examples/sec; 0.084 sec/batch)
2017-06-02 04:18:03.391254: step 66770, loss = 0.61 (1527.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:18:04.230307: step 66780, loss = 0.65 (1525.5 examples/sec; 0.084 sec/batch)
2017-06-02 04:18:05.102888: step 66790, loss = 0.82 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:18:06.112886: step 66800, loss = 0.71 (1267.3 examples/sec; 0.101 sec/batch)
2017-06-02 04:18:06.839264: step 66810, loss = 0.84 (1762.2 examples/sec; 0.073 sec/batch)
2017-06-02 04:18:07.702773: step 66820, loss = 0.69 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:18:08.606045: step 66830, loss = 0.63 (1417.1 examples/sec; 0.090 sec/batch)
2017-06-02 04:18:09.461308: step 66840, loss = 0.78 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:18:10.332052: step 66850, loss = 0.75 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:18:11.184240: step 66860, loss = 0.69 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:18:12.012835: step 66870, loss = 0.73 (1544.8 examples/sec; 0.083 sec/batch)
2017-06-02 04:18:12.856990: step 66880, loss = 0.73 (1516.3 examples/sec; 0.084 sec/batch)
2017-06-02 04:18:13.736770: step 66890, loss = 0.69 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:18:14.724199: step 66900, loss = 0.86 (1296.3 examples/sec; 0.099 sec/batch)
2017-06-02 04:18:15.505830: step 66910, loss = 0.66 (1637.6 examples/sec; 0.078 sec/batch)
2017-06-02 04:18:16.358440: step 66920, loss = 0.60 (1501.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:18:17.233173: step 66930, loss = 0.72 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:18:18.075975: step 66940, loss = 0.83 (1518.7 examples/sec; 0.084 sec/batch)
2017-06-02 04:18:18.965943: step 66950, loss = 0.78 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 04:18:19.856374: step 66960, loss = 0.65 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:18:20.737728: step 66970, loss = 0.81 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:18:21.613331: step 66980, loss = 0.71 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:18:22.484337: step 66990, loss = 0.76 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:18:23.488262: step 67000, loss = 0.71 (1275.0 examples/sec; 0.100 sec/batch)
2017-06-02 04:18:24.252389: step 67010, loss = 0.75 (1675.1 examples/sec; 0.076 sec/batch)
2017-06-02 04:18:25.140041: step 67020, loss = 0.58 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:18:26.009153: step 67030, loss = 0.61 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:18:26.879223: step 67040, loss = 0.72 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:18:27.769196: step 67050, loss = 0.61 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 04:18:28.638318: step 67060, loss = 0.57 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:18:29.531096: step 67070, loss = 0.80 (1433.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:18:30.386284: step 67080, loss = 0.66 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:18:31.269885: step 67090, loss = 0.71 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:18:32.229038: step 67100, loss = 0.59 (1334.5 examples/sec; 0.096 sec/batch)
2017-06-02 04:18:33.011916: step 67110, loss = 0.72 (1635.0 examples/sec; 0.078 sec/batch)
2017-06-02 04:18:33.918479: step 67120, loss = 0.74 (1411.9 examples/sec; 0.091 sec/batch)
2017-06-02 04:18:34.798485: step 67130, loss = 0.59 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:18:35.679387: step 67140, loss = 0.73 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:18:36.568882: step 67150, loss = 0.72 (1439.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:18:37.433593: step 67160, loss = 0.78 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:18:38.316308: step 67170, loss = 0.77 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:18:39.181632: step 67180, loss = 0.64 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:18:40.050589: step 67190, loss = 0.59 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:18:41.014813: step 67200, loss = 0.66 (1327.5 examples/sec; 0.096 sec/batch)
2017-06-02 04:18:41.786100: step 67210, loss = 0.64 (1659.6 examples/sec; 0.077 sec/batch)
2017-06-02 04:18:42.673523: step 67220, loss = 0.70 (1442.4 examples/sec; 0.089 sec/batch)
2017-06-02 04:18:43.533767: step 67230, loss = 0.73 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:18:44.420573: step 67240, loss = 0.65 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 04:18:45.290504: step 67250, loss = 0.68 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:18:46.186145: step 67260, loss = 0.70 (1429.1 examples/sec; 0.090 sec/batch)
2017-06-02 04:18:47.067779: step 67270, loss = 0.74 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:18:47.941642: step 67280, loss = 0.69 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:18:48.801963: step 67290, loss = 0.96 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:18:49.784708: step 67300, loss = 0.66 (1302.5 examples/sec; 0.098 sec/batch)
2017-06-02 04:18:50.547694: step 67310, loss = 0.73 (1677.6 examples/sec; 0.076 sec/batch)
2017-06-02 04:18:51.413442: step 67320, loss = 0.71 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:18:52.270025: step 67330, loss = 0.70 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:18:53.144408: step 67340, loss = 0.67 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:18:53.999369: step 67350, loss = 0.61 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:18:54.847376: step 67360, loss = 0.75 (1509.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:18:55.728098: step 67370, loss = 0.91 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:18:56.589583: step 67380, loss = 0.67 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:18:57.420710: step 67390, loss = 0.75 (1540.1 examples/sec; 0.083 sec/batch)
2017-06-02 04:18:58.384193: step 67400, loss = 0.74 (1328.5 examples/sec; 0.096 sec/batch)
2017-06-02 04:18:59.181254: step 67410, loss = 0.87 (1605.9 examples/sec; 0.080 sec/batch)
2017-06-02 04:19:00.067287: step 67420, loss = 0.81 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:19:00.951917: step 67430, loss = 0.89 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:19:01.857124: step 67440, loss = 0.76 (1414.0 examples/sec; 0.091 sec/batch)
2017-06-02 04:19:02.748361: step 67450, loss = 0.69 (1436.2 examples/sec; 0.089 sec/batch)
2017-06-02 04:19:03.630624: step 67460, loss = 0.71 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:19:04.510536: step 67470, loss = 0.78 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:19:05.389863: step 67480, loss = 0.82 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:19:06.278138: step 67490, loss = 0.69 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:19:07.258933: step 67500, loss = 0.69 (1305.1 examples/sec; 0.098 sec/batch)
2017-06-02 04:19:08.037006: step 67510, loss = 0.69 (1645.1 examples/sec; 0.078 sec/batch)
2017-06-02 04:19:08.901390: step 67520, loss = 0.72 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:19:09.789319: step 67530, loss = 0.64 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:19:10.656875: step 67540, loss = 0.89 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:19:11.544556: step 67550, loss = 0.68 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:19:12.437196: step 67560, loss = 0.77 (1433.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:19:13.308854: step 67570, loss = 0.72 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:19:14.190722: step 67580, loss = 0.68 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:19:15.060295: step 67590, loss = 0.68 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:19:16.043061: step 67600, loss = 0.75 (1302.4 examples/sec; 0.098 sec/batch)
2017-06-02 04:19:16.814033: step 67610, loss = 0.74 (1660.2 examples/sec; 0.077 sec/batch)
2017-06-02 04:19:17.696773: step 67620, loss = 0.72 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:19:18.561793: step 67630, loss = 0.58 (1479.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:19:19.419581: step 67640, loss = 0.81 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:19:20.291674: step 67650, loss = 0.64 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:19:21.127606: step 67660, loss = 0.71 (1531.2 examples/sec; 0.084 sec/batch)
2017-06-02 04:19:22.003369: step 67670, loss = 0.72 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:19:22.889749: step 67680, loss = 0.67 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:19:23.748076: step 67690, loss = 0.84 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:19:24.727771: step 67700, loss = 0.72 (1306.5 examples/sec; 0.098 sec/batch)
2017-06-02 04:19:25.497569: step 67710, loss = 0.67 (1662.8 examples/sec; 0.077 sec/batch)
2017-06-02 04:19:26.367444: step 67720, loss = 0.78 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:19:27.251394: step 67730, loss = 0.75 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:19:28.112363: step 67740, loss = 0.64 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:19:28.977280: step 67750, loss = 0.67 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:19:29.848824: step 67760, loss = 0.71 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:19:30.724135: step 67770, loss = 0.81 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:19:31.579047: step 67780, loss = 0.88 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:19:32.435468: step 67790, loss = 0.69 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:19:33.462802: step 67800, loss = 0.70 (1245.9 examples/sec; 0.103 sec/batch)
2017-06-02 04:19:34.207741: step 67810, loss = 0.62 (1718.3 examples/sec; 0.074 sec/batch)
2017-06-02 04:19:35.088248: step 67820, loss = 0.70 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:19:35.953574: step 67830, loss = 0.69 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:19:36.849005: step 67840, loss = 0.67 (1429.5 examples/sec; 0.090 sec/batch)
2017-06-02 04:19:37.728388: step 67850, loss = 0.75 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:19:38.613418: step 67860, loss = 0.60 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:19:39.504015: step 67870, loss = 0.68 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 04:19:40.393460: step 67880, loss = 0.73 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:19:41.273212: step 67890, loss = 0.77 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:19:42.249444: step 67900, loss = 0.70 (1311.1 examples/sec; 0.098 sec/batch)
2017-06-02 04:19:43.007036: step 67910, loss = 0.67 (1689.6 examples/sec; 0.076 sec/batch)
2017-06-02 04:19:43.866122: step 67920, loss = 0.83 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:19:44.717758: step 67930, loss = 0.74 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:19:45.601283: step 67940, loss = 0.70 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:19:46.437524: step 67950, loss = 0.67 (1530.7 examples/sec; 0.084 sec/batch)
2017-06-02 04:19:47.306518: step 67960, loss = 0.76 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:19:48.198882: step 67970, loss = 0.79 (1434.4 examples/sec; 0.089 sec/batch)
2017-06-02 04:19:49.082055: step 67980, loss = 0.80 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:19:49.966887: step 67990, loss = 0.71 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 04:19:50.954450: step 68000, loss = 0.73 (1296.1 examples/sec; 0.099 sec/batch)
2017-06-02 04:19:51.773967: step 68010, loss = 0.69 (1561.9 examples/sec; 0.082 sec/batch)
2017-06-02 04:19:52.617495: step 68020, loss = 0.76 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 04:19:53.499784: step 68030, loss = 0.86 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:19:54.349414: step 68040, loss = 0.67 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:19:55.225397: step 68050, loss = 0.73 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:19:56.082603: step 68060, loss = 0.79 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:19:56.945493: step 68070, loss = 0.68 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:19:57.825354: step 68080, loss = 0.82 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:19:58.686694: step 68090, loss = 0.70 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:19:59.625655: step 68100, loss = 0.80 (1363.2 examples/sec; 0.094 sec/batch)
2017-06-02 04:20:00.391488: step 68110, loss = 0.90 (1671.4 examples/sec; 0.077 sec/batch)
2017-06-02 04:20:01.239457: step 68120, loss = 0.66 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:20:02.107670: step 68130, loss = 0.66 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:20:02.963054: step 68140, loss = 0.72 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:20:03.844085: step 68150, loss = 0.76 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:20:04.719523: step 68160, loss = 0.67 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:20:05.558816: step 68170, loss = 0.97 (1525.1 examples/sec; 0.084 sec/batch)
2017-06-02 04:20:06.440441: step 68180, loss = 0.70 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:20:07.300803: step 68190, loss = 0.67 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:20:08.215708: step 68200, loss = 0.80 (1399.0 examples/sec; 0.091 sec/batch)
2017-06-02 04:20:08.984785: step 68210, loss = 0.74 (1664.4 examples/sec; 0.077 sec/batch)
2017-06-02 04:20:09.850761: step 68220, loss = 0.72 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:20:10.724200: step 68230, loss = 0.73 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:20:11.571218: step 68240, loss = 0.72 (1511.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:20:12.422850: step 68250, loss = 0.74 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:20:13.287848: step 68260, loss = 0.74 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:20:14.151766: step 68270, loss = 0.79 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:20:15.018374: step 68280, loss = 0.72 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:20:15.862396: step 68290, loss = 0.60 (1516.5 examples/sec; 0.084 sec/batch)
2017-06-02 04:20:16.829349: step 68300, loss = 0.64 (1323.7 examples/sec; 0.097 sec/batch)
2017-06-02 04:20:17.627873: step 68310, loss = 0.81 (1603.0 examples/sec; 0.080 sec/batch)
2017-06-02 04:20:18.502534: step 68320, loss = 0.75 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:20:19.354235: step 68330, loss = 0.64 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:20:20.225282: step 68340, loss = 0.79 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:20:21.064190: step 68350, loss = 0.80 (1525.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:20:21.933109: step 68360, loss = 0.64 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:20:22.800400: step 68370, loss = 0.65 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:20:23.673988: step 68380, loss = 0.85 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:20:24.536562: step 68390, loss = 0.55 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:20:25.486131: step 68400, loss = 0.78 (1348.0 examples/sec; 0.095 sec/batch)
2017-06-02 04:20:26.255770: step 68410, loss = 0.60 (1663.1 examples/sec; 0.077 sec/batch)
2017-06-02 04:20:27.130306: step 68420, loss = 0.80 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:20:27.986858: step 68430, loss = 0.58 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:20:28.851224: step 68440, loss = 0.75 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:20:29.715969: step 68450, loss = 0.70 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:20:30.584785: step 68460, loss = 0.69 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:20:31.460772: step 68470, loss = 0.80 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:20:32.327999: step 68480, loss = 0.66 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:20:33.201751: step 68490, loss = 0.73 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:20:34.219549: step 68500, loss = 0.63 (1257.6 examples/sec; 0.102 sec/batch)
2017-06-02 04:20:34.923873: step 68510, loss = 0.76 (1817.4 examples/sec; 0.070 sec/batch)
2017-06-02 04:20:35.771853: step 68520, loss = 0.74 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:20:36.640653: step 68530, loss = 0.85 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:20:37.505807: step 68540, loss = 0.69 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:20:38.366631: step 68550, loss = 0.74 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:20:39.242359: step 68560, loss = 0.71 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:20:40.121883: step 68570, loss = 0.78 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:20:40.974997: step 68580, loss = 0.74 (1500.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:20:41.848911: step 68590, loss = 0.54 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:20:42.852010: step 68600, loss = 0.76 (1276.1 examples/sec; 0.100 sec/batch)
2017-06-02 04:20:43.610873: step 68610, loss = 0.62 (1686.7 examples/sec; 0.076 sec/batch)
2017-06-02 04:20:44.474149: step 68620, loss = 0.64 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:20:45.329466: step 68630, loss = 0.61 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:20:46.177100: step 68640, loss = 0.75 (1510.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:20:47.063692: step 68650, loss = 0.66 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:20:47.908110: step 68660, loss = 0.77 (1515.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:20:48.736411: step 68670, loss = 0.81 (1545.3 examples/sec; 0.083 sec/batch)
2017-06-02 04:20:49.602552: step 68680, loss = 0.71 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:20:50.461992: step 68690, loss = 0.74 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:20:51.430416: step 68700, loss = 0.70 (1321.7 examples/sec; 0.097 sec/batch)
2017-06-02 04:20:52.220879: step 68710, loss = 0.64 (1619.3 examples/sec; 0.079 sec/batch)
2017-06-02 04:20:53.080078: step 68720, loss = 0.61 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:20:53.953280: step 68730, loss = 0.69 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:20:54.837246: step 68740, loss = 0.81 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:20:55.698430: step 68750, loss = 0.63 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:20:56.554063: step 68760, loss = 0.79 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:20:57.404528: step 68770, loss = 0.74 (1505.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:20:58.267920: step 68780, loss = 0.68 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:20:59.117611: step 68790, loss = 0.81 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:21:00.115226: step 68800, loss = 0.80 (1283.1 examples/sec; 0.100 sec/batch)
2017-06-02 04:21:00.834854: step 68810, loss = 0.70 (1778.7 examples/sec; 0.072 sec/batch)
2017-06-02 04:21:01.697040: step 68820, loss = 0.87 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:21:02.571210: step 68830, loss = 0.60 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:21:03.410479: step 68840, loss = 0.70 (1525.2 examples/sec; 0.084 sec/batch)
2017-06-02 04:21:04.292944: step 68850, loss = 0.70 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:21:05.151499: step 68860, loss = 0.72 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:21:06.001983: step 68870, loss = 0.90 (1505.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:21:06.824206: step 68880, loss = 0.76 (1556.8 examples/sec; 0.082 sec/batch)
2017-06-02 04:21:07.675077: step 68890, loss = 0.70 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:21:08.647947: step 68900, loss = 0.74 (1315.7 examples/sec; 0.097 sec/batch)
2017-06-02 04:21:09.415767: step 68910, loss = 0.61 (1667.1 examples/sec; 0.077 sec/batch)
2017-06-02 04:21:10.273216: step 68920, loss = 0.87 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:21:11.153621: step 68930, loss = 0.77 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:21:12.019838: step 68940, loss = 0.72 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:21:12.874408: step 68950, loss = 0.65 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:21:13.760151: step 68960, loss = 0.85 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:21:14.614984: step 68970, loss = 0.75 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:21:15.478203: step 68980, loss = 0.68 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:21:16.355559: step 68990, loss = 0.81 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:21:17.338897: step 69000, loss = 0.57 (1301.7 examples/sec; 0.098 sec/batch)
2017-06-02 04:21:18.101442: step 69010, loss = 0.85 (1678.6 examples/sec; 0.076 sec/batch)
2017-06-02 04:21:18.952363: step 69020, loss = 0.75 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:21:19.826958: step 69030, loss = 0.61 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:21:20.699146: step 69040, loss = 0.75 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:21:21.589397: step 69050, loss = 0.74 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:21:22.442258: step 69060, loss = 0.91 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:21:23.304006: step 69070, loss = 0.78 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:21:24.161655: step 69080, loss = 0.58 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:21:25.038597: step 69090, loss = 0.74 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:21:26.039485: step 69100, loss = 0.65 (1278.8 examples/sec; 0.100 sec/batch)
2017-06-02 04:21:26.778697: step 69110, loss = 0.68 (1731.6 examples/sec; 0.074 sec/batch)
2017-06-02 04:21:27.632462: step 69120, loss = 0.79 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:21:28.481577: step 69130, loss = 0.73 (1507.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:21:29.349185: step 69140, loss = 0.78 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:21:30.227794: step 69150, loss = 0.65 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:21:31.077308: step 69160, loss = 0.66 (1506.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:21:31.939899: step 69170, loss = 0.70 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:21:32.820415: step 69180, loss = 0.76 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:21:33.692271: step 69190, loss = 0.72 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:21:34.701806: step 69200, loss = 0.80 (1267.9 examples/sec; 0.101 sec/batch)
2017-06-02 04:21:35.468233: step 69210, loss = 0.65 (1670.1 examples/sec; 0.077 sec/batch)
2017-06-02 04:21:36.327609: step 69220, loss = 0.65 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:21:37.222004: step 69230, loss = 0.61 (1431.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:21:38.092324: step 69240, loss = 0.72 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:21:38.955091: step 69250, loss = 0.66 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:21:39.842188: step 69260, loss = 0.64 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:21:40.711427: step 69270, loss = 0.54 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:21:41.580125: step 69280, loss = 0.75 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:21:42.449788: step 69290, loss = 0.67 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:21:43.417719: step 69300, loss = 0.60 (1322.4 examples/sec; 0.097 sec/batch)
2017-06-02 04:21:44.192455: step 69310, loss = 0.65 (1652.2 examples/sec; 0.077 sec/batch)
2017-06-02 04:21:45.048371: step 69320, loss = 0.70 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:21:45.908786: step 69330, loss = 0.81 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:21:46.770368: step 69340, loss = 0.71 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:21:47.663556: step 69350, loss = 0.68 (1433.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:21:48.537006: step 69360, loss = 0.74 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:21:49.405577: step 69370, loss = 0.80 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:21:50.275090: step 69380, loss = 0.91 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:21:51.135605: step 69390, loss = 0.79 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:21:52.079204: step 69400, loss = 0.72 (1356.5 examples/sec; 0.094 sec/batch)
2017-06-02 04:21:52.842571: step 69410, loss = 0.76 (1676.8 examples/sec; 0.076 sec/batch)
2017-06-02 04:21:53.690662: step 69420, loss = 0.76 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:21:54.557100: step 69430, loss = 0.71 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:21:55.434537: step 69440, loss = 0.61 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:21:56.313052: step 69450, loss = 0.77 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:21:57.172621: step 69460, loss = 0.68 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:21:58.016447: step 69470, loss = 0.66 (1516.9 examples/sec; 0.084 sec/batch)
2017-06-02 04:21:58.893187: step 69480, loss = 0.65 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:21:59.757396: step 69490, loss = 0.66 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:22:00.727024: step 69500, loss = 0.66 (1320.1 examples/sec; 0.097 sec/batch)
2017-06-02 04:22:01.485115: step 69510, loss = 0.68 (1688.5 examples/sec; 0.076 sec/batch)
2017-06-02 04:22:02.347081: step 69520, loss = 0.68 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:22:03.219518: step 69530, loss = 0.55 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:22:04.060882: step 69540, loss = 0.75 (1521.3 examples/sec; 0.084 sec/batch)
2017-06-02 04:22:04.943737: step 69550, loss = 0.76 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:22:05.813600: step 69560, loss = 0.73 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:22:06.662065: step 69570, loss = 0.69 (1508.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:22:07.520435: step 69580, loss = 0.84 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:22:08.391630: step 69590, loss = 0.85 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:22:09.395665: step 69600, loss = 0.74 (1274.9 examples/sec; 0.100 sec/batch)
2017-06-02 04:22:10.120398: step 69610, loss = 0.66 (1766.2 examples/sec; 0.072 sec/batch)
2017-06-02 04:22:10.995314: step 69620, loss = 0.84 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:22:11.872996: step 69630, loss = 0.78 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:22:12.738887: step 69640, loss = 0.72 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:22:13.579565: step 69650, loss = 0.51 (1522.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:22:14.400724: step 69660, loss = 0.71 (1558.7 examples/sec; 0.082 sec/batch)
2017-06-02 04:22:15.255197: step 69670, loss = 0.71 (1498.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:22:16.108038: step 69680, loss = 0.73 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:22:16.967563: step 69690, loss = 0.81 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:22:17.941268: step 69700, loss = 0.67 (1314.6 examples/sec; 0.097 sec/batch)
2017-06-02 04:22:18.695559: step 69710, loss = 0.72 (1697.0 examples/sec; 0.075 sec/batch)
2017-06-02 04:22:19.554029: step 69720, loss = 0.77 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:22:20.409496: step 69730, loss = 0.75 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:22:21.270535: step 69740, loss = 0.72 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:22:22.162840: step 69750, loss = 0.78 (1434.4 examples/sec; 0.089 sec/batch)
2017-06-02 04:22:23.049884: step 69760, loss = 0.69 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:22:23.928320: step 69770, loss = 0.70 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:22:24.802749: step 69780, loss = 0.55 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:22:25.706618: step 69790, loss = 0.79 (1416.1 examples/sec; 0.090 sec/batch)
2017-06-02 04:22:26.720030: step 69800, loss = 0.57 (1263.1 examples/sec; 0.101 sec/batch)
2017-06-02 04:22:27.468463: step 69810, loss = 0.78 (1710.2 examples/sec; 0.075 sec/batch)
2017-06-02 04:22:28.349369: step 69820, loss = 0.68 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:22:29.241091: step 69830, loss = 0.72 (1435.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:22:30.120574: step 69840, loss = 0.71 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:22:31.028215: step 69850, loss = 0.84 (1410.3 examples/sec; 0.091 sec/batch)
2017-06-02 04:22:31.901501: step 69860, loss = 0.90 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:22:32.754948: step 69870, loss = 0.78 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:22:33.624358: step 69880, loss = 0.85 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:22:34.469746: step 69890, loss = 0.79 (1514.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:22:35.433495: step 69900, loss = 0.70 (1328.2 examples/sec; 0.096 sec/batch)
2017-06-02 04:22:36.217287: step 69910, loss = 0.73 (1633.1 examples/sec; 0.078 sec/batch)
2017-06-02 04:22:37.121993: step 69920, loss = 0.63 (1414.8 examples/sec; 0.090 sec/batch)
2017-06-02 04:22:37.990862: step 69930, loss = 0.71 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:22:38.843215: step 69940, loss = 0.75 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:22:39.720193: step 69950, loss = 0.73 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:22:40.607521: step 69960, loss = 0.65 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:22:41.449233: step 69970, loss = 0.71 (1520.7 examples/sec; 0.084 sec/batch)
2017-06-02 04:22:42.298774: step 69980, loss = 0.82 (1506.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:22:43.172467: step 69990, loss = 0.65 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:22:44.150859: step 70000, loss = 0.69 (1308.3 examples/sec; 0.098 sec/batch)
2017-06-02 04:22:44.910957: step 70010, loss = 0.74 (1684.0 examples/sec; 0.076 sec/batch)
2017-06-02 04:22:45.782418: step 70020, loss = 0.62 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:22:46.653470: step 70030, loss = 0.82 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:22:47.512148: step 70040, loss = 0.77 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:22:48.350523: step 70050, loss = 0.76 (1526.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:22:49.203763: step 70060, loss = 0.66 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:22:50.071665: step 70070, loss = 0.81 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:22:50.939352: step 70080, loss = 0.65 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:22:51.808299: step 70090, loss = 0.80 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:22:52.753270: step 70100, loss = 0.73 (1354.5 examples/sec; 0.095 sec/batch)
2017-06-02 04:22:53.534920: step 70110, loss = 0.64 (1637.6 examples/sec; 0.078 sec/batch)
2017-06-02 04:22:54.388111: step 70120, loss = 0.63 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:22:55.263860: step 70130, loss = 0.66 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:22:56.126432: step 70140, loss = 0.79 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:22:56.969662: step 70150, loss = 0.83 (1518.0 examples/sec; 0.084 sec/batch)
2017-06-02 04:22:57.841980: step 70160, loss = 0.85 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:22:58.713103: step 70170, loss = 0.83 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:22:59.567213: step 70180, loss = 0.62 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:23:00.434853: step 70190, loss = 0.71 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:23:01.398524: step 70200, loss = 0.87 (1328.2 examples/sec; 0.096 sec/batch)
2017-06-02 04:23:02.171986: step 70210, loss = 0.80 (1654.9 examples/sec; 0.077 sec/batch)
2017-06-02 04:23:03.066391: step 70220, loss = 0.78 (1431.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:23:03.940399: step 70230, loss = 0.63 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:23:04.778186: step 70240, loss = 0.64 (1527.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:23:05.636545: step 70250, loss = 0.73 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:23:06.474328: step 70260, loss = 0.73 (1527.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:23:07.343619: step 70270, loss = 0.72 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:23:08.204158: step 70280, loss = 0.57 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:23:09.092704: step 70290, loss = 0.77 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:23:10.045868: step 70300, loss = 0.80 (1342.9 examples/sec; 0.095 sec/batch)
2017-06-02 04:23:10.805126: step 70310, loss = 0.52 (1685.9 examples/sec; 0.076 sec/batch)
2017-06-02 04:23:11.678368: step 70320, loss = 0.67 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:23:12.544676: step 70330, loss = 0.56 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:23:13.418977: step 70340, loss = 0.80 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:23:14.287293: step 70350, loss = 0.71 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:23:15.152406: step 70360, loss = 0.72 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:23:15.996519: step 70370, loss = 0.77 (1516.4 examples/sec; 0.084 sec/batch)
2017-06-02 04:23:16.855672: step 70380, loss = 0.57 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:23:17.735124: step 70390, loss = 0.79 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:23:18.685512: step 70400, loss = 0.77 (1346.8 examples/sec; 0.095 sec/batch)
2017-06-02 04:23:19.451939: step 70410, loss = 0.69 (1670.1 examples/sec; 0.077 sec/batch)
2017-06-02 04:23:20.340508: step 70420, loss = 0.61 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:23:21.180694: step 70430, loss = 0.77 (1523.5 examples/sec; 0.084 sec/batch)
2017-06-02 04:23:22.053676: step 70440, loss = 0.55 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:23:22.914395: step 70450, loss = 0.68 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:23:23.758063: step 70460, loss = 0.72 (1517.2 examples/sec; 0.084 sec/batch)
2017-06-02 04:23:24.627911: step 70470, loss = 0.74 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:23:25.494999: step 70480, loss = 0.77 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:23:26.365389: step 70490, loss = 0.61 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:23:27.318795: step 70500, loss = 0.68 (1342.6 examples/sec; 0.095 sec/batch)
2017-06-02 04:23:28.087674: step 70510, loss = 0.66 (1664.7 examples/sec; 0.077 sec/batch)
2017-06-02 04:23:28.931577: step 70520, loss = 0.75 (1516.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:23:29.786871: step 70530, loss = 0.72 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:23:30.644193: step 70540, loss = 0.79 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:23:31.538393: step 70550, loss = 0.71 (1431.4 examples/sec; 0.089 sec/batch)
2017-06-02 04:23:32.413445: step 70560, loss = 0.61 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:23:33.279510: step 70570, loss = 0.58 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:23:34.120382: step 70580, loss = 0.64 (1522.2 examples/sec; 0.084 sec/batch)
2017-06-02 04:23:34.951278: step 70590, loss = 0.70 (1540.5 examples/sec; 0.083 sec/batch)
2017-06-02 04:23:35.899764: step 70600, loss = 0.71 (1349.5 examples/sec; 0.095 sec/batch)
2017-06-02 04:23:36.665787: step 70610, loss = 0.76 (1671.0 examples/sec; 0.077 sec/batch)
2017-06-02 04:23:37.540328: step 70620, loss = 0.69 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:23:38.403231: step 70630, loss = 0.63 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:23:39.262538: step 70640, loss = 0.92 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:23:40.119574: step 70650, loss = 0.68 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:23:40.978722: step 70660, loss = 0.78 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:23:41.826014: step 70670, loss = 0.64 (1510.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:23:42.677999: step 70680, loss = 0.62 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:23:43.535220: step 70690, loss = 0.96 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:23:44.476166: step 70700, loss = 0.78 (1360.3 examples/sec; 0.094 sec/batch)
2017-06-02 04:23:45.257035: step 70710, loss = 0.65 (1639.2 examples/sec; 0.078 sec/batch)
2017-06-02 04:23:46.103186: step 70720, loss = 0.84 (1512.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:23:46.989114: step 70730, loss = 0.64 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:23:47.853384: step 70740, loss = 0.74 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:23:48.716062: step 70750, loss = 0.78 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:23:49.557509: step 70760, loss = 0.75 (1521.2 examples/sec; 0.084 sec/batch)
2017-06-02 04:23:50.406252: step 70770, loss = 0.73 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:23:51.290455: step 70780, loss = 0.69 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:23:52.151202: step 70790, loss = 0.74 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:23:53.125088: step 70800, loss = 0.73 (1314.3 examples/sec; 0.097 sec/batch)
2017-06-02 04:23:53.900223: step 70810, loss = 0.73 (1651.3 examples/sec; 0.078 sec/batch)
2017-06-02 04:23:54.760243: step 70820, loss = 0.76 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:23:55.630037: step 70830, loss = 0.95 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:23:56.490914: step 70840, loss = 0.67 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:23:57.317751: step 70850, loss = 0.55 (1548.1 examples/sec; 0.083 sec/batch)
2017-06-02 04:23:58.180650: step 70860, loss = 0.79 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:23:59.047141: step 70870, loss = 0.61 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:23:59.887826: step 70880, loss = 0.86 (1522.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:24:00.739449: step 70890, loss = 0.71 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:24:01.712438: step 70900, loss = 0.68 (1315.5 examples/sec; 0.097 sec/batch)
2017-06-02 04:24:02.494750: step 70910, loss = 0.76 (1636.2 examples/sec; 0.078 sec/batch)
2017-06-02 04:24:03.342826: step 70920, loss = 0.64 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:24:04.222664: step 70930, loss = 0.68 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:24:05.090617: step 70940, loss = 0.79 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:24:05.952413: step 70950, loss = 0.69 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:24:06.818503: step 70960, loss = 0.87 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:24:07.699094: step 70970, loss = 0.80 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:24:08.562367: step 70980, loss = 0.78 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:24:09.433351: step 70990, loss = 0.61 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:24:10.402523: step 71000, loss = 0.67 (1320.7 examples/sec; 0.097 sec/batch)
2017-06-02 04:24:11.182991: step 71010, loss = 0.87 (1640.0 examples/sec; 0.078 sec/batch)
2017-06-02 04:24:12.030168: step 71020, loss = 0.72 (1510.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:24:12.892282: step 71030, loss = 0.67 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:24:13.762830: step 71040, loss = 0.86 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:24:14.594989: step 71050, loss = 0.68 (1538.2 examples/sec; 0.083 sec/batch)
2017-06-02 04:24:15.439727: step 71060, loss = 0.75 (1515.3 examples/sec; 0.084 sec/batch)
2017-06-02 04:24:16.311402: step 71070, loss = 0.77 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:24:17.171891: step 71080, loss = 0.71 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:24:18.033958: step 71090, loss = 0.86 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:24:18.996324: step 71100, loss = 0.57 (1330.1 examples/sec; 0.096 sec/batch)
2017-06-02 04:24:19.763946: step 71110, loss = 0.66 (1667.5 examples/sec; 0.077 sec/batch)
2017-06-02 04:24:20.638463: step 71120, loss = 0.79 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:24:21.503974: step 71130, loss = 0.72 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:24:22.350256: step 71140, loss = 0.88 (1512.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:24:23.232938: step 71150, loss = 0.67 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:24:24.094557: step 71160, loss = 0.87 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:24:24.973199: step 71170, loss = 0.71 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:24:25.805616: step 71180, loss = 0.81 (1537.7 examples/sec; 0.083 sec/batch)
2017-06-02 04:24:26.649360: step 71190, loss = 0.74 (1517.0 examples/sec; 0.084 sec/batch)
2017-06-02 04:24:27.603464: step 71200, loss = 0.66 (1341.6 examples/sec; 0.095 sec/batch)
2017-06-02 04:24:28.378054: step 71210, loss = 0.60 (1652.5 examples/sec; 0.077 sec/batch)
2017-06-02 04:24:29.254332: step 71220, loss = 0.73 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:24:30.118191: step 71230, loss = 0.70 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:24:30.988638: step 71240, loss = 0.65 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:24:31.861002: step 71250, loss = 0.76 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:24:32.733300: step 71260, loss = 0.75 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:24:33.606721: step 71270, loss = 0.78 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:24:34.470119: step 71280, loss = 0.72 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:24:35.348082: step 71290, loss = 0.77 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:24:36.314514: step 71300, loss = 0.73 (1324.5 examples/sec; 0.097 sec/batch)
2017-06-02 04:24:37.074853: step 71310, loss = 0.64 (1683.4 examples/sec; 0.076 sec/batch)
2017-06-02 04:24:37.937484: step 71320, loss = 0.64 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:24:38.780058: step 71330, loss = 0.67 (1519.2 examples/sec; 0.084 sec/batch)
2017-06-02 04:24:39.627596: step 71340, loss = 0.73 (1510.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:24:40.502023: step 71350, loss = 0.71 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:24:41.369622: step 71360, loss = 0.67 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:24:42.217392: step 71370, loss = 0.65 (1509.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:24:43.086032: step 71380, loss = 0.78 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:24:43.963583: step 71390, loss = 0.77 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:24:44.927268: step 71400, loss = 0.73 (1328.2 examples/sec; 0.096 sec/batch)
2017-06-02 04:24:45.694579: step 71410, loss = 0.58 (1668.2 examples/sec; 0.077 sec/batch)
2017-06-02 04:24:46.587931: step 71420, loss = 0.88 (1432.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:24:47.480978: step 71430, loss = 0.74 (1433.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:24:48.368367: step 71440, loss = 0.89 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:24:49.263143: step 71450, loss = 0.66 (1430.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:24:50.147862: step 71460, loss = 0.79 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:24:51.012801: step 71470, loss = 0.72 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:24:51.882257: step 71480, loss = 0.64 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:24:52.767897: step 71490, loss = 0.84 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:24:53.732163: step 71500, loss = 0.80 (1327.4 examples/sec; 0.096 sec/batch)
2017-06-02 04:24:54.506440: step 71510, loss = 0.70 (1653.1 examples/sec; 0.077 sec/batch)
2017-06-02 04:24:55.390452: step 71520, loss = 0.69 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:24:56.272378: step 71530, loss = 0.74 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:24:57.164845: step 71540, loss = 0.74 (1434.2 examples/sec; 0.089 sec/batch)
2017-06-02 04:24:58.044479: step 71550, loss = 0.80 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:24:58.916445: step 71560, loss = 0.83 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:24:59.796005: step 71570, loss = 0.80 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:25:00.625340: step 71580, loss = 0.63 (1543.4 examples/sec; 0.083 sec/batch)
2017-06-02 04:25:01.524376: step 71590, loss = 0.62 (1423.7 examples/sec; 0.090 sec/batch)
2017-06-02 04:25:02.491442: step 71600, loss = 0.64 (1323.6 examples/sec; 0.097 sec/batch)
2017-06-02 04:25:03.231160: step 71610, loss = 0.70 (1730.4 examples/sec; 0.074 sec/batch)
2017-06-02 04:25:04.122965: step 71620, loss = 0.76 (1435.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:25:04.974297: step 71630, loss = 0.72 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:25:05.864201: step 71640, loss = 0.74 (1438.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:25:06.730994: step 71650, loss = 0.72 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:25:07.628638: step 71660, loss = 0.71 (1426.0 examples/sec; 0.090 sec/batch)
2017-06-02 04:25:08.480437: step 71670, loss = 0.67 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:25:09.354093: step 71680, loss = 0.88 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:25:10.225670: step 71690, loss = 0.53 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:25:11.201366: step 71700, loss = 0.89 (1311.9 examples/sec; 0.098 sec/batch)
2017-06-02 04:25:11.965782: step 71710, loss = 0.79 (1674.5 examples/sec; 0.076 sec/batch)
2017-06-02 04:25:12.822653: step 71720, loss = 0.71 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:25:13.666950: step 71730, loss = 0.79 (1516.0 examples/sec; 0.084 sec/batch)
2017-06-02 04:25:14.542765: step 71740, loss = 0.66 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:25:15.393533: step 71750, loss = 0.67 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:25:16.257062: step 71760, loss = 0.69 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:25:17.113859: step 71770, loss = 0.74 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:25:17.973487: step 71780, loss = 0.81 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:25:18.829322: step 71790, loss = 0.80 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:25:19.831981: step 71800, loss = 0.61 (1276.6 examples/sec; 0.100 sec/batch)
2017-06-02 04:25:20.562973: step 71810, loss = 0.63 (1751.0 examples/sec; 0.073 sec/batch)
2017-06-02 04:25:21.417087: step 71820, loss = 0.72 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:25:22.291824: step 71830, loss = 0.86 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:25:23.132742: step 71840, loss = 0.68 (1522.2 examples/sec; 0.084 sec/batch)
2017-06-02 04:25:23.993093: step 71850, loss = 0.58 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:25:24.861245: step 71860, loss = 0.80 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:25:25.730480: step 71870, loss = 0.84 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:25:26.570487: step 71880, loss = 0.58 (1523.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:25:27.415717: step 71890, loss = 0.71 (1514.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:25:28.369497: step 71900, loss = 0.85 (1342.0 examples/sec; 0.095 sec/batch)
2017-06-02 04:25:29.132054: step 71910, loss = 0.79 (1678.6 examples/sec; 0.076 sec/batch)
2017-06-02 04:25:30.033324: step 71920, loss = 0.65 (1420.2 examples/sec; 0.090 sec/batch)
2017-06-02 04:25:30.924326: step 71930, loss = 0.70 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:25:31.796300: step 71940, loss = 0.57 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:25:32.673390: step 71950, loss = 0.76 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:25:33.542979: step 71960, loss = 0.74 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:25:34.384718: step 71970, loss = 0.69 (1520.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:25:35.249168: step 71980, loss = 0.79 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:25:36.105113: step 71990, loss = 0.72 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:25:37.062580: step 72000, loss = 0.70 (1336.8 examples/sec; 0.096 sec/batch)
2017-06-02 04:25:37.843039: step 72010, loss = 0.58 (1640.1 examples/sec; 0.078 sec/batch)
2017-06-02 04:25:38.694111: step 72020, loss = 0.73 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:25:39.555028: step 72030, loss = 0.73 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:25:40.411602: step 72040, loss = 0.74 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:25:41.236381: step 72050, loss = 0.61 (1551.9 examples/sec; 0.082 sec/batch)
2017-06-02 04:25:42.106624: step 72060, loss = 0.59 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:25:42.992996: step 72070, loss = 0.88 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:25:43.848584: step 72080, loss = 0.71 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:25:44.737816: step 72090, loss = 0.71 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:25:45.701248: step 72100, loss = 0.60 (1328.6 examples/sec; 0.096 sec/batch)
2017-06-02 04:25:46.461124: step 72110, loss = 0.76 (1684.5 examples/sec; 0.076 sec/batch)
2017-06-02 04:25:47.327272: step 72120, loss = 0.76 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:25:48.185132: step 72130, loss = 0.79 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:25:49.027094: step 72140, loss = 0.80 (1520.3 examples/sec; 0.084 sec/batch)
2017-06-02 04:25:49.905370: step 72150, loss = 0.59 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:25:50.784316: step 72160, loss = 0.79 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:25:51.641450: step 72170, loss = 0.71 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:25:52.508118: step 72180, loss = 0.72 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:25:53.382678: step 72190, loss = 0.70 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:25:54.314697: step 72200, loss = 0.70 (1373.4 examples/sec; 0.093 sec/batch)
2017-06-02 04:25:55.079466: step 72210, loss = 0.67 (1673.7 examples/sec; 0.076 sec/batch)
2017-06-02 04:25:55.912618: step 72220, loss = 0.72 (1536.3 examples/sec; 0.083 sec/batch)
2017-06-02 04:25:56.779403: step 72230, loss = 0.64 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:25:57.627972: step 72240, loss = 0.85 (1508.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:25:58.501744: step 72250, loss = 0.69 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:25:59.381896: step 72260, loss = 0.67 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:26:00.247133: step 72270, loss = 0.77 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:26:01.109714: step 72280, loss = 0.70 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:26:01.972386: step 72290, loss = 0.73 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:26:02.905586: step 72300, loss = 0.75 (1371.6 examples/sec; 0.093 sec/batch)
2017-06-02 04:26:03.652554: step 72310, loss = 0.67 (1713.6 examples/sec; 0.075 sec/batch)
2017-06-02 04:26:04.509881: step 72320, loss = 0.64 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:26:05.383016: step 72330, loss = 0.78 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:26:06.267206: step 72340, loss = 0.64 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:26:07.114234: step 72350, loss = 0.80 (1511.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:26:07.993385: step 72360, loss = 0.82 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:26:08.870557: step 72370, loss = 0.66 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:26:09.734993: step 72380, loss = 0.73 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:26:10.624079: step 72390, loss = 0.69 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:26:11.635515: step 72400, loss = 0.62 (1265.5 examples/sec; 0.101 sec/batch)
2017-06-02 04:26:12.398705: step 72410, loss = 0.72 (1677.2 examples/sec; 0.076 sec/batch)
2017-06-02 04:26:13.271238: step 72420, loss = 0.79 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:26:14.159720: step 72430, loss = 0.69 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:26:15.043707: step 72440, loss = 0.67 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:26:15.923044: step 72450, loss = 0.69 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:26:16.788736: step 72460, loss = 0.86 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:26:17.631579: step 72470, loss = 0.68 (1518.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:26:18.494798: step 72480, loss = 0.72 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:26:19.391803: step 72490, loss = 0.78 (1427.0 examples/sec; 0.090 sec/batch)
2017-06-02 04:26:20.354980: step 72500, loss = 0.75 (1328.9 examples/sec; 0.096 sec/batch)
2017-06-02 04:26:21.112877: step 72510, loss = 0.79 (1688.9 examples/sec; 0.076 sec/batch)
2017-06-02 04:26:21.965687: step 72520, loss = 0.78 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:26:22.828508: step 72530, loss = 0.80 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:26:23.704431: step 72540, loss = 0.72 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:26:24.590515: step 72550, loss = 0.66 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:26:25.430772: step 72560, loss = 0.76 (1523.4 examples/sec; 0.084 sec/batch)
2017-06-02 04:26:26.294692: step 72570, loss = 0.81 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:26:27.169053: step 72580, loss = 0.58 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:26:28.022380: step 72590, loss = 0.73 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:26:28.937739: step 72600, loss = 0.72 (1398.4 examples/sec; 0.092 sec/batch)
2017-06-02 04:26:29.678533: step 72610, loss = 0.74 (1727.9 examples/sec; 0.074 sec/batch)
2017-06-02 04:26:30.522339: step 72620, loss = 0.82 (1516.9 examples/sec; 0.084 sec/batch)
2017-06-02 04:26:31.397232: step 72630, loss = 0.76 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:26:32.271363: step 72640, loss = 0.62 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:26:33.126928: step 72650, loss = 0.65 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:26:33.981654: step 72660, loss = 0.75 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:26:34.847059: step 72670, loss = 0.84 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:26:35.696364: step 72680, loss = 0.64 (1507.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:26:36.548807: step 72690, loss = 0.87 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:26:37.548780: step 72700, loss = 0.83 (1280.0 examples/sec; 0.100 sec/batch)
2017-06-02 04:26:38.288321: step 72710, loss = 0.66 (1730.8 examples/sec; 0.074 sec/batch)
2017-06-02 04:26:39.146356: step 72720, loss = 0.74 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:26:39.998114: step 72730, loss = 0.63 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:26:40.833760: step 72740, loss = 0.70 (1531.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:26:41.696894: step 72750, loss = 0.70 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:26:42.548052: step 72760, loss = 0.74 (1503.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:26:43.441251: step 72770, loss = 0.74 (1433.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:26:44.311119: step 72780, loss = 0.81 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:26:45.158350: step 72790, loss = 0.60 (1510.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:26:46.098373: step 72800, loss = 0.67 (1361.7 examples/sec; 0.094 sec/batch)
2017-06-02 04:26:46.843579: step 72810, loss = 0.68 (1717.7 examples/sec; 0.075 sec/batch)
2017-06-02 04:26:47.685164: step 72820, loss = 0.61 (1520.9 examples/sec; 0.084 sec/batch)
2017-06-02 04:26:48.558673: step 72830, loss = 0.58 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:26:49.448028: step 72840, loss = 0.59 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:26:50.306374: step 72850, loss = 0.76 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:26:51.197017: step 72860, loss = 0.90 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:26:52.057877: step 72870, loss = 0.72 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:26:52.945678: step 72880, loss = 0.68 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:26:53.834233: step 72890, loss = 0.85 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:26:54.793093: step 72900, loss = 0.73 (1334.9 examples/sec; 0.096 sec/batch)
2017-06-02 04:26:55.545244: step 72910, loss = 0.86 (1701.9 examples/sec; 0.075 sec/batch)
2017-06-02 04:26:56.419690: step 72920, loss = 0.48 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:26:57.260097: step 72930, loss = 0.70 (1523.1 examples/sec; 0.084 sec/batch)
2017-06-02 04:26:58.141894: step 72940, loss = 0.68 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:26:58.984392: step 72950, loss = 0.75 (1519.3 examples/sec; 0.084 sec/batch)
2017-06-02 04:26:59.838313: step 72960, loss = 0.72 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:27:00.673165: step 72970, loss = 0.79 (1533.2 examples/sec; 0.083 sec/batch)
2017-06-02 04:27:01.552518: step 72980, loss = 0.73 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:27:02.414338: step 72990, loss = 0.75 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:27:03.364596: step 73000, loss = 0.81 (1347.0 examples/sec; 0.095 sec/batch)
2017-06-02 04:27:04.132549: step 73010, loss = 0.72 (1666.8 examples/sec; 0.077 sec/batch)
2017-06-02 04:27:04.972219: step 73020, loss = 0.88 (1524.4 examples/sec; 0.084 sec/batch)
2017-06-02 04:27:05.844128: step 73030, loss = 0.77 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:27:06.715616: step 73040, loss = 0.60 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:27:07.580658: step 73050, loss = 0.75 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:27:08.467666: step 73060, loss = 0.70 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:27:09.314812: step 73070, loss = 0.69 (1511.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:27:10.164399: step 73080, loss = 0.73 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:27:11.054043: step 73090, loss = 0.93 (1438.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:27:11.994318: step 73100, loss = 0.68 (1361.3 examples/sec; 0.094 sec/batch)
2017-06-02 04:27:12.779661: step 73110, loss = 0.81 (1629.9 examples/sec; 0.079 sec/batch)
2017-06-02 04:27:13.648347: step 73120, loss = 0.66 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:27:14.491801: step 73130, loss = 0.61 (1517.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:27:15.349386: step 73140, loss = 0.80 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:27:16.204743: step 73150, loss = 0.70 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:27:17.095605: step 73160, loss = 0.74 (1436.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:27:17.957822: step 73170, loss = 0.61 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:27:18.841375: step 73180, loss = 0.84 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:27:19.696035: step 73190, loss = 0.58 (1497.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:27:20.665520: step 73200, loss = 0.60 (1320.3 examples/sec; 0.097 sec/batch)
2017-06-02 04:27:21.447406: step 73210, loss = 0.83 (1637.1 examples/sec; 0.078 sec/batch)
2017-06-02 04:27:22.304138: step 73220, loss = 0.59 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:27:23.181791: step 73230, loss = 0.85 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:27:24.047667: step 73240, loss = 0.67 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:27:24.892104: step 73250, loss = 0.81 (1515.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:27:25.769066: step 73260, loss = 0.73 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:27:26.614921: step 73270, loss = 0.66 (1513.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:27:27.485693: step 73280, loss = 0.84 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:27:28.357557: step 73290, loss = 0.74 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:27:29.302034: step 73300, loss = 0.80 (1355.2 examples/sec; 0.094 sec/batch)
2017-06-02 04:27:30.098539: step 73310, loss = 0.82 (1607.0 examples/sec; 0.080 sec/batch)
2017-06-02 04:27:30.971062: step 73320, loss = 0.71 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:27:31.823302: step 73330, loss = 0.75 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:27:32.698323: step 73340, loss = 0.71 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:27:33.571166: step 73350, loss = 0.73 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:27:34.427612: step 73360, loss = 0.75 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:27:35.296140: step 73370, loss = 0.52 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:27:36.176246: step 73380, loss = 0.63 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:27:37.073125: step 73390, loss = 0.69 (1427.2 examples/sec; 0.090 sec/batch)
2017-06-02 04:27:38.058526: step 73400, loss = 0.78 (1299.0 examples/sec; 0.099 sec/batch)
2017-06-02 04:27:38.857862: step 73410, loss = 0.75 (1601.3 examples/sec; 0.080 sec/batch)
2017-06-02 04:27:39.726983: step 73420, loss = 0.75 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:27:40.597785: step 73430, loss = 0.65 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:27:41.449806: step 73440, loss = 0.63 (1502.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:27:42.309505: step 73450, loss = 0.63 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:27:43.183832: step 73460, loss = 0.73 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:27:44.031191: step 73470, loss = 0.71 (1510.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:27:44.906986: step 73480, loss = 0.57 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:27:45.757567: step 73490, loss = 0.59 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:27:46.718267: step 73500, loss = 0.76 (1332.4 examples/sec; 0.096 sec/batch)
2017-06-02 04:27:47.483545: step 73510, loss = 0.74 (1672.6 examples/sec; 0.077 sec/batch)
2017-06-02 04:27:48.346087: step 73520, loss = 0.67 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:27:49.202520: step 73530, loss = 0.88 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:27:50.074351: step 73540, loss = 0.74 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:27:50.937641: step 73550, loss = 0.71 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:27:51.804238: step 73560, loss = 0.74 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:27:52.651966: step 73570, loss = 0.67 (1509.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:27:53.514423: step 73580, loss = 0.78 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:27:54.377248: step 73590, loss = 0.71 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:27:55.356175: step 73600, loss = 0.73 (1307.5 examples/sec; 0.098 sec/batch)
2017-06-02 04:27:56.115431: step 73610, loss = 0.94 (1685.9 examples/sec; 0.076 sec/batch)
2017-06-02 04:27:56.967836: step 73620, loss = 0.70 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:27:57.803511: step 73630, loss = 0.73 (1531.7 examples/sec; 0.084 sec/batch)
2017-06-02 04:27:58.670616: step 73640, loss = 0.77 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:27:59.565909: step 73650, loss = 0.87 (1429.7 examples/sec; 0.090 sec/batch)
2017-06-02 04:28:00.411169: step 73660, loss = 0.80 (1514.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:28:01.289528: step 73670, loss = 0.57 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:28:02.151500: step 73680, loss = 0.78 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:28:03.020389: step 73690, loss = 0.58 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:28:04.015425: step 73700, loss = 0.76 (1286.4 examples/sec; 0.100 sec/batch)
2017-06-02 04:28:04.785247: step 73710, loss = 0.99 (1662.7 examples/sec; 0.077 sec/batch)
2017-06-02 04:28:05.630982: step 73720, loss = 0.72 (1513.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:28:06.475659: step 73730, loss = 0.68 (1515.4 examples/sec; 0.084 sec/batch)
2017-06-02 04:28:07.346288: step 73740, loss = 0.76 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:28:08.231795: step 73750, loss = 0.57 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:28:09.116116: step 73760, loss = 0.86 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:28:09.963075: step 73770, loss = 0.67 (1511.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:28:10.830305: step 73780, loss = 0.77 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:28:11.687398: step 73790, loss = 0.81 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:28:12.657571: step 73800, loss = 0.60 (1319.4 examples/sec; 0.097 sec/batch)
2017-06-02 04:28:13.405532: step 73810, loss = 0.68 (1711.3 examples/sec; 0.075 sec/batch)
2017-06-02 04:28:14.261077: step 73820, loss = 0.84 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:28:15.142880: step 73830, loss = 0.71 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:28:16.027277: step 73840, loss = 0.56 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:28:16.889752: step 73850, loss = 0.84 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:28:17.740346: step 73860, loss = 0.72 (1504.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:28:18.594757: step 73870, loss = 0.56 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:28:19.474660: step 73880, loss = 0.66 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:28:20.336294: step 73890, loss = 0.77 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:28:21.284078: step 73900, loss = 0.77 (1350.5 examples/sec; 0.095 sec/batch)
2017-06-02 04:28:22.029296: step 73910, loss = 0.66 (1717.6 examples/sec; 0.075 sec/batch)
2017-06-02 04:28:22.901753: step 73920, loss = 0.59 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:28:23.770148: step 73930, loss = 0.69 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:28:24.657186: step 73940, loss = 0.72 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:28:25.525100: step 73950, loss = 0.84 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:28:26.371117: step 73960, loss = 0.86 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:28:27.216587: step 73970, loss = 0.83 (1514.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:28:28.086094: step 73980, loss = 0.80 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:28:28.959054: step 73990, loss = 0.70 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:28:29.915782: step 74000, loss = 0.74 (1337.9 examples/sec; 0.096 sec/batch)
2017-06-02 04:28:30.697682: step 74010, loss = 0.70 (1637.0 examples/sec; 0.078 sec/batch)
2017-06-02 04:28:31.568247: step 74020, loss = 0.67 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:28:32.430409: step 74030, loss = 0.78 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:28:33.299193: step 74040, loss = 0.67 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:28:34.195364: step 74050, loss = 0.85 (1428.3 examples/sec; 0.090 sec/batch)
2017-06-02 04:28:35.061579: step 74060, loss = 0.74 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:28:35.941227: step 74070, loss = 0.80 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:28:36.821197: step 74080, loss = 0.78 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:28:37.717096: step 74090, loss = 0.69 (1428.7 examples/sec; 0.090 sec/batch)
2017-06-02 04:28:38.694216: step 74100, loss = 0.71 (1310.0 examples/sec; 0.098 sec/batch)
2017-06-02 04:28:39.484024: step 74110, loss = 0.81 (1620.6 examples/sec; 0.079 sec/batch)
2017-06-02 04:28:40.336979: step 74120, loss = 0.87 (1500.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:28:41.213944: step 74130, loss = 0.85 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:28:42.099801: step 74140, loss = 0.79 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:28:42.971904: step 74150, loss = 0.96 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:28:43.824021: step 74160, loss = 0.83 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:28:44.693485: step 74170, loss = 0.73 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:28:45.560755: step 74180, loss = 0.67 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:28:46.427675: step 74190, loss = 0.89 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:28:47.384678: step 74200, loss = 0.74 (1337.5 examples/sec; 0.096 sec/batch)
2017-06-02 04:28:48.155233: step 74210, loss = 0.75 (1661.1 examples/sec; 0.077 sec/batch)
2017-06-02 04:28:49.021796: step 74220, loss = 0.82 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:28:49.872890: step 74230, loss = 0.83 (1503.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:28:50.733473: step 74240, loss = 0.66 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:28:51.603383: step 74250, loss = 0.71 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:28:52.449046: step 74260, loss = 0.60 (1513.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:28:53.343133: step 74270, loss = 0.63 (1431.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:28:54.193233: step 74280, loss = 0.67 (1505.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:28:55.067448: step 74290, loss = 0.61 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:28:56.038202: step 74300, loss = 0.66 (1318.6 examples/sec; 0.097 sec/batch)
2017-06-02 04:28:56.803518: step 74310, loss = 0.78 (1672.5 examples/sec; 0.077 sec/batch)
2017-06-02 04:28:57.678135: step 74320, loss = 0.73 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:28:58.524263: step 74330, loss = 0.64 (1512.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:28:59.393647: step 74340, loss = 0.75 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:29:00.244520: step 74350, loss = 0.78 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:29:01.107835: step 74360, loss = 0.55 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:29:01.956209: step 74370, loss = 0.88 (1508.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:29:02.817814: step 74380, loss = 0.67 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:29:03.660904: step 74390, loss = 0.74 (1518.2 examples/sec; 0.084 sec/batch)
2017-06-02 04:29:04.610119: step 74400, loss = 0.73 (1348.5 examples/sec; 0.095 sec/batch)
2017-06-02 04:29:05.384896: step 74410, loss = 0.77 (1652.1 examples/sec; 0.077 sec/batch)
2017-06-02 04:29:06.247554: step 74420, loss = 0.75 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:29:07.118520: step 74430, loss = 0.73 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:29:07.997869: step 74440, loss = 0.61 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:29:08.857471: step 74450, loss = 0.75 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:29:09.748573: step 74460, loss = 1.00 (1436.4 examples/sec; 0.089 sec/batch)
2017-06-02 04:29:10.591913: step 74470, loss = 0.79 (1517.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:29:11.483267: step 74480, loss = 0.75 (1436.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:29:12.324744: step 74490, loss = 0.85 (1521.1 examples/sec; 0.084 sec/batch)
2017-06-02 04:29:13.293646: step 74500, loss = 0.86 (1321.1 examples/sec; 0.097 sec/batch)
2017-06-02 04:29:14.055577: step 74510, loss = 0.75 (1679.9 examples/sec; 0.076 sec/batch)
2017-06-02 04:29:14.931010: step 74520, loss = 0.67 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:29:15.815437: step 74530, loss = 0.65 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:29:16.696716: step 74540, loss = 0.76 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:29:17.554242: step 74550, loss = 0.76 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:29:18.401313: step 74560, loss = 0.74 (1511.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:29:19.255341: step 74570, loss = 0.73 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:29:20.111258: step 74580, loss = 0.79 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:29:20.945097: step 74590, loss = 0.58 (1535.1 examples/sec; 0.083 sec/batch)
2017-06-02 04:29:21.942138: step 74600, loss = 0.79 (1283.8 examples/sec; 0.100 sec/batch)
2017-06-02 04:29:22.689941: step 74610, loss = 0.61 (1711.7 examples/sec; 0.075 sec/batch)
2017-06-02 04:29:23.552952: step 74620, loss = 0.68 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:29:24.430285: step 74630, loss = 0.77 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:29:25.297664: step 74640, loss = 0.76 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:29:26.142396: step 74650, loss = 0.69 (1515.3 examples/sec; 0.084 sec/batch)
2017-06-02 04:29:27.020327: step 74660, loss = 0.69 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:29:27.882739: step 74670, loss = 0.83 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:29:28.730644: step 74680, loss = 0.77 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:29:29.596135: step 74690, loss = 0.81 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:29:30.547021: step 74700, loss = 0.69 (1346.1 examples/sec; 0.095 sec/batch)
2017-06-02 04:29:31.324025: step 74710, loss = 0.74 (1647.3 examples/sec; 0.078 sec/batch)
2017-06-02 04:29:32.202173: step 74720, loss = 0.74 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:29:33.084858: step 74730, loss = 0.84 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:29:33.951041: step 74740, loss = 0.75 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:29:34.786869: step 74750, loss = 0.80 (1531.4 examples/sec; 0.084 sec/batch)
2017-06-02 04:29:35.652972: step 74760, loss = 0.65 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:29:36.510850: step 74770, loss = 0.61 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:29:37.348163: step 74780, loss = 0.69 (1528.7 examples/sec; 0.084 sec/batch)
2017-06-02 04:29:38.218190: step 74790, loss = 0.80 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:29:39.156842: step 74800, loss = 0.72 (1363.7 examples/sec; 0.094 sec/batch)
2017-06-02 04:29:39.923047: step 74810, loss = 0.64 (1670.6 examples/sec; 0.077 sec/batch)
2017-06-02 04:29:40.784568: step 74820, loss = 0.74 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:29:41.643484: step 74830, loss = 0.83 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:29:42.486867: step 74840, loss = 0.69 (1517.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:29:43.375198: step 74850, loss = 0.70 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:29:44.210633: step 74860, loss = 0.85 (1532.1 examples/sec; 0.084 sec/batch)
2017-06-02 04:29:45.084868: step 74870, loss = 0.65 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:29:45.943399: step 74880, loss = 0.69 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:29:46.795053: step 74890, loss = 0.55 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:29:47.781055: step 74900, loss = 0.84 (1298.2 examples/sec; 0.099 sec/batch)
2017-06-02 04:29:48.530414: step 74910, loss = 0.72 (1708.1 examples/sec; 0.075 sec/batch)
2017-06-02 04:29:49.363563: step 74920, loss = 0.75 (1536.3 examples/sec; 0.083 sec/batch)
2017-06-02 04:29:50.224930: step 74930, loss = 0.67 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:29:51.083653: step 74940, loss = 0.65 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:29:52.011525: step 74950, loss = 0.70 (1379.5 examples/sec; 0.093 sec/batch)
2017-06-02 04:29:52.858350: step 74960, loss = 0.73 (1511.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:29:53.731060: step 74970, loss = 0.70 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:29:54.617801: step 74980, loss = 0.72 (1443.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:29:55.516357: step 74990, loss = 0.83 (1424.5 examples/sec; 0.090 sec/batch)
2017-06-02 04:29:56.480400: step 75000, loss = 0.55 (1327.7 examples/sec; 0.096 sec/batch)
2017-06-02 04:29:57.245373: step 75010, loss = 0.66 (1673.3 examples/sec; 0.076 sec/batch)
2017-06-02 04:29:58.110331: step 75020, loss = 0.66 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:29:58.967398: step 75030, loss = 0.76 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:29:59.824069: step 75040, loss = 0.77 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:30:00.696627: step 75050, loss = 0.55 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:30:01.547307: step 75060, loss = 0.85 (1504.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:30:02.437421: step 75070, loss = 0.59 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:30:03.301972: step 75080, loss = 0.76 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:30:04.156110: step 75090, loss = 0.78 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:30:05.108652: step 75100, loss = 0.61 (1343.8 examples/sec; 0.095 sec/batch)
2017-06-02 04:30:05.877846: step 75110, loss = 0.80 (1664.1 examples/sec; 0.077 sec/batch)
2017-06-02 04:30:06.725833: step 75120, loss = 0.77 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:30:07.603824: step 75130, loss = 0.77 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:30:08.454711: step 75140, loss = 0.72 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:30:09.330201: step 75150, loss = 0.78 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:30:10.205020: step 75160, loss = 0.67 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:30:11.051927: step 75170, loss = 0.82 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:30:11.940381: step 75180, loss = 0.73 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:30:12.796306: step 75190, loss = 0.73 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:30:13.770572: step 75200, loss = 0.71 (1313.8 examples/sec; 0.097 sec/batch)
2017-06-02 04:30:14.553119: step 75210, loss = 0.78 (1635.7 examples/sec; 0.078 sec/batch)
2017-06-02 04:30:15.429896: step 75220, loss = 0.67 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:30:16.316291: step 75230, loss = 0.80 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:30:17.205669: step 75240, loss = 0.83 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 04:30:18.064314: step 75250, loss = 0.71 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:30:18.934664: step 75260, loss = 0.96 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:30:19.837763: step 75270, loss = 0.80 (1417.3 examples/sec; 0.090 sec/batch)
2017-06-02 04:30:20.692229: step 75280, loss = 0.75 (1498.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:30:21.552611: step 75290, loss = 0.79 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:30:22.526375: step 75300, loss = 0.84 (1314.5 examples/sec; 0.097 sec/batch)
2017-06-02 04:30:23.291380: step 75310, loss = 0.68 (1673.2 examples/sec; 0.077 sec/batch)
2017-06-02 04:30:24.157197: step 75320, loss = 0.62 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:30:25.042147: step 75330, loss = 0.94 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:30:25.932738: step 75340, loss = 0.59 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:30:26.792551: step 75350, loss = 0.69 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:30:27.642776: step 75360, loss = 0.64 (1505.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:30:28.490843: step 75370, loss = 0.76 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:30:29.349808: step 75380, loss = 0.70 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:30:30.212149: step 75390, loss = 0.70 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:30:31.212872: step 75400, loss = 0.79 (1279.1 examples/sec; 0.100 sec/batch)
2017-06-02 04:30:31.938719: step 75410, loss = 0.73 (1763.5 examples/sec; 0.073 sec/batch)
2017-06-02 04:30:32.788625: step 75420, loss = 0.93 (1506.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:30:33.676263: step 75430, loss = 0.63 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:30:34.510908: step 75440, loss = 0.83 (1533.6 examples/sec; 0.083 sec/batch)
2017-06-02 04:30:35.365456: step 75450, loss = 0.69 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:30:36.201564: step 75460, loss = 0.75 (1530.9 examples/sec; 0.084 sec/batch)
2017-06-02 04:30:37.042634: step 75470, loss = 0.72 (1521.9 examples/sec; 0.084 sec/batch)
2017-06-02 04:30:37.910198: step 75480, loss = 0.73 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:30:38.740306: step 75490, loss = 0.66 (1542.0 examples/sec; 0.083 sec/batch)
2017-06-02 04:30:39.738879: step 75500, loss = 0.72 (1281.8 examples/sec; 0.100 sec/batch)
2017-06-02 04:30:40.482508: step 75510, loss = 0.72 (1721.3 examples/sec; 0.074 sec/batch)
2017-06-02 04:30:41.365680: step 75520, loss = 0.73 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:30:42.218897: step 75530, loss = 0.83 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:30:43.105489: step 75540, loss = 0.76 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:30:43.973883: step 75550, loss = 0.73 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:30:44.841481: step 75560, loss = 0.66 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:30:45.719259: step 75570, loss = 0.87 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:30:46.622042: step 75580, loss = 0.61 (1417.9 examples/sec; 0.090 sec/batch)
2017-06-02 04:30:47.507584: step 75590, loss = 0.89 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 04:30:48.475396: step 75600, loss = 0.85 (1322.6 examples/sec; 0.097 sec/batch)
2017-06-02 04:30:49.269684: step 75610, loss = 0.69 (1611.5 examples/sec; 0.079 sec/batch)
2017-06-02 04:30:50.132218: step 75620, loss = 0.75 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:30:50.975878: step 75630, loss = 0.63 (1517.2 examples/sec; 0.084 sec/batch)
2017-06-02 04:30:51.838826: step 75640, loss = 0.85 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:30:52.716590: step 75650, loss = 0.82 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:30:53.603659: step 75660, loss = 0.77 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:30:54.435500: step 75670, loss = 0.69 (1538.8 examples/sec; 0.083 sec/batch)
2017-06-02 04:30:55.262408: step 75680, loss = 0.72 (1547.9 examples/sec; 0.083 sec/batch)
2017-06-02 04:30:56.145609: step 75690, loss = 0.61 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:30:57.129789: step 75700, loss = 0.69 (1300.6 examples/sec; 0.098 sec/batch)
2017-06-02 04:30:57.907738: step 75710, loss = 0.72 (1645.4 examples/sec; 0.078 sec/batch)
2017-06-02 04:30:58.774445: step 75720, loss = 0.77 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:30:59.640291: step 75730, loss = 0.74 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:31:00.488524: step 75740, loss = 0.87 (1509.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:31:01.339194: step 75750, loss = 0.75 (1504.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:31:02.201941: step 75760, loss = 0.67 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:31:03.056878: step 75770, loss = 0.70 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:31:03.932476: step 75780, loss = 0.64 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:31:04.811338: step 75790, loss = 0.66 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:31:05.751354: step 75800, loss = 0.60 (1361.7 examples/sec; 0.094 sec/batch)
2017-06-02 04:31:06.515777: step 75810, loss = 0.64 (1674.5 examples/sec; 0.076 sec/batch)
2017-06-02 04:31:07.395610: step 75820, loss = 0.73 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:31:08.274372: step 75830, loss = 0.74 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:31:09.132291: step 75840, loss = 0.83 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:31:10.007262: step 75850, loss = 0.62 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:31:10.857054: step 75860, loss = 0.71 (1506.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:31:11.741995: step 75870, loss = 0.82 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:31:12.613482: step 75880, loss = 0.70 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:31:13.491883: step 75890, loss = 0.67 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:31:14.467416: step 75900, loss = 0.66 (1312.1 examples/sec; 0.098 sec/batch)
2017-06-02 04:31:15.253529: step 75910, loss = 0.67 (1628.3 examples/sec; 0.079 sec/batch)
2017-06-02 04:31:16.136790: step 75920, loss = 0.69 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:31:17.025872: step 75930, loss = 0.78 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:31:17.920494: step 75940, loss = 0.65 (1430.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:31:18.819392: step 75950, loss = 0.82 (1424.0 examples/sec; 0.090 sec/batch)
2017-06-02 04:31:19.699292: step 75960, loss = 0.62 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:31:20.586241: step 75970, loss = 0.72 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:31:21.460123: step 75980, loss = 0.70 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:31:22.338521: step 75990, loss = 0.68 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:31:23.322749: step 76000, loss = 0.68 (1300.5 examples/sec; 0.098 sec/batch)
2017-06-02 04:31:24.085524: step 76010, loss = 0.90 (1678.1 examples/sec; 0.076 sec/batch)
2017-06-02 04:31:24.961275: step 76020, loss = 0.89 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:31:25.815857: step 76030, loss = 0.72 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:31:26.696800: step 76040, loss = 0.71 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:31:27.564495: step 76050, loss = 0.89 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:31:28.443260: step 76060, loss = 0.80 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:31:29.297356: step 76070, loss = 0.69 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:31:30.178312: step 76080, loss = 0.78 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:31:31.028654: step 76090, loss = 0.77 (1505.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:31:31.996362: step 76100, loss = 0.74 (1322.6 examples/sec; 0.097 sec/batch)
2017-06-02 04:31:32.778575: step 76110, loss = 0.73 (1636.4 examples/sec; 0.078 sec/batch)
2017-06-02 04:31:33.642517: step 76120, loss = 0.83 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:31:34.505272: step 76130, loss = 0.65 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:31:35.367140: step 76140, loss = 0.78 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:31:36.257535: step 76150, loss = 0.70 (1437.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:31:37.146704: step 76160, loss = 0.92 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:31:37.996682: step 76170, loss = 0.83 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:31:38.865116: step 76180, loss = 0.81 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:31:39.739168: step 76190, loss = 0.77 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:31:40.705581: step 76200, loss = 0.73 (1324.5 examples/sec; 0.097 sec/batch)
2017-06-02 04:31:41.480488: step 76210, loss = 0.64 (1651.8 examples/sec; 0.077 sec/batch)
2017-06-02 04:31:42.340240: step 76220, loss = 0.66 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:31:43.192263: step 76230, loss = 0.88 (1502.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:31:44.043353: step 76240, loss = 0.73 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:31:44.904870: step 76250, loss = 0.69 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:31:45.753600: step 76260, loss = 0.74 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:31:46.600525: step 76270, loss = 0.72 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:31:47.469469: step 76280, loss = 0.68 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:31:48.329272: step 76290, loss = 0.73 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:31:49.307482: step 76300, loss = 0.67 (1308.5 examples/sec; 0.098 sec/batch)
2017-06-02 04:31:50.073082: step 76310, loss = 0.68 (1671.9 examples/sec; 0.077 sec/batch)
2017-06-02 04:31:50.928682: step 76320, loss = 0.51 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:31:51.795465: step 76330, loss = 0.80 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:31:52.642286: step 76340, loss = 0.85 (1511.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:31:53.498076: step 76350, loss = 0.63 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:31:54.368510: step 76360, loss = 0.62 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:31:55.211234: step 76370, loss = 0.83 (1518.9 examples/sec; 0.084 sec/batch)
2017-06-02 04:31:56.072420: step 76380, loss = 0.62 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:31:56.930968: step 76390, loss = 0.82 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:31:57.917211: step 76400, loss = 0.79 (1297.8 examples/sec; 0.099 sec/batch)
2017-06-02 04:31:58.685802: step 76410, loss = 0.72 (1665.4 examples/sec; 0.077 sec/batch)
2017-06-02 04:31:59.578718: step 76420, loss = 0.75 (1433.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:32:00.456802: step 76430, loss = 0.67 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:32:01.339529: step 76440, loss = 0.75 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:32:02.172763: step 76450, loss = 0.77 (1536.2 examples/sec; 0.083 sec/batch)
2017-06-02 04:32:03.010560: step 76460, loss = 0.79 (1527.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:32:03.881335: step 76470, loss = 0.79 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:32:04.765096: step 76480, loss = 0.83 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:32:05.617537: step 76490, loss = 0.73 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:32:06.567237: step 76500, loss = 0.77 (1347.8 examples/sec; 0.095 sec/batch)
2017-06-02 04:32:07.361387: step 76510, loss = 0.75 (1611.8 examples/sec; 0.079 sec/batch)
2017-06-02 04:32:08.214725: step 76520, loss = 0.67 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:32:09.085944: step 76530, loss = 0.67 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:32:09.955381: step 76540, loss = 0.77 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:32:10.820108: step 76550, loss = 0.74 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:32:11.680471: step 76560, loss = 0.60 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:32:12.547979: step 76570, loss = 0.71 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:32:13.388789: step 76580, loss = 0.86 (1522.4 examples/sec; 0.084 sec/batch)
2017-06-02 04:32:14.291060: step 76590, loss = 0.75 (1418.6 examples/sec; 0.090 sec/batch)
2017-06-02 04:32:15.287132: step 76600, loss = 0.73 (1285.1 examples/sec; 0.100 sec/batch)
2017-06-02 04:32:16.066854: step 76610, loss = 0.66 (1641.6 examples/sec; 0.078 sec/batch)
2017-06-02 04:32:16.949319: step 76620, loss = 0.74 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:32:17.814951: step 76630, loss = 0.79 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:32:18.681984: step 76640, loss = 0.79 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:32:19.545615: step 76650, loss = 0.70 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:32:20.398040: step 76660, loss = 0.70 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:32:21.253828: step 76670, loss = 0.71 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:32:22.121227: step 76680, loss = 0.67 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:32:22.966048: step 76690, loss = 0.72 (1515.1 examples/sec; 0.084 sec/batch)
2017-06-02 04:32:23.926984: step 76700, loss = 0.77 (1332.0 examples/sec; 0.096 sec/batch)
2017-06-02 04:32:24.693405: step 76710, loss = 0.73 (1670.1 examples/sec; 0.077 sec/batch)
2017-06-02 04:32:25.576176: step 76720, loss = 0.69 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:32:26.433471: step 76730, loss = 0.67 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:32:27.288678: step 76740, loss = 0.70 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:32:28.156356: step 76750, loss = 0.75 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:32:29.014466: step 76760, loss = 0.71 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:32:29.871656: step 76770, loss = 0.68 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:32:30.728497: step 76780, loss = 0.64 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:32:31.588688: step 76790, loss = 0.86 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:32:32.528353: step 76800, loss = 0.74 (1362.2 examples/sec; 0.094 sec/batch)
2017-06-02 04:32:33.296049: step 76810, loss = 0.67 (1667.4 examples/sec; 0.077 sec/batch)
2017-06-02 04:32:34.157517: step 76820, loss = 0.64 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:32:35.033840: step 76830, loss = 0.73 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:32:35.895605: step 76840, loss = 0.69 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:32:36.777303: step 76850, loss = 0.72 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:32:37.659082: step 76860, loss = 0.57 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:32:38.540513: step 76870, loss = 0.62 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:32:39.418728: step 76880, loss = 0.67 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:32:40.296358: step 76890, loss = 0.81 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:32:41.264050: step 76900, loss = 0.61 (1322.7 examples/sec; 0.097 sec/batch)
2017-06-02 04:32:42.042807: step 76910, loss = 0.75 (1643.6 examples/sec; 0.078 sec/batch)
2017-06-02 04:32:42.921135: step 76920, loss = 0.85 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:32:43.820813: step 76930, loss = 0.73 (1422.7 examples/sec; 0.090 sec/batch)
2017-06-02 04:32:44.671649: step 76940, loss = 0.74 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:32:45.538786: step 76950, loss = 0.85 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:32:46.400065: step 76960, loss = 0.69 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:32:47.276387: step 76970, loss = 0.71 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:32:48.138614: step 76980, loss = 0.69 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:32:48.994130: step 76990, loss = 0.69 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:32:49.941715: step 77000, loss = 0.62 (1350.8 examples/sec; 0.095 sec/batch)
2017-06-02 04:32:50.719365: step 77010, loss = 0.81 (1646.0 examples/sec; 0.078 sec/batch)
2017-06-02 04:32:51.572062: step 77020, loss = 0.71 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:32:52.424571: step 77030, loss = 0.69 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:32:53.265363: step 77040, loss = 0.64 (1522.4 examples/sec; 0.084 sec/batch)
2017-06-02 04:32:54.137699: step 77050, loss = 0.69 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:32:54.984402: step 77060, loss = 0.65 (1511.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:32:55.873613: step 77070, loss = 0.64 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:32:56.781426: step 77080, loss = 0.66 (1410.0 examples/sec; 0.091 sec/batch)
2017-06-02 04:32:57.674296: step 77090, loss = 0.59 (1433.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:32:58.625200: step 77100, loss = 0.80 (1346.1 examples/sec; 0.095 sec/batch)
2017-06-02 04:32:59.413024: step 77110, loss = 0.80 (1624.7 examples/sec; 0.079 sec/batch)
2017-06-02 04:33:00.270541: step 77120, loss = 0.69 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:33:01.143852: step 77130, loss = 0.72 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:33:02.034589: step 77140, loss = 0.76 (1437.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:33:02.926315: step 77150, loss = 0.71 (1435.4 examples/sec; 0.089 sec/batch)
2017-06-02 04:33:03.818302: step 77160, loss = 0.74 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:33:04.697852: step 77170, loss = 0.67 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:33:05.545660: step 77180, loss = 0.73 (1509.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:33:06.396418: step 77190, loss = 0.74 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:33:07.377353: step 77200, loss = 0.73 (1304.9 examples/sec; 0.098 sec/batch)
2017-06-02 04:33:08.127490: step 77210, loss = 0.84 (1706.3 examples/sec; 0.075 sec/batch)
2017-06-02 04:33:08.974135: step 77220, loss = 0.59 (1511.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:33:09.851500: step 77230, loss = 0.71 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:33:10.716853: step 77240, loss = 0.60 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:33:11.586765: step 77250, loss = 0.65 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:33:12.445817: step 77260, loss = 0.97 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:33:13.320577: step 77270, loss = 0.73 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:33:14.180468: step 77280, loss = 0.70 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:33:15.017046: step 77290, loss = 0.68 (1530.0 examples/sec; 0.084 sec/batch)
2017-06-02 04:33:15.981240: step 77300, loss = 0.70 (1327.5 examples/sec; 0.096 sec/batch)
2017-06-02 04:33:16.748669: step 77310, loss = 0.65 (1667.9 examples/sec; 0.077 sec/batch)
2017-06-02 04:33:17.595560: step 77320, loss = 0.78 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:33:18.467802: step 77330, loss = 0.83 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:33:19.359504: step 77340, loss = 0.66 (1435.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:33:20.226423: step 77350, loss = 0.73 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:33:21.071666: step 77360, loss = 0.59 (1514.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:33:21.908235: step 77370, loss = 0.63 (1530.1 examples/sec; 0.084 sec/batch)
2017-06-02 04:33:22.746924: step 77380, loss = 0.65 (1526.2 examples/sec; 0.084 sec/batch)
2017-06-02 04:33:23.615044: step 77390, loss = 0.64 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:33:24.556935: step 77400, loss = 0.85 (1359.0 examples/sec; 0.094 sec/batch)
2017-06-02 04:33:25.322058: step 77410, loss = 0.68 (1672.9 examples/sec; 0.077 sec/batch)
2017-06-02 04:33:26.175072: step 77420, loss = 0.76 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:33:27.041841: step 77430, loss = 0.68 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:33:27.925485: step 77440, loss = 0.66 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:33:28.780510: step 77450, loss = 0.81 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:33:29.624260: step 77460, loss = 0.71 (1517.0 examples/sec; 0.084 sec/batch)
2017-06-02 04:33:30.468828: step 77470, loss = 0.67 (1515.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:33:31.336835: step 77480, loss = 0.71 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:33:32.217899: step 77490, loss = 0.76 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:33:33.205009: step 77500, loss = 0.63 (1296.7 examples/sec; 0.099 sec/batch)
2017-06-02 04:33:33.941942: step 77510, loss = 0.62 (1736.9 examples/sec; 0.074 sec/batch)
2017-06-02 04:33:34.801195: step 77520, loss = 0.72 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:33:35.650543: step 77530, loss = 0.66 (1507.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:33:36.516300: step 77540, loss = 0.63 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:33:37.349604: step 77550, loss = 0.71 (1536.0 examples/sec; 0.083 sec/batch)
2017-06-02 04:33:38.197143: step 77560, loss = 0.80 (1510.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:33:39.044946: step 77570, loss = 0.63 (1509.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:33:39.909953: step 77580, loss = 0.75 (1479.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:33:40.767999: step 77590, loss = 0.62 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:33:41.720728: step 77600, loss = 0.74 (1343.5 examples/sec; 0.095 sec/batch)
2017-06-02 04:33:42.484025: step 77610, loss = 0.70 (1676.9 examples/sec; 0.076 sec/batch)
2017-06-02 04:33:43.332923: step 77620, loss = 0.80 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:33:44.191387: step 77630, loss = 0.66 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:33:45.029653: step 77640, loss = 0.79 (1527.0 examples/sec; 0.084 sec/batch)
2017-06-02 04:33:45.883846: step 77650, loss = 0.64 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:33:46.753598: step 77660, loss = 0.60 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:33:47.599336: step 77670, loss = 0.54 (1513.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:33:48.448588: step 77680, loss = 0.68 (1507.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:33:49.310125: step 77690, loss = 0.68 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:33:50.281542: step 77700, loss = 0.64 (1317.7 examples/sec; 0.097 sec/batch)
2017-06-02 04:33:50.992692: step 77710, loss = 0.75 (1799.9 examples/sec; 0.071 sec/batch)
2017-06-02 04:33:51.834501: step 77720, loss = 0.75 (1520.5 examples/sec; 0.084 sec/batch)
2017-06-02 04:33:52.697184: step 77730, loss = 0.72 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:33:53.551872: step 77740, loss = 0.86 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:33:54.419837: step 77750, loss = 0.77 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:33:55.298112: step 77760, loss = 0.65 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:33:56.165845: step 77770, loss = 0.72 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:33:57.040768: step 77780, loss = 0.60 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:33:57.914574: step 77790, loss = 0.55 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:33:58.865026: step 77800, loss = 0.72 (1346.7 examples/sec; 0.095 sec/batch)
2017-06-02 04:33:59.636997: step 77810, loss = 0.65 (1658.1 examples/sec; 0.077 sec/batch)
2017-06-02 04:34:00.478883: step 77820, loss = 0.69 (1520.4 examples/sec; 0.084 sec/batch)
2017-06-02 04:34:01.349560: step 77830, loss = 0.87 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:34:02.217474: step 77840, loss = 0.74 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:34:03.055646: step 77850, loss = 0.75 (1527.1 examples/sec; 0.084 sec/batch)
2017-06-02 04:34:03.911836: step 77860, loss = 0.74 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:34:04.783547: step 77870, loss = 0.76 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:34:05.636793: step 77880, loss = 0.77 (1500.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:34:06.505152: step 77890, loss = 0.65 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:34:07.468943: step 77900, loss = 0.84 (1328.1 examples/sec; 0.096 sec/batch)
2017-06-02 04:34:08.263011: step 77910, loss = 0.79 (1612.0 examples/sec; 0.079 sec/batch)
2017-06-02 04:34:09.163307: step 77920, loss = 0.61 (1421.8 examples/sec; 0.090 sec/batch)
2017-06-02 04:34:10.002253: step 77930, loss = 0.69 (1525.7 examples/sec; 0.084 sec/batch)
2017-06-02 04:34:10.895611: step 77940, loss = 0.67 (1432.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:34:11.780022: step 77950, loss = 0.72 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:34:12.647839: step 77960, loss = 0.79 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:34:13.536473: step 77970, loss = 0.71 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 04:34:14.407884: step 77980, loss = 0.69 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:34:15.266721: step 77990, loss = 0.87 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:34:16.246754: step 78000, loss = 0.73 (1306.1 examples/sec; 0.098 sec/batch)
2017-06-02 04:34:17.046642: step 78010, loss = 0.75 (1600.2 examples/sec; 0.080 sec/batch)
2017-06-02 04:34:17.930282: step 78020, loss = 0.81 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:34:18.807353: step 78030, loss = 0.62 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:34:19.655966: step 78040, loss = 0.80 (1508.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:34:20.517127: step 78050, loss = 0.70 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:34:21.385159: step 78060, loss = 0.72 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:34:22.252122: step 78070, loss = 0.76 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:34:23.134114: step 78080, loss = 0.61 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:34:24.013460: step 78090, loss = 0.85 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:34:24.998687: step 78100, loss = 0.64 (1299.2 examples/sec; 0.099 sec/batch)
2017-06-02 04:34:25.781506: step 78110, loss = 0.55 (1635.1 examples/sec; 0.078 sec/batch)
2017-06-02 04:34:26.669683: step 78120, loss = 0.56 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 04:34:27.554680: step 78130, loss = 0.74 (1446.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:34:28.443217: step 78140, loss = 0.80 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:34:29.340264: step 78150, loss = 0.58 (1426.9 examples/sec; 0.090 sec/batch)
2017-06-02 04:34:30.220267: step 78160, loss = 0.87 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:34:31.097432: step 78170, loss = 0.59 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:34:31.969400: step 78180, loss = 0.67 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:34:32.845630: step 78190, loss = 0.58 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:34:33.842920: step 78200, loss = 0.79 (1283.5 examples/sec; 0.100 sec/batch)
2017-06-02 04:34:34.619214: step 78210, loss = 0.81 (1648.9 examples/sec; 0.078 sec/batch)
2017-06-02 04:34:35.490627: step 78220, loss = 0.74 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:34:36.361943: step 78230, loss = 0.70 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:34:37.228355: step 78240, loss = 0.84 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:34:38.094901: step 78250, loss = 0.65 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:34:38.929587: step 78260, loss = 0.81 (1533.5 examples/sec; 0.083 sec/batch)
2017-06-02 04:34:39.763398: step 78270, loss = 0.68 (1535.1 examples/sec; 0.083 sec/batch)
2017-06-02 04:34:40.633032: step 78280, loss = 0.65 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:34:41.517523: step 78290, loss = 0.66 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:34:42.468595: step 78300, loss = 0.91 (1345.8 examples/sec; 0.095 sec/batch)
2017-06-02 04:34:43.258875: step 78310, loss = 0.82 (1619.7 examples/sec; 0.079 sec/batch)
2017-06-02 04:34:44.122154: step 78320, loss = 0.79 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:34:44.984946: step 78330, loss = 0.87 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:34:45.847385: step 78340, loss = 0.60 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:34:46.718914: step 78350, loss = 0.89 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:34:47.560476: step 78360, loss = 0.78 (1521.0 examples/sec; 0.084 sec/batch)
2017-06-02 04:34:48.420699: step 78370, loss = 0.66 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:34:49.260533: step 78380, loss = 0.82 (1524.1 examples/sec; 0.084 sec/batch)
2017-06-02 04:34:50.115635: step 78390, loss = 0.67 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:34:51.093390: step 78400, loss = 0.81 (1309.1 examples/sec; 0.098 sec/batch)
2017-06-02 04:34:51.852800: step 78410, loss = 0.77 (1685.5 examples/sec; 0.076 sec/batch)
2017-06-02 04:34:52.714912: step 78420, loss = 0.76 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:34:53.574397: step 78430, loss = 0.73 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:34:54.455373: step 78440, loss = 0.60 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:34:55.339377: step 78450, loss = 0.61 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:34:56.208031: step 78460, loss = 0.75 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:34:57.089018: step 78470, loss = 0.75 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:34:57.926429: step 78480, loss = 0.69 (1528.5 examples/sec; 0.084 sec/batch)
2017-06-02 04:34:58.807227: step 78490, loss = 0.80 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:34:59.769042: step 78500, loss = 0.79 (1330.8 examples/sec; 0.096 sec/batch)
2017-06-02 04:35:00.550496: step 78510, loss = 0.69 (1638.0 examples/sec; 0.078 sec/batch)
2017-06-02 04:35:01.439073: step 78520, loss = 0.84 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:35:02.310202: step 78530, loss = 0.66 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:35:03.179577: step 78540, loss = 0.71 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:35:04.058848: step 78550, loss = 0.69 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:35:04.928559: step 78560, loss = 0.70 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:35:05.807830: step 78570, loss = 0.79 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:35:06.681003: step 78580, loss = 0.65 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:35:07.539606: step 78590, loss = 0.70 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:35:08.511753: step 78600, loss = 0.65 (1316.7 examples/sec; 0.097 sec/batch)
2017-06-02 04:35:09.288351: step 78610, loss = 0.62 (1648.2 examples/sec; 0.078 sec/batch)
2017-06-02 04:35:10.161037: step 78620, loss = 0.76 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:35:11.001610: step 78630, loss = 0.69 (1522.9 examples/sec; 0.084 sec/batch)
2017-06-02 04:35:11.881904: step 78640, loss = 0.77 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:35:12.743989: step 78650, loss = 0.65 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:35:13.609382: step 78660, loss = 0.81 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:35:14.473827: step 78670, loss = 0.68 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:35:15.351141: step 78680, loss = 0.75 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:35:16.223529: step 78690, loss = 0.72 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:35:17.250256: step 78700, loss = 0.73 (1246.7 examples/sec; 0.103 sec/batch)
2017-06-02 04:35:17.972628: step 78710, loss = 0.77 (1772.0 examples/sec; 0.072 sec/batch)
2017-06-02 04:35:18.849399: step 78720, loss = 0.71 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:35:19.710166: step 78730, loss = 0.58 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:35:20.583711: step 78740, loss = 0.65 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:35:21.429827: step 78750, loss = 0.74 (1512.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:35:22.295842: step 78760, loss = 0.74 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:35:23.137628: step 78770, loss = 0.77 (1520.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:35:24.007131: step 78780, loss = 0.72 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:35:24.871983: step 78790, loss = 0.64 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:35:25.847705: step 78800, loss = 0.73 (1311.9 examples/sec; 0.098 sec/batch)
2017-06-02 04:35:26.564151: step 78810, loss = 0.77 (1786.6 examples/sec; 0.072 sec/batch)
2017-06-02 04:35:27.420530: step 78820, loss = 0.61 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:35:28.294022: step 78830, loss = 0.75 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:35:29.154658: step 78840, loss = 0.76 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:35:30.036805: step 78850, loss = 0.70 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:35:30.904760: step 78860, loss = 0.85 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:35:31.756301: step 78870, loss = 0.60 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:35:32.636312: step 78880, loss = 0.56 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:35:33.491986: step 78890, loss = 0.73 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:35:34.512360: step 78900, loss = 0.57 (1254.4 examples/sec; 0.102 sec/batch)
2017-06-02 04:35:35.254390: step 78910, loss = 0.88 (1725.0 examples/sec; 0.074 sec/batch)
2017-06-02 04:35:36.099227: step 78920, loss = 0.71 (1515.1 examples/sec; 0.084 sec/batch)
2017-06-02 04:35:36.964242: step 78930, loss = 0.76 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:35:37.837360: step 78940, loss = 0.77 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:35:38.689663: step 78950, loss = 0.76 (1501.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:35:39.535192: step 78960, loss = 0.77 (1513.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:35:40.400994: step 78970, loss = 0.61 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:35:41.237584: step 78980, loss = 0.76 (1530.0 examples/sec; 0.084 sec/batch)
2017-06-02 04:35:42.088768: step 78990, loss = 0.62 (1503.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:35:43.046196: step 79000, loss = 0.74 (1336.9 examples/sec; 0.096 sec/batch)
2017-06-02 04:35:43.793742: step 79010, loss = 0.67 (1712.3 examples/sec; 0.075 sec/batch)
2017-06-02 04:35:44.642461: step 79020, loss = 0.63 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:35:45.503278: step 79030, loss = 0.70 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:35:46.373297: step 79040, loss = 0.65 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:35:47.224670: step 79050, loss = 0.69 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:35:48.076809: step 79060, loss = 0.70 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:35:48.935436: step 79070, loss = 0.63 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:35:49.778201: step 79080, loss = 0.81 (1518.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:35:50.646948: step 79090, loss = 0.68 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:35:51.604396: step 79100, loss = 0.86 (1336.9 examples/sec; 0.096 sec/batch)
2017-06-02 04:35:52.365298: step 79110, loss = 0.71 (1682.2 examples/sec; 0.076 sec/batch)
2017-06-02 04:35:53.222847: step 79120, loss = 0.74 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:35:54.053933: step 79130, loss = 0.88 (1540.1 examples/sec; 0.083 sec/batch)
2017-06-02 04:35:54.905163: step 79140, loss = 0.79 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:35:55.747612: step 79150, loss = 0.64 (1519.4 examples/sec; 0.084 sec/batch)
2017-06-02 04:35:56.630304: step 79160, loss = 0.83 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:35:57.471416: step 79170, loss = 0.80 (1521.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:35:58.336252: step 79180, loss = 0.73 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:35:59.221597: step 79190, loss = 0.61 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:36:00.169411: step 79200, loss = 0.75 (1350.5 examples/sec; 0.095 sec/batch)
2017-06-02 04:36:00.940680: step 79210, loss = 0.59 (1659.6 examples/sec; 0.077 sec/batch)
2017-06-02 04:36:01.811076: step 79220, loss = 0.80 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:36:02.660623: step 79230, loss = 0.65 (1506.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:36:03.494999: step 79240, loss = 0.75 (1534.1 examples/sec; 0.083 sec/batch)
2017-06-02 04:36:04.339839: step 79250, loss = 0.54 (1515.1 examples/sec; 0.084 sec/batch)
2017-06-02 04:36:05.189773: step 79260, loss = 0.73 (1506.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:36:06.068675: step 79270, loss = 0.65 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:36:06.936652: step 79280, loss = 0.72 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:36:07.781325: step 79290, loss = 0.81 (1515.4 examples/sec; 0.084 sec/batch)
2017-06-02 04:36:08.745225: step 79300, loss = 0.71 (1327.9 examples/sec; 0.096 sec/batch)
2017-06-02 04:36:09.526756: step 79310, loss = 0.74 (1637.8 examples/sec; 0.078 sec/batch)
2017-06-02 04:36:10.397469: step 79320, loss = 0.79 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:36:11.292048: step 79330, loss = 0.73 (1430.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:36:12.146001: step 79340, loss = 0.64 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:36:13.047375: step 79350, loss = 0.68 (1420.0 examples/sec; 0.090 sec/batch)
2017-06-02 04:36:13.928898: step 79360, loss = 0.64 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:36:14.821518: step 79370, loss = 0.67 (1434.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:36:15.710673: step 79380, loss = 0.65 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:36:16.564787: step 79390, loss = 0.69 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:36:17.529453: step 79400, loss = 0.75 (1326.9 examples/sec; 0.096 sec/batch)
2017-06-02 04:36:18.306339: step 79410, loss = 0.63 (1647.6 examples/sec; 0.078 sec/batch)
2017-06-02 04:36:19.181375: step 79420, loss = 0.72 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:36:20.050796: step 79430, loss = 0.70 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:36:20.940819: step 79440, loss = 0.80 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 04:36:21.818868: step 79450, loss = 0.64 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:36:22.686967: step 79460, loss = 0.79 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:36:23.561850: step 79470, loss = 0.67 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:36:24.448550: step 79480, loss = 0.78 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:36:25.332667: step 79490, loss = 0.81 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:36:26.311637: step 79500, loss = 0.66 (1307.5 examples/sec; 0.098 sec/batch)
2017-06-02 04:36:27.097171: step 79510, loss = 0.77 (1629.5 examples/sec; 0.079 sec/batch)
2017-06-02 04:36:27.980766: step 79520, loss = 0.73 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:36:28.868505: step 79530, loss = 0.69 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:36:29.724744: step 79540, loss = 0.71 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:36:30.582125: step 79550, loss = 0.69 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:36:31.452479: step 79560, loss = 0.74 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:36:32.318819: step 79570, loss = 0.70 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:36:33.186396: step 79580, loss = 0.72 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:36:34.079016: step 79590, loss = 0.64 (1434.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:36:35.078419: step 79600, loss = 0.76 (1280.8 examples/sec; 0.100 sec/batch)
2017-06-02 04:36:35.810623: step 79610, loss = 0.73 (1748.1 examples/sec; 0.073 sec/batch)
2017-06-02 04:36:36.670581: step 79620, loss = 0.64 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:36:37.536386: step 79630, loss = 0.68 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:36:38.411810: step 79640, loss = 0.73 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:36:39.264614: step 79650, loss = 0.55 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:36:40.127627: step 79660, loss = 0.76 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:36:41.015571: step 79670, loss = 0.72 (1441.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:36:41.868671: step 79680, loss = 0.85 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:36:42.715987: step 79690, loss = 0.86 (1510.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:36:43.712435: step 79700, loss = 0.80 (1284.6 examples/sec; 0.100 sec/batch)
2017-06-02 04:36:44.444013: step 79710, loss = 0.75 (1749.7 examples/sec; 0.073 sec/batch)
2017-06-02 04:36:45.287305: step 79720, loss = 0.74 (1517.9 examples/sec; 0.084 sec/batch)
2017-06-02 04:36:46.137843: step 79730, loss = 0.70 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:36:46.972841: step 79740, loss = 0.87 (1532.9 examples/sec; 0.084 sec/batch)
2017-06-02 04:36:47.820907: step 79750, loss = 0.66 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:36:48.681992: step 79760, loss = 0.76 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:36:49.542626: step 79770, loss = 0.72 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:36:50.379919: step 79780, loss = 0.78 (1528.7 examples/sec; 0.084 sec/batch)
2017-06-02 04:36:51.241832: step 79790, loss = 0.72 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:36:52.258830: step 79800, loss = 0.79 (1258.6 examples/sec; 0.102 sec/batch)
2017-06-02 04:36:52.959749: step 79810, loss = 1.03 (1826.2 examples/sec; 0.070 sec/batch)
2017-06-02 04:36:53.822073: step 79820, loss = 0.66 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:36:54.692391: step 79830, loss = 0.77 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:36:55.549533: step 79840, loss = 0.72 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:36:56.426519: step 79850, loss = 0.73 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:36:57.321745: step 79860, loss = 0.68 (1429.8 examples/sec; 0.090 sec/batch)
2017-06-02 04:36:58.182485: step 79870, loss = 0.84 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:36:59.006786: step 79880, loss = 0.59 (1552.8 examples/sec; 0.082 sec/batch)
2017-06-02 04:36:59.859299: step 79890, loss = 0.73 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:37:00.827582: step 79900, loss = 0.78 (1321.9 examples/sec; 0.097 sec/batch)
2017-06-02 04:37:01.597059: step 79910, loss = 0.77 (1663.5 examples/sec; 0.077 sec/batch)
2017-06-02 04:37:02.464187: step 79920, loss = 0.76 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:37:03.335865: step 79930, loss = 0.66 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:37:04.190050: step 79940, loss = 0.67 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:37:05.073359: step 79950, loss = 0.69 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:37:05.936243: step 79960, loss = 0.61 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:37:06.785604: step 79970, loss = 0.73 (1507.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:37:07.654091: step 79980, loss = 0.92 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:37:08.528920: step 79990, loss = 0.69 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:37:09.497086: step 80000, loss = 0.73 (1322.1 examples/sec; 0.097 sec/batch)
2017-06-02 04:37:10.270986: step 80010, loss = 0.68 (1653.9 examples/sec; 0.077 sec/batch)
2017-06-02 04:37:11.133377: step 80020, loss = 0.86 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:37:12.016344: step 80030, loss = 0.82 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:37:12.901230: step 80040, loss = 0.78 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:37:13.801584: step 80050, loss = 0.66 (1421.7 examples/sec; 0.090 sec/batch)
2017-06-02 04:37:14.661014: step 80060, loss = 0.83 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:37:15.497681: step 80070, loss = 0.58 (1529.9 examples/sec; 0.084 sec/batch)
2017-06-02 04:37:16.394996: step 80080, loss = 0.78 (1426.5 examples/sec; 0.090 sec/batch)
2017-06-02 04:37:17.243151: step 80090, loss = 0.65 (1509.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:37:18.196270: step 80100, loss = 0.70 (1343.0 examples/sec; 0.095 sec/batch)
2017-06-02 04:37:18.982476: step 80110, loss = 0.71 (1628.1 examples/sec; 0.079 sec/batch)
2017-06-02 04:37:19.840676: step 80120, loss = 0.66 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:37:20.713673: step 80130, loss = 0.73 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:37:21.549704: step 80140, loss = 0.75 (1531.0 examples/sec; 0.084 sec/batch)
2017-06-02 04:37:22.395670: step 80150, loss = 0.65 (1513.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:37:23.237144: step 80160, loss = 0.66 (1521.1 examples/sec; 0.084 sec/batch)
2017-06-02 04:37:24.090956: step 80170, loss = 0.72 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:37:24.939904: step 80180, loss = 0.77 (1507.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:37:25.833042: step 80190, loss = 0.61 (1433.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:37:26.788765: step 80200, loss = 0.66 (1339.3 examples/sec; 0.096 sec/batch)
2017-06-02 04:37:27.589305: step 80210, loss = 0.62 (1598.9 examples/sec; 0.080 sec/batch)
2017-06-02 04:37:28.463674: step 80220, loss = 0.69 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:37:29.322160: step 80230, loss = 0.59 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:37:30.186505: step 80240, loss = 0.64 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:37:31.063404: step 80250, loss = 0.72 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:37:31.901764: step 80260, loss = 0.63 (1526.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:37:32.755354: step 80270, loss = 0.66 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:37:33.617298: step 80280, loss = 0.86 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:37:34.489983: step 80290, loss = 0.74 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:37:35.444025: step 80300, loss = 0.70 (1341.7 examples/sec; 0.095 sec/batch)
2017-06-02 04:37:36.230181: step 80310, loss = 0.69 (1628.2 examples/sec; 0.079 sec/batch)
2017-06-02 04:37:37.080493: step 80320, loss = 0.80 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:37:37.940167: step 80330, loss = 0.58 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:37:38.823407: step 80340, loss = 0.85 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:37:39.665161: step 80350, loss = 0.85 (1520.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:37:40.527337: step 80360, loss = 0.85 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:37:41.361130: step 80370, loss = 0.79 (1535.1 examples/sec; 0.083 sec/batch)
2017-06-02 04:37:42.208498: step 80380, loss = 0.67 (1510.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:37:43.046229: step 80390, loss = 0.60 (1527.9 examples/sec; 0.084 sec/batch)
2017-06-02 04:37:43.986386: step 80400, loss = 0.71 (1361.5 examples/sec; 0.094 sec/batch)
2017-06-02 04:37:44.777883: step 80410, loss = 0.66 (1617.2 examples/sec; 0.079 sec/batch)
2017-06-02 04:37:45.622954: step 80420, loss = 0.57 (1514.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:37:46.466665: step 80430, loss = 0.76 (1517.1 examples/sec; 0.084 sec/batch)
2017-06-02 04:37:47.334523: step 80440, loss = 0.71 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:37:48.185001: step 80450, loss = 0.69 (1505.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:37:49.006347: step 80460, loss = 0.58 (1558.4 examples/sec; 0.082 sec/batch)
2017-06-02 04:37:49.855534: step 80470, loss = 0.56 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:37:50.704673: step 80480, loss = 0.62 (1507.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:37:51.569771: step 80490, loss = 0.66 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:37:52.527772: step 80500, loss = 0.70 (1336.1 examples/sec; 0.096 sec/batch)
2017-06-02 04:37:53.318555: step 80510, loss = 0.68 (1618.7 examples/sec; 0.079 sec/batch)
2017-06-02 04:37:54.200674: step 80520, loss = 0.56 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:37:55.054268: step 80530, loss = 0.65 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:37:55.915182: step 80540, loss = 0.59 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:37:56.776529: step 80550, loss = 0.59 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:37:57.637118: step 80560, loss = 0.67 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:37:58.487690: step 80570, loss = 0.82 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:37:59.357096: step 80580, loss = 0.71 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:38:00.212824: step 80590, loss = 0.65 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:38:01.148333: step 80600, loss = 0.73 (1368.2 examples/sec; 0.094 sec/batch)
2017-06-02 04:38:01.911963: step 80610, loss = 0.53 (1676.2 examples/sec; 0.076 sec/batch)
2017-06-02 04:38:02.764814: step 80620, loss = 0.68 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:38:03.630495: step 80630, loss = 0.81 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:38:04.476179: step 80640, loss = 0.67 (1513.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:38:05.349227: step 80650, loss = 0.72 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:38:06.199754: step 80660, loss = 0.76 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:38:07.074986: step 80670, loss = 0.71 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:38:07.941067: step 80680, loss = 0.97 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:38:08.798282: step 80690, loss = 0.65 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:38:09.838143: step 80700, loss = 0.68 (1230.9 examples/sec; 0.104 sec/batch)
2017-06-02 04:38:10.553407: step 80710, loss = 0.87 (1789.6 examples/sec; 0.072 sec/batch)
2017-06-02 04:38:11.432125: step 80720, loss = 0.77 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:38:12.271068: step 80730, loss = 0.65 (1525.7 examples/sec; 0.084 sec/batch)
2017-06-02 04:38:13.129378: step 80740, loss = 0.74 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:38:13.983627: step 80750, loss = 0.71 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:38:14.846860: step 80760, loss = 0.81 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:38:15.695261: step 80770, loss = 0.70 (1508.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:38:16.575504: step 80780, loss = 0.74 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:38:17.390011: step 80790, loss = 0.69 (1571.5 examples/sec; 0.081 sec/batch)
2017-06-02 04:38:18.343606: step 80800, loss = 0.73 (1342.3 examples/sec; 0.095 sec/batch)
2017-06-02 04:38:19.123069: step 80810, loss = 0.68 (1642.2 examples/sec; 0.078 sec/batch)
2017-06-02 04:38:19.984910: step 80820, loss = 0.66 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:38:20.841746: step 80830, loss = 0.75 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:38:21.693382: step 80840, loss = 0.72 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:38:22.572175: step 80850, loss = 0.75 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:38:23.441747: step 80860, loss = 0.74 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:38:24.307391: step 80870, loss = 0.62 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:38:25.193293: step 80880, loss = 0.71 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:38:26.064414: step 80890, loss = 0.64 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:38:27.016147: step 80900, loss = 0.68 (1344.9 examples/sec; 0.095 sec/batch)
2017-06-02 04:38:27.778887: step 80910, loss = 0.91 (1678.2 examples/sec; 0.076 sec/batch)
2017-06-02 04:38:28.642973: step 80920, loss = 0.87 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:38:29.494277: step 80930, loss = 0.84 (1503.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:38:30.330614: step 80940, loss = 0.75 (1530.5 examples/sec; 0.084 sec/batch)
2017-06-02 04:38:31.208441: step 80950, loss = 0.76 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:38:32.054069: step 80960, loss = 0.86 (1513.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:38:32.896828: step 80970, loss = 0.66 (1518.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:38:33.757651: step 80980, loss = 0.85 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:38:34.621051: step 80990, loss = 0.64 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:38:35.589668: step 81000, loss = 0.59 (1321.5 examples/sec; 0.097 sec/batch)
2017-06-02 04:38:36.338007: step 81010, loss = 0.48 (1710.5 examples/sec; 0.075 sec/batch)
2017-06-02 04:38:37.195388: step 81020, loss = 0.77 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:38:38.055540: step 81030, loss = 0.82 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:38:38.906380: step 81040, loss = 0.84 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:38:39.794170: step 81050, loss = 0.64 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:38:40.688033: step 81060, loss = 0.73 (1432.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:38:41.551433: step 81070, loss = 0.75 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:38:42.406425: step 81080, loss = 0.72 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:38:43.251637: step 81090, loss = 0.68 (1514.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:38:44.218337: step 81100, loss = 0.68 (1324.1 examples/sec; 0.097 sec/batch)
2017-06-02 04:38:44.985079: step 81110, loss = 0.73 (1669.4 examples/sec; 0.077 sec/batch)
2017-06-02 04:38:45.881721: step 81120, loss = 0.69 (1427.5 examples/sec; 0.090 sec/batch)
2017-06-02 04:38:46.749796: step 81130, loss = 0.66 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:38:47.620230: step 81140, loss = 0.66 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:38:48.501309: step 81150, loss = 0.88 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:38:49.371919: step 81160, loss = 0.62 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:38:50.218959: step 81170, loss = 0.83 (1511.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:38:51.057152: step 81180, loss = 0.69 (1527.1 examples/sec; 0.084 sec/batch)
2017-06-02 04:38:51.934971: step 81190, loss = 0.83 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:38:52.974060: step 81200, loss = 0.70 (1231.8 examples/sec; 0.104 sec/batch)
2017-06-02 04:38:53.700704: step 81210, loss = 0.66 (1761.5 examples/sec; 0.073 sec/batch)
2017-06-02 04:38:54.576468: step 81220, loss = 0.86 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:38:55.453633: step 81230, loss = 0.86 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:38:56.317163: step 81240, loss = 0.80 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:38:57.204532: step 81250, loss = 0.57 (1442.4 examples/sec; 0.089 sec/batch)
2017-06-02 04:38:58.092029: step 81260, loss = 0.77 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:38:58.971532: step 81270, loss = 0.74 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:38:59.852161: step 81280, loss = 0.71 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:39:00.728560: step 81290, loss = 0.85 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:39:01.704925: step 81300, loss = 0.68 (1311.0 examples/sec; 0.098 sec/batch)
2017-06-02 04:39:02.490252: step 81310, loss = 0.67 (1629.9 examples/sec; 0.079 sec/batch)
2017-06-02 04:39:03.377635: step 81320, loss = 0.76 (1442.4 examples/sec; 0.089 sec/batch)
2017-06-02 04:39:04.256701: step 81330, loss = 0.83 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:39:05.125704: step 81340, loss = 0.63 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:39:06.004826: step 81350, loss = 0.74 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:39:06.881431: step 81360, loss = 0.64 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:39:07.758497: step 81370, loss = 0.76 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:39:08.666173: step 81380, loss = 0.68 (1410.2 examples/sec; 0.091 sec/batch)
2017-06-02 04:39:09.540154: step 81390, loss = 0.71 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:39:10.507141: step 81400, loss = 0.64 (1323.7 examples/sec; 0.097 sec/batch)
2017-06-02 04:39:11.276694: step 81410, loss = 0.73 (1663.3 examples/sec; 0.077 sec/batch)
2017-06-02 04:39:12.129842: step 81420, loss = 0.63 (1500.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:39:13.001800: step 81430, loss = 0.91 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:39:13.865606: step 81440, loss = 0.75 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:39:14.733536: step 81450, loss = 0.81 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:39:15.595389: step 81460, loss = 0.64 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:39:16.453170: step 81470, loss = 0.70 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:39:17.300067: step 81480, loss = 0.73 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:39:18.148111: step 81490, loss = 0.75 (1509.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:39:19.138816: step 81500, loss = 0.66 (1292.0 examples/sec; 0.099 sec/batch)
2017-06-02 04:39:19.911214: step 81510, loss = 0.65 (1657.2 examples/sec; 0.077 sec/batch)
2017-06-02 04:39:20.778659: step 81520, loss = 0.83 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:39:21.658837: step 81530, loss = 0.72 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:39:22.527356: step 81540, loss = 0.72 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:39:23.404686: step 81550, loss = 0.76 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:39:24.273359: step 81560, loss = 0.75 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:39:25.147030: step 81570, loss = 0.71 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:39:26.025987: step 81580, loss = 0.79 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:39:26.872219: step 81590, loss = 0.62 (1512.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:39:27.822655: step 81600, loss = 0.73 (1346.7 examples/sec; 0.095 sec/batch)
2017-06-02 04:39:28.580535: step 81610, loss = 0.60 (1688.9 examples/sec; 0.076 sec/batch)
2017-06-02 04:39:29.428741: step 81620, loss = 0.77 (1509.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:39:30.290510: step 81630, loss = 0.75 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:39:31.124656: step 81640, loss = 0.77 (1534.5 examples/sec; 0.083 sec/batch)
2017-06-02 04:39:31.970893: step 81650, loss = 0.61 (1512.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:39:32.816757: step 81660, loss = 0.75 (1513.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:39:33.691984: step 81670, loss = 0.63 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:39:34.553974: step 81680, loss = 0.69 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:39:35.431008: step 81690, loss = 0.67 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:39:36.414863: step 81700, loss = 0.61 (1300.9 examples/sec; 0.098 sec/batch)
2017-06-02 04:39:37.188202: step 81710, loss = 0.69 (1655.2 examples/sec; 0.077 sec/batch)
2017-06-02 04:39:38.043160: step 81720, loss = 0.66 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:39:38.911554: step 81730, loss = 0.81 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:39:39.791084: step 81740, loss = 0.79 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:39:40.674708: step 81750, loss = 0.65 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:39:41.514718: step 81760, loss = 0.64 (1523.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:39:42.365670: step 81770, loss = 0.69 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:39:43.216206: step 81780, loss = 0.84 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:39:44.085986: step 81790, loss = 0.79 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:39:45.034806: step 81800, loss = 0.60 (1349.0 examples/sec; 0.095 sec/batch)
2017-06-02 04:39:45.810769: step 81810, loss = 0.67 (1649.6 examples/sec; 0.078 sec/batch)
2017-06-02 04:39:46.661638: step 81820, loss = 0.76 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:39:47.530921: step 81830, loss = 0.72 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:39:48.368235: step 81840, loss = 0.68 (1528.7 examples/sec; 0.084 sec/batch)
2017-06-02 04:39:49.217457: step 81850, loss = 0.77 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:39:50.072425: step 81860, loss = 0.65 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:39:50.914179: step 81870, loss = 0.80 (1520.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:39:51.835394: step 81880, loss = 0.76 (1389.5 examples/sec; 0.092 sec/batch)
2017-06-02 04:39:52.710206: step 81890, loss = 0.74 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:39:53.673762: step 81900, loss = 0.87 (1328.4 examples/sec; 0.096 sec/batch)
2017-06-02 04:39:54.440784: step 81910, loss = 0.68 (1668.8 examples/sec; 0.077 sec/batch)
2017-06-02 04:39:55.279273: step 81920, loss = 0.64 (1526.5 examples/sec; 0.084 sec/batch)
2017-06-02 04:39:56.159617: step 81930, loss = 0.77 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:39:57.018827: step 81940, loss = 0.70 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:39:57.864201: step 81950, loss = 0.66 (1514.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:39:58.710226: step 81960, loss = 0.67 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:39:59.556947: step 81970, loss = 0.67 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:40:00.392271: step 81980, loss = 0.72 (1532.3 examples/sec; 0.084 sec/batch)
2017-06-02 04:40:01.219229: step 81990, loss = 0.74 (1547.8 examples/sec; 0.083 sec/batch)
2017-06-02 04:40:02.177930: step 82000, loss = 0.79 (1335.1 examples/sec; 0.096 sec/batch)
2017-06-02 04:40:02.937117: step 82010, loss = 0.79 (1686.0 examples/sec; 0.076 sec/batch)
2017-06-02 04:40:03.796281: step 82020, loss = 0.97 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:40:04.648184: step 82030, loss = 0.83 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:40:05.525158: step 82040, loss = 0.73 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:40:06.383766: step 82050, loss = 0.67 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:40:07.237184: step 82060, loss = 0.81 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:40:08.092939: step 82070, loss = 0.64 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:40:08.928068: step 82080, loss = 0.76 (1532.7 examples/sec; 0.084 sec/batch)
2017-06-02 04:40:09.813304: step 82090, loss = 0.66 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:40:10.744805: step 82100, loss = 0.63 (1374.1 examples/sec; 0.093 sec/batch)
2017-06-02 04:40:11.509800: step 82110, loss = 0.62 (1673.2 examples/sec; 0.076 sec/batch)
2017-06-02 04:40:12.381367: step 82120, loss = 0.93 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:40:13.244215: step 82130, loss = 0.71 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:40:14.092723: step 82140, loss = 0.71 (1508.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:40:14.965434: step 82150, loss = 0.75 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:40:15.820022: step 82160, loss = 0.64 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:40:16.684224: step 82170, loss = 0.79 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:40:17.539746: step 82180, loss = 0.67 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:40:18.432981: step 82190, loss = 0.79 (1433.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:40:19.378918: step 82200, loss = 0.73 (1353.1 examples/sec; 0.095 sec/batch)
2017-06-02 04:40:20.157541: step 82210, loss = 0.66 (1643.9 examples/sec; 0.078 sec/batch)
2017-06-02 04:40:21.021452: step 82220, loss = 0.76 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:40:21.873005: step 82230, loss = 0.69 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:40:22.722261: step 82240, loss = 0.64 (1507.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:40:23.588640: step 82250, loss = 0.82 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:40:24.469346: step 82260, loss = 0.63 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:40:25.343505: step 82270, loss = 0.68 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:40:26.188926: step 82280, loss = 0.68 (1514.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:40:27.053686: step 82290, loss = 0.90 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:40:28.014498: step 82300, loss = 0.67 (1332.2 examples/sec; 0.096 sec/batch)
2017-06-02 04:40:28.746330: step 82310, loss = 0.82 (1749.0 examples/sec; 0.073 sec/batch)
2017-06-02 04:40:29.590635: step 82320, loss = 0.69 (1516.0 examples/sec; 0.084 sec/batch)
2017-06-02 04:40:30.438672: step 82330, loss = 0.70 (1509.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:40:31.309631: step 82340, loss = 0.69 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:40:32.147481: step 82350, loss = 0.69 (1527.7 examples/sec; 0.084 sec/batch)
2017-06-02 04:40:33.004179: step 82360, loss = 0.61 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:40:33.850945: step 82370, loss = 0.74 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:40:34.706235: step 82380, loss = 0.64 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:40:35.575224: step 82390, loss = 0.64 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:40:36.588202: step 82400, loss = 0.66 (1263.6 examples/sec; 0.101 sec/batch)
2017-06-02 04:40:37.348976: step 82410, loss = 0.77 (1682.5 examples/sec; 0.076 sec/batch)
2017-06-02 04:40:38.232289: step 82420, loss = 0.57 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:40:39.116600: step 82430, loss = 0.60 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:40:39.978393: step 82440, loss = 0.65 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:40:40.849829: step 82450, loss = 0.62 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:40:41.697656: step 82460, loss = 0.80 (1509.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:40:42.534661: step 82470, loss = 0.60 (1529.2 examples/sec; 0.084 sec/batch)
2017-06-02 04:40:43.395264: step 82480, loss = 0.61 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:40:44.232677: step 82490, loss = 0.77 (1528.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:40:45.208723: step 82500, loss = 0.69 (1311.3 examples/sec; 0.098 sec/batch)
2017-06-02 04:40:45.956081: step 82510, loss = 0.57 (1712.7 examples/sec; 0.075 sec/batch)
2017-06-02 04:40:46.798980: step 82520, loss = 0.76 (1518.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:40:47.649595: step 82530, loss = 0.90 (1504.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:40:48.509946: step 82540, loss = 0.60 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:40:49.402201: step 82550, loss = 0.69 (1434.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:40:50.274018: step 82560, loss = 0.66 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:40:51.139674: step 82570, loss = 0.70 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:40:52.022002: step 82580, loss = 0.78 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:40:52.891288: step 82590, loss = 0.79 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:40:53.871204: step 82600, loss = 0.74 (1306.2 examples/sec; 0.098 sec/batch)
2017-06-02 04:40:54.635907: step 82610, loss = 0.66 (1673.9 examples/sec; 0.076 sec/batch)
2017-06-02 04:40:55.507699: step 82620, loss = 0.68 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:40:56.349992: step 82630, loss = 0.63 (1519.7 examples/sec; 0.084 sec/batch)
2017-06-02 04:40:57.207648: step 82640, loss = 0.58 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:40:58.116339: step 82650, loss = 0.62 (1408.6 examples/sec; 0.091 sec/batch)
2017-06-02 04:40:58.943457: step 82660, loss = 0.67 (1547.5 examples/sec; 0.083 sec/batch)
2017-06-02 04:40:59.800434: step 82670, loss = 0.89 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:41:00.690957: step 82680, loss = 0.71 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:41:01.551434: step 82690, loss = 0.75 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:41:02.561883: step 82700, loss = 0.65 (1266.8 examples/sec; 0.101 sec/batch)
2017-06-02 04:41:03.269373: step 82710, loss = 0.65 (1809.2 examples/sec; 0.071 sec/batch)
2017-06-02 04:41:04.149093: step 82720, loss = 0.74 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:41:05.029224: step 82730, loss = 0.72 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:41:05.888405: step 82740, loss = 0.76 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:41:06.749870: step 82750, loss = 0.61 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:41:07.598085: step 82760, loss = 0.64 (1509.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:41:08.459978: step 82770, loss = 0.69 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:41:09.328226: step 82780, loss = 0.73 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:41:10.183548: step 82790, loss = 0.81 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:41:11.150362: step 82800, loss = 0.66 (1323.9 examples/sec; 0.097 sec/batch)
2017-06-02 04:41:11.911772: step 82810, loss = 0.66 (1681.1 examples/sec; 0.076 sec/batch)
2017-06-02 04:41:12.758996: step 82820, loss = 0.71 (1510.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:41:13.601057: step 82830, loss = 0.71 (1520.1 examples/sec; 0.084 sec/batch)
2017-06-02 04:41:14.451857: step 82840, loss = 0.60 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:41:15.320625: step 82850, loss = 0.62 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:41:16.161166: step 82860, loss = 0.61 (1522.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:41:17.009762: step 82870, loss = 0.80 (1508.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:41:17.903454: step 82880, loss = 0.61 (1432.2 examples/sec; 0.089 sec/batch)
2017-06-02 04:41:18.768069: step 82890, loss = 0.74 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:41:19.751822: step 82900, loss = 0.52 (1301.1 examples/sec; 0.098 sec/batch)
2017-06-02 04:41:20.523654: step 82910, loss = 0.71 (1658.4 examples/sec; 0.077 sec/batch)
2017-06-02 04:41:21.413896: step 82920, loss = 0.72 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:41:22.287896: step 82930, loss = 0.66 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:41:23.156933: step 82940, loss = 0.65 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:41:23.998424: step 82950, loss = 0.65 (1521.1 examples/sec; 0.084 sec/batch)
2017-06-02 04:41:24.873758: step 82960, loss = 0.74 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:41:25.734032: step 82970, loss = 0.65 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:41:26.593675: step 82980, loss = 0.69 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:41:27.471017: step 82990, loss = 0.76 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:41:28.411387: step 83000, loss = 0.65 (1361.2 examples/sec; 0.094 sec/batch)
2017-06-02 04:41:29.197448: step 83010, loss = 0.83 (1628.4 examples/sec; 0.079 sec/batch)
2017-06-02 04:41:30.070091: step 83020, loss = 0.78 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:41:30.918067: step 83030, loss = 0.71 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:41:31.770744: step 83040, loss = 0.59 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:41:32.638114: step 83050, loss = 0.91 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:41:33.518381: step 83060, loss = 0.83 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:41:34.392473: step 83070, loss = 0.75 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:41:35.267023: step 83080, loss = 0.71 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:41:36.133684: step 83090, loss = 0.83 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:41:37.088930: step 83100, loss = 0.76 (1340.0 examples/sec; 0.096 sec/batch)
2017-06-02 04:41:37.848331: step 83110, loss = 0.73 (1685.5 examples/sec; 0.076 sec/batch)
2017-06-02 04:41:38.694512: step 83120, loss = 0.92 (1512.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:41:39.543206: step 83130, loss = 0.64 (1508.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:41:40.391141: step 83140, loss = 0.79 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:41:41.265325: step 83150, loss = 0.62 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:41:42.128087: step 83160, loss = 0.71 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:41:42.988737: step 83170, loss = 0.69 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:41:43.855604: step 83180, loss = 0.61 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:41:44.710348: step 83190, loss = 0.78 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:41:45.694790: step 83200, loss = 0.70 (1300.2 examples/sec; 0.098 sec/batch)
2017-06-02 04:41:46.462745: step 83210, loss = 0.72 (1666.8 examples/sec; 0.077 sec/batch)
2017-06-02 04:41:47.319012: step 83220, loss = 0.72 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:41:48.216045: step 83230, loss = 0.64 (1426.9 examples/sec; 0.090 sec/batch)
2017-06-02 04:41:49.087567: step 83240, loss = 0.66 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:41:49.955433: step 83250, loss = 0.77 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:41:50.833453: step 83260, loss = 0.75 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:41:51.688332: step 83270, loss = 0.83 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:41:52.552668: step 83280, loss = 0.68 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:41:53.446304: step 83290, loss = 0.68 (1432.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:41:54.424834: step 83300, loss = 0.67 (1308.1 examples/sec; 0.098 sec/batch)
2017-06-02 04:41:55.210091: step 83310, loss = 0.70 (1630.0 examples/sec; 0.079 sec/batch)
2017-06-02 04:41:56.051013: step 83320, loss = 0.80 (1522.1 examples/sec; 0.084 sec/batch)
2017-06-02 04:41:56.907702: step 83330, loss = 0.73 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:41:57.765519: step 83340, loss = 0.70 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:41:58.623402: step 83350, loss = 0.58 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:41:59.525759: step 83360, loss = 0.71 (1418.5 examples/sec; 0.090 sec/batch)
2017-06-02 04:42:00.400718: step 83370, loss = 0.72 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:42:01.258486: step 83380, loss = 0.81 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:42:02.128739: step 83390, loss = 0.73 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:42:03.111936: step 83400, loss = 0.94 (1301.9 examples/sec; 0.098 sec/batch)
2017-06-02 04:42:03.873212: step 83410, loss = 0.81 (1681.4 examples/sec; 0.076 sec/batch)
2017-06-02 04:42:04.760066: step 83420, loss = 0.78 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:42:05.615427: step 83430, loss = 0.69 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:42:06.468791: step 83440, loss = 0.61 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:42:07.338658: step 83450, loss = 0.98 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:42:08.204023: step 83460, loss = 0.79 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:42:09.087295: step 83470, loss = 0.84 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:42:09.951416: step 83480, loss = 0.75 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:42:10.785648: step 83490, loss = 0.64 (1534.4 examples/sec; 0.083 sec/batch)
2017-06-02 04:42:11.731429: step 83500, loss = 0.62 (1353.4 examples/sec; 0.095 sec/batch)
2017-06-02 04:42:12.498748: step 83510, loss = 0.83 (1668.2 examples/sec; 0.077 sec/batch)
2017-06-02 04:42:13.367286: step 83520, loss = 0.68 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:42:14.208655: step 83530, loss = 0.81 (1521.3 examples/sec; 0.084 sec/batch)
2017-06-02 04:42:15.085256: step 83540, loss = 0.75 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:42:15.959142: step 83550, loss = 0.64 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:42:16.804119: step 83560, loss = 0.71 (1514.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:42:17.682030: step 83570, loss = 0.67 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:42:18.541336: step 83580, loss = 0.78 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:42:19.395844: step 83590, loss = 0.79 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:42:20.365891: step 83600, loss = 0.80 (1319.5 examples/sec; 0.097 sec/batch)
2017-06-02 04:42:21.134352: step 83610, loss = 0.62 (1665.7 examples/sec; 0.077 sec/batch)
2017-06-02 04:42:21.987851: step 83620, loss = 0.66 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:42:22.841457: step 83630, loss = 0.78 (1499.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:42:23.705110: step 83640, loss = 0.65 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:42:24.582822: step 83650, loss = 0.81 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:42:25.458360: step 83660, loss = 0.69 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:42:26.290961: step 83670, loss = 0.64 (1537.4 examples/sec; 0.083 sec/batch)
2017-06-02 04:42:27.179465: step 83680, loss = 0.69 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:42:28.053675: step 83690, loss = 0.68 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:42:29.083457: step 83700, loss = 0.84 (1243.0 examples/sec; 0.103 sec/batch)
2017-06-02 04:42:29.830103: step 83710, loss = 0.74 (1714.3 examples/sec; 0.075 sec/batch)
2017-06-02 04:42:30.702220: step 83720, loss = 0.52 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:42:31.578504: step 83730, loss = 0.78 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:42:32.440894: step 83740, loss = 0.77 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:42:33.296306: step 83750, loss = 0.78 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:42:34.143866: step 83760, loss = 0.76 (1510.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:42:34.998846: step 83770, loss = 0.81 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:42:35.839907: step 83780, loss = 0.70 (1521.9 examples/sec; 0.084 sec/batch)
2017-06-02 04:42:36.727190: step 83790, loss = 0.76 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:42:37.675146: step 83800, loss = 0.75 (1350.3 examples/sec; 0.095 sec/batch)
2017-06-02 04:42:38.464035: step 83810, loss = 0.59 (1622.5 examples/sec; 0.079 sec/batch)
2017-06-02 04:42:39.316299: step 83820, loss = 0.70 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:42:40.172386: step 83830, loss = 0.77 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:42:41.042074: step 83840, loss = 0.57 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:42:41.878843: step 83850, loss = 0.82 (1529.7 examples/sec; 0.084 sec/batch)
2017-06-02 04:42:42.736986: step 83860, loss = 0.82 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:42:43.608781: step 83870, loss = 0.84 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:42:44.488871: step 83880, loss = 0.73 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:42:45.349331: step 83890, loss = 0.64 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:42:46.299052: step 83900, loss = 0.66 (1347.7 examples/sec; 0.095 sec/batch)
2017-06-02 04:42:47.079286: step 83910, loss = 0.85 (1640.5 examples/sec; 0.078 sec/batch)
2017-06-02 04:42:47.940793: step 83920, loss = 0.74 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:42:48.792994: step 83930, loss = 0.70 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:42:49.669143: step 83940, loss = 0.68 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:42:50.533167: step 83950, loss = 0.74 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:42:51.405077: step 83960, loss = 0.66 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:42:52.241654: step 83970, loss = 0.80 (1530.1 examples/sec; 0.084 sec/batch)
2017-06-02 04:42:53.111440: step 83980, loss = 0.69 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:42:53.947034: step 83990, loss = 0.66 (1531.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:42:54.903422: step 84000, loss = 0.62 (1338.4 examples/sec; 0.096 sec/batch)
2017-06-02 04:42:55.675385: step 84010, loss = 0.72 (1658.1 examples/sec; 0.077 sec/batch)
2017-06-02 04:42:56.523454: step 84020, loss = 0.86 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:42:57.391634: step 84030, loss = 0.72 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:42:58.223133: step 84040, loss = 0.67 (1539.4 examples/sec; 0.083 sec/batch)
2017-06-02 04:42:59.085992: step 84050, loss = 0.79 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:42:59.938789: step 84060, loss = 0.55 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:43:00.798272: step 84070, loss = 0.87 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:43:01.665415: step 84080, loss = 0.66 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:43:02.529827: step 84090, loss = 0.69 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:43:03.466299: step 84100, loss = 0.61 (1366.8 examples/sec; 0.094 sec/batch)
2017-06-02 04:43:04.222312: step 84110, loss = 0.73 (1693.1 examples/sec; 0.076 sec/batch)
2017-06-02 04:43:05.088918: step 84120, loss = 0.80 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:43:05.950883: step 84130, loss = 0.66 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:43:06.817477: step 84140, loss = 0.63 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:43:07.650090: step 84150, loss = 0.76 (1537.3 examples/sec; 0.083 sec/batch)
2017-06-02 04:43:08.489335: step 84160, loss = 0.73 (1525.2 examples/sec; 0.084 sec/batch)
2017-06-02 04:43:09.348259: step 84170, loss = 0.59 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:43:10.209076: step 84180, loss = 0.65 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:43:11.077938: step 84190, loss = 0.73 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:43:12.045953: step 84200, loss = 0.67 (1322.3 examples/sec; 0.097 sec/batch)
2017-06-02 04:43:12.818052: step 84210, loss = 0.76 (1657.8 examples/sec; 0.077 sec/batch)
2017-06-02 04:43:13.671150: step 84220, loss = 0.83 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:43:14.519108: step 84230, loss = 0.77 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:43:15.379870: step 84240, loss = 0.81 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:43:16.240982: step 84250, loss = 0.88 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:43:17.101612: step 84260, loss = 0.61 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:43:17.962608: step 84270, loss = 0.73 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:43:18.825454: step 84280, loss = 0.65 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:43:19.718718: step 84290, loss = 0.70 (1432.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:43:20.678272: step 84300, loss = 0.88 (1334.0 examples/sec; 0.096 sec/batch)
2017-06-02 04:43:21.443773: step 84310, loss = 0.79 (1672.1 examples/sec; 0.077 sec/batch)
2017-06-02 04:43:22.317417: step 84320, loss = 0.69 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:43:23.192299: step 84330, loss = 0.67 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:43:24.059867: step 84340, loss = 0.85 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:43:24.945851: step 84350, loss = 0.70 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:43:25.820600: step 84360, loss = 0.65 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:43:26.685492: step 84370, loss = 0.84 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:43:27.557867: step 84380, loss = 0.67 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:43:28.424809: step 84390, loss = 0.59 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:43:29.361808: step 84400, loss = 0.76 (1366.1 examples/sec; 0.094 sec/batch)
2017-06-02 04:43:30.123263: step 84410, loss = 0.82 (1681.1 examples/sec; 0.076 sec/batch)
2017-06-02 04:43:30.971226: step 84420, loss = 0.61 (1509.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:43:31.833054: step 84430, loss = 0.63 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:43:32.694820: step 84440, loss = 0.68 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:43:33.552234: step 84450, loss = 0.74 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:43:34.396175: step 84460, loss = 0.64 (1516.7 examples/sec; 0.084 sec/batch)
2017-06-02 04:43:35.249911: step 84470, loss = 0.78 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:43:36.095017: step 84480, loss = 0.72 (1514.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:43:36.947699: step 84490, loss = 0.66 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:43:37.929878: step 84500, loss = 0.67 (1303.2 examples/sec; 0.098 sec/batch)
2017-06-02 04:43:38.670793: step 84510, loss = 0.67 (1727.6 examples/sec; 0.074 sec/batch)
2017-06-02 04:43:39.530519: step 84520, loss = 0.64 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:43:40.409186: step 84530, loss = 0.55 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:43:41.261270: step 84540, loss = 0.87 (1502.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:43:42.123141: step 84550, loss = 0.69 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:43:43.009753: step 84560, loss = 0.76 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:43:43.858355: step 84570, loss = 0.73 (1508.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:43:44.733248: step 84580, loss = 0.80 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:43:45.566213: step 84590, loss = 0.73 (1536.7 examples/sec; 0.083 sec/batch)
2017-06-02 04:43:46.538090: step 84600, loss = 0.77 (1317.0 examples/sec; 0.097 sec/batch)
2017-06-02 04:43:47.305808: step 84610, loss = 0.78 (1667.3 examples/sec; 0.077 sec/batch)
2017-06-02 04:43:48.168684: step 84620, loss = 0.66 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:43:49.020283: step 84630, loss = 0.94 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:43:49.876150: step 84640, loss = 0.78 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:43:50.746013: step 84650, loss = 0.76 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:43:51.608436: step 84660, loss = 0.90 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:43:52.471958: step 84670, loss = 0.72 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:43:53.372301: step 84680, loss = 0.75 (1421.7 examples/sec; 0.090 sec/batch)
2017-06-02 04:43:54.226171: step 84690, loss = 0.91 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:43:55.147138: step 84700, loss = 0.75 (1389.8 examples/sec; 0.092 sec/batch)
2017-06-02 04:43:55.921024: step 84710, loss = 0.69 (1654.0 examples/sec; 0.077 sec/batch)
2017-06-02 04:43:56.788616: step 84720, loss = 0.64 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:43:57.643805: step 84730, loss = 0.80 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:43:58.500836: step 84740, loss = 0.66 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:43:59.362990: step 84750, loss = 0.81 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:44:00.212961: step 84760, loss = 0.72 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:44:01.094804: step 84770, loss = 0.87 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:44:01.947040: step 84780, loss = 0.67 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:44:02.799223: step 84790, loss = 0.73 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:44:03.746871: step 84800, loss = 0.61 (1350.7 examples/sec; 0.095 sec/batch)
2017-06-02 04:44:04.521910: step 84810, loss = 0.78 (1651.5 examples/sec; 0.078 sec/batch)
2017-06-02 04:44:05.374354: step 84820, loss = 0.72 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:44:06.223856: step 84830, loss = 0.71 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:44:07.087142: step 84840, loss = 0.72 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:44:07.961401: step 84850, loss = 0.66 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:44:08.794870: step 84860, loss = 0.64 (1535.8 examples/sec; 0.083 sec/batch)
2017-06-02 04:44:09.653341: step 84870, loss = 0.76 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:44:10.509608: step 84880, loss = 0.71 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:44:11.361598: step 84890, loss = 0.74 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:44:12.325186: step 84900, loss = 0.72 (1328.4 examples/sec; 0.096 sec/batch)
2017-06-02 04:44:13.071888: step 84910, loss = 0.73 (1714.2 examples/sec; 0.075 sec/batch)
2017-06-02 04:44:13.927857: step 84920, loss = 0.76 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:44:14.797983: step 84930, loss = 0.70 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:44:15.636145: step 84940, loss = 0.59 (1527.1 examples/sec; 0.084 sec/batch)
2017-06-02 04:44:16.503200: step 84950, loss = 0.77 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:44:17.382854: step 84960, loss = 0.69 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:44:18.256206: step 84970, loss = 0.78 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:44:19.134113: step 84980, loss = 0.85 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:44:19.998436: step 84990, loss = 0.59 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:44:20.944048: step 85000, loss = 0.77 (1353.6 examples/sec; 0.095 sec/batch)
2017-06-02 04:44:21.738356: step 85010, loss = 0.75 (1611.5 examples/sec; 0.079 sec/batch)
2017-06-02 04:44:22.593239: step 85020, loss = 0.58 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:44:23.472994: step 85030, loss = 0.71 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:44:24.359954: step 85040, loss = 0.85 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:44:25.214289: step 85050, loss = 0.81 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:44:26.077375: step 85060, loss = 0.76 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:44:26.962819: step 85070, loss = 0.78 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:44:27.847374: step 85080, loss = 0.72 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:44:28.702778: step 85090, loss = 0.76 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:44:29.643963: step 85100, loss = 0.68 (1360.0 examples/sec; 0.094 sec/batch)
2017-06-02 04:44:30.414646: step 85110, loss = 0.69 (1660.9 examples/sec; 0.077 sec/batch)
2017-06-02 04:44:31.285688: step 85120, loss = 0.62 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:44:32.147459: step 85130, loss = 1.02 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:44:33.000477: step 85140, loss = 0.75 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:44:33.885771: step 85150, loss = 0.73 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:44:34.756852: step 85160, loss = 0.73 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:44:35.639025: step 85170, loss = 0.81 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:44:36.490209: step 85180, loss = 0.74 (1503.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:44:37.366031: step 85190, loss = 0.58 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:44:38.332173: step 85200, loss = 0.74 (1324.8 examples/sec; 0.097 sec/batch)
2017-06-02 04:44:39.108033: step 85210, loss = 0.75 (1649.8 examples/sec; 0.078 sec/batch)
2017-06-02 04:44:39.961040: step 85220, loss = 0.69 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:44:40.817803: step 85230, loss = 0.73 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:44:41.670787: step 85240, loss = 0.70 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:44:42.526431: step 85250, loss = 0.81 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:44:43.378506: step 85260, loss = 0.62 (1502.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:44:44.243389: step 85270, loss = 0.70 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:44:45.087621: step 85280, loss = 0.76 (1516.2 examples/sec; 0.084 sec/batch)
2017-06-02 04:44:45.937255: step 85290, loss = 0.66 (1507.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:44:46.901369: step 85300, loss = 0.78 (1326.9 examples/sec; 0.096 sec/batch)
2017-06-02 04:44:47.660794: step 85310, loss = 0.75 (1685.5 examples/sec; 0.076 sec/batch)
2017-06-02 04:44:48.535420: step 85320, loss = 0.75 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:44:49.429005: step 85330, loss = 0.79 (1432.4 examples/sec; 0.089 sec/batch)
2017-06-02 04:44:50.299317: step 85340, loss = 0.71 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:44:51.171587: step 85350, loss = 0.67 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:44:52.053230: step 85360, loss = 0.61 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:44:52.931265: step 85370, loss = 0.73 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:44:53.810930: step 85380, loss = 0.69 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:44:54.659055: step 85390, loss = 0.79 (1509.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:44:55.652616: step 85400, loss = 0.86 (1288.3 examples/sec; 0.099 sec/batch)
2017-06-02 04:44:56.433441: step 85410, loss = 0.61 (1639.3 examples/sec; 0.078 sec/batch)
2017-06-02 04:44:57.313640: step 85420, loss = 0.60 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:44:58.168214: step 85430, loss = 0.75 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:44:59.039113: step 85440, loss = 0.79 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:44:59.926827: step 85450, loss = 0.72 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:45:00.797820: step 85460, loss = 0.76 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:45:01.664807: step 85470, loss = 0.70 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:45:02.555402: step 85480, loss = 0.71 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 04:45:03.425928: step 85490, loss = 0.59 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:45:04.402049: step 85500, loss = 0.77 (1311.3 examples/sec; 0.098 sec/batch)
2017-06-02 04:45:05.187964: step 85510, loss = 0.68 (1628.7 examples/sec; 0.079 sec/batch)
2017-06-02 04:45:06.070660: step 85520, loss = 0.84 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:45:06.935603: step 85530, loss = 0.83 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:45:07.800851: step 85540, loss = 0.64 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:45:08.664702: step 85550, loss = 0.84 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:45:09.532826: step 85560, loss = 0.61 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:45:10.387173: step 85570, loss = 0.67 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:45:11.240290: step 85580, loss = 0.77 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:45:12.118977: step 85590, loss = 0.82 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:45:13.064577: step 85600, loss = 0.69 (1353.6 examples/sec; 0.095 sec/batch)
2017-06-02 04:45:13.828155: step 85610, loss = 0.82 (1676.3 examples/sec; 0.076 sec/batch)
2017-06-02 04:45:14.700249: step 85620, loss = 0.64 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:45:15.544001: step 85630, loss = 0.70 (1517.0 examples/sec; 0.084 sec/batch)
2017-06-02 04:45:16.413267: step 85640, loss = 0.72 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:45:17.264871: step 85650, loss = 0.70 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:45:18.140560: step 85660, loss = 0.65 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:45:18.995878: step 85670, loss = 0.68 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:45:19.875081: step 85680, loss = 0.63 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:45:20.725999: step 85690, loss = 0.75 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:45:21.690744: step 85700, loss = 0.62 (1326.8 examples/sec; 0.096 sec/batch)
2017-06-02 04:45:22.460718: step 85710, loss = 0.67 (1662.4 examples/sec; 0.077 sec/batch)
2017-06-02 04:45:23.320057: step 85720, loss = 0.63 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:45:24.186243: step 85730, loss = 0.87 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:45:25.055443: step 85740, loss = 0.67 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:45:25.928262: step 85750, loss = 0.78 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:45:26.795917: step 85760, loss = 0.73 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:45:27.626022: step 85770, loss = 0.74 (1542.0 examples/sec; 0.083 sec/batch)
2017-06-02 04:45:28.459808: step 85780, loss = 0.80 (1535.2 examples/sec; 0.083 sec/batch)
2017-06-02 04:45:29.303610: step 85790, loss = 0.72 (1516.9 examples/sec; 0.084 sec/batch)
2017-06-02 04:45:30.271458: step 85800, loss = 0.76 (1322.5 examples/sec; 0.097 sec/batch)
2017-06-02 04:45:31.043103: step 85810, loss = 0.66 (1658.8 examples/sec; 0.077 sec/batch)
2017-06-02 04:45:31.872169: step 85820, loss = 0.69 (1543.9 examples/sec; 0.083 sec/batch)
2017-06-02 04:45:32.744369: step 85830, loss = 0.89 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:45:33.599849: step 85840, loss = 0.80 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:45:34.461986: step 85850, loss = 0.65 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:45:35.329852: step 85860, loss = 0.57 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:45:36.169321: step 85870, loss = 0.67 (1524.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:45:37.023179: step 85880, loss = 0.69 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:45:37.893155: step 85890, loss = 0.75 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:45:38.864563: step 85900, loss = 0.72 (1317.7 examples/sec; 0.097 sec/batch)
2017-06-02 04:45:39.642796: step 85910, loss = 0.68 (1644.8 examples/sec; 0.078 sec/batch)
2017-06-02 04:45:40.549834: step 85920, loss = 0.84 (1411.2 examples/sec; 0.091 sec/batch)
2017-06-02 04:45:41.448303: step 85930, loss = 0.65 (1424.6 examples/sec; 0.090 sec/batch)
2017-06-02 04:45:42.321358: step 85940, loss = 0.61 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:45:43.180322: step 85950, loss = 0.68 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:45:44.056183: step 85960, loss = 0.79 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:45:44.957945: step 85970, loss = 0.73 (1419.4 examples/sec; 0.090 sec/batch)
2017-06-02 04:45:45.839509: step 85980, loss = 0.74 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:45:46.724895: step 85990, loss = 0.67 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:45:47.711974: step 86000, loss = 0.71 (1296.7 examples/sec; 0.099 sec/batch)
2017-06-02 04:45:48.498546: step 86010, loss = 0.70 (1627.3 examples/sec; 0.079 sec/batch)
2017-06-02 04:45:49.364121: step 86020, loss = 0.75 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:45:50.251678: step 86030, loss = 0.71 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 04:45:51.128121: step 86040, loss = 0.71 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:45:51.983576: step 86050, loss = 0.62 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:45:52.856367: step 86060, loss = 0.69 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:45:53.725625: step 86070, loss = 0.79 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:45:54.596407: step 86080, loss = 0.66 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:45:55.438625: step 86090, loss = 0.64 (1519.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:45:56.418473: step 86100, loss = 0.65 (1306.3 examples/sec; 0.098 sec/batch)
2017-06-02 04:45:57.198106: step 86110, loss = 0.77 (1641.8 examples/sec; 0.078 sec/batch)
2017-06-02 04:45:58.077556: step 86120, loss = 0.65 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:45:58.933803: step 86130, loss = 0.75 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:45:59.795801: step 86140, loss = 0.67 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:46:00.650391: step 86150, loss = 0.68 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:46:01.504382: step 86160, loss = 0.71 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:46:02.374325: step 86170, loss = 0.69 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:46:03.223908: step 86180, loss = 0.83 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:46:04.081791: step 86190, loss = 0.90 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:46:05.014067: step 86200, loss = 0.68 (1373.0 examples/sec; 0.093 sec/batch)
2017-06-02 04:46:05.773852: step 86210, loss = 0.83 (1684.7 examples/sec; 0.076 sec/batch)
2017-06-02 04:46:06.634293: step 86220, loss = 0.62 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:46:07.496014: step 86230, loss = 0.63 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:46:08.374224: step 86240, loss = 0.73 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:46:09.254535: step 86250, loss = 0.58 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:46:10.082702: step 86260, loss = 0.79 (1545.6 examples/sec; 0.083 sec/batch)
2017-06-02 04:46:10.941827: step 86270, loss = 0.65 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:46:11.780518: step 86280, loss = 0.72 (1526.2 examples/sec; 0.084 sec/batch)
2017-06-02 04:46:12.639152: step 86290, loss = 0.65 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:46:13.649774: step 86300, loss = 0.86 (1266.6 examples/sec; 0.101 sec/batch)
2017-06-02 04:46:14.359261: step 86310, loss = 0.75 (1804.1 examples/sec; 0.071 sec/batch)
2017-06-02 04:46:15.221683: step 86320, loss = 0.65 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:46:16.094018: step 86330, loss = 0.76 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:46:16.976843: step 86340, loss = 0.59 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:46:17.837233: step 86350, loss = 0.64 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:46:18.706501: step 86360, loss = 0.83 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:46:19.559661: step 86370, loss = 0.79 (1500.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:46:20.422513: step 86380, loss = 0.79 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:46:21.310426: step 86390, loss = 0.71 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:46:22.272081: step 86400, loss = 0.60 (1331.0 examples/sec; 0.096 sec/batch)
2017-06-02 04:46:23.027355: step 86410, loss = 0.80 (1694.8 examples/sec; 0.076 sec/batch)
2017-06-02 04:46:23.902595: step 86420, loss = 0.73 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:46:24.777149: step 86430, loss = 0.79 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:46:25.633793: step 86440, loss = 0.79 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:46:26.521874: step 86450, loss = 0.59 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:46:27.405109: step 86460, loss = 0.72 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:46:28.273797: step 86470, loss = 0.72 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:46:29.137658: step 86480, loss = 0.62 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:46:30.005897: step 86490, loss = 0.77 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:46:30.992017: step 86500, loss = 0.68 (1298.0 examples/sec; 0.099 sec/batch)
2017-06-02 04:46:31.775387: step 86510, loss = 0.81 (1634.0 examples/sec; 0.078 sec/batch)
2017-06-02 04:46:32.613706: step 86520, loss = 0.64 (1526.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:46:33.476618: step 86530, loss = 0.76 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:46:34.343606: step 86540, loss = 0.62 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:46:35.207759: step 86550, loss = 0.64 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:46:36.072634: step 86560, loss = 0.66 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:46:36.953303: step 86570, loss = 0.76 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:46:37.844550: step 86580, loss = 0.67 (1436.2 examples/sec; 0.089 sec/batch)
2017-06-02 04:46:38.719992: step 86590, loss = 0.73 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:46:39.700502: step 86600, loss = 0.78 (1305.4 examples/sec; 0.098 sec/batch)
2017-06-02 04:46:40.455904: step 86610, loss = 0.71 (1694.5 examples/sec; 0.076 sec/batch)
2017-06-02 04:46:41.345083: step 86620, loss = 0.66 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:46:42.186062: step 86630, loss = 0.74 (1522.0 examples/sec; 0.084 sec/batch)
2017-06-02 04:46:43.032812: step 86640, loss = 0.80 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:46:43.900840: step 86650, loss = 0.72 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:46:44.776655: step 86660, loss = 0.92 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:46:45.645369: step 86670, loss = 0.71 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:46:46.512781: step 86680, loss = 0.82 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:46:47.370457: step 86690, loss = 0.60 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:46:48.342047: step 86700, loss = 0.68 (1317.4 examples/sec; 0.097 sec/batch)
2017-06-02 04:46:49.109122: step 86710, loss = 0.80 (1668.7 examples/sec; 0.077 sec/batch)
2017-06-02 04:46:49.977828: step 86720, loss = 0.62 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:46:50.852423: step 86730, loss = 0.79 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:46:51.700188: step 86740, loss = 0.73 (1509.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:46:52.553912: step 86750, loss = 0.69 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:46:53.414402: step 86760, loss = 0.66 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:46:54.303589: step 86770, loss = 0.89 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:46:55.187022: step 86780, loss = 0.68 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:46:56.066080: step 86790, loss = 0.57 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:46:57.029749: step 86800, loss = 0.66 (1328.3 examples/sec; 0.096 sec/batch)
2017-06-02 04:46:57.788996: step 86810, loss = 0.65 (1685.9 examples/sec; 0.076 sec/batch)
2017-06-02 04:46:58.657162: step 86820, loss = 1.03 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:46:59.504126: step 86830, loss = 0.91 (1511.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:47:00.374010: step 86840, loss = 0.69 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:47:01.243200: step 86850, loss = 0.65 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:47:02.098063: step 86860, loss = 0.63 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:47:02.943993: step 86870, loss = 0.71 (1513.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:47:03.799825: step 86880, loss = 0.66 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:47:04.632757: step 86890, loss = 0.72 (1536.8 examples/sec; 0.083 sec/batch)
2017-06-02 04:47:05.597656: step 86900, loss = 0.70 (1326.6 examples/sec; 0.096 sec/batch)
2017-06-02 04:47:06.355282: step 86910, loss = 0.68 (1689.5 examples/sec; 0.076 sec/batch)
2017-06-02 04:47:07.213071: step 86920, loss = 0.68 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:47:08.097546: step 86930, loss = 0.60 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:47:08.954293: step 86940, loss = 0.66 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:47:09.804318: step 86950, loss = 0.68 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:47:10.657436: step 86960, loss = 0.78 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:47:11.501202: step 86970, loss = 0.70 (1517.0 examples/sec; 0.084 sec/batch)
2017-06-02 04:47:12.378138: step 86980, loss = 0.91 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:47:13.250701: step 86990, loss = 0.64 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:47:14.227927: step 87000, loss = 0.50 (1309.8 examples/sec; 0.098 sec/batch)
2017-06-02 04:47:15.000261: step 87010, loss = 0.61 (1657.3 examples/sec; 0.077 sec/batch)
2017-06-02 04:47:15.848405: step 87020, loss = 0.77 (1509.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:47:16.695087: step 87030, loss = 0.65 (1511.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:47:17.577139: step 87040, loss = 0.69 (1451.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:47:18.435550: step 87050, loss = 0.73 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:47:19.305336: step 87060, loss = 0.77 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:47:20.152435: step 87070, loss = 0.76 (1511.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:47:20.986204: step 87080, loss = 0.75 (1535.2 examples/sec; 0.083 sec/batch)
2017-06-02 04:47:21.835245: step 87090, loss = 0.59 (1507.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:47:22.792032: step 87100, loss = 0.74 (1337.8 examples/sec; 0.096 sec/batch)
2017-06-02 04:47:23.549688: step 87110, loss = 0.60 (1689.4 examples/sec; 0.076 sec/batch)
2017-06-02 04:47:24.381953: step 87120, loss = 0.65 (1538.0 examples/sec; 0.083 sec/batch)
2017-06-02 04:47:25.246108: step 87130, loss = 0.96 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:47:26.116554: step 87140, loss = 0.76 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:47:26.978680: step 87150, loss = 0.63 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:47:27.843253: step 87160, loss = 0.67 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:47:28.687809: step 87170, loss = 0.61 (1515.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:47:29.551477: step 87180, loss = 0.85 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:47:30.426708: step 87190, loss = 0.79 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:47:31.398103: step 87200, loss = 0.79 (1317.7 examples/sec; 0.097 sec/batch)
2017-06-02 04:47:32.181900: step 87210, loss = 0.82 (1633.1 examples/sec; 0.078 sec/batch)
2017-06-02 04:47:33.068363: step 87220, loss = 0.87 (1443.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:47:33.948559: step 87230, loss = 0.65 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:47:34.812805: step 87240, loss = 0.83 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:47:35.685028: step 87250, loss = 0.74 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:47:36.545552: step 87260, loss = 0.73 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:47:37.426171: step 87270, loss = 0.62 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:47:38.284780: step 87280, loss = 0.63 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:47:39.155186: step 87290, loss = 0.63 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:47:40.120429: step 87300, loss = 0.66 (1326.1 examples/sec; 0.097 sec/batch)
2017-06-02 04:47:40.826037: step 87310, loss = 0.63 (1814.0 examples/sec; 0.071 sec/batch)
2017-06-02 04:47:41.693956: step 87320, loss = 0.71 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:47:42.544140: step 87330, loss = 0.68 (1505.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:47:43.401955: step 87340, loss = 0.67 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:47:44.278865: step 87350, loss = 0.66 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:47:45.146200: step 87360, loss = 0.56 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:47:45.980376: step 87370, loss = 0.66 (1534.5 examples/sec; 0.083 sec/batch)
2017-06-02 04:47:46.824031: step 87380, loss = 0.65 (1517.2 examples/sec; 0.084 sec/batch)
2017-06-02 04:47:47.669862: step 87390, loss = 0.58 (1513.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:47:48.613315: step 87400, loss = 0.63 (1356.7 examples/sec; 0.094 sec/batch)
2017-06-02 04:47:49.392749: step 87410, loss = 0.89 (1642.2 examples/sec; 0.078 sec/batch)
2017-06-02 04:47:50.256311: step 87420, loss = 0.89 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:47:51.115943: step 87430, loss = 0.73 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:47:51.978473: step 87440, loss = 0.63 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:47:52.825419: step 87450, loss = 0.71 (1511.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:47:53.700881: step 87460, loss = 0.78 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:47:54.581464: step 87470, loss = 0.66 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:47:55.425964: step 87480, loss = 0.68 (1515.7 examples/sec; 0.084 sec/batch)
2017-06-02 04:47:56.305024: step 87490, loss = 0.77 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:47:57.251918: step 87500, loss = 0.76 (1351.8 examples/sec; 0.095 sec/batch)
2017-06-02 04:47:58.031305: step 87510, loss = 0.63 (1642.3 examples/sec; 0.078 sec/batch)
2017-06-02 04:47:58.896046: step 87520, loss = 0.64 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:47:59.749590: step 87530, loss = 0.69 (1499.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:48:00.578690: step 87540, loss = 0.58 (1543.8 examples/sec; 0.083 sec/batch)
2017-06-02 04:48:01.453961: step 87550, loss = 0.77 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:48:02.286261: step 87560, loss = 0.70 (1537.9 examples/sec; 0.083 sec/batch)
2017-06-02 04:48:03.121498: step 87570, loss = 0.83 (1532.5 examples/sec; 0.084 sec/batch)
2017-06-02 04:48:04.001204: step 87580, loss = 0.63 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:48:04.854956: step 87590, loss = 0.66 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:48:05.822259: step 87600, loss = 0.74 (1323.3 examples/sec; 0.097 sec/batch)
2017-06-02 04:48:06.582930: step 87610, loss = 0.73 (1682.8 examples/sec; 0.076 sec/batch)
2017-06-02 04:48:07.443743: step 87620, loss = 0.60 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:48:08.323184: step 87630, loss = 0.67 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:48:09.146114: step 87640, loss = 0.74 (1555.4 examples/sec; 0.082 sec/batch)
2017-06-02 04:48:10.010039: step 87650, loss = 0.68 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:48:10.869907: step 87660, loss = 0.71 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:48:11.753800: step 87670, loss = 0.66 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:48:12.620334: step 87680, loss = 0.67 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:48:13.488643: step 87690, loss = 0.82 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:48:14.461155: step 87700, loss = 0.69 (1316.2 examples/sec; 0.097 sec/batch)
2017-06-02 04:48:15.232583: step 87710, loss = 0.80 (1659.3 examples/sec; 0.077 sec/batch)
2017-06-02 04:48:16.067047: step 87720, loss = 0.79 (1533.9 examples/sec; 0.083 sec/batch)
2017-06-02 04:48:16.902219: step 87730, loss = 0.62 (1532.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:48:17.762551: step 87740, loss = 0.68 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:48:18.629513: step 87750, loss = 0.71 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:48:19.484552: step 87760, loss = 0.65 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:48:20.336486: step 87770, loss = 0.65 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:48:21.192968: step 87780, loss = 0.72 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:48:22.043214: step 87790, loss = 0.67 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:48:23.006764: step 87800, loss = 0.56 (1328.4 examples/sec; 0.096 sec/batch)
2017-06-02 04:48:23.785640: step 87810, loss = 0.73 (1643.4 examples/sec; 0.078 sec/batch)
2017-06-02 04:48:24.649356: step 87820, loss = 0.64 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:48:25.496141: step 87830, loss = 0.74 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:48:26.365364: step 87840, loss = 0.66 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:48:27.240054: step 87850, loss = 0.77 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:48:28.109263: step 87860, loss = 0.74 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:48:28.965071: step 87870, loss = 0.71 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:48:29.830881: step 87880, loss = 0.70 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:48:30.675417: step 87890, loss = 0.72 (1515.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:48:31.619184: step 87900, loss = 0.70 (1356.3 examples/sec; 0.094 sec/batch)
2017-06-02 04:48:32.391710: step 87910, loss = 0.70 (1656.9 examples/sec; 0.077 sec/batch)
2017-06-02 04:48:33.227382: step 87920, loss = 0.63 (1531.7 examples/sec; 0.084 sec/batch)
2017-06-02 04:48:34.093789: step 87930, loss = 0.80 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:48:34.950671: step 87940, loss = 0.65 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:48:35.800549: step 87950, loss = 0.72 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:48:36.667345: step 87960, loss = 0.78 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:48:37.548849: step 87970, loss = 0.80 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:48:38.419007: step 87980, loss = 0.86 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:48:39.303049: step 87990, loss = 0.71 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:48:40.263963: step 88000, loss = 0.71 (1332.1 examples/sec; 0.096 sec/batch)
2017-06-02 04:48:41.033781: step 88010, loss = 0.64 (1662.8 examples/sec; 0.077 sec/batch)
2017-06-02 04:48:41.892981: step 88020, loss = 0.63 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:48:42.765619: step 88030, loss = 0.84 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:48:43.663300: step 88040, loss = 0.77 (1425.9 examples/sec; 0.090 sec/batch)
2017-06-02 04:48:44.490441: step 88050, loss = 0.72 (1547.5 examples/sec; 0.083 sec/batch)
2017-06-02 04:48:45.347853: step 88060, loss = 0.66 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:48:46.247962: step 88070, loss = 0.75 (1422.0 examples/sec; 0.090 sec/batch)
2017-06-02 04:48:47.096652: step 88080, loss = 0.76 (1508.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:48:47.959375: step 88090, loss = 0.59 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:48:48.919041: step 88100, loss = 0.65 (1333.8 examples/sec; 0.096 sec/batch)
2017-06-02 04:48:49.690623: step 88110, loss = 0.55 (1658.9 examples/sec; 0.077 sec/batch)
2017-06-02 04:48:50.553547: step 88120, loss = 0.71 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:48:51.441727: step 88130, loss = 0.76 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:48:52.318050: step 88140, loss = 0.68 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:48:53.156125: step 88150, loss = 0.74 (1527.3 examples/sec; 0.084 sec/batch)
2017-06-02 04:48:54.006942: step 88160, loss = 0.70 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:48:54.866670: step 88170, loss = 0.75 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:48:55.742919: step 88180, loss = 0.57 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:48:56.591690: step 88190, loss = 0.63 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:48:57.552330: step 88200, loss = 0.77 (1332.5 examples/sec; 0.096 sec/batch)
2017-06-02 04:48:58.311443: step 88210, loss = 0.65 (1686.2 examples/sec; 0.076 sec/batch)
2017-06-02 04:48:59.213541: step 88220, loss = 0.80 (1418.9 examples/sec; 0.090 sec/batch)
2017-06-02 04:49:00.063915: step 88230, loss = 0.61 (1505.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:49:00.932567: step 88240, loss = 0.78 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:49:01.802413: step 88250, loss = 0.70 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:49:02.670225: step 88260, loss = 0.60 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:49:03.533158: step 88270, loss = 0.79 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:49:04.400678: step 88280, loss = 0.85 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:49:05.283792: step 88290, loss = 0.63 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:49:06.250127: step 88300, loss = 0.65 (1324.6 examples/sec; 0.097 sec/batch)
2017-06-02 04:49:07.009746: step 88310, loss = 0.77 (1685.1 examples/sec; 0.076 sec/batch)
2017-06-02 04:49:07.872994: step 88320, loss = 0.86 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:49:08.717654: step 88330, loss = 0.60 (1515.4 examples/sec; 0.084 sec/batch)
2017-06-02 04:49:09.587262: step 88340, loss = 0.69 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:49:10.455848: step 88350, loss = 0.62 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:49:11.338900: step 88360, loss = 0.76 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:49:12.192830: step 88370, loss = 0.77 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:49:13.073748: step 88380, loss = 0.65 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:49:13.932105: step 88390, loss = 0.70 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:49:14.869055: step 88400, loss = 0.68 (1366.1 examples/sec; 0.094 sec/batch)
2017-06-02 04:49:15.640312: step 88410, loss = 0.74 (1659.6 examples/sec; 0.077 sec/batch)
2017-06-02 04:49:16.495546: step 88420, loss = 0.72 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:49:17.349163: step 88430, loss = 0.61 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:49:18.194108: step 88440, loss = 0.75 (1514.9 examples/sec; 0.084 sec/batch)
2017-06-02 04:49:19.077921: step 88450, loss = 0.68 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:49:19.936585: step 88460, loss = 0.66 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:49:20.825568: step 88470, loss = 0.71 (1439.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:49:21.693016: step 88480, loss = 0.72 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:49:22.576411: step 88490, loss = 0.89 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:49:23.605432: step 88500, loss = 0.82 (1243.9 examples/sec; 0.103 sec/batch)
2017-06-02 04:49:24.321276: step 88510, loss = 0.73 (1788.1 examples/sec; 0.072 sec/batch)
2017-06-02 04:49:25.188828: step 88520, loss = 0.73 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:49:26.077106: step 88530, loss = 0.80 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:49:26.946123: step 88540, loss = 0.73 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:49:27.837682: step 88550, loss = 0.80 (1435.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:49:28.709910: step 88560, loss = 0.76 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:49:29.583347: step 88570, loss = 0.58 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:49:30.453979: step 88580, loss = 0.72 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:49:31.300291: step 88590, loss = 0.64 (1512.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:49:32.287451: step 88600, loss = 0.67 (1296.6 examples/sec; 0.099 sec/batch)
2017-06-02 04:49:33.052686: step 88610, loss = 0.66 (1672.7 examples/sec; 0.077 sec/batch)
2017-06-02 04:49:33.926303: step 88620, loss = 0.81 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:49:34.809286: step 88630, loss = 0.72 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:49:35.689528: step 88640, loss = 0.58 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:49:36.535781: step 88650, loss = 0.92 (1512.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:49:37.406635: step 88660, loss = 0.77 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:49:38.270881: step 88670, loss = 0.71 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:49:39.099852: step 88680, loss = 0.77 (1544.1 examples/sec; 0.083 sec/batch)
2017-06-02 04:49:39.965072: step 88690, loss = 0.71 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:49:40.943671: step 88700, loss = 0.61 (1308.0 examples/sec; 0.098 sec/batch)
2017-06-02 04:49:41.687932: step 88710, loss = 0.84 (1719.8 examples/sec; 0.074 sec/batch)
2017-06-02 04:49:42.564197: step 88720, loss = 0.70 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:49:43.436078: step 88730, loss = 0.63 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:49:44.291620: step 88740, loss = 0.63 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:49:45.178590: step 88750, loss = 0.56 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:49:46.036794: step 88760, loss = 0.64 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:49:46.901552: step 88770, loss = 0.79 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:49:47.755683: step 88780, loss = 0.87 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:49:48.630969: step 88790, loss = 0.74 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:49:49.612003: step 88800, loss = 0.58 (1304.7 examples/sec; 0.098 sec/batch)
2017-06-02 04:49:50.401563: step 88810, loss = 0.72 (1621.2 examples/sec; 0.079 sec/batch)
2017-06-02 04:49:51.374362: step 88820, loss = 0.67 (1315.8 examples/sec; 0.097 sec/batch)
2017-06-02 04:49:52.245282: step 88830, loss = 0.67 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:49:53.116164: step 88840, loss = 0.68 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:49:53.985150: step 88850, loss = 0.73 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:49:54.838239: step 88860, loss = 0.63 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:49:55.677867: step 88870, loss = 0.62 (1524.5 examples/sec; 0.084 sec/batch)
2017-06-02 04:49:56.565628: step 88880, loss = 0.61 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:49:57.404052: step 88890, loss = 0.80 (1526.7 examples/sec; 0.084 sec/batch)
2017-06-02 04:49:58.358939: step 88900, loss = 0.79 (1340.5 examples/sec; 0.095 sec/batch)
2017-06-02 04:49:59.115999: step 88910, loss = 0.84 (1690.8 examples/sec; 0.076 sec/batch)
2017-06-02 04:49:59.993064: step 88920, loss = 0.59 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:50:00.842057: step 88930, loss = 0.66 (1507.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:50:01.690468: step 88940, loss = 0.72 (1508.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:50:02.556556: step 88950, loss = 0.71 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:50:03.416961: step 88960, loss = 0.90 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:50:04.299305: step 88970, loss = 0.65 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:50:05.197017: step 88980, loss = 0.73 (1425.8 examples/sec; 0.090 sec/batch)
2017-06-02 04:50:06.079760: step 88990, loss = 0.68 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:50:07.075224: step 89000, loss = 0.73 (1285.8 examples/sec; 0.100 sec/batch)
2017-06-02 04:50:07.829373: step 89010, loss = 0.71 (1697.3 examples/sec; 0.075 sec/batch)
2017-06-02 04:50:08.712925: step 89020, loss = 0.74 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:50:09.599405: step 89030, loss = 0.68 (1443.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:50:10.458561: step 89040, loss = 0.71 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:50:11.351590: step 89050, loss = 0.83 (1433.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:50:12.229988: step 89060, loss = 0.76 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:50:13.103978: step 89070, loss = 0.76 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:50:13.995183: step 89080, loss = 0.68 (1436.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:50:14.876531: step 89090, loss = 0.78 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:50:15.837389: step 89100, loss = 0.81 (1332.2 examples/sec; 0.096 sec/batch)
2017-06-02 04:50:16.624905: step 89110, loss = 0.96 (1625.3 examples/sec; 0.079 sec/batch)
2017-06-02 04:50:17.476945: step 89120, loss = 0.70 (1502.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:50:18.333675: step 89130, loss = 0.66 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:50:19.191854: step 89140, loss = 0.91 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:50:20.089383: step 89150, loss = 0.78 (1426.1 examples/sec; 0.090 sec/batch)
2017-06-02 04:50:20.955137: step 89160, loss = 0.90 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:50:21.843796: step 89170, loss = 0.90 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 04:50:22.693637: step 89180, loss = 0.78 (1506.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:50:23.530803: step 89190, loss = 0.70 (1529.0 examples/sec; 0.084 sec/batch)
2017-06-02 04:50:24.490321: step 89200, loss = 0.66 (1334.0 examples/sec; 0.096 sec/batch)
2017-06-02 04:50:25.267402: step 89210, loss = 0.68 (1647.2 examples/sec; 0.078 sec/batch)
2017-06-02 04:50:26.145684: step 89220, loss = 0.73 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:50:27.032510: step 89230, loss = 0.79 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:50:27.889013: step 89240, loss = 0.60 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:50:28.747770: step 89250, loss = 0.75 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:50:29.585670: step 89260, loss = 0.68 (1527.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:50:30.462658: step 89270, loss = 0.81 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:50:31.337720: step 89280, loss = 0.74 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:50:32.221620: step 89290, loss = 0.77 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:50:33.196479: step 89300, loss = 0.75 (1313.0 examples/sec; 0.097 sec/batch)
2017-06-02 04:50:33.967017: step 89310, loss = 0.74 (1661.2 examples/sec; 0.077 sec/batch)
2017-06-02 04:50:34.803213: step 89320, loss = 0.58 (1530.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:50:35.679845: step 89330, loss = 0.66 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:50:36.547762: step 89340, loss = 0.61 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:50:37.384532: step 89350, loss = 0.65 (1529.7 examples/sec; 0.084 sec/batch)
2017-06-02 04:50:38.234870: step 89360, loss = 0.81 (1505.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:50:39.079911: step 89370, loss = 0.67 (1514.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:50:39.953946: step 89380, loss = 0.58 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:50:40.800208: step 89390, loss = 0.70 (1512.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:50:41.744941: step 89400, loss = 0.73 (1354.9 examples/sec; 0.094 sec/batch)
2017-06-02 04:50:42.535504: step 89410, loss = 0.61 (1619.1 examples/sec; 0.079 sec/batch)
2017-06-02 04:50:43.409558: step 89420, loss = 0.97 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:50:44.267746: step 89430, loss = 0.77 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:50:45.156556: step 89440, loss = 0.64 (1440.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:50:46.033656: step 89450, loss = 0.74 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:50:46.896928: step 89460, loss = 0.65 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:50:47.755838: step 89470, loss = 0.77 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:50:48.624400: step 89480, loss = 0.64 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:50:49.487337: step 89490, loss = 0.67 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:50:50.448699: step 89500, loss = 0.74 (1331.4 examples/sec; 0.096 sec/batch)
2017-06-02 04:50:51.214813: step 89510, loss = 0.65 (1670.8 examples/sec; 0.077 sec/batch)
2017-06-02 04:50:52.101156: step 89520, loss = 0.93 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 04:50:52.962089: step 89530, loss = 0.63 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:50:53.834708: step 89540, loss = 0.68 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:50:54.704360: step 89550, loss = 0.67 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:50:55.585233: step 89560, loss = 0.93 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:50:56.442924: step 89570, loss = 0.63 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:50:57.327116: step 89580, loss = 0.82 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:50:58.186072: step 89590, loss = 0.75 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:50:59.149071: step 89600, loss = 0.69 (1329.2 examples/sec; 0.096 sec/batch)
2017-06-02 04:50:59.943476: step 89610, loss = 0.70 (1611.3 examples/sec; 0.079 sec/batch)
2017-06-02 04:51:00.818009: step 89620, loss = 0.79 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:51:01.683455: step 89630, loss = 0.84 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:51:02.522814: step 89640, loss = 0.60 (1525.0 examples/sec; 0.084 sec/batch)
2017-06-02 04:51:03.385354: step 89650, loss = 0.73 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:51:04.243903: step 89660, loss = 0.64 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:51:05.068925: step 89670, loss = 0.75 (1551.5 examples/sec; 0.083 sec/batch)
2017-06-02 04:51:05.935598: step 89680, loss = 0.73 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:51:06.789951: step 89690, loss = 0.72 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:51:07.770087: step 89700, loss = 0.86 (1305.9 examples/sec; 0.098 sec/batch)
2017-06-02 04:51:08.532939: step 89710, loss = 0.82 (1677.9 examples/sec; 0.076 sec/batch)
2017-06-02 04:51:09.419000: step 89720, loss = 0.73 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:51:10.284023: step 89730, loss = 0.78 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:51:11.115625: step 89740, loss = 0.87 (1539.2 examples/sec; 0.083 sec/batch)
2017-06-02 04:51:11.977007: step 89750, loss = 0.65 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:51:12.829208: step 89760, loss = 0.65 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:51:13.690083: step 89770, loss = 0.83 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:51:14.547826: step 89780, loss = 0.66 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:51:15.406561: step 89790, loss = 0.68 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:51:16.398481: step 89800, loss = 0.90 (1290.4 examples/sec; 0.099 sec/batch)
2017-06-02 04:51:17.082751: step 89810, loss = 0.72 (1870.7 examples/sec; 0.068 sec/batch)
2017-06-02 04:51:17.922684: step 89820, loss = 0.72 (1523.9 examples/sec; 0.084 sec/batch)
2017-06-02 04:51:18.778658: step 89830, loss = 0.73 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:51:19.636587: step 89840, loss = 0.65 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:51:20.485045: step 89850, loss = 0.74 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:51:21.321494: step 89860, loss = 0.69 (1530.3 examples/sec; 0.084 sec/batch)
2017-06-02 04:51:22.132606: step 89870, loss = 0.76 (1578.1 examples/sec; 0.081 sec/batch)
2017-06-02 04:51:22.997737: step 89880, loss = 0.90 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:51:23.861660: step 89890, loss = 0.72 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:51:24.829740: step 89900, loss = 0.53 (1322.2 examples/sec; 0.097 sec/batch)
2017-06-02 04:51:25.589158: step 89910, loss = 0.58 (1685.5 examples/sec; 0.076 sec/batch)
2017-06-02 04:51:26.449943: step 89920, loss = 0.64 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:51:27.312180: step 89930, loss = 0.83 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:51:28.200199: step 89940, loss = 0.65 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 04:51:29.059026: step 89950, loss = 0.92 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:51:29.925082: step 89960, loss = 0.91 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:51:30.766259: step 89970, loss = 0.78 (1521.7 examples/sec; 0.084 sec/batch)
2017-06-02 04:51:31.617075: step 89980, loss = 0.70 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:51:32.470014: step 89990, loss = 0.68 (1500.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:51:33.415175: step 90000, loss = 0.60 (1354.3 examples/sec; 0.095 sec/batch)
2017-06-02 04:51:34.190166: step 90010, loss = 0.80 (1651.7 examples/sec; 0.077 sec/batch)
2017-06-02 04:51:35.065745: step 90020, loss = 0.75 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:51:35.931192: step 90030, loss = 0.78 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:51:36.794455: step 90040, loss = 0.71 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:51:37.651524: step 90050, loss = 0.76 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:51:38.509954: step 90060, loss = 0.72 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:51:39.339418: step 90070, loss = 0.66 (1543.2 examples/sec; 0.083 sec/batch)
2017-06-02 04:51:40.198502: step 90080, loss = 0.66 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:51:41.061067: step 90090, loss = 0.79 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:51:42.007790: step 90100, loss = 0.68 (1352.0 examples/sec; 0.095 sec/batch)
2017-06-02 04:51:42.770653: step 90110, loss = 0.82 (1677.9 examples/sec; 0.076 sec/batch)
2017-06-02 04:51:43.647940: step 90120, loss = 0.77 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:51:44.484389: step 90130, loss = 0.64 (1530.3 examples/sec; 0.084 sec/batch)
2017-06-02 04:51:45.341703: step 90140, loss = 0.55 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:51:46.209161: step 90150, loss = 0.74 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:51:47.080679: step 90160, loss = 0.68 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:51:47.918318: step 90170, loss = 0.80 (1528.1 examples/sec; 0.084 sec/batch)
2017-06-02 04:51:48.770029: step 90180, loss = 0.73 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:51:49.618458: step 90190, loss = 0.94 (1508.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:51:50.588568: step 90200, loss = 0.75 (1319.4 examples/sec; 0.097 sec/batch)
2017-06-02 04:51:51.377403: step 90210, loss = 0.84 (1622.6 examples/sec; 0.079 sec/batch)
2017-06-02 04:51:52.261742: step 90220, loss = 0.67 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:51:53.096980: step 90230, loss = 0.97 (1532.5 examples/sec; 0.084 sec/batch)
2017-06-02 04:51:53.968707: step 90240, loss = 0.82 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:51:54.804809: step 90250, loss = 0.71 (1530.9 examples/sec; 0.084 sec/batch)
2017-06-02 04:51:55.663748: step 90260, loss = 0.72 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:51:56.525306: step 90270, loss = 0.62 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:51:57.373056: step 90280, loss = 0.73 (1509.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:51:58.231302: step 90290, loss = 0.65 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:51:59.177040: step 90300, loss = 0.67 (1353.4 examples/sec; 0.095 sec/batch)
2017-06-02 04:51:59.938371: step 90310, loss = 0.59 (1681.3 examples/sec; 0.076 sec/batch)
2017-06-02 04:52:00.814096: step 90320, loss = 0.63 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:52:01.674288: step 90330, loss = 0.62 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:52:02.530551: step 90340, loss = 0.82 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:52:03.403436: step 90350, loss = 0.72 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:52:04.268185: step 90360, loss = 0.70 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:52:05.136420: step 90370, loss = 0.61 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:52:06.016853: step 90380, loss = 0.66 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:52:06.878844: step 90390, loss = 0.64 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:52:07.857414: step 90400, loss = 0.59 (1308.0 examples/sec; 0.098 sec/batch)
2017-06-02 04:52:08.621047: step 90410, loss = 0.81 (1676.2 examples/sec; 0.076 sec/batch)
2017-06-02 04:52:09.452103: step 90420, loss = 0.70 (1540.2 examples/sec; 0.083 sec/batch)
2017-06-02 04:52:10.278591: step 90430, loss = 0.74 (1548.7 examples/sec; 0.083 sec/batch)
2017-06-02 04:52:11.111190: step 90440, loss = 0.66 (1537.3 examples/sec; 0.083 sec/batch)
2017-06-02 04:52:11.981544: step 90450, loss = 0.76 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:52:12.816130: step 90460, loss = 0.63 (1533.7 examples/sec; 0.083 sec/batch)
2017-06-02 04:52:13.652128: step 90470, loss = 0.66 (1531.1 examples/sec; 0.084 sec/batch)
2017-06-02 04:52:14.493693: step 90480, loss = 0.55 (1521.0 examples/sec; 0.084 sec/batch)
2017-06-02 04:52:15.336235: step 90490, loss = 0.72 (1519.2 examples/sec; 0.084 sec/batch)
2017-06-02 04:52:16.270541: step 90500, loss = 0.66 (1370.0 examples/sec; 0.093 sec/batch)
2017-06-02 04:52:17.023656: step 90510, loss = 0.63 (1699.6 examples/sec; 0.075 sec/batch)
2017-06-02 04:52:17.890263: step 90520, loss = 0.70 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:52:18.733607: step 90530, loss = 0.61 (1517.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:52:19.578152: step 90540, loss = 0.73 (1515.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:52:20.416690: step 90550, loss = 0.53 (1526.5 examples/sec; 0.084 sec/batch)
2017-06-02 04:52:21.291521: step 90560, loss = 0.67 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:52:22.129966: step 90570, loss = 0.82 (1526.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:52:22.983810: step 90580, loss = 0.83 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:52:23.843369: step 90590, loss = 0.69 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:52:24.854587: step 90600, loss = 0.73 (1265.8 examples/sec; 0.101 sec/batch)
2017-06-02 04:52:25.564711: step 90610, loss = 0.72 (1802.5 examples/sec; 0.071 sec/batch)
2017-06-02 04:52:26.441802: step 90620, loss = 0.60 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:52:27.282378: step 90630, loss = 0.72 (1522.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:52:28.122869: step 90640, loss = 0.78 (1522.9 examples/sec; 0.084 sec/batch)
2017-06-02 04:52:28.998837: step 90650, loss = 0.62 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:52:29.858258: step 90660, loss = 0.82 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:52:30.705337: step 90670, loss = 0.63 (1511.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:52:31.577963: step 90680, loss = 0.56 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:52:32.444187: step 90690, loss = 0.62 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:52:33.404220: step 90700, loss = 0.74 (1333.3 examples/sec; 0.096 sec/batch)
2017-06-02 04:52:34.194319: step 90710, loss = 0.65 (1620.1 examples/sec; 0.079 sec/batch)
2017-06-02 04:52:35.057122: step 90720, loss = 0.76 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:52:35.904140: step 90730, loss = 0.75 (1511.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:52:36.766427: step 90740, loss = 0.65 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:52:37.606325: step 90750, loss = 0.66 (1524.0 examples/sec; 0.084 sec/batch)
2017-06-02 04:52:38.451278: step 90760, loss = 0.70 (1514.9 examples/sec; 0.084 sec/batch)
2017-06-02 04:52:39.324783: step 90770, loss = 0.74 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:52:40.156814: step 90780, loss = 0.76 (1538.4 examples/sec; 0.083 sec/batch)
2017-06-02 04:52:41.011568: step 90790, loss = 0.85 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:52:41.966806: step 90800, loss = 0.76 (1340.0 examples/sec; 0.096 sec/batch)
2017-06-02 04:52:42.705421: step 90810, loss = 0.86 (1733.0 examples/sec; 0.074 sec/batch)
2017-06-02 04:52:43.569267: step 90820, loss = 0.77 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:52:44.421976: step 90830, loss = 0.72 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:52:45.263586: step 90840, loss = 0.76 (1520.9 examples/sec; 0.084 sec/batch)
2017-06-02 04:52:46.105354: step 90850, loss = 0.70 (1520.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:52:46.957316: step 90860, loss = 0.76 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:52:47.837666: step 90870, loss = 0.73 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:52:48.675249: step 90880, loss = 0.81 (1528.2 examples/sec; 0.084 sec/batch)
2017-06-02 04:52:49.520483: step 90890, loss = 0.62 (1514.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:52:50.487322: step 90900, loss = 0.72 (1323.9 examples/sec; 0.097 sec/batch)
2017-06-02 04:52:51.273703: step 90910, loss = 0.77 (1627.7 examples/sec; 0.079 sec/batch)
2017-06-02 04:52:52.132246: step 90920, loss = 0.82 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:52:53.007054: step 90930, loss = 0.78 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:52:53.864746: step 90940, loss = 0.64 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:52:54.718570: step 90950, loss = 0.89 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:52:55.575939: step 90960, loss = 0.67 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:52:56.415522: step 90970, loss = 0.73 (1524.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:52:57.280593: step 90980, loss = 0.69 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:52:58.139849: step 90990, loss = 0.83 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:52:59.057479: step 91000, loss = 0.86 (1394.9 examples/sec; 0.092 sec/batch)
2017-06-02 04:52:59.785205: step 91010, loss = 0.70 (1758.9 examples/sec; 0.073 sec/batch)
2017-06-02 04:53:00.634941: step 91020, loss = 0.64 (1506.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:53:01.478468: step 91030, loss = 0.80 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 04:53:02.349881: step 91040, loss = 0.80 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:53:03.216572: step 91050, loss = 0.78 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:53:04.069206: step 91060, loss = 0.66 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:53:04.941787: step 91070, loss = 0.66 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:53:05.820398: step 91080, loss = 0.80 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:53:06.682942: step 91090, loss = 0.68 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:53:07.647164: step 91100, loss = 0.70 (1327.5 examples/sec; 0.096 sec/batch)
2017-06-02 04:53:08.417040: step 91110, loss = 0.66 (1662.6 examples/sec; 0.077 sec/batch)
2017-06-02 04:53:09.297133: step 91120, loss = 0.71 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:53:10.154186: step 91130, loss = 0.78 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:53:10.983853: step 91140, loss = 0.92 (1542.8 examples/sec; 0.083 sec/batch)
2017-06-02 04:53:11.842168: step 91150, loss = 0.72 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:53:12.691659: step 91160, loss = 0.76 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:53:13.557449: step 91170, loss = 0.76 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:53:14.404269: step 91180, loss = 0.68 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:53:15.272308: step 91190, loss = 0.66 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:53:16.225343: step 91200, loss = 0.66 (1343.1 examples/sec; 0.095 sec/batch)
2017-06-02 04:53:16.977306: step 91210, loss = 0.81 (1702.2 examples/sec; 0.075 sec/batch)
2017-06-02 04:53:17.828563: step 91220, loss = 0.69 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:53:18.681197: step 91230, loss = 0.67 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:53:19.526037: step 91240, loss = 0.65 (1515.1 examples/sec; 0.084 sec/batch)
2017-06-02 04:53:20.393649: step 91250, loss = 0.64 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:53:21.257844: step 91260, loss = 0.73 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:53:22.095420: step 91270, loss = 0.84 (1528.2 examples/sec; 0.084 sec/batch)
2017-06-02 04:53:22.934587: step 91280, loss = 0.68 (1525.3 examples/sec; 0.084 sec/batch)
2017-06-02 04:53:23.797002: step 91290, loss = 0.70 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:53:24.765865: step 91300, loss = 0.86 (1321.1 examples/sec; 0.097 sec/batch)
2017-06-02 04:53:25.524529: step 91310, loss = 0.63 (1687.2 examples/sec; 0.076 sec/batch)
2017-06-02 04:53:26.417559: step 91320, loss = 0.70 (1433.3 examples/sec; 0.089 sec/batch)
2017-06-02 04:53:27.299345: step 91330, loss = 0.67 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:53:28.160267: step 91340, loss = 0.66 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:53:28.989711: step 91350, loss = 0.64 (1543.2 examples/sec; 0.083 sec/batch)
2017-06-02 04:53:29.831028: step 91360, loss = 0.60 (1521.4 examples/sec; 0.084 sec/batch)
2017-06-02 04:53:30.688897: step 91370, loss = 0.59 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:53:31.544282: step 91380, loss = 0.62 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:53:32.414069: step 91390, loss = 0.73 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:53:33.381223: step 91400, loss = 0.76 (1323.5 examples/sec; 0.097 sec/batch)
2017-06-02 04:53:34.152467: step 91410, loss = 0.66 (1659.7 examples/sec; 0.077 sec/batch)
2017-06-02 04:53:34.991302: step 91420, loss = 0.72 (1525.9 examples/sec; 0.084 sec/batch)
2017-06-02 04:53:35.845595: step 91430, loss = 0.63 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:53:36.723353: step 91440, loss = 0.71 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:53:37.594587: step 91450, loss = 0.79 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:53:38.439148: step 91460, loss = 0.79 (1515.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:53:39.301022: step 91470, loss = 0.61 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:53:40.147687: step 91480, loss = 0.91 (1511.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:53:41.034346: step 91490, loss = 0.78 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:53:41.977199: step 91500, loss = 0.77 (1357.6 examples/sec; 0.094 sec/batch)
2017-06-02 04:53:42.742847: step 91510, loss = 0.62 (1671.8 examples/sec; 0.077 sec/batch)
2017-06-02 04:53:43.586667: step 91520, loss = 0.62 (1516.9 examples/sec; 0.084 sec/batch)
2017-06-02 04:53:44.469599: step 91530, loss = 0.68 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:53:45.334077: step 91540, loss = 0.73 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:53:46.167205: step 91550, loss = 0.80 (1536.4 examples/sec; 0.083 sec/batch)
2017-06-02 04:53:47.043462: step 91560, loss = 0.75 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:53:47.913435: step 91570, loss = 0.70 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:53:48.779879: step 91580, loss = 0.85 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:53:49.621456: step 91590, loss = 0.76 (1521.0 examples/sec; 0.084 sec/batch)
2017-06-02 04:53:50.595078: step 91600, loss = 0.73 (1314.7 examples/sec; 0.097 sec/batch)
2017-06-02 04:53:51.353499: step 91610, loss = 0.63 (1687.7 examples/sec; 0.076 sec/batch)
2017-06-02 04:53:52.219332: step 91620, loss = 0.70 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:53:53.088521: step 91630, loss = 0.82 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:53:53.946247: step 91640, loss = 0.65 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:53:54.809600: step 91650, loss = 0.74 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:53:55.661760: step 91660, loss = 0.76 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:53:56.546042: step 91670, loss = 0.72 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:53:57.403677: step 91680, loss = 0.72 (1492.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:53:58.236245: step 91690, loss = 0.67 (1537.4 examples/sec; 0.083 sec/batch)
2017-06-02 04:53:59.226579: step 91700, loss = 0.86 (1292.5 examples/sec; 0.099 sec/batch)
2017-06-02 04:54:00.002574: step 91710, loss = 0.63 (1649.5 examples/sec; 0.078 sec/batch)
2017-06-02 04:54:00.852559: step 91720, loss = 0.69 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:54:01.739701: step 91730, loss = 0.72 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:54:02.608752: step 91740, loss = 0.70 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:54:03.466103: step 91750, loss = 0.77 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:54:04.339423: step 91760, loss = 0.70 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:54:05.194953: step 91770, loss = 0.70 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:54:06.058119: step 91780, loss = 0.82 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:54:06.886097: step 91790, loss = 0.68 (1545.9 examples/sec; 0.083 sec/batch)
2017-06-02 04:54:07.879563: step 91800, loss = 0.78 (1288.4 examples/sec; 0.099 sec/batch)
2017-06-02 04:54:08.639543: step 91810, loss = 0.73 (1684.2 examples/sec; 0.076 sec/batch)
2017-06-02 04:54:09.507603: step 91820, loss = 0.79 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:54:10.350677: step 91830, loss = 0.81 (1518.2 examples/sec; 0.084 sec/batch)
2017-06-02 04:54:11.199428: step 91840, loss = 0.73 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:54:12.030331: step 91850, loss = 0.62 (1540.5 examples/sec; 0.083 sec/batch)
2017-06-02 04:54:12.908127: step 91860, loss = 0.72 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:54:13.759715: step 91870, loss = 0.72 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:54:14.625485: step 91880, loss = 0.70 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:54:15.509671: step 91890, loss = 0.66 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:54:16.442280: step 91900, loss = 0.70 (1372.5 examples/sec; 0.093 sec/batch)
2017-06-02 04:54:17.203131: step 91910, loss = 0.75 (1682.3 examples/sec; 0.076 sec/batch)
2017-06-02 04:54:18.060229: step 91920, loss = 0.66 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:54:18.894445: step 91930, loss = 0.65 (1534.4 examples/sec; 0.083 sec/batch)
2017-06-02 04:54:19.749850: step 91940, loss = 0.63 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:54:20.620440: step 91950, loss = 0.68 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:54:21.484594: step 91960, loss = 0.67 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:54:22.331006: step 91970, loss = 0.66 (1512.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:54:23.196646: step 91980, loss = 0.76 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:54:24.049942: step 91990, loss = 0.84 (1500.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:54:25.005039: step 92000, loss = 0.76 (1340.2 examples/sec; 0.096 sec/batch)
2017-06-02 04:54:25.789629: step 92010, loss = 0.57 (1631.4 examples/sec; 0.078 sec/batch)
2017-06-02 04:54:26.644898: step 92020, loss = 0.58 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:54:27.505313: step 92030, loss = 0.56 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:54:28.343685: step 92040, loss = 0.66 (1526.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:54:29.216090: step 92050, loss = 0.82 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:54:30.057836: step 92060, loss = 0.66 (1520.7 examples/sec; 0.084 sec/batch)
2017-06-02 04:54:30.895575: step 92070, loss = 0.67 (1527.9 examples/sec; 0.084 sec/batch)
2017-06-02 04:54:31.766637: step 92080, loss = 0.61 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:54:32.633479: step 92090, loss = 0.75 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:54:33.603407: step 92100, loss = 0.62 (1319.7 examples/sec; 0.097 sec/batch)
2017-06-02 04:54:34.389668: step 92110, loss = 0.79 (1628.0 examples/sec; 0.079 sec/batch)
2017-06-02 04:54:35.253196: step 92120, loss = 0.68 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:54:36.105027: step 92130, loss = 0.82 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:54:36.960566: step 92140, loss = 0.65 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:54:37.779059: step 92150, loss = 0.70 (1563.8 examples/sec; 0.082 sec/batch)
2017-06-02 04:54:38.627331: step 92160, loss = 0.78 (1508.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:54:39.513191: step 92170, loss = 0.70 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:54:40.366147: step 92180, loss = 0.63 (1500.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:54:41.200037: step 92190, loss = 0.58 (1535.0 examples/sec; 0.083 sec/batch)
2017-06-02 04:54:42.160628: step 92200, loss = 0.71 (1332.5 examples/sec; 0.096 sec/batch)
2017-06-02 04:54:42.916512: step 92210, loss = 0.78 (1693.4 examples/sec; 0.076 sec/batch)
2017-06-02 04:54:43.769283: step 92220, loss = 0.83 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:54:44.620852: step 92230, loss = 0.62 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:54:45.475743: step 92240, loss = 0.67 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:54:46.342064: step 92250, loss = 0.69 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:54:47.188192: step 92260, loss = 0.81 (1512.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:54:48.029185: step 92270, loss = 0.63 (1522.0 examples/sec; 0.084 sec/batch)
2017-06-02 04:54:48.903217: step 92280, loss = 0.61 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:54:49.725008: step 92290, loss = 0.62 (1557.6 examples/sec; 0.082 sec/batch)
2017-06-02 04:54:50.681109: step 92300, loss = 0.68 (1338.8 examples/sec; 0.096 sec/batch)
2017-06-02 04:54:51.426474: step 92310, loss = 0.72 (1717.3 examples/sec; 0.075 sec/batch)
2017-06-02 04:54:52.283240: step 92320, loss = 0.72 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:54:53.135596: step 92330, loss = 0.69 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:54:53.999574: step 92340, loss = 0.69 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:54:54.853230: step 92350, loss = 0.62 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:54:55.675135: step 92360, loss = 0.74 (1557.4 examples/sec; 0.082 sec/batch)
2017-06-02 04:54:56.521917: step 92370, loss = 0.68 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:54:57.360826: step 92380, loss = 0.78 (1525.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:54:58.203157: step 92390, loss = 0.61 (1519.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:54:59.167546: step 92400, loss = 0.77 (1327.3 examples/sec; 0.096 sec/batch)
2017-06-02 04:54:59.940056: step 92410, loss = 0.64 (1656.9 examples/sec; 0.077 sec/batch)
2017-06-02 04:55:00.797906: step 92420, loss = 0.60 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:55:01.650659: step 92430, loss = 0.74 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:55:02.487243: step 92440, loss = 0.58 (1530.0 examples/sec; 0.084 sec/batch)
2017-06-02 04:55:03.363358: step 92450, loss = 0.64 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:55:04.236440: step 92460, loss = 0.59 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:55:05.086958: step 92470, loss = 0.60 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:55:05.949071: step 92480, loss = 0.70 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:55:06.823914: step 92490, loss = 0.73 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:55:07.775020: step 92500, loss = 0.78 (1345.8 examples/sec; 0.095 sec/batch)
2017-06-02 04:55:08.535795: step 92510, loss = 0.67 (1682.5 examples/sec; 0.076 sec/batch)
2017-06-02 04:55:09.370580: step 92520, loss = 0.71 (1533.3 examples/sec; 0.083 sec/batch)
2017-06-02 04:55:10.236411: step 92530, loss = 0.59 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:55:11.079749: step 92540, loss = 0.81 (1517.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:55:11.947812: step 92550, loss = 0.74 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:55:12.811871: step 92560, loss = 0.68 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:55:13.693125: step 92570, loss = 0.59 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:55:14.548931: step 92580, loss = 0.91 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:55:15.400694: step 92590, loss = 0.82 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:55:16.367041: step 92600, loss = 0.84 (1324.6 examples/sec; 0.097 sec/batch)
2017-06-02 04:55:17.132758: step 92610, loss = 0.76 (1671.7 examples/sec; 0.077 sec/batch)
2017-06-02 04:55:18.015722: step 92620, loss = 0.73 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:55:18.888443: step 92630, loss = 0.70 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:55:19.763568: step 92640, loss = 0.56 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:55:20.611591: step 92650, loss = 0.87 (1509.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:55:21.481460: step 92660, loss = 0.79 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:55:22.292369: step 92670, loss = 0.61 (1578.5 examples/sec; 0.081 sec/batch)
2017-06-02 04:55:23.136055: step 92680, loss = 0.82 (1517.2 examples/sec; 0.084 sec/batch)
2017-06-02 04:55:24.023245: step 92690, loss = 0.64 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 04:55:24.978912: step 92700, loss = 0.86 (1339.4 examples/sec; 0.096 sec/batch)
2017-06-02 04:55:25.743609: step 92710, loss = 0.59 (1673.9 examples/sec; 0.076 sec/batch)
2017-06-02 04:55:26.583287: step 92720, loss = 0.69 (1524.4 examples/sec; 0.084 sec/batch)
2017-06-02 04:55:27.439224: step 92730, loss = 0.82 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:55:28.284011: step 92740, loss = 0.84 (1515.2 examples/sec; 0.084 sec/batch)
2017-06-02 04:55:29.163276: step 92750, loss = 0.71 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:55:30.039126: step 92760, loss = 0.74 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:55:30.919452: step 92770, loss = 0.90 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:55:31.770576: step 92780, loss = 0.65 (1503.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:55:32.611170: step 92790, loss = 0.71 (1522.7 examples/sec; 0.084 sec/batch)
2017-06-02 04:55:33.573226: step 92800, loss = 0.71 (1330.5 examples/sec; 0.096 sec/batch)
2017-06-02 04:55:34.334594: step 92810, loss = 0.65 (1681.2 examples/sec; 0.076 sec/batch)
2017-06-02 04:55:35.199791: step 92820, loss = 0.73 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:55:36.049402: step 92830, loss = 0.67 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:55:36.896063: step 92840, loss = 0.85 (1511.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:55:37.731651: step 92850, loss = 0.76 (1531.9 examples/sec; 0.084 sec/batch)
2017-06-02 04:55:38.595615: step 92860, loss = 0.58 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:55:39.465682: step 92870, loss = 0.61 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:55:40.313453: step 92880, loss = 0.74 (1509.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:55:41.169999: step 92890, loss = 0.80 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:55:42.110808: step 92900, loss = 0.64 (1360.5 examples/sec; 0.094 sec/batch)
2017-06-02 04:55:42.861909: step 92910, loss = 0.70 (1704.2 examples/sec; 0.075 sec/batch)
2017-06-02 04:55:43.741786: step 92920, loss = 0.74 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:55:44.612878: step 92930, loss = 0.79 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:55:45.479747: step 92940, loss = 0.82 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:55:46.345232: step 92950, loss = 0.72 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:55:47.226921: step 92960, loss = 0.71 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:55:48.112148: step 92970, loss = 0.60 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:55:48.991924: step 92980, loss = 0.68 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:55:49.849580: step 92990, loss = 0.65 (1492.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:55:50.802295: step 93000, loss = 0.47 (1343.5 examples/sec; 0.095 sec/batch)
2017-06-02 04:55:51.562548: step 93010, loss = 0.78 (1683.7 examples/sec; 0.076 sec/batch)
2017-06-02 04:55:52.422801: step 93020, loss = 0.74 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:55:53.292430: step 93030, loss = 0.61 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:55:54.148548: step 93040, loss = 0.70 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:55:54.992436: step 93050, loss = 0.63 (1516.8 examples/sec; 0.084 sec/batch)
2017-06-02 04:55:55.857026: step 93060, loss = 0.66 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:55:56.721236: step 93070, loss = 0.73 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:55:57.585793: step 93080, loss = 0.75 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:55:58.425874: step 93090, loss = 0.65 (1523.7 examples/sec; 0.084 sec/batch)
2017-06-02 04:55:59.407767: step 93100, loss = 0.67 (1303.6 examples/sec; 0.098 sec/batch)
2017-06-02 04:56:00.185101: step 93110, loss = 0.63 (1646.6 examples/sec; 0.078 sec/batch)
2017-06-02 04:56:01.054975: step 93120, loss = 0.76 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:56:01.931504: step 93130, loss = 0.81 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:56:02.792839: step 93140, loss = 0.73 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:56:03.665874: step 93150, loss = 0.84 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:56:04.545639: step 93160, loss = 0.89 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:56:05.407907: step 93170, loss = 0.62 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:56:06.263853: step 93180, loss = 0.67 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:56:07.144316: step 93190, loss = 0.73 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:56:08.104706: step 93200, loss = 0.74 (1332.8 examples/sec; 0.096 sec/batch)
2017-06-02 04:56:08.879648: step 93210, loss = 0.75 (1651.7 examples/sec; 0.077 sec/batch)
2017-06-02 04:56:09.740368: step 93220, loss = 0.64 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:56:10.584475: step 93230, loss = 0.80 (1516.4 examples/sec; 0.084 sec/batch)
2017-06-02 04:56:11.467691: step 93240, loss = 0.69 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:56:12.317833: step 93250, loss = 0.77 (1505.6 examples/sec; 0.085 sec/batch)
2017-06-02 04:56:13.166918: step 93260, loss = 0.63 (1507.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:56:14.035448: step 93270, loss = 0.69 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:56:14.900536: step 93280, loss = 0.73 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:56:15.752844: step 93290, loss = 0.75 (1501.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:56:16.769023: step 93300, loss = 0.77 (1259.6 examples/sec; 0.102 sec/batch)
2017-06-02 04:56:17.495611: step 93310, loss = 0.71 (1761.7 examples/sec; 0.073 sec/batch)
2017-06-02 04:56:18.358556: step 93320, loss = 0.60 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:56:19.232812: step 93330, loss = 0.65 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:56:20.088245: step 93340, loss = 0.74 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:56:20.964949: step 93350, loss = 0.76 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:56:21.812515: step 93360, loss = 0.70 (1510.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:56:22.682085: step 93370, loss = 0.60 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:56:23.537061: step 93380, loss = 0.70 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:56:24.405841: step 93390, loss = 0.61 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:56:25.358498: step 93400, loss = 0.66 (1343.6 examples/sec; 0.095 sec/batch)
2017-06-02 04:56:26.145591: step 93410, loss = 0.72 (1626.3 examples/sec; 0.079 sec/batch)
2017-06-02 04:56:27.001509: step 93420, loss = 0.60 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:56:27.856563: step 93430, loss = 0.82 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:56:28.738930: step 93440, loss = 0.60 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:56:29.606665: step 93450, loss = 0.81 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:56:30.482434: step 93460, loss = 0.73 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:56:31.375193: step 93470, loss = 0.62 (1433.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:56:32.229704: step 93480, loss = 0.73 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:56:33.096630: step 93490, loss = 0.78 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:56:34.052635: step 93500, loss = 0.65 (1338.9 examples/sec; 0.096 sec/batch)
2017-06-02 04:56:34.829607: step 93510, loss = 0.67 (1647.4 examples/sec; 0.078 sec/batch)
2017-06-02 04:56:35.691110: step 93520, loss = 0.80 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:56:36.579404: step 93530, loss = 0.62 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 04:56:37.477174: step 93540, loss = 0.75 (1425.8 examples/sec; 0.090 sec/batch)
2017-06-02 04:56:38.378823: step 93550, loss = 0.73 (1419.6 examples/sec; 0.090 sec/batch)
2017-06-02 04:56:39.262986: step 93560, loss = 0.76 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:56:40.146109: step 93570, loss = 0.78 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:56:41.017743: step 93580, loss = 0.61 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:56:41.907485: step 93590, loss = 0.66 (1438.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:56:42.916684: step 93600, loss = 0.81 (1268.3 examples/sec; 0.101 sec/batch)
2017-06-02 04:56:43.636760: step 93610, loss = 0.77 (1777.7 examples/sec; 0.072 sec/batch)
2017-06-02 04:56:44.523297: step 93620, loss = 0.73 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 04:56:45.395873: step 93630, loss = 0.77 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:56:46.284377: step 93640, loss = 0.70 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 04:56:47.148037: step 93650, loss = 0.70 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:56:48.021519: step 93660, loss = 0.71 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:56:48.894139: step 93670, loss = 0.73 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:56:49.766112: step 93680, loss = 0.57 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:56:50.633878: step 93690, loss = 0.57 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:56:51.642253: step 93700, loss = 0.80 (1269.4 examples/sec; 0.101 sec/batch)
2017-06-02 04:56:52.364243: step 93710, loss = 0.63 (1772.9 examples/sec; 0.072 sec/batch)
2017-06-02 04:56:53.239679: step 93720, loss = 0.58 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:56:54.121532: step 93730, loss = 0.71 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:56:55.005975: step 93740, loss = 0.67 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:56:55.875986: step 93750, loss = 0.70 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:56:56.729975: step 93760, loss = 0.70 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:56:57.575301: step 93770, loss = 0.59 (1514.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:56:58.431953: step 93780, loss = 0.65 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:56:59.288225: step 93790, loss = 0.53 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:57:00.266968: step 93800, loss = 0.87 (1307.8 examples/sec; 0.098 sec/batch)
2017-06-02 04:57:01.058976: step 93810, loss = 0.67 (1616.2 examples/sec; 0.079 sec/batch)
2017-06-02 04:57:01.928865: step 93820, loss = 0.66 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:57:02.820265: step 93830, loss = 0.75 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:57:03.692834: step 93840, loss = 0.76 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:57:04.575139: step 93850, loss = 0.68 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 04:57:05.439427: step 93860, loss = 0.82 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:57:06.308119: step 93870, loss = 0.80 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:57:07.170948: step 93880, loss = 0.76 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:57:08.016212: step 93890, loss = 0.73 (1514.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:57:08.985820: step 93900, loss = 0.60 (1320.1 examples/sec; 0.097 sec/batch)
2017-06-02 04:57:09.768787: step 93910, loss = 0.91 (1634.8 examples/sec; 0.078 sec/batch)
2017-06-02 04:57:10.638915: step 93920, loss = 0.65 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:57:11.511654: step 93930, loss = 0.74 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:57:12.392979: step 93940, loss = 0.74 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:57:13.258232: step 93950, loss = 0.72 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:57:14.139718: step 93960, loss = 0.83 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:57:15.014151: step 93970, loss = 0.70 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:57:15.888901: step 93980, loss = 0.81 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:57:16.779718: step 93990, loss = 0.59 (1436.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:57:17.776649: step 94000, loss = 0.63 (1284.0 examples/sec; 0.100 sec/batch)
2017-06-02 04:57:18.522948: step 94010, loss = 0.76 (1715.1 examples/sec; 0.075 sec/batch)
2017-06-02 04:57:19.367278: step 94020, loss = 0.73 (1516.0 examples/sec; 0.084 sec/batch)
2017-06-02 04:57:20.248588: step 94030, loss = 0.84 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:57:21.087599: step 94040, loss = 0.76 (1525.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:57:21.944638: step 94050, loss = 0.68 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:57:22.803147: step 94060, loss = 0.60 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:57:23.646121: step 94070, loss = 0.71 (1518.4 examples/sec; 0.084 sec/batch)
2017-06-02 04:57:24.508584: step 94080, loss = 0.96 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:57:25.383337: step 94090, loss = 0.64 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:57:26.360713: step 94100, loss = 0.65 (1309.6 examples/sec; 0.098 sec/batch)
2017-06-02 04:57:27.138290: step 94110, loss = 0.62 (1646.1 examples/sec; 0.078 sec/batch)
2017-06-02 04:57:28.001902: step 94120, loss = 0.66 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:57:28.892962: step 94130, loss = 0.63 (1436.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:57:29.758967: step 94140, loss = 0.72 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:57:30.630535: step 94150, loss = 0.68 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:57:31.507851: step 94160, loss = 0.74 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:57:32.379120: step 94170, loss = 0.74 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:57:33.240765: step 94180, loss = 0.60 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:57:34.092658: step 94190, loss = 0.77 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:57:35.056560: step 94200, loss = 0.82 (1327.9 examples/sec; 0.096 sec/batch)
2017-06-02 04:57:35.820035: step 94210, loss = 0.68 (1676.5 examples/sec; 0.076 sec/batch)
2017-06-02 04:57:36.686963: step 94220, loss = 0.67 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:57:37.542087: step 94230, loss = 0.77 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:57:38.404375: step 94240, loss = 0.74 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:57:39.276774: step 94250, loss = 0.68 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:57:40.149503: step 94260, loss = 0.71 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:57:41.005942: step 94270, loss = 0.69 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:57:41.874983: step 94280, loss = 0.75 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:57:42.757205: step 94290, loss = 0.62 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:57:43.713943: step 94300, loss = 0.76 (1337.8 examples/sec; 0.096 sec/batch)
2017-06-02 04:57:44.482834: step 94310, loss = 0.72 (1664.8 examples/sec; 0.077 sec/batch)
2017-06-02 04:57:45.371603: step 94320, loss = 0.64 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 04:57:46.233967: step 94330, loss = 0.85 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:57:47.083270: step 94340, loss = 0.69 (1507.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:57:47.948574: step 94350, loss = 0.76 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:57:48.822551: step 94360, loss = 0.67 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:57:49.686717: step 94370, loss = 0.85 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:57:50.515077: step 94380, loss = 0.64 (1545.2 examples/sec; 0.083 sec/batch)
2017-06-02 04:57:51.382534: step 94390, loss = 0.62 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:57:52.341432: step 94400, loss = 0.62 (1334.9 examples/sec; 0.096 sec/batch)
2017-06-02 04:57:53.099234: step 94410, loss = 0.66 (1689.1 examples/sec; 0.076 sec/batch)
2017-06-02 04:57:53.954410: step 94420, loss = 0.70 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:57:54.812933: step 94430, loss = 0.68 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 04:57:55.694476: step 94440, loss = 0.79 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 04:57:56.548403: step 94450, loss = 0.68 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:57:57.404918: step 94460, loss = 0.80 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:57:58.251804: step 94470, loss = 0.82 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:57:59.093431: step 94480, loss = 0.70 (1520.9 examples/sec; 0.084 sec/batch)
2017-06-02 04:57:59.952682: step 94490, loss = 0.62 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:58:00.912993: step 94500, loss = 0.86 (1332.9 examples/sec; 0.096 sec/batch)
2017-06-02 04:58:01.688324: step 94510, loss = 0.77 (1650.9 examples/sec; 0.078 sec/batch)
2017-06-02 04:58:02.549306: step 94520, loss = 0.77 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 04:58:03.426574: step 94530, loss = 0.72 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:58:04.297473: step 94540, loss = 0.68 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:58:05.165892: step 94550, loss = 0.67 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:58:06.020399: step 94560, loss = 0.73 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:58:06.868233: step 94570, loss = 0.70 (1509.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:58:07.735343: step 94580, loss = 0.66 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:58:08.592989: step 94590, loss = 0.73 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:58:09.538968: step 94600, loss = 0.74 (1353.1 examples/sec; 0.095 sec/batch)
2017-06-02 04:58:10.278877: step 94610, loss = 0.67 (1730.0 examples/sec; 0.074 sec/batch)
2017-06-02 04:58:11.135092: step 94620, loss = 0.75 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:58:11.974187: step 94630, loss = 0.75 (1525.4 examples/sec; 0.084 sec/batch)
2017-06-02 04:58:12.878819: step 94640, loss = 0.66 (1414.9 examples/sec; 0.090 sec/batch)
2017-06-02 04:58:13.753103: step 94650, loss = 0.71 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:58:14.595464: step 94660, loss = 0.73 (1519.5 examples/sec; 0.084 sec/batch)
2017-06-02 04:58:15.469210: step 94670, loss = 0.79 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:58:16.317282: step 94680, loss = 0.67 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:58:17.198447: step 94690, loss = 0.70 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:58:18.178420: step 94700, loss = 0.82 (1306.1 examples/sec; 0.098 sec/batch)
2017-06-02 04:58:18.954267: step 94710, loss = 0.78 (1649.8 examples/sec; 0.078 sec/batch)
2017-06-02 04:58:19.823872: step 94720, loss = 0.64 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:58:20.685675: step 94730, loss = 0.70 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:58:21.575259: step 94740, loss = 0.65 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 04:58:22.440432: step 94750, loss = 0.79 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 04:58:23.306232: step 94760, loss = 0.74 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:58:24.171187: step 94770, loss = 0.81 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:58:25.026450: step 94780, loss = 0.63 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:58:25.910125: step 94790, loss = 0.68 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:58:26.857926: step 94800, loss = 0.75 (1350.5 examples/sec; 0.095 sec/batch)
2017-06-02 04:58:27.615382: step 94810, loss = 0.85 (1689.9 examples/sec; 0.076 sec/batch)
2017-06-02 04:58:28.473114: step 94820, loss = 0.70 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:58:29.326962: step 94830, loss = 0.73 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 04:58:30.173236: step 94840, loss = 0.74 (1512.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:58:31.017186: step 94850, loss = 0.70 (1516.7 examples/sec; 0.084 sec/batch)
2017-06-02 04:58:31.840199: step 94860, loss = 0.66 (1555.2 examples/sec; 0.082 sec/batch)
2017-06-02 04:58:32.714721: step 94870, loss = 0.57 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:58:33.567509: step 94880, loss = 0.75 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:58:34.412796: step 94890, loss = 0.61 (1514.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:58:35.376718: step 94900, loss = 0.71 (1327.9 examples/sec; 0.096 sec/batch)
2017-06-02 04:58:36.160715: step 94910, loss = 0.75 (1632.6 examples/sec; 0.078 sec/batch)
2017-06-02 04:58:37.026696: step 94920, loss = 0.66 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:58:37.882402: step 94930, loss = 0.78 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:58:38.722078: step 94940, loss = 0.58 (1524.4 examples/sec; 0.084 sec/batch)
2017-06-02 04:58:39.573008: step 94950, loss = 0.78 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:58:40.440064: step 94960, loss = 0.76 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:58:41.306129: step 94970, loss = 0.64 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 04:58:42.169478: step 94980, loss = 0.63 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:58:43.049845: step 94990, loss = 0.65 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:58:44.064307: step 95000, loss = 0.71 (1261.8 examples/sec; 0.101 sec/batch)
2017-06-02 04:58:44.814374: step 95010, loss = 0.67 (1706.5 examples/sec; 0.075 sec/batch)
2017-06-02 04:58:45.634074: step 95020, loss = 0.83 (1561.6 examples/sec; 0.082 sec/batch)
2017-06-02 04:58:46.492360: step 95030, loss = 0.60 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:58:47.372250: step 95040, loss = 0.68 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:58:48.247705: step 95050, loss = 0.78 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:58:49.116456: step 95060, loss = 0.87 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 04:58:50.005047: step 95070, loss = 0.71 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 04:58:50.907160: step 95080, loss = 0.70 (1418.9 examples/sec; 0.090 sec/batch)
2017-06-02 04:58:51.749707: step 95090, loss = 0.66 (1519.2 examples/sec; 0.084 sec/batch)
2017-06-02 04:58:52.714511: step 95100, loss = 0.58 (1326.7 examples/sec; 0.096 sec/batch)
2017-06-02 04:58:53.482002: step 95110, loss = 0.58 (1667.8 examples/sec; 0.077 sec/batch)
2017-06-02 04:58:54.344093: step 95120, loss = 0.81 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:58:55.224032: step 95130, loss = 0.74 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:58:56.078617: step 95140, loss = 0.72 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 04:58:56.942146: step 95150, loss = 0.77 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:58:57.810401: step 95160, loss = 0.61 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:58:58.678929: step 95170, loss = 0.73 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 04:58:59.540216: step 95180, loss = 0.80 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:59:00.402484: step 95190, loss = 0.72 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:59:01.361717: step 95200, loss = 0.69 (1334.4 examples/sec; 0.096 sec/batch)
2017-06-02 04:59:02.127626: step 95210, loss = 0.74 (1671.2 examples/sec; 0.077 sec/batch)
2017-06-02 04:59:02.983120: step 95220, loss = 0.77 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:59:03.864494: step 95230, loss = 0.83 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:59:04.715355: step 95240, loss = 0.83 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:59:05.575523: step 95250, loss = 0.65 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:59:06.422934: step 95260, loss = 0.62 (1510.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:59:07.272615: step 95270, loss = 0.73 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:59:08.143210: step 95280, loss = 0.71 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:59:09.019386: step 95290, loss = 0.67 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:59:09.984354: step 95300, loss = 0.80 (1326.5 examples/sec; 0.096 sec/batch)
2017-06-02 04:59:10.734020: step 95310, loss = 0.60 (1707.4 examples/sec; 0.075 sec/batch)
2017-06-02 04:59:11.612405: step 95320, loss = 0.70 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:59:12.492066: step 95330, loss = 0.69 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:59:13.372968: step 95340, loss = 0.69 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 04:59:14.242722: step 95350, loss = 0.56 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 04:59:15.124527: step 95360, loss = 0.69 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 04:59:16.003724: step 95370, loss = 0.75 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:59:16.874448: step 95380, loss = 0.53 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:59:17.724073: step 95390, loss = 0.71 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 04:59:18.709473: step 95400, loss = 0.63 (1299.0 examples/sec; 0.099 sec/batch)
2017-06-02 04:59:19.481652: step 95410, loss = 0.55 (1657.6 examples/sec; 0.077 sec/batch)
2017-06-02 04:59:20.316330: step 95420, loss = 0.61 (1533.5 examples/sec; 0.083 sec/batch)
2017-06-02 04:59:21.198795: step 95430, loss = 0.75 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 04:59:22.054835: step 95440, loss = 0.74 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:59:22.914786: step 95450, loss = 0.63 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 04:59:23.788418: step 95460, loss = 0.66 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:59:24.631853: step 95470, loss = 0.69 (1517.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:59:25.509020: step 95480, loss = 0.84 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 04:59:26.352122: step 95490, loss = 0.61 (1518.2 examples/sec; 0.084 sec/batch)
2017-06-02 04:59:27.345492: step 95500, loss = 0.71 (1288.5 examples/sec; 0.099 sec/batch)
2017-06-02 04:59:28.107464: step 95510, loss = 0.58 (1679.9 examples/sec; 0.076 sec/batch)
2017-06-02 04:59:28.959002: step 95520, loss = 0.73 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:59:29.854839: step 95530, loss = 0.69 (1428.8 examples/sec; 0.090 sec/batch)
2017-06-02 04:59:30.705341: step 95540, loss = 0.68 (1505.0 examples/sec; 0.085 sec/batch)
2017-06-02 04:59:31.581514: step 95550, loss = 0.77 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 04:59:32.464086: step 95560, loss = 0.92 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 04:59:33.323982: step 95570, loss = 0.56 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 04:59:34.193589: step 95580, loss = 0.74 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 04:59:35.047088: step 95590, loss = 0.99 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 04:59:36.018008: step 95600, loss = 0.74 (1318.3 examples/sec; 0.097 sec/batch)
2017-06-02 04:59:36.760614: step 95610, loss = 0.60 (1723.7 examples/sec; 0.074 sec/batch)
2017-06-02 04:59:37.625910: step 95620, loss = 0.81 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 04:59:38.487575: step 95630, loss = 0.62 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 04:59:39.336187: step 95640, loss = 0.76 (1508.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:59:40.206276: step 95650, loss = 0.61 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 04:59:41.069346: step 95660, loss = 0.77 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 04:59:41.903208: step 95670, loss = 0.70 (1535.0 examples/sec; 0.083 sec/batch)
2017-06-02 04:59:42.758630: step 95680, loss = 0.74 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:59:43.642342: step 95690, loss = 0.74 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 04:59:44.597629: step 95700, loss = 0.74 (1339.9 examples/sec; 0.096 sec/batch)
2017-06-02 04:59:45.316946: step 95710, loss = 0.71 (1779.5 examples/sec; 0.072 sec/batch)
2017-06-02 04:59:46.172561: step 95720, loss = 0.66 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 04:59:47.047673: step 95730, loss = 0.87 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 04:59:47.900223: step 95740, loss = 0.69 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 04:59:48.753953: step 95750, loss = 0.64 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 04:59:49.593520: step 95760, loss = 0.64 (1524.6 examples/sec; 0.084 sec/batch)
2017-06-02 04:59:50.443528: step 95770, loss = 0.66 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 04:59:51.374913: step 95780, loss = 0.67 (1374.3 examples/sec; 0.093 sec/batch)
2017-06-02 04:59:52.241407: step 95790, loss = 0.62 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 04:59:53.207087: step 95800, loss = 0.73 (1325.5 examples/sec; 0.097 sec/batch)
2017-06-02 04:59:53.973225: step 95810, loss = 0.65 (1670.7 examples/sec; 0.077 sec/batch)
2017-06-02 04:59:54.836152: step 95820, loss = 0.68 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 04:59:55.700534: step 95830, loss = 0.69 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 04:59:56.548664: step 95840, loss = 0.74 (1509.2 examples/sec; 0.085 sec/batch)
2017-06-02 04:59:57.413398: step 95850, loss = 0.73 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 04:59:58.236786: step 95860, loss = 0.79 (1554.6 examples/sec; 0.082 sec/batch)
2017-06-02 04:59:59.107798: step 95870, loss = 0.59 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 04:59:59.969833: step 95880, loss = 0.73 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:00:00.798406: step 95890, loss = 0.64 (1544.8 examples/sec; 0.083 sec/batch)
2017-06-02 05:00:01.742719: step 95900, loss = 0.69 (1355.5 examples/sec; 0.094 sec/batch)
2017-06-02 05:00:02.522742: step 95910, loss = 0.78 (1641.0 examples/sec; 0.078 sec/batch)
2017-06-02 05:00:03.373971: step 95920, loss = 0.65 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:00:04.239628: step 95930, loss = 0.72 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:00:05.081911: step 95940, loss = 0.77 (1519.7 examples/sec; 0.084 sec/batch)
2017-06-02 05:00:05.938139: step 95950, loss = 0.64 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:00:06.756396: step 95960, loss = 0.69 (1564.3 examples/sec; 0.082 sec/batch)
2017-06-02 05:00:07.588658: step 95970, loss = 0.77 (1538.0 examples/sec; 0.083 sec/batch)
2017-06-02 05:00:08.449700: step 95980, loss = 0.67 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:00:09.302781: step 95990, loss = 0.79 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:00:10.279167: step 96000, loss = 0.76 (1310.9 examples/sec; 0.098 sec/batch)
2017-06-02 05:00:11.040363: step 96010, loss = 0.63 (1681.6 examples/sec; 0.076 sec/batch)
2017-06-02 05:00:11.901950: step 96020, loss = 0.61 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:00:12.774472: step 96030, loss = 0.72 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:00:13.639744: step 96040, loss = 0.73 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:00:14.506091: step 96050, loss = 0.69 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:00:15.373635: step 96060, loss = 0.74 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:00:16.263328: step 96070, loss = 0.80 (1438.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:00:17.145772: step 96080, loss = 0.59 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:00:18.008345: step 96090, loss = 0.63 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:00:18.974428: step 96100, loss = 0.57 (1324.9 examples/sec; 0.097 sec/batch)
2017-06-02 05:00:19.760185: step 96110, loss = 0.72 (1629.0 examples/sec; 0.079 sec/batch)
2017-06-02 05:00:20.641071: step 96120, loss = 0.69 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:00:21.490874: step 96130, loss = 0.88 (1506.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:00:22.360581: step 96140, loss = 0.76 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:00:23.238279: step 96150, loss = 0.75 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:00:24.104643: step 96160, loss = 0.76 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:00:24.942850: step 96170, loss = 0.61 (1527.1 examples/sec; 0.084 sec/batch)
2017-06-02 05:00:25.821758: step 96180, loss = 0.63 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:00:26.693004: step 96190, loss = 0.77 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:00:27.641307: step 96200, loss = 0.56 (1349.8 examples/sec; 0.095 sec/batch)
2017-06-02 05:00:28.412806: step 96210, loss = 0.61 (1659.1 examples/sec; 0.077 sec/batch)
2017-06-02 05:00:29.307467: step 96220, loss = 0.82 (1430.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:00:30.162277: step 96230, loss = 0.79 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:00:31.051065: step 96240, loss = 0.66 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:00:31.941365: step 96250, loss = 0.76 (1437.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:00:32.824867: step 96260, loss = 0.53 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:00:33.667490: step 96270, loss = 0.66 (1519.0 examples/sec; 0.084 sec/batch)
2017-06-02 05:00:34.514627: step 96280, loss = 0.64 (1511.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:00:35.368441: step 96290, loss = 0.87 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:00:36.335944: step 96300, loss = 0.76 (1323.0 examples/sec; 0.097 sec/batch)
2017-06-02 05:00:37.070069: step 96310, loss = 0.68 (1743.6 examples/sec; 0.073 sec/batch)
2017-06-02 05:00:37.936075: step 96320, loss = 0.58 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:00:38.789758: step 96330, loss = 0.74 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:00:39.646341: step 96340, loss = 0.90 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:00:40.499640: step 96350, loss = 0.67 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:00:41.366916: step 96360, loss = 0.62 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:00:42.222025: step 96370, loss = 0.69 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:00:43.089628: step 96380, loss = 0.71 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:00:43.937207: step 96390, loss = 0.79 (1510.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:00:44.907680: step 96400, loss = 0.70 (1319.0 examples/sec; 0.097 sec/batch)
2017-06-02 05:00:45.664127: step 96410, loss = 0.67 (1692.1 examples/sec; 0.076 sec/batch)
2017-06-02 05:00:46.508956: step 96420, loss = 0.71 (1515.1 examples/sec; 0.084 sec/batch)
2017-06-02 05:00:47.382306: step 96430, loss = 0.70 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:00:48.232912: step 96440, loss = 0.67 (1504.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:00:49.118301: step 96450, loss = 0.63 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:00:49.967668: step 96460, loss = 0.60 (1507.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:00:50.816770: step 96470, loss = 0.63 (1507.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:00:51.669672: step 96480, loss = 0.77 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:00:52.553364: step 96490, loss = 0.89 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:00:53.513500: step 96500, loss = 0.88 (1333.1 examples/sec; 0.096 sec/batch)
2017-06-02 05:00:54.288707: step 96510, loss = 0.74 (1651.2 examples/sec; 0.078 sec/batch)
2017-06-02 05:00:55.132096: step 96520, loss = 0.74 (1517.7 examples/sec; 0.084 sec/batch)
2017-06-02 05:00:55.997909: step 96530, loss = 0.59 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:00:56.866467: step 96540, loss = 0.74 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:00:57.740051: step 96550, loss = 0.75 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:00:58.610274: step 96560, loss = 0.76 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:00:59.487501: step 96570, loss = 0.63 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:01:00.362680: step 96580, loss = 0.85 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:01:01.244410: step 96590, loss = 0.70 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:01:02.199355: step 96600, loss = 0.48 (1340.4 examples/sec; 0.095 sec/batch)
2017-06-02 05:01:02.942817: step 96610, loss = 0.73 (1721.7 examples/sec; 0.074 sec/batch)
2017-06-02 05:01:03.793385: step 96620, loss = 0.76 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:01:04.636563: step 96630, loss = 0.71 (1518.1 examples/sec; 0.084 sec/batch)
2017-06-02 05:01:05.507972: step 96640, loss = 0.88 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:01:06.336578: step 96650, loss = 0.70 (1544.8 examples/sec; 0.083 sec/batch)
2017-06-02 05:01:07.205794: step 96660, loss = 0.55 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:01:08.063888: step 96670, loss = 0.56 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:01:08.922707: step 96680, loss = 0.66 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:01:09.774475: step 96690, loss = 0.73 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:01:10.709044: step 96700, loss = 0.54 (1369.6 examples/sec; 0.093 sec/batch)
2017-06-02 05:01:11.487636: step 96710, loss = 0.82 (1644.0 examples/sec; 0.078 sec/batch)
2017-06-02 05:01:12.327597: step 96720, loss = 0.67 (1523.9 examples/sec; 0.084 sec/batch)
2017-06-02 05:01:13.175891: step 96730, loss = 0.59 (1508.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:01:14.058516: step 96740, loss = 0.57 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:01:14.929183: step 96750, loss = 0.68 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:01:15.797815: step 96760, loss = 0.64 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:01:16.657426: step 96770, loss = 0.67 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:01:17.508920: step 96780, loss = 0.78 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:01:18.384700: step 96790, loss = 0.64 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:01:19.352029: step 96800, loss = 0.73 (1323.2 examples/sec; 0.097 sec/batch)
2017-06-02 05:01:20.132083: step 96810, loss = 0.65 (1640.9 examples/sec; 0.078 sec/batch)
2017-06-02 05:01:21.002292: step 96820, loss = 0.67 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:01:21.869011: step 96830, loss = 0.62 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:01:22.703506: step 96840, loss = 0.80 (1533.8 examples/sec; 0.083 sec/batch)
2017-06-02 05:01:23.571494: step 96850, loss = 0.67 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:01:24.434654: step 96860, loss = 0.75 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:01:25.294394: step 96870, loss = 0.59 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:01:26.139399: step 96880, loss = 0.66 (1514.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:01:27.008664: step 96890, loss = 0.67 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:01:27.995000: step 96900, loss = 0.72 (1297.7 examples/sec; 0.099 sec/batch)
2017-06-02 05:01:28.704646: step 96910, loss = 0.77 (1803.7 examples/sec; 0.071 sec/batch)
2017-06-02 05:01:29.551675: step 96920, loss = 0.78 (1511.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:01:30.410505: step 96930, loss = 0.62 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:01:31.281972: step 96940, loss = 0.71 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:01:32.139903: step 96950, loss = 0.72 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:01:33.011544: step 96960, loss = 0.79 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:01:33.883899: step 96970, loss = 0.72 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:01:34.779406: step 96980, loss = 0.78 (1429.4 examples/sec; 0.090 sec/batch)
2017-06-02 05:01:35.655759: step 96990, loss = 0.70 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:01:36.626993: step 97000, loss = 0.64 (1317.9 examples/sec; 0.097 sec/batch)
2017-06-02 05:01:37.394207: step 97010, loss = 0.75 (1668.4 examples/sec; 0.077 sec/batch)
2017-06-02 05:01:38.248363: step 97020, loss = 0.65 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:01:39.130784: step 97030, loss = 0.57 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:01:40.008326: step 97040, loss = 0.67 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:01:40.872499: step 97050, loss = 0.75 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:01:41.732539: step 97060, loss = 0.77 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:01:42.588967: step 97070, loss = 0.80 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:01:43.475303: step 97080, loss = 0.82 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:01:44.358668: step 97090, loss = 0.67 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:01:45.340220: step 97100, loss = 0.57 (1304.1 examples/sec; 0.098 sec/batch)
2017-06-02 05:01:46.085756: step 97110, loss = 0.58 (1716.9 examples/sec; 0.075 sec/batch)
2017-06-02 05:01:46.963428: step 97120, loss = 0.55 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:01:47.841345: step 97130, loss = 0.76 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:01:48.704597: step 97140, loss = 0.70 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:01:49.578977: step 97150, loss = 0.80 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:01:50.440462: step 97160, loss = 0.76 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:01:51.332870: step 97170, loss = 0.67 (1434.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:01:52.186983: step 97180, loss = 0.59 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:01:53.060280: step 97190, loss = 0.65 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:01:54.032453: step 97200, loss = 0.81 (1316.6 examples/sec; 0.097 sec/batch)
2017-06-02 05:01:54.805691: step 97210, loss = 0.75 (1655.4 examples/sec; 0.077 sec/batch)
2017-06-02 05:01:55.665558: step 97220, loss = 0.67 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:01:56.505650: step 97230, loss = 0.94 (1523.6 examples/sec; 0.084 sec/batch)
2017-06-02 05:01:57.362832: step 97240, loss = 0.67 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:01:58.216143: step 97250, loss = 0.74 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:01:59.074106: step 97260, loss = 0.80 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:01:59.937946: step 97270, loss = 0.81 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:02:00.808373: step 97280, loss = 0.73 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:02:01.693585: step 97290, loss = 0.67 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 05:02:02.645861: step 97300, loss = 0.70 (1344.1 examples/sec; 0.095 sec/batch)
2017-06-02 05:02:03.395423: step 97310, loss = 0.74 (1707.7 examples/sec; 0.075 sec/batch)
2017-06-02 05:02:04.232355: step 97320, loss = 0.74 (1529.4 examples/sec; 0.084 sec/batch)
2017-06-02 05:02:05.087030: step 97330, loss = 0.67 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:02:05.928952: step 97340, loss = 0.80 (1520.3 examples/sec; 0.084 sec/batch)
2017-06-02 05:02:06.802079: step 97350, loss = 0.70 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:02:07.653332: step 97360, loss = 0.78 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:02:08.511246: step 97370, loss = 0.85 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:02:09.353987: step 97380, loss = 0.81 (1518.8 examples/sec; 0.084 sec/batch)
2017-06-02 05:02:10.201819: step 97390, loss = 0.70 (1509.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:02:11.163013: step 97400, loss = 0.61 (1331.6 examples/sec; 0.096 sec/batch)
2017-06-02 05:02:11.918343: step 97410, loss = 0.68 (1694.6 examples/sec; 0.076 sec/batch)
2017-06-02 05:02:12.796331: step 97420, loss = 0.80 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:02:13.637417: step 97430, loss = 0.76 (1521.8 examples/sec; 0.084 sec/batch)
2017-06-02 05:02:14.475489: step 97440, loss = 0.66 (1527.3 examples/sec; 0.084 sec/batch)
2017-06-02 05:02:15.344503: step 97450, loss = 0.70 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:02:16.191537: step 97460, loss = 0.70 (1511.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:02:17.038831: step 97470, loss = 0.65 (1510.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:02:17.901891: step 97480, loss = 0.64 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:02:18.749261: step 97490, loss = 0.54 (1510.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:02:19.724104: step 97500, loss = 0.70 (1313.0 examples/sec; 0.097 sec/batch)
2017-06-02 05:02:20.516482: step 97510, loss = 0.68 (1615.4 examples/sec; 0.079 sec/batch)
2017-06-02 05:02:21.398129: step 97520, loss = 0.63 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:02:22.274885: step 97530, loss = 0.64 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:02:23.158677: step 97540, loss = 0.65 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:02:24.013835: step 97550, loss = 0.89 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:02:24.890690: step 97560, loss = 0.70 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:02:25.775453: step 97570, loss = 0.82 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:02:26.646411: step 97580, loss = 0.59 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:02:27.521274: step 97590, loss = 0.81 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:02:28.501054: step 97600, loss = 0.66 (1306.4 examples/sec; 0.098 sec/batch)
2017-06-02 05:02:29.249741: step 97610, loss = 0.54 (1709.7 examples/sec; 0.075 sec/batch)
2017-06-02 05:02:30.120165: step 97620, loss = 0.62 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:02:30.985244: step 97630, loss = 0.82 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:02:31.845815: step 97640, loss = 0.67 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:02:32.701696: step 97650, loss = 0.71 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:02:33.556425: step 97660, loss = 0.73 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:02:34.407394: step 97670, loss = 0.69 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:02:35.261069: step 97680, loss = 0.63 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:02:36.137089: step 97690, loss = 0.67 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:02:37.094141: step 97700, loss = 0.71 (1337.4 examples/sec; 0.096 sec/batch)
2017-06-02 05:02:37.852620: step 97710, loss = 0.65 (1687.6 examples/sec; 0.076 sec/batch)
2017-06-02 05:02:38.715112: step 97720, loss = 0.63 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:02:39.591989: step 97730, loss = 0.60 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:02:40.435200: step 97740, loss = 0.53 (1518.0 examples/sec; 0.084 sec/batch)
2017-06-02 05:02:41.292506: step 97750, loss = 0.82 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:02:42.165709: step 97760, loss = 0.64 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:02:43.037643: step 97770, loss = 0.72 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:02:43.911574: step 97780, loss = 0.67 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:02:44.767906: step 97790, loss = 0.67 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:02:45.730590: step 97800, loss = 0.68 (1329.6 examples/sec; 0.096 sec/batch)
2017-06-02 05:02:46.483383: step 97810, loss = 0.74 (1700.3 examples/sec; 0.075 sec/batch)
2017-06-02 05:02:47.344016: step 97820, loss = 0.56 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:02:48.204577: step 97830, loss = 0.63 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:02:49.044369: step 97840, loss = 0.77 (1524.2 examples/sec; 0.084 sec/batch)
2017-06-02 05:02:49.895858: step 97850, loss = 0.68 (1503.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:02:50.761128: step 97860, loss = 0.72 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:02:51.625558: step 97870, loss = 0.75 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:02:52.489095: step 97880, loss = 0.65 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:02:53.340869: step 97890, loss = 0.73 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:02:54.290899: step 97900, loss = 0.85 (1347.3 examples/sec; 0.095 sec/batch)
2017-06-02 05:02:55.054324: step 97910, loss = 0.79 (1676.7 examples/sec; 0.076 sec/batch)
2017-06-02 05:02:55.916948: step 97920, loss = 0.70 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:02:56.783450: step 97930, loss = 0.60 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:02:57.658535: step 97940, loss = 0.61 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:02:58.490295: step 97950, loss = 0.67 (1538.9 examples/sec; 0.083 sec/batch)
2017-06-02 05:02:59.331981: step 97960, loss = 0.80 (1520.8 examples/sec; 0.084 sec/batch)
2017-06-02 05:03:00.179459: step 97970, loss = 0.69 (1510.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:03:01.032027: step 97980, loss = 0.65 (1501.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:03:01.894853: step 97990, loss = 0.63 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:03:02.889805: step 98000, loss = 0.70 (1286.5 examples/sec; 0.099 sec/batch)
2017-06-02 05:03:03.625045: step 98010, loss = 0.63 (1741.0 examples/sec; 0.074 sec/batch)
2017-06-02 05:03:04.498628: step 98020, loss = 0.73 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:03:05.366515: step 98030, loss = 0.77 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:03:06.238828: step 98040, loss = 0.78 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:03:07.109863: step 98050, loss = 0.71 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:03:07.983395: step 98060, loss = 0.66 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:03:08.861354: step 98070, loss = 0.88 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:03:09.710978: step 98080, loss = 0.74 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:03:10.566281: step 98090, loss = 0.71 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:03:11.530549: step 98100, loss = 0.56 (1327.4 examples/sec; 0.096 sec/batch)
2017-06-02 05:03:12.279058: step 98110, loss = 0.75 (1710.1 examples/sec; 0.075 sec/batch)
2017-06-02 05:03:13.140341: step 98120, loss = 0.65 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:03:14.005265: step 98130, loss = 0.80 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:03:14.862384: step 98140, loss = 0.72 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:03:15.723780: step 98150, loss = 0.71 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:03:16.590370: step 98160, loss = 0.57 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:03:17.473629: step 98170, loss = 0.83 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:03:18.327470: step 98180, loss = 0.68 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:03:19.196590: step 98190, loss = 0.66 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:03:20.179206: step 98200, loss = 0.73 (1302.6 examples/sec; 0.098 sec/batch)
2017-06-02 05:03:20.962710: step 98210, loss = 0.66 (1633.7 examples/sec; 0.078 sec/batch)
2017-06-02 05:03:21.836816: step 98220, loss = 0.69 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:03:22.716393: step 98230, loss = 0.66 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:03:23.584047: step 98240, loss = 0.86 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:03:24.423062: step 98250, loss = 0.62 (1525.6 examples/sec; 0.084 sec/batch)
2017-06-02 05:03:25.271927: step 98260, loss = 0.69 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:03:26.132232: step 98270, loss = 0.71 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:03:26.991057: step 98280, loss = 0.64 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:03:27.864914: step 98290, loss = 0.57 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:03:28.855017: step 98300, loss = 0.76 (1292.8 examples/sec; 0.099 sec/batch)
2017-06-02 05:03:29.579867: step 98310, loss = 0.69 (1765.9 examples/sec; 0.072 sec/batch)
2017-06-02 05:03:30.429333: step 98320, loss = 0.78 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:03:31.277485: step 98330, loss = 0.79 (1509.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:03:32.136460: step 98340, loss = 0.66 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:03:33.033700: step 98350, loss = 0.70 (1426.6 examples/sec; 0.090 sec/batch)
2017-06-02 05:03:33.884689: step 98360, loss = 0.63 (1504.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:03:34.767340: step 98370, loss = 0.81 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:03:35.626496: step 98380, loss = 0.80 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:03:36.510219: step 98390, loss = 0.63 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:03:37.458879: step 98400, loss = 0.61 (1349.3 examples/sec; 0.095 sec/batch)
2017-06-02 05:03:38.253871: step 98410, loss = 0.64 (1610.1 examples/sec; 0.079 sec/batch)
2017-06-02 05:03:39.120863: step 98420, loss = 0.62 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:03:39.987474: step 98430, loss = 0.70 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:03:40.866792: step 98440, loss = 0.71 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:03:41.732906: step 98450, loss = 0.70 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:03:42.572387: step 98460, loss = 0.77 (1524.8 examples/sec; 0.084 sec/batch)
2017-06-02 05:03:43.451789: step 98470, loss = 0.58 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:03:44.328281: step 98480, loss = 0.91 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:03:45.218504: step 98490, loss = 0.68 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 05:03:46.198205: step 98500, loss = 0.66 (1306.5 examples/sec; 0.098 sec/batch)
2017-06-02 05:03:46.971220: step 98510, loss = 0.83 (1655.9 examples/sec; 0.077 sec/batch)
2017-06-02 05:03:47.843289: step 98520, loss = 0.77 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:03:48.710520: step 98530, loss = 0.60 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:03:49.574569: step 98540, loss = 0.70 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:03:50.420716: step 98550, loss = 0.67 (1512.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:03:51.306285: step 98560, loss = 0.75 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:03:52.166536: step 98570, loss = 0.71 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:03:53.036027: step 98580, loss = 0.78 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:03:53.896128: step 98590, loss = 0.78 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:03:54.856716: step 98600, loss = 0.69 (1332.5 examples/sec; 0.096 sec/batch)
2017-06-02 05:03:55.634376: step 98610, loss = 0.62 (1646.0 examples/sec; 0.078 sec/batch)
2017-06-02 05:03:56.459791: step 98620, loss = 0.72 (1550.7 examples/sec; 0.083 sec/batch)
2017-06-02 05:03:57.348676: step 98630, loss = 0.54 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 05:03:58.225158: step 98640, loss = 0.55 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:03:59.104028: step 98650, loss = 0.66 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:03:59.946326: step 98660, loss = 0.70 (1519.6 examples/sec; 0.084 sec/batch)
2017-06-02 05:04:00.808199: step 98670, loss = 0.58 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:04:01.683623: step 98680, loss = 0.67 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:04:02.542639: step 98690, loss = 0.67 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:04:03.519526: step 98700, loss = 0.69 (1310.3 examples/sec; 0.098 sec/batch)
2017-06-02 05:04:04.263819: step 98710, loss = 0.71 (1719.7 examples/sec; 0.074 sec/batch)
2017-06-02 05:04:05.143237: step 98720, loss = 0.64 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:04:06.009524: step 98730, loss = 0.69 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:04:06.860014: step 98740, loss = 0.71 (1505.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:04:07.738546: step 98750, loss = 0.67 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:04:08.574846: step 98760, loss = 0.62 (1530.6 examples/sec; 0.084 sec/batch)
2017-06-02 05:04:09.422224: step 98770, loss = 0.69 (1510.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:04:10.281017: step 98780, loss = 0.79 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:04:11.127594: step 98790, loss = 0.74 (1512.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:04:12.080552: step 98800, loss = 0.60 (1343.2 examples/sec; 0.095 sec/batch)
2017-06-02 05:04:12.876049: step 98810, loss = 0.65 (1609.0 examples/sec; 0.080 sec/batch)
2017-06-02 05:04:13.752046: step 98820, loss = 0.76 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:04:14.605417: step 98830, loss = 0.79 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:04:15.487340: step 98840, loss = 0.69 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:04:16.350238: step 98850, loss = 0.77 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:04:17.220367: step 98860, loss = 0.69 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:04:18.084476: step 98870, loss = 0.77 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:04:18.935407: step 98880, loss = 0.82 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:04:19.782574: step 98890, loss = 0.86 (1510.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:04:20.755499: step 98900, loss = 0.78 (1315.6 examples/sec; 0.097 sec/batch)
2017-06-02 05:04:21.533006: step 98910, loss = 0.75 (1646.3 examples/sec; 0.078 sec/batch)
2017-06-02 05:04:22.385578: step 98920, loss = 0.60 (1501.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:04:23.258972: step 98930, loss = 0.80 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:04:24.118251: step 98940, loss = 0.62 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:04:24.987016: step 98950, loss = 0.87 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:04:25.822444: step 98960, loss = 0.70 (1532.2 examples/sec; 0.084 sec/batch)
2017-06-02 05:04:26.686669: step 98970, loss = 0.67 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:04:27.562574: step 98980, loss = 0.62 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:04:28.426686: step 98990, loss = 0.79 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:04:29.368220: step 99000, loss = 0.74 (1359.5 examples/sec; 0.094 sec/batch)
2017-06-02 05:04:30.141697: step 99010, loss = 0.61 (1654.9 examples/sec; 0.077 sec/batch)
2017-06-02 05:04:30.991601: step 99020, loss = 0.53 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:04:31.855805: step 99030, loss = 0.68 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:04:32.729833: step 99040, loss = 0.67 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:04:33.601043: step 99050, loss = 0.77 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:04:34.432120: step 99060, loss = 0.81 (1540.1 examples/sec; 0.083 sec/batch)
2017-06-02 05:04:35.282957: step 99070, loss = 0.75 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:04:36.135511: step 99080, loss = 0.70 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:04:36.996909: step 99090, loss = 0.70 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:04:37.993871: step 99100, loss = 0.72 (1284.1 examples/sec; 0.100 sec/batch)
2017-06-02 05:04:38.733536: step 99110, loss = 0.72 (1730.2 examples/sec; 0.074 sec/batch)
2017-06-02 05:04:39.596204: step 99120, loss = 0.75 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:04:40.447288: step 99130, loss = 0.75 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:04:41.318029: step 99140, loss = 0.69 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:04:42.195934: step 99150, loss = 0.57 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:04:43.045580: step 99160, loss = 0.73 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:04:43.909642: step 99170, loss = 0.63 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:04:44.752822: step 99180, loss = 0.73 (1518.1 examples/sec; 0.084 sec/batch)
2017-06-02 05:04:45.652228: step 99190, loss = 0.75 (1423.2 examples/sec; 0.090 sec/batch)
2017-06-02 05:04:46.638595: step 99200, loss = 0.87 (1297.7 examples/sec; 0.099 sec/batch)
2017-06-02 05:04:47.366029: step 99210, loss = 0.74 (1759.6 examples/sec; 0.073 sec/batch)
2017-06-02 05:04:48.241320: step 99220, loss = 0.81 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:04:49.095741: step 99230, loss = 0.73 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:04:49.990084: step 99240, loss = 0.57 (1431.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:04:50.857605: step 99250, loss = 0.81 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:04:51.725286: step 99260, loss = 0.67 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:04:52.579103: step 99270, loss = 0.64 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:04:53.439037: step 99280, loss = 0.62 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:04:54.316125: step 99290, loss = 0.78 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:04:55.263778: step 99300, loss = 0.71 (1350.7 examples/sec; 0.095 sec/batch)
2017-06-02 05:04:56.019698: step 99310, loss = 0.65 (1693.3 examples/sec; 0.076 sec/batch)
2017-06-02 05:04:56.883357: step 99320, loss = 0.64 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:04:57.751844: step 99330, loss = 0.58 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:04:58.629566: step 99340, loss = 0.62 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:04:59.520492: step 99350, loss = 0.67 (1436.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:05:00.403010: step 99360, loss = 0.75 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:05:01.280833: step 99370, loss = 0.73 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:05:02.153109: step 99380, loss = 0.80 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:05:03.007283: step 99390, loss = 0.83 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:05:04.035680: step 99400, loss = 0.70 (1244.6 examples/sec; 0.103 sec/batch)
2017-06-02 05:05:04.748679: step 99410, loss = 0.59 (1795.3 examples/sec; 0.071 sec/batch)
2017-06-02 05:05:05.618428: step 99420, loss = 0.61 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:05:06.499747: step 99430, loss = 0.76 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:05:07.360879: step 99440, loss = 0.64 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:05:08.240778: step 99450, loss = 0.70 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:05:09.090343: step 99460, loss = 0.75 (1506.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:05:09.959441: step 99470, loss = 0.70 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:05:10.840846: step 99480, loss = 0.76 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:05:11.728649: step 99490, loss = 0.69 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 05:05:12.691219: step 99500, loss = 0.82 (1329.8 examples/sec; 0.096 sec/batch)
2017-06-02 05:05:13.456852: step 99510, loss = 0.66 (1671.8 examples/sec; 0.077 sec/batch)
2017-06-02 05:05:14.325756: step 99520, loss = 0.72 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:05:15.180513: step 99530, loss = 0.85 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:05:16.059627: step 99540, loss = 0.62 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:05:16.942380: step 99550, loss = 0.62 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:05:17.800790: step 99560, loss = 0.70 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:05:18.684713: step 99570, loss = 0.71 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:05:19.559016: step 99580, loss = 0.74 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:05:20.449228: step 99590, loss = 0.65 (1437.9 examples/sec; 0.089 sec/batch)
2017-06-02 05:05:21.441130: step 99600, loss = 0.61 (1290.5 examples/sec; 0.099 sec/batch)
2017-06-02 05:05:22.225519: step 99610, loss = 0.63 (1631.8 examples/sec; 0.078 sec/batch)
2017-06-02 05:05:23.082987: step 99620, loss = 0.69 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:05:23.956381: step 99630, loss = 0.65 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:05:24.827224: step 99640, loss = 0.66 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:05:25.718702: step 99650, loss = 0.69 (1435.8 examples/sec; 0.089 sec/batch)
2017-06-02 05:05:26.608599: step 99660, loss = 0.76 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:05:27.488434: step 99670, loss = 0.67 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:05:28.345806: step 99680, loss = 0.65 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:05:29.226724: step 99690, loss = 0.83 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:05:30.195795: step 99700, loss = 0.86 (1320.9 examples/sec; 0.097 sec/batch)
2017-06-02 05:05:30.975737: step 99710, loss = 0.73 (1641.2 examples/sec; 0.078 sec/batch)
2017-06-02 05:05:31.877661: step 99720, loss = 0.77 (1419.2 examples/sec; 0.090 sec/batch)
2017-06-02 05:05:32.753783: step 99730, loss = 0.60 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:05:33.608428: step 99740, loss = 0.68 (1497.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:05:34.470571: step 99750, loss = 0.72 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:05:35.370054: step 99760, loss = 0.74 (1423.0 examples/sec; 0.090 sec/batch)
2017-06-02 05:05:36.240799: step 99770, loss = 0.69 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:05:37.115310: step 99780, loss = 0.74 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:05:37.961484: step 99790, loss = 0.67 (1512.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:05:38.921796: step 99800, loss = 0.64 (1332.9 examples/sec; 0.096 sec/batch)
2017-06-02 05:05:39.681989: step 99810, loss = 0.70 (1683.8 examples/sec; 0.076 sec/batch)
2017-06-02 05:05:40.548544: step 99820, loss = 0.66 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:05:41.388896: step 99830, loss = 0.63 (1523.2 examples/sec; 0.084 sec/batch)
2017-06-02 05:05:42.244173: step 99840, loss = 0.60 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:05:43.118633: step 99850, loss = 0.67 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:05:43.981849: step 99860, loss = 0.73 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:05:44.866959: step 99870, loss = 0.67 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 05:05:45.727084: step 99880, loss = 0.62 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:05:46.603267: step 99890, loss = 0.64 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:05:47.638176: step 99900, loss = 0.59 (1236.8 examples/sec; 0.103 sec/batch)
2017-06-02 05:05:48.350983: step 99910, loss = 0.57 (1795.7 examples/sec; 0.071 sec/batch)
2017-06-02 05:05:49.236231: step 99920, loss = 0.75 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 05:05:50.083810: step 99930, loss = 0.61 (1510.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:05:50.982168: step 99940, loss = 0.58 (1424.8 examples/sec; 0.090 sec/batch)
2017-06-02 05:05:51.825359: step 99950, loss = 0.63 (1518.1 examples/sec; 0.084 sec/batch)
2017-06-02 05:05:52.693368: step 99960, loss = 0.69 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:05:53.577403: step 99970, loss = 0.73 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:05:54.446441: step 99980, loss = 0.76 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:05:55.332744: step 99990, loss = 0.70 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:05:56.286911: step 100000, loss = 0.69 (1341.5 examples/sec; 0.095 sec/batch)
2017-06-02 05:05:57.076311: step 100010, loss = 0.68 (1621.5 examples/sec; 0.079 sec/batch)
2017-06-02 05:05:57.925158: step 100020, loss = 0.70 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:05:58.798020: step 100030, loss = 0.68 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:05:59.684832: step 100040, loss = 0.65 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:06:00.562489: step 100050, loss = 0.82 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:06:01.428988: step 100060, loss = 0.80 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:06:02.289179: step 100070, loss = 0.69 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:06:03.139877: step 100080, loss = 0.69 (1504.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:06:04.030334: step 100090, loss = 0.69 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 05:06:05.009078: step 100100, loss = 0.87 (1307.8 examples/sec; 0.098 sec/batch)
2017-06-02 05:06:05.786256: step 100110, loss = 0.63 (1647.0 examples/sec; 0.078 sec/batch)
2017-06-02 05:06:06.652845: step 100120, loss = 0.66 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:06:07.523193: step 100130, loss = 0.61 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:06:08.386958: step 100140, loss = 0.60 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:06:09.225337: step 100150, loss = 0.86 (1526.8 examples/sec; 0.084 sec/batch)
2017-06-02 05:06:10.094997: step 100160, loss = 0.73 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:06:10.939815: step 100170, loss = 0.74 (1515.1 examples/sec; 0.084 sec/batch)
2017-06-02 05:06:11.814830: step 100180, loss = 0.94 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:06:12.661250: step 100190, loss = 0.68 (1512.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:06:13.648695: step 100200, loss = 0.85 (1296.3 examples/sec; 0.099 sec/batch)
2017-06-02 05:06:14.412523: step 100210, loss = 0.71 (1675.8 examples/sec; 0.076 sec/batch)
2017-06-02 05:06:15.277629: step 100220, loss = 0.78 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:06:16.148845: step 100230, loss = 0.68 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:06:16.991939: step 100240, loss = 0.75 (1518.2 examples/sec; 0.084 sec/batch)
2017-06-02 05:06:17.852599: step 100250, loss = 0.59 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:06:18.713769: step 100260, loss = 0.90 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:06:19.556812: step 100270, loss = 0.73 (1518.3 examples/sec; 0.084 sec/batch)
2017-06-02 05:06:20.436442: step 100280, loss = 0.64 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:06:21.290898: step 100290, loss = 0.84 (1498.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:06:22.245255: step 100300, loss = 0.84 (1341.2 examples/sec; 0.095 sec/batch)
2017-06-02 05:06:23.023536: step 100310, loss = 0.66 (1644.7 examples/sec; 0.078 sec/batch)
2017-06-02 05:06:23.899286: step 100320, loss = 1.02 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:06:24.750889: step 100330, loss = 0.73 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:06:25.623388: step 100340, loss = 0.62 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:06:26.501181: step 100350, loss = 0.72 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:06:27.364831: step 100360, loss = 0.71 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:06:28.245805: step 100370, loss = 0.64 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:06:29.079091: step 100380, loss = 0.65 (1536.0 examples/sec; 0.083 sec/batch)
2017-06-02 05:06:29.931678: step 100390, loss = 0.65 (1501.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:06:30.885442: step 100400, loss = 0.58 (1342.1 examples/sec; 0.095 sec/batch)
2017-06-02 05:06:31.657942: step 100410, loss = 0.62 (1657.0 examples/sec; 0.077 sec/batch)
2017-06-02 05:06:32.520423: step 100420, loss = 0.65 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:06:33.376841: step 100430, loss = 0.75 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:06:34.219534: step 100440, loss = 0.60 (1518.9 examples/sec; 0.084 sec/batch)
2017-06-02 05:06:35.112669: step 100450, loss = 0.63 (1433.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:06:35.986361: step 100460, loss = 0.64 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:06:36.867028: step 100470, loss = 0.77 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:06:37.739762: step 100480, loss = 0.71 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:06:38.601715: step 100490, loss = 0.72 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:06:39.577613: step 100500, loss = 0.63 (1311.6 examples/sec; 0.098 sec/batch)
2017-06-02 05:06:40.374014: step 100510, loss = 0.65 (1607.2 examples/sec; 0.080 sec/batch)
2017-06-02 05:06:41.248180: step 100520, loss = 0.77 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:06:42.115027: step 100530, loss = 0.73 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:06:42.996068: step 100540, loss = 0.69 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:06:43.868837: step 100550, loss = 0.71 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:06:44.748695: step 100560, loss = 0.77 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:06:45.603236: step 100570, loss = 0.69 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:06:46.459150: step 100580, loss = 0.61 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:06:47.326535: step 100590, loss = 0.70 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:06:48.320946: step 100600, loss = 0.80 (1287.2 examples/sec; 0.099 sec/batch)
2017-06-02 05:06:49.029674: step 100610, loss = 0.60 (1806.1 examples/sec; 0.071 sec/batch)
2017-06-02 05:06:49.878364: step 100620, loss = 0.75 (1508.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:06:50.724804: step 100630, loss = 0.55 (1512.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:06:51.623824: step 100640, loss = 0.68 (1423.8 examples/sec; 0.090 sec/batch)
2017-06-02 05:06:52.486954: step 100650, loss = 0.67 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:06:53.345925: step 100660, loss = 0.68 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:06:54.226821: step 100670, loss = 0.70 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:06:55.096192: step 100680, loss = 0.63 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:06:55.992827: step 100690, loss = 0.56 (1427.5 examples/sec; 0.090 sec/batch)
2017-06-02 05:06:56.989928: step 100700, loss = 0.81 (1283.7 examples/sec; 0.100 sec/batch)
2017-06-02 05:06:57.762440: step 100710, loss = 0.62 (1656.9 examples/sec; 0.077 sec/batch)
2017-06-02 05:06:58.603173: step 100720, loss = 0.75 (1522.5 examples/sec; 0.084 sec/batch)
2017-06-02 05:06:59.488238: step 100730, loss = 0.70 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:07:00.363960: step 100740, loss = 0.72 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:07:01.245917: step 100750, loss = 0.87 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:07:02.130499: step 100760, loss = 0.67 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:07:03.007299: step 100770, loss = 0.58 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:07:03.891903: step 100780, loss = 0.77 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:07:04.778612: step 100790, loss = 0.68 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 05:07:05.760146: step 100800, loss = 0.76 (1304.1 examples/sec; 0.098 sec/batch)
2017-06-02 05:07:06.490238: step 100810, loss = 0.69 (1753.3 examples/sec; 0.073 sec/batch)
2017-06-02 05:07:07.349416: step 100820, loss = 0.66 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:07:08.225229: step 100830, loss = 0.71 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:07:09.114220: step 100840, loss = 0.61 (1439.8 examples/sec; 0.089 sec/batch)
2017-06-02 05:07:09.973182: step 100850, loss = 0.72 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:07:10.856844: step 100860, loss = 0.67 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:07:11.731976: step 100870, loss = 0.63 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:07:12.573767: step 100880, loss = 0.66 (1520.6 examples/sec; 0.084 sec/batch)
2017-06-02 05:07:13.435907: step 100890, loss = 0.70 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:07:14.381036: step 100900, loss = 0.65 (1354.3 examples/sec; 0.095 sec/batch)
2017-06-02 05:07:15.166311: step 100910, loss = 0.84 (1630.0 examples/sec; 0.079 sec/batch)
2017-06-02 05:07:16.015129: step 100920, loss = 0.65 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:07:16.869141: step 100930, loss = 0.64 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:07:17.728585: step 100940, loss = 0.68 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:07:18.601451: step 100950, loss = 0.63 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:07:19.483509: step 100960, loss = 0.58 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:07:20.330077: step 100970, loss = 0.66 (1512.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:07:21.168104: step 100980, loss = 0.66 (1527.4 examples/sec; 0.084 sec/batch)
2017-06-02 05:07:22.047223: step 100990, loss = 0.72 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:07:23.001103: step 101000, loss = 0.63 (1341.9 examples/sec; 0.095 sec/batch)
2017-06-02 05:07:23.785871: step 101010, loss = 0.77 (1631.0 examples/sec; 0.078 sec/batch)
2017-06-02 05:07:24.659934: step 101020, loss = 0.64 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:07:25.503174: step 101030, loss = 0.67 (1518.0 examples/sec; 0.084 sec/batch)
2017-06-02 05:07:26.368993: step 101040, loss = 0.69 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:07:27.231952: step 101050, loss = 0.81 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:07:28.078354: step 101060, loss = 0.63 (1512.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:07:28.951329: step 101070, loss = 0.73 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:07:29.817211: step 101080, loss = 0.70 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:07:30.653184: step 101090, loss = 0.75 (1531.2 examples/sec; 0.084 sec/batch)
2017-06-02 05:07:31.636709: step 101100, loss = 0.74 (1301.4 examples/sec; 0.098 sec/batch)
2017-06-02 05:07:32.418582: step 101110, loss = 0.70 (1637.1 examples/sec; 0.078 sec/batch)
2017-06-02 05:07:33.301873: step 101120, loss = 0.63 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:07:34.177486: step 101130, loss = 0.66 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:07:35.051599: step 101140, loss = 0.79 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:07:35.908852: step 101150, loss = 0.66 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:07:36.790973: step 101160, loss = 0.82 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:07:37.643987: step 101170, loss = 0.62 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:07:38.491472: step 101180, loss = 0.67 (1510.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:07:39.345067: step 101190, loss = 0.76 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:07:40.284061: step 101200, loss = 0.60 (1363.2 examples/sec; 0.094 sec/batch)
2017-06-02 05:07:41.065908: step 101210, loss = 0.87 (1637.1 examples/sec; 0.078 sec/batch)
2017-06-02 05:07:41.917237: step 101220, loss = 0.61 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:07:42.785991: step 101230, loss = 0.56 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:07:43.634260: step 101240, loss = 0.75 (1509.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:07:44.479696: step 101250, loss = 0.62 (1514.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:07:45.352942: step 101260, loss = 0.64 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:07:46.222241: step 101270, loss = 0.77 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:07:47.077722: step 101280, loss = 0.67 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:07:47.960990: step 101290, loss = 0.80 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:07:48.932081: step 101300, loss = 0.80 (1318.1 examples/sec; 0.097 sec/batch)
2017-06-02 05:07:49.688602: step 101310, loss = 0.62 (1692.0 examples/sec; 0.076 sec/batch)
2017-06-02 05:07:50.541487: step 101320, loss = 0.70 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:07:51.404326: step 101330, loss = 0.69 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:07:52.279712: step 101340, loss = 0.72 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:07:53.152085: step 101350, loss = 0.81 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:07:54.006714: step 101360, loss = 0.70 (1497.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:07:54.867919: step 101370, loss = 0.59 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:07:55.691581: step 101380, loss = 0.56 (1554.0 examples/sec; 0.082 sec/batch)
2017-06-02 05:07:56.558272: step 101390, loss = 0.53 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:07:57.514518: step 101400, loss = 0.71 (1338.6 examples/sec; 0.096 sec/batch)
2017-06-02 05:07:58.286698: step 101410, loss = 0.67 (1657.6 examples/sec; 0.077 sec/batch)
2017-06-02 05:07:59.162017: step 101420, loss = 0.72 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:08:00.037621: step 101430, loss = 0.83 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:08:00.926182: step 101440, loss = 0.63 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 05:08:01.792122: step 101450, loss = 0.74 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:08:02.665707: step 101460, loss = 0.84 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:08:03.515204: step 101470, loss = 0.62 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:08:04.378776: step 101480, loss = 0.71 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:08:05.252028: step 101490, loss = 0.73 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:08:06.202604: step 101500, loss = 0.77 (1346.5 examples/sec; 0.095 sec/batch)
2017-06-02 05:08:06.978366: step 101510, loss = 0.76 (1650.0 examples/sec; 0.078 sec/batch)
2017-06-02 05:08:07.822684: step 101520, loss = 0.76 (1516.0 examples/sec; 0.084 sec/batch)
2017-06-02 05:08:08.706965: step 101530, loss = 0.62 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:08:09.541188: step 101540, loss = 0.64 (1534.3 examples/sec; 0.083 sec/batch)
2017-06-02 05:08:10.398766: step 101550, loss = 0.67 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:08:11.265147: step 101560, loss = 0.62 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:08:12.144216: step 101570, loss = 0.79 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:08:13.017524: step 101580, loss = 0.62 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:08:13.866724: step 101590, loss = 0.71 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:08:14.816078: step 101600, loss = 0.75 (1348.3 examples/sec; 0.095 sec/batch)
2017-06-02 05:08:15.590066: step 101610, loss = 0.65 (1653.8 examples/sec; 0.077 sec/batch)
2017-06-02 05:08:16.436125: step 101620, loss = 0.65 (1512.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:08:17.324952: step 101630, loss = 0.88 (1440.1 examples/sec; 0.089 sec/batch)
2017-06-02 05:08:18.177393: step 101640, loss = 0.77 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:08:19.050812: step 101650, loss = 0.74 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:08:19.910882: step 101660, loss = 0.65 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:08:20.744878: step 101670, loss = 0.72 (1534.8 examples/sec; 0.083 sec/batch)
2017-06-02 05:08:21.624373: step 101680, loss = 0.70 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:08:22.470668: step 101690, loss = 0.59 (1512.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:08:23.440473: step 101700, loss = 0.81 (1319.8 examples/sec; 0.097 sec/batch)
2017-06-02 05:08:24.219687: step 101710, loss = 0.76 (1642.7 examples/sec; 0.078 sec/batch)
2017-06-02 05:08:25.079001: step 101720, loss = 0.59 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:08:25.954159: step 101730, loss = 0.73 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:08:26.808289: step 101740, loss = 0.74 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:08:27.697073: step 101750, loss = 0.73 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:08:28.579033: step 101760, loss = 0.64 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:08:29.441251: step 101770, loss = 0.61 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:08:30.304119: step 101780, loss = 0.76 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:08:31.163941: step 101790, loss = 0.73 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:08:32.123898: step 101800, loss = 0.81 (1333.4 examples/sec; 0.096 sec/batch)
2017-06-02 05:08:32.897766: step 101810, loss = 0.70 (1654.1 examples/sec; 0.077 sec/batch)
2017-06-02 05:08:33.766578: step 101820, loss = 0.55 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:08:34.640316: step 101830, loss = 0.84 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:08:35.536738: step 101840, loss = 0.59 (1427.9 examples/sec; 0.090 sec/batch)
2017-06-02 05:08:36.389561: step 101850, loss = 0.78 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:08:37.243412: step 101860, loss = 0.85 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:08:38.118409: step 101870, loss = 0.67 (1462.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:08:38.956787: step 101880, loss = 0.68 (1526.8 examples/sec; 0.084 sec/batch)
2017-06-02 05:08:39.825494: step 101890, loss = 0.69 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:08:40.810367: step 101900, loss = 0.78 (1299.6 examples/sec; 0.098 sec/batch)
2017-06-02 05:08:41.592890: step 101910, loss = 0.70 (1635.8 examples/sec; 0.078 sec/batch)
2017-06-02 05:08:42.441226: step 101920, loss = 0.75 (1508.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:08:43.315538: step 101930, loss = 0.84 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:08:44.198247: step 101940, loss = 0.68 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:08:45.052146: step 101950, loss = 0.62 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:08:45.910416: step 101960, loss = 0.74 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:08:46.804561: step 101970, loss = 0.63 (1431.5 examples/sec; 0.089 sec/batch)
2017-06-02 05:08:47.698303: step 101980, loss = 0.70 (1432.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:08:48.586133: step 101990, loss = 0.73 (1441.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:08:49.574752: step 102000, loss = 0.58 (1294.7 examples/sec; 0.099 sec/batch)
2017-06-02 05:08:50.359501: step 102010, loss = 0.55 (1631.1 examples/sec; 0.078 sec/batch)
2017-06-02 05:08:51.242870: step 102020, loss = 0.89 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:08:52.126173: step 102030, loss = 0.65 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:08:53.026515: step 102040, loss = 0.46 (1421.7 examples/sec; 0.090 sec/batch)
2017-06-02 05:08:53.890282: step 102050, loss = 0.68 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:08:54.761300: step 102060, loss = 0.67 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:08:55.605927: step 102070, loss = 0.72 (1515.5 examples/sec; 0.084 sec/batch)
2017-06-02 05:08:56.471842: step 102080, loss = 0.70 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:08:57.352792: step 102090, loss = 0.81 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:08:58.333414: step 102100, loss = 0.75 (1305.3 examples/sec; 0.098 sec/batch)
2017-06-02 05:08:59.100980: step 102110, loss = 0.75 (1667.6 examples/sec; 0.077 sec/batch)
2017-06-02 05:08:59.975770: step 102120, loss = 0.71 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:09:00.851294: step 102130, loss = 0.65 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:09:01.723493: step 102140, loss = 0.80 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:09:02.587697: step 102150, loss = 0.62 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:09:03.439121: step 102160, loss = 0.84 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:09:04.312476: step 102170, loss = 0.72 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:09:05.190546: step 102180, loss = 0.65 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:09:06.055581: step 102190, loss = 0.61 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:09:07.018163: step 102200, loss = 0.71 (1329.8 examples/sec; 0.096 sec/batch)
2017-06-02 05:09:07.801235: step 102210, loss = 0.65 (1634.6 examples/sec; 0.078 sec/batch)
2017-06-02 05:09:08.667704: step 102220, loss = 0.84 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:09:09.521913: step 102230, loss = 0.64 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:09:10.382321: step 102240, loss = 0.70 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:09:11.264312: step 102250, loss = 0.75 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:09:12.144394: step 102260, loss = 0.67 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:09:13.006072: step 102270, loss = 0.63 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:09:13.892136: step 102280, loss = 0.78 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 05:09:14.749555: step 102290, loss = 0.67 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:09:15.765032: step 102300, loss = 0.67 (1260.5 examples/sec; 0.102 sec/batch)
2017-06-02 05:09:16.498559: step 102310, loss = 0.68 (1745.0 examples/sec; 0.073 sec/batch)
2017-06-02 05:09:17.359500: step 102320, loss = 0.77 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:09:18.212549: step 102330, loss = 0.57 (1500.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:09:19.082656: step 102340, loss = 0.78 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:09:19.916831: step 102350, loss = 0.77 (1534.4 examples/sec; 0.083 sec/batch)
2017-06-02 05:09:20.772316: step 102360, loss = 0.67 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:09:21.642488: step 102370, loss = 0.67 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:09:22.474120: step 102380, loss = 0.75 (1539.2 examples/sec; 0.083 sec/batch)
2017-06-02 05:09:23.349998: step 102390, loss = 0.64 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:09:24.299427: step 102400, loss = 0.74 (1348.2 examples/sec; 0.095 sec/batch)
2017-06-02 05:09:25.069327: step 102410, loss = 0.81 (1662.5 examples/sec; 0.077 sec/batch)
2017-06-02 05:09:25.951582: step 102420, loss = 0.79 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:09:26.809278: step 102430, loss = 0.65 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:09:27.687188: step 102440, loss = 0.80 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:09:28.562548: step 102450, loss = 0.95 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:09:29.401606: step 102460, loss = 0.81 (1525.5 examples/sec; 0.084 sec/batch)
2017-06-02 05:09:30.257918: step 102470, loss = 0.75 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:09:31.125636: step 102480, loss = 0.82 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:09:31.981422: step 102490, loss = 0.73 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:09:32.970706: step 102500, loss = 0.66 (1293.8 examples/sec; 0.099 sec/batch)
2017-06-02 05:09:33.709963: step 102510, loss = 0.74 (1731.5 examples/sec; 0.074 sec/batch)
2017-06-02 05:09:34.557943: step 102520, loss = 0.68 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:09:35.419097: step 102530, loss = 0.76 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:09:36.251652: step 102540, loss = 0.52 (1537.4 examples/sec; 0.083 sec/batch)
2017-06-02 05:09:37.107580: step 102550, loss = 0.66 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:09:37.978948: step 102560, loss = 0.67 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:09:38.848331: step 102570, loss = 0.63 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:09:39.710086: step 102580, loss = 0.69 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:09:40.558170: step 102590, loss = 0.64 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:09:41.514956: step 102600, loss = 0.71 (1337.8 examples/sec; 0.096 sec/batch)
2017-06-02 05:09:42.280592: step 102610, loss = 0.66 (1671.9 examples/sec; 0.077 sec/batch)
2017-06-02 05:09:43.140972: step 102620, loss = 0.73 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:09:44.031983: step 102630, loss = 0.73 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 05:09:44.903069: step 102640, loss = 0.70 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:09:45.779607: step 102650, loss = 0.62 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:09:46.625970: step 102660, loss = 0.68 (1512.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:09:47.504013: step 102670, loss = 0.62 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:09:48.400583: step 102680, loss = 0.74 (1427.7 examples/sec; 0.090 sec/batch)
2017-06-02 05:09:49.285331: step 102690, loss = 0.80 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:09:50.257506: step 102700, loss = 0.77 (1316.6 examples/sec; 0.097 sec/batch)
2017-06-02 05:09:51.049871: step 102710, loss = 0.73 (1615.4 examples/sec; 0.079 sec/batch)
2017-06-02 05:09:52.002693: step 102720, loss = 0.61 (1343.4 examples/sec; 0.095 sec/batch)
2017-06-02 05:09:52.880378: step 102730, loss = 0.48 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:09:53.769541: step 102740, loss = 0.65 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 05:09:54.631587: step 102750, loss = 0.70 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:09:55.519722: step 102760, loss = 0.66 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:09:56.388050: step 102770, loss = 0.75 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:09:57.282379: step 102780, loss = 0.61 (1431.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:09:58.169379: step 102790, loss = 0.68 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 05:09:59.180991: step 102800, loss = 0.78 (1265.3 examples/sec; 0.101 sec/batch)
2017-06-02 05:09:59.919312: step 102810, loss = 0.58 (1733.7 examples/sec; 0.074 sec/batch)
2017-06-02 05:10:00.809933: step 102820, loss = 0.64 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:10:01.691250: step 102830, loss = 0.85 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:10:02.575452: step 102840, loss = 0.73 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:10:03.436880: step 102850, loss = 0.62 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:10:04.303204: step 102860, loss = 0.75 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:10:05.172078: step 102870, loss = 0.75 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:10:06.014849: step 102880, loss = 0.65 (1518.8 examples/sec; 0.084 sec/batch)
2017-06-02 05:10:06.861198: step 102890, loss = 0.73 (1512.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:10:07.838181: step 102900, loss = 0.65 (1310.2 examples/sec; 0.098 sec/batch)
2017-06-02 05:10:08.554709: step 102910, loss = 0.73 (1786.4 examples/sec; 0.072 sec/batch)
2017-06-02 05:10:09.427896: step 102920, loss = 0.68 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:10:10.285791: step 102930, loss = 0.77 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:10:11.161908: step 102940, loss = 0.65 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:10:12.052412: step 102950, loss = 0.81 (1437.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:10:12.931567: step 102960, loss = 0.74 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:10:13.820163: step 102970, loss = 0.75 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 05:10:14.675795: step 102980, loss = 0.65 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:10:15.554655: step 102990, loss = 0.82 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:10:16.523655: step 103000, loss = 0.77 (1321.0 examples/sec; 0.097 sec/batch)
2017-06-02 05:10:17.312811: step 103010, loss = 0.64 (1622.0 examples/sec; 0.079 sec/batch)
2017-06-02 05:10:18.184768: step 103020, loss = 0.60 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:10:19.067080: step 103030, loss = 0.56 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:10:19.934950: step 103040, loss = 0.57 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:10:20.800489: step 103050, loss = 0.67 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:10:21.666287: step 103060, loss = 0.79 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:10:22.518584: step 103070, loss = 0.71 (1501.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:10:23.392551: step 103080, loss = 0.59 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:10:24.257521: step 103090, loss = 0.75 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:10:25.234281: step 103100, loss = 0.65 (1310.5 examples/sec; 0.098 sec/batch)
2017-06-02 05:10:26.000990: step 103110, loss = 0.60 (1669.5 examples/sec; 0.077 sec/batch)
2017-06-02 05:10:26.860822: step 103120, loss = 0.73 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:10:27.695082: step 103130, loss = 0.64 (1534.3 examples/sec; 0.083 sec/batch)
2017-06-02 05:10:28.563473: step 103140, loss = 0.61 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:10:29.428402: step 103150, loss = 0.79 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:10:30.317311: step 103160, loss = 0.63 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 05:10:31.183026: step 103170, loss = 0.69 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:10:32.046708: step 103180, loss = 0.75 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:10:32.908774: step 103190, loss = 0.76 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:10:33.908927: step 103200, loss = 0.74 (1279.8 examples/sec; 0.100 sec/batch)
2017-06-02 05:10:34.694785: step 103210, loss = 0.74 (1628.8 examples/sec; 0.079 sec/batch)
2017-06-02 05:10:35.588564: step 103220, loss = 0.67 (1432.1 examples/sec; 0.089 sec/batch)
2017-06-02 05:10:36.459541: step 103230, loss = 0.61 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:10:37.333297: step 103240, loss = 0.69 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:10:38.189829: step 103250, loss = 0.74 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:10:39.056306: step 103260, loss = 0.62 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:10:39.948175: step 103270, loss = 0.58 (1435.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:10:40.840145: step 103280, loss = 0.82 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 05:10:41.714275: step 103290, loss = 0.77 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:10:42.654047: step 103300, loss = 0.86 (1362.0 examples/sec; 0.094 sec/batch)
2017-06-02 05:10:43.466989: step 103310, loss = 0.75 (1574.5 examples/sec; 0.081 sec/batch)
2017-06-02 05:10:44.342322: step 103320, loss = 0.67 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:10:45.223739: step 103330, loss = 0.68 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:10:46.099905: step 103340, loss = 0.67 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:10:46.960665: step 103350, loss = 0.68 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:10:47.833635: step 103360, loss = 0.62 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:10:48.732379: step 103370, loss = 0.65 (1424.2 examples/sec; 0.090 sec/batch)
2017-06-02 05:10:49.611991: step 103380, loss = 0.67 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:10:50.472854: step 103390, loss = 0.75 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:10:51.410967: step 103400, loss = 0.69 (1364.4 examples/sec; 0.094 sec/batch)
2017-06-02 05:10:52.194828: step 103410, loss = 0.58 (1632.9 examples/sec; 0.078 sec/batch)
2017-06-02 05:10:53.055326: step 103420, loss = 0.78 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:10:53.938793: step 103430, loss = 0.79 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:10:54.830953: step 103440, loss = 0.52 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:10:55.694296: step 103450, loss = 0.73 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:10:56.560300: step 103460, loss = 0.56 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:10:57.440248: step 103470, loss = 0.66 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:10:58.329101: step 103480, loss = 0.76 (1440.1 examples/sec; 0.089 sec/batch)
2017-06-02 05:10:59.193279: step 103490, loss = 0.67 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:11:00.133264: step 103500, loss = 0.70 (1361.7 examples/sec; 0.094 sec/batch)
2017-06-02 05:11:00.894550: step 103510, loss = 0.71 (1681.4 examples/sec; 0.076 sec/batch)
2017-06-02 05:11:01.753142: step 103520, loss = 0.68 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:11:02.640323: step 103530, loss = 0.74 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 05:11:03.495311: step 103540, loss = 0.79 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:11:04.338695: step 103550, loss = 0.51 (1517.7 examples/sec; 0.084 sec/batch)
2017-06-02 05:11:05.180437: step 103560, loss = 0.79 (1520.7 examples/sec; 0.084 sec/batch)
2017-06-02 05:11:06.045538: step 103570, loss = 0.57 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:11:06.922355: step 103580, loss = 0.68 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:11:07.771243: step 103590, loss = 0.69 (1507.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:11:08.773641: step 103600, loss = 0.68 (1276.9 examples/sec; 0.100 sec/batch)
2017-06-02 05:11:09.491653: step 103610, loss = 0.66 (1782.7 examples/sec; 0.072 sec/batch)
2017-06-02 05:11:10.338671: step 103620, loss = 0.72 (1511.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:11:11.204343: step 103630, loss = 0.73 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:11:12.081030: step 103640, loss = 0.70 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:11:12.951171: step 103650, loss = 0.59 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:11:13.802937: step 103660, loss = 0.73 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:11:14.656441: step 103670, loss = 0.67 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:11:15.508427: step 103680, loss = 0.78 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:11:16.398121: step 103690, loss = 0.67 (1438.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:11:17.342922: step 103700, loss = 0.51 (1354.8 examples/sec; 0.094 sec/batch)
2017-06-02 05:11:18.126117: step 103710, loss = 0.70 (1634.3 examples/sec; 0.078 sec/batch)
2017-06-02 05:11:18.990108: step 103720, loss = 0.93 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:11:19.843944: step 103730, loss = 0.62 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:11:20.701102: step 103740, loss = 0.82 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:11:21.580540: step 103750, loss = 0.76 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:11:22.435249: step 103760, loss = 0.63 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:11:23.293083: step 103770, loss = 0.67 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:11:24.135177: step 103780, loss = 0.77 (1520.0 examples/sec; 0.084 sec/batch)
2017-06-02 05:11:24.986105: step 103790, loss = 0.60 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:11:25.946241: step 103800, loss = 0.55 (1333.1 examples/sec; 0.096 sec/batch)
2017-06-02 05:11:26.739027: step 103810, loss = 0.82 (1614.6 examples/sec; 0.079 sec/batch)
2017-06-02 05:11:27.611717: step 103820, loss = 0.73 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:11:28.489202: step 103830, loss = 0.80 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:11:29.346393: step 103840, loss = 0.66 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:11:30.227388: step 103850, loss = 0.64 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:11:31.082769: step 103860, loss = 0.68 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:11:31.932235: step 103870, loss = 0.71 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:11:32.801141: step 103880, loss = 0.68 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:11:33.666302: step 103890, loss = 0.76 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:11:34.615922: step 103900, loss = 0.70 (1347.9 examples/sec; 0.095 sec/batch)
2017-06-02 05:11:35.417824: step 103910, loss = 0.57 (1596.2 examples/sec; 0.080 sec/batch)
2017-06-02 05:11:36.247389: step 103920, loss = 0.75 (1543.0 examples/sec; 0.083 sec/batch)
2017-06-02 05:11:37.097918: step 103930, loss = 0.68 (1505.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:11:37.982422: step 103940, loss = 0.70 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:11:38.847028: step 103950, loss = 0.71 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:11:39.720028: step 103960, loss = 0.71 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:11:40.586943: step 103970, loss = 0.60 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:11:41.445142: step 103980, loss = 0.62 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:11:42.303563: step 103990, loss = 0.82 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:11:43.258826: step 104000, loss = 0.77 (1339.9 examples/sec; 0.096 sec/batch)
2017-06-02 05:11:44.067028: step 104010, loss = 0.64 (1583.8 examples/sec; 0.081 sec/batch)
2017-06-02 05:11:44.946456: step 104020, loss = 0.69 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:11:45.821142: step 104030, loss = 0.71 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:11:46.686578: step 104040, loss = 0.65 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:11:47.548582: step 104050, loss = 0.89 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:11:48.424503: step 104060, loss = 0.75 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:11:49.281628: step 104070, loss = 0.64 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:11:50.158421: step 104080, loss = 0.71 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:11:51.020832: step 104090, loss = 0.70 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:11:52.055400: step 104100, loss = 0.89 (1237.2 examples/sec; 0.103 sec/batch)
2017-06-02 05:11:52.770955: step 104110, loss = 0.72 (1788.8 examples/sec; 0.072 sec/batch)
2017-06-02 05:11:53.642866: step 104120, loss = 0.86 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:11:54.495361: step 104130, loss = 0.77 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:11:55.370156: step 104140, loss = 0.72 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:11:56.227401: step 104150, loss = 0.56 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:11:57.103921: step 104160, loss = 0.76 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:11:57.965092: step 104170, loss = 0.72 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:11:58.829514: step 104180, loss = 0.69 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:11:59.704006: step 104190, loss = 0.69 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:12:00.644722: step 104200, loss = 0.66 (1360.7 examples/sec; 0.094 sec/batch)
2017-06-02 05:12:01.424287: step 104210, loss = 0.78 (1641.9 examples/sec; 0.078 sec/batch)
2017-06-02 05:12:02.286116: step 104220, loss = 0.69 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:12:03.170857: step 104230, loss = 0.70 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:12:04.030221: step 104240, loss = 0.67 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:12:04.914457: step 104250, loss = 0.68 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:12:05.802236: step 104260, loss = 0.60 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 05:12:06.662211: step 104270, loss = 0.72 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:12:07.505678: step 104280, loss = 0.67 (1517.5 examples/sec; 0.084 sec/batch)
2017-06-02 05:12:08.368901: step 104290, loss = 0.69 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:12:09.328902: step 104300, loss = 0.80 (1333.3 examples/sec; 0.096 sec/batch)
2017-06-02 05:12:10.116974: step 104310, loss = 0.72 (1624.2 examples/sec; 0.079 sec/batch)
2017-06-02 05:12:10.986328: step 104320, loss = 0.78 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:12:11.871462: step 104330, loss = 0.51 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 05:12:12.756169: step 104340, loss = 0.85 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:12:13.630777: step 104350, loss = 0.73 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:12:14.481756: step 104360, loss = 0.68 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:12:15.342631: step 104370, loss = 0.56 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:12:16.202847: step 104380, loss = 0.73 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:12:17.070941: step 104390, loss = 0.64 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:12:18.030144: step 104400, loss = 0.73 (1334.4 examples/sec; 0.096 sec/batch)
2017-06-02 05:12:18.815539: step 104410, loss = 0.70 (1629.8 examples/sec; 0.079 sec/batch)
2017-06-02 05:12:19.691744: step 104420, loss = 0.89 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:12:20.556485: step 104430, loss = 0.80 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:12:21.419370: step 104440, loss = 0.80 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:12:22.285418: step 104450, loss = 0.94 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:12:23.119256: step 104460, loss = 0.64 (1535.1 examples/sec; 0.083 sec/batch)
2017-06-02 05:12:23.978980: step 104470, loss = 0.61 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:12:24.833818: step 104480, loss = 0.75 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:12:25.702714: step 104490, loss = 0.78 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:12:26.649384: step 104500, loss = 0.82 (1352.1 examples/sec; 0.095 sec/batch)
2017-06-02 05:12:27.401426: step 104510, loss = 0.68 (1702.0 examples/sec; 0.075 sec/batch)
2017-06-02 05:12:28.255683: step 104520, loss = 0.78 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:12:29.108311: step 104530, loss = 0.72 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:12:29.958957: step 104540, loss = 0.78 (1504.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:12:30.811386: step 104550, loss = 0.58 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:12:31.645397: step 104560, loss = 0.73 (1534.7 examples/sec; 0.083 sec/batch)
2017-06-02 05:12:32.518626: step 104570, loss = 0.79 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:12:33.366825: step 104580, loss = 0.81 (1509.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:12:34.235010: step 104590, loss = 0.54 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:12:35.219286: step 104600, loss = 0.67 (1300.4 examples/sec; 0.098 sec/batch)
2017-06-02 05:12:36.010695: step 104610, loss = 0.66 (1617.4 examples/sec; 0.079 sec/batch)
2017-06-02 05:12:36.885515: step 104620, loss = 0.80 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:12:37.764633: step 104630, loss = 0.74 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:12:38.624720: step 104640, loss = 0.71 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:12:39.503556: step 104650, loss = 0.65 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:12:40.374026: step 104660, loss = 0.64 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:12:41.253158: step 104670, loss = 0.69 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:12:42.122832: step 104680, loss = 0.90 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:12:42.993835: step 104690, loss = 0.77 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:12:43.994247: step 104700, loss = 0.88 (1279.5 examples/sec; 0.100 sec/batch)
2017-06-02 05:12:44.722476: step 104710, loss = 0.66 (1757.7 examples/sec; 0.073 sec/batch)
2017-06-02 05:12:45.573226: step 104720, loss = 0.66 (1504.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:12:46.428287: step 104730, loss = 0.58 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:12:47.307223: step 104740, loss = 0.66 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:12:48.173686: step 104750, loss = 0.71 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:12:49.037400: step 104760, loss = 0.66 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:12:49.879861: step 104770, loss = 0.67 (1519.4 examples/sec; 0.084 sec/batch)
2017-06-02 05:12:50.762642: step 104780, loss = 0.61 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:12:51.639323: step 104790, loss = 0.77 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:12:52.605657: step 104800, loss = 0.66 (1324.6 examples/sec; 0.097 sec/batch)
2017-06-02 05:12:53.385720: step 104810, loss = 0.71 (1640.9 examples/sec; 0.078 sec/batch)
2017-06-02 05:12:54.248252: step 104820, loss = 0.67 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:12:55.120176: step 104830, loss = 0.84 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:12:55.999217: step 104840, loss = 0.79 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:12:56.845100: step 104850, loss = 0.79 (1513.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:12:57.710305: step 104860, loss = 0.86 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:12:58.569008: step 104870, loss = 0.66 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:12:59.420793: step 104880, loss = 0.71 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:13:00.286324: step 104890, loss = 0.71 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:13:01.257292: step 104900, loss = 0.58 (1318.3 examples/sec; 0.097 sec/batch)
2017-06-02 05:13:02.035934: step 104910, loss = 0.80 (1643.9 examples/sec; 0.078 sec/batch)
2017-06-02 05:13:02.881605: step 104920, loss = 0.64 (1513.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:13:03.739889: step 104930, loss = 0.69 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:13:04.598404: step 104940, loss = 0.61 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:13:05.502343: step 104950, loss = 0.68 (1416.0 examples/sec; 0.090 sec/batch)
2017-06-02 05:13:06.354217: step 104960, loss = 0.68 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:13:07.235410: step 104970, loss = 0.67 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:13:08.104000: step 104980, loss = 0.95 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:13:08.965626: step 104990, loss = 0.76 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:13:09.923500: step 105000, loss = 0.72 (1336.3 examples/sec; 0.096 sec/batch)
2017-06-02 05:13:10.693972: step 105010, loss = 0.68 (1661.3 examples/sec; 0.077 sec/batch)
2017-06-02 05:13:11.577846: step 105020, loss = 0.66 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:13:12.435295: step 105030, loss = 0.74 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:13:13.294971: step 105040, loss = 0.64 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:13:14.145495: step 105050, loss = 0.53 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:13:15.003905: step 105060, loss = 0.75 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:13:15.869702: step 105070, loss = 0.77 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:13:16.728208: step 105080, loss = 0.74 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:13:17.612961: step 105090, loss = 0.67 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:13:18.566602: step 105100, loss = 0.68 (1342.2 examples/sec; 0.095 sec/batch)
2017-06-02 05:13:19.345873: step 105110, loss = 0.84 (1642.6 examples/sec; 0.078 sec/batch)
2017-06-02 05:13:20.241973: step 105120, loss = 0.69 (1428.4 examples/sec; 0.090 sec/batch)
2017-06-02 05:13:21.102859: step 105130, loss = 0.68 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:13:21.959404: step 105140, loss = 0.66 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:13:22.811088: step 105150, loss = 0.69 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:13:23.680789: step 105160, loss = 0.69 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:13:24.537126: step 105170, loss = 0.75 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:13:25.403032: step 105180, loss = 0.86 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:13:26.270397: step 105190, loss = 0.67 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:13:27.281714: step 105200, loss = 0.79 (1265.7 examples/sec; 0.101 sec/batch)
2017-06-02 05:13:28.022458: step 105210, loss = 0.61 (1728.0 examples/sec; 0.074 sec/batch)
2017-06-02 05:13:28.863956: step 105220, loss = 0.70 (1521.1 examples/sec; 0.084 sec/batch)
2017-06-02 05:13:29.719976: step 105230, loss = 0.60 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:13:30.563228: step 105240, loss = 0.74 (1517.9 examples/sec; 0.084 sec/batch)
2017-06-02 05:13:31.434086: step 105250, loss = 0.93 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:13:32.308422: step 105260, loss = 0.65 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:13:33.198210: step 105270, loss = 0.71 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 05:13:34.041899: step 105280, loss = 0.64 (1517.1 examples/sec; 0.084 sec/batch)
2017-06-02 05:13:34.914999: step 105290, loss = 0.60 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:13:35.851271: step 105300, loss = 0.87 (1367.1 examples/sec; 0.094 sec/batch)
2017-06-02 05:13:36.611230: step 105310, loss = 0.71 (1684.3 examples/sec; 0.076 sec/batch)
2017-06-02 05:13:37.479299: step 105320, loss = 0.72 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:13:38.332540: step 105330, loss = 0.76 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:13:39.206086: step 105340, loss = 0.72 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:13:40.065391: step 105350, loss = 0.55 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:13:40.942459: step 105360, loss = 0.59 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:13:41.801100: step 105370, loss = 0.80 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:13:42.652243: step 105380, loss = 0.59 (1503.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:13:43.521060: step 105390, loss = 0.68 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:13:44.536346: step 105400, loss = 0.78 (1260.7 examples/sec; 0.102 sec/batch)
2017-06-02 05:13:45.259419: step 105410, loss = 0.67 (1770.2 examples/sec; 0.072 sec/batch)
2017-06-02 05:13:46.132656: step 105420, loss = 0.71 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:13:46.978407: step 105430, loss = 0.60 (1513.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:13:47.860411: step 105440, loss = 0.72 (1451.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:13:48.730439: step 105450, loss = 0.71 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:13:49.576431: step 105460, loss = 0.60 (1513.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:13:50.447586: step 105470, loss = 0.80 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:13:51.309958: step 105480, loss = 0.62 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:13:52.163606: step 105490, loss = 0.86 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:13:53.129821: step 105500, loss = 0.70 (1324.8 examples/sec; 0.097 sec/batch)
2017-06-02 05:13:53.879028: step 105510, loss = 0.68 (1708.5 examples/sec; 0.075 sec/batch)
2017-06-02 05:13:54.728877: step 105520, loss = 0.66 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:13:55.595367: step 105530, loss = 0.78 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:13:56.432463: step 105540, loss = 0.72 (1529.1 examples/sec; 0.084 sec/batch)
2017-06-02 05:13:57.271033: step 105550, loss = 0.81 (1526.4 examples/sec; 0.084 sec/batch)
2017-06-02 05:13:58.129555: step 105560, loss = 0.93 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:13:58.997141: step 105570, loss = 0.69 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:13:59.845595: step 105580, loss = 0.73 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:14:00.738395: step 105590, loss = 0.80 (1433.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:14:01.706320: step 105600, loss = 0.68 (1322.4 examples/sec; 0.097 sec/batch)
2017-06-02 05:14:02.489023: step 105610, loss = 0.82 (1635.3 examples/sec; 0.078 sec/batch)
2017-06-02 05:14:03.368431: step 105620, loss = 0.66 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:14:04.239891: step 105630, loss = 0.60 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:14:05.091547: step 105640, loss = 0.53 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:14:05.947872: step 105650, loss = 0.69 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:14:06.802237: step 105660, loss = 0.72 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:14:07.667395: step 105670, loss = 0.75 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:14:08.542320: step 105680, loss = 0.66 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:14:09.411001: step 105690, loss = 0.71 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:14:10.369300: step 105700, loss = 0.62 (1335.7 examples/sec; 0.096 sec/batch)
2017-06-02 05:14:11.129233: step 105710, loss = 0.80 (1684.4 examples/sec; 0.076 sec/batch)
2017-06-02 05:14:12.017378: step 105720, loss = 0.73 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:14:12.868573: step 105730, loss = 0.73 (1503.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:14:13.727270: step 105740, loss = 0.63 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:14:14.580167: step 105750, loss = 0.68 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:14:15.481056: step 105760, loss = 0.61 (1420.8 examples/sec; 0.090 sec/batch)
2017-06-02 05:14:16.361790: step 105770, loss = 0.59 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:14:17.227709: step 105780, loss = 0.66 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:14:18.083850: step 105790, loss = 0.75 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:14:19.043175: step 105800, loss = 0.80 (1334.2 examples/sec; 0.096 sec/batch)
2017-06-02 05:14:19.804258: step 105810, loss = 0.72 (1681.8 examples/sec; 0.076 sec/batch)
2017-06-02 05:14:20.672160: step 105820, loss = 0.74 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:14:21.539938: step 105830, loss = 0.74 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:14:22.377528: step 105840, loss = 0.69 (1528.2 examples/sec; 0.084 sec/batch)
2017-06-02 05:14:23.241798: step 105850, loss = 0.79 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:14:24.135171: step 105860, loss = 0.73 (1432.8 examples/sec; 0.089 sec/batch)
2017-06-02 05:14:25.019493: step 105870, loss = 0.75 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:14:25.902127: step 105880, loss = 0.74 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:14:26.797643: step 105890, loss = 0.71 (1429.3 examples/sec; 0.090 sec/batch)
2017-06-02 05:14:27.800765: step 105900, loss = 0.68 (1276.0 examples/sec; 0.100 sec/batch)
2017-06-02 05:14:28.566235: step 105910, loss = 0.62 (1672.2 examples/sec; 0.077 sec/batch)
2017-06-02 05:14:29.410606: step 105920, loss = 0.63 (1515.9 examples/sec; 0.084 sec/batch)
2017-06-02 05:14:30.285902: step 105930, loss = 0.69 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:14:31.157611: step 105940, loss = 0.69 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:14:32.039905: step 105950, loss = 0.72 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:14:32.907184: step 105960, loss = 0.72 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:14:33.765815: step 105970, loss = 0.66 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:14:34.623571: step 105980, loss = 0.66 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:14:35.481581: step 105990, loss = 0.67 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:14:36.408244: step 106000, loss = 0.54 (1381.3 examples/sec; 0.093 sec/batch)
2017-06-02 05:14:37.180180: step 106010, loss = 0.74 (1658.2 examples/sec; 0.077 sec/batch)
2017-06-02 05:14:38.034048: step 106020, loss = 0.60 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:14:38.880378: step 106030, loss = 0.72 (1512.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:14:39.736942: step 106040, loss = 0.72 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:14:40.607764: step 106050, loss = 0.73 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:14:41.467318: step 106060, loss = 0.58 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:14:42.335496: step 106070, loss = 0.64 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:14:43.218702: step 106080, loss = 0.77 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:14:44.097167: step 106090, loss = 0.61 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:14:45.046445: step 106100, loss = 0.70 (1348.4 examples/sec; 0.095 sec/batch)
2017-06-02 05:14:45.807041: step 106110, loss = 0.84 (1682.9 examples/sec; 0.076 sec/batch)
2017-06-02 05:14:46.689352: step 106120, loss = 0.59 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:14:47.551165: step 106130, loss = 0.78 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:14:48.410887: step 106140, loss = 0.76 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:14:49.256688: step 106150, loss = 0.71 (1513.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:14:50.158200: step 106160, loss = 0.82 (1419.8 examples/sec; 0.090 sec/batch)
2017-06-02 05:14:51.023719: step 106170, loss = 0.62 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:14:51.894388: step 106180, loss = 0.64 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:14:52.777252: step 106190, loss = 0.68 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:14:53.733128: step 106200, loss = 0.59 (1339.1 examples/sec; 0.096 sec/batch)
2017-06-02 05:14:54.511042: step 106210, loss = 0.60 (1645.4 examples/sec; 0.078 sec/batch)
2017-06-02 05:14:55.385169: step 106220, loss = 0.64 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:14:56.244630: step 106230, loss = 0.85 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:14:57.104053: step 106240, loss = 0.65 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:14:57.972662: step 106250, loss = 0.68 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:14:58.848579: step 106260, loss = 0.71 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:14:59.722545: step 106270, loss = 0.61 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:15:00.590658: step 106280, loss = 0.66 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:15:01.459944: step 106290, loss = 0.71 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:15:02.405620: step 106300, loss = 0.79 (1353.5 examples/sec; 0.095 sec/batch)
2017-06-02 05:15:03.206769: step 106310, loss = 0.74 (1597.7 examples/sec; 0.080 sec/batch)
2017-06-02 05:15:04.088593: step 106320, loss = 0.83 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:15:04.966523: step 106330, loss = 0.68 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:15:05.816653: step 106340, loss = 0.58 (1505.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:15:06.678568: step 106350, loss = 0.66 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:15:07.549657: step 106360, loss = 0.69 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:15:08.418586: step 106370, loss = 0.56 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:15:09.308397: step 106380, loss = 0.60 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 05:15:10.158449: step 106390, loss = 0.58 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:15:11.085893: step 106400, loss = 0.81 (1380.2 examples/sec; 0.093 sec/batch)
2017-06-02 05:15:11.858508: step 106410, loss = 0.67 (1656.7 examples/sec; 0.077 sec/batch)
2017-06-02 05:15:12.726795: step 106420, loss = 0.72 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:15:13.582013: step 106430, loss = 0.71 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:15:14.406329: step 106440, loss = 0.70 (1552.8 examples/sec; 0.082 sec/batch)
2017-06-02 05:15:15.298325: step 106450, loss = 0.55 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 05:15:16.189339: step 106460, loss = 0.54 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 05:15:17.063223: step 106470, loss = 0.65 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:15:17.921547: step 106480, loss = 0.55 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:15:18.779904: step 106490, loss = 0.74 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:15:19.807055: step 106500, loss = 0.66 (1246.2 examples/sec; 0.103 sec/batch)
2017-06-02 05:15:20.546139: step 106510, loss = 0.73 (1731.9 examples/sec; 0.074 sec/batch)
2017-06-02 05:15:21.428439: step 106520, loss = 0.76 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:15:22.309303: step 106530, loss = 0.60 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:15:23.210542: step 106540, loss = 0.56 (1420.3 examples/sec; 0.090 sec/batch)
2017-06-02 05:15:24.076892: step 106550, loss = 0.71 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:15:24.945118: step 106560, loss = 0.76 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:15:25.809071: step 106570, loss = 0.89 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:15:26.681185: step 106580, loss = 0.62 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:15:27.538968: step 106590, loss = 0.79 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:15:28.508714: step 106600, loss = 0.57 (1319.9 examples/sec; 0.097 sec/batch)
2017-06-02 05:15:29.268242: step 106610, loss = 0.66 (1685.3 examples/sec; 0.076 sec/batch)
2017-06-02 05:15:30.131649: step 106620, loss = 0.64 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:15:30.991164: step 106630, loss = 0.71 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:15:31.844553: step 106640, loss = 0.62 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:15:32.696031: step 106650, loss = 0.90 (1503.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:15:33.558390: step 106660, loss = 0.72 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:15:34.414033: step 106670, loss = 0.74 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:15:35.298458: step 106680, loss = 0.71 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:15:36.155613: step 106690, loss = 0.70 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:15:37.148854: step 106700, loss = 0.65 (1288.7 examples/sec; 0.099 sec/batch)
2017-06-02 05:15:37.881162: step 106710, loss = 0.68 (1747.9 examples/sec; 0.073 sec/batch)
2017-06-02 05:15:38.752359: step 106720, loss = 0.74 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:15:39.594350: step 106730, loss = 0.72 (1520.2 examples/sec; 0.084 sec/batch)
2017-06-02 05:15:40.454847: step 106740, loss = 0.58 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:15:41.315182: step 106750, loss = 0.73 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:15:42.156865: step 106760, loss = 0.88 (1520.8 examples/sec; 0.084 sec/batch)
2017-06-02 05:15:43.024361: step 106770, loss = 0.69 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:15:43.919169: step 106780, loss = 0.61 (1430.5 examples/sec; 0.089 sec/batch)
2017-06-02 05:15:44.794890: step 106790, loss = 0.61 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:15:45.795384: step 106800, loss = 0.80 (1279.4 examples/sec; 0.100 sec/batch)
2017-06-02 05:15:46.506338: step 106810, loss = 0.74 (1800.4 examples/sec; 0.071 sec/batch)
2017-06-02 05:15:47.378411: step 106820, loss = 0.72 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:15:48.259214: step 106830, loss = 0.71 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:15:49.114455: step 106840, loss = 0.63 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:15:49.968707: step 106850, loss = 0.69 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:15:50.812095: step 106860, loss = 0.67 (1517.7 examples/sec; 0.084 sec/batch)
2017-06-02 05:15:51.693756: step 106870, loss = 0.74 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:15:52.539007: step 106880, loss = 0.77 (1514.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:15:53.394299: step 106890, loss = 0.63 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:15:54.357102: step 106900, loss = 0.66 (1329.4 examples/sec; 0.096 sec/batch)
2017-06-02 05:15:55.118505: step 106910, loss = 0.68 (1681.1 examples/sec; 0.076 sec/batch)
2017-06-02 05:15:55.991708: step 106920, loss = 0.77 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:15:56.862787: step 106930, loss = 0.69 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:15:57.741085: step 106940, loss = 0.67 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:15:58.601483: step 106950, loss = 0.66 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:15:59.456804: step 106960, loss = 0.62 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:16:00.325764: step 106970, loss = 0.73 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:16:01.195586: step 106980, loss = 0.84 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:16:02.062816: step 106990, loss = 0.67 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:16:03.018280: step 107000, loss = 0.58 (1339.7 examples/sec; 0.096 sec/batch)
2017-06-02 05:16:03.793056: step 107010, loss = 0.67 (1652.1 examples/sec; 0.077 sec/batch)
2017-06-02 05:16:04.648601: step 107020, loss = 0.73 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:16:05.487866: step 107030, loss = 0.73 (1525.1 examples/sec; 0.084 sec/batch)
2017-06-02 05:16:06.350439: step 107040, loss = 0.74 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:16:07.193410: step 107050, loss = 0.77 (1518.4 examples/sec; 0.084 sec/batch)
2017-06-02 05:16:08.031656: step 107060, loss = 0.58 (1527.0 examples/sec; 0.084 sec/batch)
2017-06-02 05:16:08.891325: step 107070, loss = 0.69 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:16:09.724173: step 107080, loss = 0.54 (1536.9 examples/sec; 0.083 sec/batch)
2017-06-02 05:16:10.568667: step 107090, loss = 0.81 (1515.7 examples/sec; 0.084 sec/batch)
2017-06-02 05:16:11.536379: step 107100, loss = 0.70 (1322.7 examples/sec; 0.097 sec/batch)
2017-06-02 05:16:12.307568: step 107110, loss = 0.59 (1659.8 examples/sec; 0.077 sec/batch)
2017-06-02 05:16:13.153865: step 107120, loss = 0.61 (1512.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:16:14.005221: step 107130, loss = 0.61 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:16:14.868736: step 107140, loss = 0.71 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:16:15.702237: step 107150, loss = 0.59 (1535.7 examples/sec; 0.083 sec/batch)
2017-06-02 05:16:16.543784: step 107160, loss = 0.82 (1521.0 examples/sec; 0.084 sec/batch)
2017-06-02 05:16:17.414903: step 107170, loss = 0.69 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:16:18.248297: step 107180, loss = 0.61 (1535.9 examples/sec; 0.083 sec/batch)
2017-06-02 05:16:19.111452: step 107190, loss = 0.58 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:16:20.057298: step 107200, loss = 0.74 (1353.3 examples/sec; 0.095 sec/batch)
2017-06-02 05:16:20.803916: step 107210, loss = 0.61 (1714.4 examples/sec; 0.075 sec/batch)
2017-06-02 05:16:21.683331: step 107220, loss = 0.75 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:16:22.550545: step 107230, loss = 0.58 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:16:23.414911: step 107240, loss = 0.67 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:16:24.303602: step 107250, loss = 0.73 (1440.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:16:25.167705: step 107260, loss = 0.75 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:16:26.027776: step 107270, loss = 0.69 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:16:26.890177: step 107280, loss = 0.75 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:16:27.755230: step 107290, loss = 0.62 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:16:28.734580: step 107300, loss = 0.64 (1306.9 examples/sec; 0.098 sec/batch)
2017-06-02 05:16:29.458761: step 107310, loss = 0.62 (1767.5 examples/sec; 0.072 sec/batch)
2017-06-02 05:16:30.340693: step 107320, loss = 0.63 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:16:31.223065: step 107330, loss = 0.83 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:16:32.087845: step 107340, loss = 0.72 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:16:32.978416: step 107350, loss = 0.82 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:16:33.850865: step 107360, loss = 0.51 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:16:34.735906: step 107370, loss = 0.57 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:16:35.600146: step 107380, loss = 0.73 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:16:36.470638: step 107390, loss = 0.74 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:16:37.428038: step 107400, loss = 0.64 (1336.9 examples/sec; 0.096 sec/batch)
2017-06-02 05:16:38.180676: step 107410, loss = 0.79 (1700.7 examples/sec; 0.075 sec/batch)
2017-06-02 05:16:39.019100: step 107420, loss = 0.69 (1526.7 examples/sec; 0.084 sec/batch)
2017-06-02 05:16:39.872668: step 107430, loss = 0.64 (1499.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:16:40.704623: step 107440, loss = 0.90 (1538.6 examples/sec; 0.083 sec/batch)
2017-06-02 05:16:41.544330: step 107450, loss = 0.69 (1524.3 examples/sec; 0.084 sec/batch)
2017-06-02 05:16:42.427511: step 107460, loss = 0.71 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:16:43.286711: step 107470, loss = 0.76 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:16:44.144607: step 107480, loss = 0.52 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:16:45.006257: step 107490, loss = 0.71 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:16:45.967223: step 107500, loss = 0.67 (1332.0 examples/sec; 0.096 sec/batch)
2017-06-02 05:16:46.739086: step 107510, loss = 0.76 (1658.3 examples/sec; 0.077 sec/batch)
2017-06-02 05:16:47.596564: step 107520, loss = 0.72 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:16:48.465885: step 107530, loss = 0.75 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:16:49.347541: step 107540, loss = 0.74 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:16:50.209970: step 107550, loss = 0.61 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:16:51.071365: step 107560, loss = 0.81 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:16:51.930740: step 107570, loss = 0.63 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:16:52.812759: step 107580, loss = 0.77 (1451.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:16:53.683493: step 107590, loss = 0.75 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:16:54.652042: step 107600, loss = 0.62 (1321.6 examples/sec; 0.097 sec/batch)
2017-06-02 05:16:55.409253: step 107610, loss = 0.77 (1690.4 examples/sec; 0.076 sec/batch)
2017-06-02 05:16:56.261701: step 107620, loss = 0.69 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:16:57.125055: step 107630, loss = 0.79 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:16:57.994354: step 107640, loss = 0.71 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:16:58.864070: step 107650, loss = 0.78 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:16:59.727313: step 107660, loss = 0.76 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:17:00.593833: step 107670, loss = 0.61 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:17:01.461024: step 107680, loss = 0.71 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:17:02.332420: step 107690, loss = 0.65 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:17:03.365757: step 107700, loss = 0.77 (1238.7 examples/sec; 0.103 sec/batch)
2017-06-02 05:17:04.061759: step 107710, loss = 0.72 (1839.1 examples/sec; 0.070 sec/batch)
2017-06-02 05:17:04.953462: step 107720, loss = 0.77 (1435.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:17:05.823687: step 107730, loss = 0.56 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:17:06.680037: step 107740, loss = 0.73 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:17:07.549219: step 107750, loss = 0.71 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:17:08.427310: step 107760, loss = 0.59 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:17:09.301325: step 107770, loss = 0.65 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:17:10.166816: step 107780, loss = 0.63 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:17:11.055650: step 107790, loss = 0.71 (1440.1 examples/sec; 0.089 sec/batch)
2017-06-02 05:17:12.034260: step 107800, loss = 0.69 (1308.0 examples/sec; 0.098 sec/batch)
2017-06-02 05:17:12.781412: step 107810, loss = 0.70 (1713.2 examples/sec; 0.075 sec/batch)
2017-06-02 05:17:13.649237: step 107820, loss = 0.74 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:17:14.515191: step 107830, loss = 0.79 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:17:15.392540: step 107840, loss = 0.67 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:17:16.284313: step 107850, loss = 0.76 (1435.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:17:17.119439: step 107860, loss = 0.68 (1532.7 examples/sec; 0.084 sec/batch)
2017-06-02 05:17:17.977116: step 107870, loss = 0.59 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:17:18.868163: step 107880, loss = 0.75 (1436.5 examples/sec; 0.089 sec/batch)
2017-06-02 05:17:19.737116: step 107890, loss = 0.67 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:17:20.693642: step 107900, loss = 0.67 (1338.2 examples/sec; 0.096 sec/batch)
2017-06-02 05:17:21.487639: step 107910, loss = 0.67 (1612.1 examples/sec; 0.079 sec/batch)
2017-06-02 05:17:22.351570: step 107920, loss = 0.75 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:17:23.222217: step 107930, loss = 0.79 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:17:24.095135: step 107940, loss = 0.73 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:17:24.981376: step 107950, loss = 0.61 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:17:25.854506: step 107960, loss = 0.77 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:17:26.735191: step 107970, loss = 0.59 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:17:27.606399: step 107980, loss = 0.76 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:17:28.469893: step 107990, loss = 0.72 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:17:29.427599: step 108000, loss = 0.72 (1336.5 examples/sec; 0.096 sec/batch)
2017-06-02 05:17:30.188564: step 108010, loss = 0.78 (1682.1 examples/sec; 0.076 sec/batch)
2017-06-02 05:17:31.037545: step 108020, loss = 0.65 (1507.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:17:31.914389: step 108030, loss = 0.71 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:17:32.756694: step 108040, loss = 0.55 (1519.7 examples/sec; 0.084 sec/batch)
2017-06-02 05:17:33.637668: step 108050, loss = 0.70 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:17:34.491377: step 108060, loss = 0.71 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:17:35.350990: step 108070, loss = 0.77 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:17:36.203675: step 108080, loss = 0.72 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:17:37.070460: step 108090, loss = 0.77 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:17:38.032377: step 108100, loss = 0.78 (1330.7 examples/sec; 0.096 sec/batch)
2017-06-02 05:17:38.768292: step 108110, loss = 0.89 (1739.3 examples/sec; 0.074 sec/batch)
2017-06-02 05:17:39.631572: step 108120, loss = 0.70 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:17:40.454107: step 108130, loss = 0.66 (1556.2 examples/sec; 0.082 sec/batch)
2017-06-02 05:17:41.331615: step 108140, loss = 0.62 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:17:42.188684: step 108150, loss = 0.71 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:17:43.042984: step 108160, loss = 0.59 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:17:43.896463: step 108170, loss = 0.68 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:17:44.747562: step 108180, loss = 0.58 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:17:45.606329: step 108190, loss = 0.57 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:17:46.518301: step 108200, loss = 0.69 (1403.6 examples/sec; 0.091 sec/batch)
2017-06-02 05:17:47.277194: step 108210, loss = 0.60 (1686.7 examples/sec; 0.076 sec/batch)
2017-06-02 05:17:48.158589: step 108220, loss = 0.67 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:17:49.004934: step 108230, loss = 0.52 (1512.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:17:49.853163: step 108240, loss = 0.59 (1509.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:17:50.713134: step 108250, loss = 0.70 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:17:51.584751: step 108260, loss = 0.70 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:17:52.463429: step 108270, loss = 0.70 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:17:53.306432: step 108280, loss = 0.69 (1518.4 examples/sec; 0.084 sec/batch)
2017-06-02 05:17:54.160500: step 108290, loss = 0.66 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:17:55.112019: step 108300, loss = 0.69 (1345.2 examples/sec; 0.095 sec/batch)
2017-06-02 05:17:55.894968: step 108310, loss = 0.71 (1634.8 examples/sec; 0.078 sec/batch)
2017-06-02 05:17:56.764004: step 108320, loss = 0.66 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:17:57.645379: step 108330, loss = 0.66 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:17:58.517120: step 108340, loss = 0.83 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:17:59.369697: step 108350, loss = 0.62 (1501.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:18:00.246195: step 108360, loss = 0.67 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:18:01.103829: step 108370, loss = 0.65 (1492.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:18:01.985764: step 108380, loss = 0.61 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:18:02.844111: step 108390, loss = 0.79 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:18:03.818126: step 108400, loss = 0.77 (1314.1 examples/sec; 0.097 sec/batch)
2017-06-02 05:18:04.609310: step 108410, loss = 0.79 (1617.8 examples/sec; 0.079 sec/batch)
2017-06-02 05:18:05.468993: step 108420, loss = 0.70 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:18:06.348557: step 108430, loss = 0.60 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:18:07.223389: step 108440, loss = 0.53 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:18:08.102386: step 108450, loss = 0.79 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:18:08.966782: step 108460, loss = 0.63 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:18:09.840815: step 108470, loss = 0.63 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:18:10.710452: step 108480, loss = 0.65 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:18:11.562461: step 108490, loss = 0.63 (1502.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:18:12.496826: step 108500, loss = 0.60 (1369.9 examples/sec; 0.093 sec/batch)
2017-06-02 05:18:13.242328: step 108510, loss = 0.61 (1717.0 examples/sec; 0.075 sec/batch)
2017-06-02 05:18:14.102692: step 108520, loss = 0.77 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:18:14.953969: step 108530, loss = 0.66 (1503.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:18:15.831446: step 108540, loss = 0.82 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:18:16.697396: step 108550, loss = 0.66 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:18:17.575644: step 108560, loss = 0.65 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:18:18.438081: step 108570, loss = 0.64 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:18:19.317408: step 108580, loss = 0.77 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:18:20.192463: step 108590, loss = 0.73 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:18:21.155518: step 108600, loss = 0.65 (1329.1 examples/sec; 0.096 sec/batch)
2017-06-02 05:18:21.955282: step 108610, loss = 0.74 (1600.5 examples/sec; 0.080 sec/batch)
2017-06-02 05:18:22.815989: step 108620, loss = 0.54 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:18:23.687302: step 108630, loss = 0.62 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:18:24.534980: step 108640, loss = 0.91 (1510.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:18:25.425252: step 108650, loss = 0.84 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 05:18:26.275269: step 108660, loss = 0.58 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:18:27.130541: step 108670, loss = 0.62 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:18:28.013244: step 108680, loss = 0.84 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:18:28.891903: step 108690, loss = 0.62 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:18:29.895644: step 108700, loss = 0.75 (1275.2 examples/sec; 0.100 sec/batch)
2017-06-02 05:18:30.651459: step 108710, loss = 0.78 (1693.5 examples/sec; 0.076 sec/batch)
2017-06-02 05:18:31.516169: step 108720, loss = 0.83 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:18:32.358126: step 108730, loss = 0.66 (1520.3 examples/sec; 0.084 sec/batch)
2017-06-02 05:18:33.215920: step 108740, loss = 0.85 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:18:34.086410: step 108750, loss = 0.66 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:18:34.947200: step 108760, loss = 0.74 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:18:35.822869: step 108770, loss = 0.57 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:18:36.671651: step 108780, loss = 0.84 (1508.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:18:37.559205: step 108790, loss = 0.68 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:18:38.530696: step 108800, loss = 0.75 (1317.6 examples/sec; 0.097 sec/batch)
2017-06-02 05:18:39.305688: step 108810, loss = 0.73 (1651.6 examples/sec; 0.077 sec/batch)
2017-06-02 05:18:40.142555: step 108820, loss = 0.77 (1529.5 examples/sec; 0.084 sec/batch)
2017-06-02 05:18:40.994211: step 108830, loss = 0.77 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:18:41.862752: step 108840, loss = 0.63 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:18:42.746144: step 108850, loss = 0.68 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:18:43.636076: step 108860, loss = 0.68 (1438.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:18:44.475242: step 108870, loss = 0.55 (1525.3 examples/sec; 0.084 sec/batch)
2017-06-02 05:18:45.338218: step 108880, loss = 0.60 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:18:46.173907: step 108890, loss = 0.79 (1531.7 examples/sec; 0.084 sec/batch)
2017-06-02 05:18:47.159194: step 108900, loss = 0.90 (1299.1 examples/sec; 0.099 sec/batch)
2017-06-02 05:18:47.940984: step 108910, loss = 0.77 (1637.3 examples/sec; 0.078 sec/batch)
2017-06-02 05:18:48.820593: step 108920, loss = 0.86 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:18:49.672997: step 108930, loss = 0.65 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:18:50.534972: step 108940, loss = 0.61 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:18:51.414548: step 108950, loss = 0.60 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:18:52.295554: step 108960, loss = 0.71 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:18:53.176138: step 108970, loss = 0.62 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:18:54.082985: step 108980, loss = 0.60 (1411.5 examples/sec; 0.091 sec/batch)
2017-06-02 05:18:54.926870: step 108990, loss = 0.52 (1516.8 examples/sec; 0.084 sec/batch)
2017-06-02 05:18:55.853397: step 109000, loss = 0.68 (1381.5 examples/sec; 0.093 sec/batch)
2017-06-02 05:18:56.620810: step 109010, loss = 0.80 (1667.9 examples/sec; 0.077 sec/batch)
2017-06-02 05:18:57.492284: step 109020, loss = 0.84 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:18:58.348663: step 109030, loss = 0.70 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:18:59.209432: step 109040, loss = 0.81 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:19:00.087906: step 109050, loss = 0.55 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:19:00.953013: step 109060, loss = 0.79 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:19:01.805463: step 109070, loss = 0.58 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:19:02.650854: step 109080, loss = 0.74 (1514.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:19:03.512817: step 109090, loss = 0.79 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:19:04.474336: step 109100, loss = 0.85 (1331.2 examples/sec; 0.096 sec/batch)
2017-06-02 05:19:05.263855: step 109110, loss = 0.81 (1621.2 examples/sec; 0.079 sec/batch)
2017-06-02 05:19:06.154317: step 109120, loss = 0.54 (1437.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:19:06.982839: step 109130, loss = 0.71 (1544.9 examples/sec; 0.083 sec/batch)
2017-06-02 05:19:07.868405: step 109140, loss = 0.75 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:19:08.748807: step 109150, loss = 0.75 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:19:09.635525: step 109160, loss = 0.64 (1443.5 examples/sec; 0.089 sec/batch)
2017-06-02 05:19:10.483242: step 109170, loss = 0.62 (1509.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:19:11.361338: step 109180, loss = 0.67 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:19:12.230684: step 109190, loss = 0.64 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:19:13.230093: step 109200, loss = 0.63 (1280.8 examples/sec; 0.100 sec/batch)
2017-06-02 05:19:13.981347: step 109210, loss = 0.67 (1703.8 examples/sec; 0.075 sec/batch)
2017-06-02 05:19:14.831446: step 109220, loss = 0.86 (1505.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:19:15.683062: step 109230, loss = 0.65 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:19:16.532574: step 109240, loss = 0.65 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:19:17.387948: step 109250, loss = 0.72 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:19:18.268824: step 109260, loss = 0.54 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:19:19.120416: step 109270, loss = 0.76 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:19:19.972811: step 109280, loss = 0.53 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:19:20.858769: step 109290, loss = 0.76 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 05:19:21.792378: step 109300, loss = 0.58 (1371.0 examples/sec; 0.093 sec/batch)
2017-06-02 05:19:22.545314: step 109310, loss = 0.75 (1700.0 examples/sec; 0.075 sec/batch)
2017-06-02 05:19:23.394524: step 109320, loss = 0.80 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:19:24.273114: step 109330, loss = 0.67 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:19:25.114692: step 109340, loss = 0.84 (1520.9 examples/sec; 0.084 sec/batch)
2017-06-02 05:19:25.999069: step 109350, loss = 0.58 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:19:26.875038: step 109360, loss = 0.64 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:19:27.761064: step 109370, loss = 0.69 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:19:28.625688: step 109380, loss = 0.63 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:19:29.485185: step 109390, loss = 0.70 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:19:30.442108: step 109400, loss = 0.62 (1337.5 examples/sec; 0.096 sec/batch)
2017-06-02 05:19:31.207461: step 109410, loss = 0.83 (1672.4 examples/sec; 0.077 sec/batch)
2017-06-02 05:19:32.070373: step 109420, loss = 0.68 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:19:32.922864: step 109430, loss = 0.51 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:19:33.794491: step 109440, loss = 0.58 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:19:34.640867: step 109450, loss = 0.69 (1512.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:19:35.526445: step 109460, loss = 0.77 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:19:36.419879: step 109470, loss = 0.78 (1432.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:19:37.311837: step 109480, loss = 0.73 (1435.1 examples/sec; 0.089 sec/batch)
2017-06-02 05:19:38.215352: step 109490, loss = 0.53 (1416.7 examples/sec; 0.090 sec/batch)
2017-06-02 05:19:39.183706: step 109500, loss = 0.69 (1321.8 examples/sec; 0.097 sec/batch)
2017-06-02 05:19:39.967980: step 109510, loss = 0.61 (1632.1 examples/sec; 0.078 sec/batch)
2017-06-02 05:19:40.843034: step 109520, loss = 0.78 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:19:41.722449: step 109530, loss = 0.73 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:19:42.590968: step 109540, loss = 0.89 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:19:43.444742: step 109550, loss = 0.74 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:19:44.336094: step 109560, loss = 0.70 (1436.0 examples/sec; 0.089 sec/batch)
2017-06-02 05:19:45.209697: step 109570, loss = 0.82 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:19:46.100002: step 109580, loss = 0.64 (1437.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:19:46.968229: step 109590, loss = 0.59 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:19:47.940848: step 109600, loss = 0.74 (1316.0 examples/sec; 0.097 sec/batch)
2017-06-02 05:19:48.705789: step 109610, loss = 0.63 (1673.4 examples/sec; 0.076 sec/batch)
2017-06-02 05:19:49.594549: step 109620, loss = 0.76 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:19:50.454805: step 109630, loss = 0.71 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:19:51.379189: step 109640, loss = 0.79 (1384.7 examples/sec; 0.092 sec/batch)
2017-06-02 05:19:52.242800: step 109650, loss = 0.69 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:19:53.105718: step 109660, loss = 0.66 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:19:53.953404: step 109670, loss = 0.60 (1510.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:19:54.829185: step 109680, loss = 0.75 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:19:55.701059: step 109690, loss = 0.74 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:19:56.651544: step 109700, loss = 0.73 (1346.7 examples/sec; 0.095 sec/batch)
2017-06-02 05:19:57.431588: step 109710, loss = 0.81 (1640.9 examples/sec; 0.078 sec/batch)
2017-06-02 05:19:58.280020: step 109720, loss = 0.73 (1508.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:19:59.135686: step 109730, loss = 0.62 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:19:59.962792: step 109740, loss = 0.75 (1547.6 examples/sec; 0.083 sec/batch)
2017-06-02 05:20:00.852060: step 109750, loss = 0.70 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:20:01.701088: step 109760, loss = 0.73 (1507.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:20:02.563995: step 109770, loss = 0.64 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:20:03.434084: step 109780, loss = 0.56 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:20:04.303089: step 109790, loss = 0.67 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:20:05.246979: step 109800, loss = 0.74 (1356.1 examples/sec; 0.094 sec/batch)
2017-06-02 05:20:06.001478: step 109810, loss = 0.74 (1696.5 examples/sec; 0.075 sec/batch)
2017-06-02 05:20:06.876914: step 109820, loss = 0.64 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:20:07.734647: step 109830, loss = 0.64 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:20:08.600156: step 109840, loss = 0.88 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:20:09.462464: step 109850, loss = 0.67 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:20:10.287423: step 109860, loss = 0.71 (1551.6 examples/sec; 0.082 sec/batch)
2017-06-02 05:20:11.158264: step 109870, loss = 0.70 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:20:12.003106: step 109880, loss = 0.64 (1515.1 examples/sec; 0.084 sec/batch)
2017-06-02 05:20:12.880376: step 109890, loss = 0.73 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:20:13.806118: step 109900, loss = 0.61 (1382.7 examples/sec; 0.093 sec/batch)
2017-06-02 05:20:14.611957: step 109910, loss = 0.74 (1588.4 examples/sec; 0.081 sec/batch)
2017-06-02 05:20:15.483877: step 109920, loss = 0.80 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:20:16.360180: step 109930, loss = 0.69 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:20:17.229019: step 109940, loss = 0.49 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:20:18.089778: step 109950, loss = 0.80 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:20:18.976109: step 109960, loss = 0.77 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:20:19.849144: step 109970, loss = 0.67 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:20:20.724724: step 109980, loss = 0.70 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:20:21.581446: step 109990, loss = 0.64 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:20:22.499116: step 110000, loss = 0.67 (1394.8 examples/sec; 0.092 sec/batch)
2017-06-02 05:20:23.284761: step 110010, loss = 0.82 (1629.3 examples/sec; 0.079 sec/batch)
2017-06-02 05:20:24.172570: step 110020, loss = 0.63 (1441.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:20:25.062014: step 110030, loss = 0.64 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 05:20:25.955606: step 110040, loss = 0.77 (1432.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:20:26.818527: step 110050, loss = 0.66 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:20:27.701935: step 110060, loss = 0.56 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:20:28.562144: step 110070, loss = 0.76 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:20:29.417981: step 110080, loss = 0.67 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:20:30.285379: step 110090, loss = 0.70 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:20:31.259261: step 110100, loss = 0.72 (1314.3 examples/sec; 0.097 sec/batch)
2017-06-02 05:20:32.024138: step 110110, loss = 0.71 (1673.5 examples/sec; 0.076 sec/batch)
2017-06-02 05:20:32.906743: step 110120, loss = 0.68 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:20:33.755202: step 110130, loss = 0.69 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:20:34.614644: step 110140, loss = 0.77 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:20:35.456792: step 110150, loss = 0.80 (1519.9 examples/sec; 0.084 sec/batch)
2017-06-02 05:20:36.334767: step 110160, loss = 0.72 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:20:37.215128: step 110170, loss = 0.62 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:20:38.087881: step 110180, loss = 0.64 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:20:38.959374: step 110190, loss = 0.65 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:20:39.939980: step 110200, loss = 0.74 (1305.3 examples/sec; 0.098 sec/batch)
2017-06-02 05:20:40.716639: step 110210, loss = 0.63 (1648.1 examples/sec; 0.078 sec/batch)
2017-06-02 05:20:41.586388: step 110220, loss = 0.75 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:20:42.456268: step 110230, loss = 0.65 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:20:43.316307: step 110240, loss = 0.64 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:20:44.209822: step 110250, loss = 0.69 (1432.6 examples/sec; 0.089 sec/batch)
2017-06-02 05:20:45.075356: step 110260, loss = 0.73 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:20:45.926337: step 110270, loss = 0.54 (1504.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:20:46.797877: step 110280, loss = 0.76 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:20:47.657774: step 110290, loss = 0.74 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:20:48.649499: step 110300, loss = 0.62 (1290.7 examples/sec; 0.099 sec/batch)
2017-06-02 05:20:49.438417: step 110310, loss = 0.85 (1622.5 examples/sec; 0.079 sec/batch)
2017-06-02 05:20:50.301306: step 110320, loss = 0.59 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:20:51.190235: step 110330, loss = 0.66 (1439.9 examples/sec; 0.089 sec/batch)
2017-06-02 05:20:52.074567: step 110340, loss = 0.70 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:20:52.941192: step 110350, loss = 0.60 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:20:53.819134: step 110360, loss = 0.68 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:20:54.685253: step 110370, loss = 0.69 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:20:55.546630: step 110380, loss = 0.82 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:20:56.404449: step 110390, loss = 0.50 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:20:57.369728: step 110400, loss = 0.68 (1326.1 examples/sec; 0.097 sec/batch)
2017-06-02 05:20:58.164129: step 110410, loss = 0.64 (1611.3 examples/sec; 0.079 sec/batch)
2017-06-02 05:20:58.998696: step 110420, loss = 0.74 (1533.7 examples/sec; 0.083 sec/batch)
2017-06-02 05:20:59.868619: step 110430, loss = 0.77 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:21:00.743267: step 110440, loss = 0.69 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:21:01.591389: step 110450, loss = 0.67 (1509.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:21:02.447275: step 110460, loss = 0.74 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:21:03.333897: step 110470, loss = 0.83 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:21:04.197462: step 110480, loss = 0.74 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:21:05.064243: step 110490, loss = 0.61 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:21:06.042844: step 110500, loss = 0.78 (1308.0 examples/sec; 0.098 sec/batch)
2017-06-02 05:21:06.814722: step 110510, loss = 0.71 (1658.3 examples/sec; 0.077 sec/batch)
2017-06-02 05:21:07.681595: step 110520, loss = 0.70 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:21:08.543248: step 110530, loss = 0.74 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:21:09.412614: step 110540, loss = 0.77 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:21:10.268830: step 110550, loss = 0.67 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:21:11.141058: step 110560, loss = 0.56 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:21:12.005105: step 110570, loss = 0.65 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:21:12.890700: step 110580, loss = 0.60 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:21:13.757053: step 110590, loss = 0.68 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:21:14.713377: step 110600, loss = 0.69 (1338.5 examples/sec; 0.096 sec/batch)
2017-06-02 05:21:15.487540: step 110610, loss = 0.56 (1653.4 examples/sec; 0.077 sec/batch)
2017-06-02 05:21:16.377468: step 110620, loss = 0.82 (1438.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:21:17.278458: step 110630, loss = 0.64 (1420.7 examples/sec; 0.090 sec/batch)
2017-06-02 05:21:18.131094: step 110640, loss = 0.71 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:21:18.977893: step 110650, loss = 0.66 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:21:19.836356: step 110660, loss = 0.72 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:21:20.680028: step 110670, loss = 0.63 (1517.2 examples/sec; 0.084 sec/batch)
2017-06-02 05:21:21.540762: step 110680, loss = 0.64 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:21:22.385454: step 110690, loss = 0.73 (1515.4 examples/sec; 0.084 sec/batch)
2017-06-02 05:21:23.340180: step 110700, loss = 0.71 (1340.7 examples/sec; 0.095 sec/batch)
2017-06-02 05:21:24.110873: step 110710, loss = 0.73 (1660.9 examples/sec; 0.077 sec/batch)
2017-06-02 05:21:24.951135: step 110720, loss = 0.69 (1523.3 examples/sec; 0.084 sec/batch)
2017-06-02 05:21:25.804356: step 110730, loss = 0.78 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:21:26.643282: step 110740, loss = 0.56 (1525.8 examples/sec; 0.084 sec/batch)
2017-06-02 05:21:27.488844: step 110750, loss = 0.62 (1513.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:21:28.326489: step 110760, loss = 0.75 (1528.1 examples/sec; 0.084 sec/batch)
2017-06-02 05:21:29.190403: step 110770, loss = 0.81 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:21:30.046948: step 110780, loss = 0.64 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:21:30.936696: step 110790, loss = 0.60 (1438.6 examples/sec; 0.089 sec/batch)
2017-06-02 05:21:31.932671: step 110800, loss = 0.75 (1285.2 examples/sec; 0.100 sec/batch)
2017-06-02 05:21:32.651651: step 110810, loss = 0.85 (1780.3 examples/sec; 0.072 sec/batch)
2017-06-02 05:21:33.549207: step 110820, loss = 0.65 (1426.1 examples/sec; 0.090 sec/batch)
2017-06-02 05:21:34.425634: step 110830, loss = 0.61 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:21:35.281553: step 110840, loss = 0.64 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:21:36.157750: step 110850, loss = 0.76 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:21:37.047975: step 110860, loss = 0.69 (1437.9 examples/sec; 0.089 sec/batch)
2017-06-02 05:21:37.893690: step 110870, loss = 0.58 (1513.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:21:38.750856: step 110880, loss = 0.71 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:21:39.596446: step 110890, loss = 0.78 (1513.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:21:40.566047: step 110900, loss = 0.56 (1320.1 examples/sec; 0.097 sec/batch)
2017-06-02 05:21:41.337225: step 110910, loss = 0.78 (1659.8 examples/sec; 0.077 sec/batch)
2017-06-02 05:21:42.170516: step 110920, loss = 0.76 (1536.1 examples/sec; 0.083 sec/batch)
2017-06-02 05:21:43.036561: step 110930, loss = 0.77 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:21:43.913886: step 110940, loss = 0.79 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:21:44.779519: step 110950, loss = 0.73 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:21:45.638217: step 110960, loss = 0.65 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:21:46.507571: step 110970, loss = 0.65 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:21:47.376573: step 110980, loss = 0.67 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:21:48.239578: step 110990, loss = 0.63 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:21:49.207978: step 111000, loss = 0.70 (1321.8 examples/sec; 0.097 sec/batch)
2017-06-02 05:21:49.978508: step 111010, loss = 0.60 (1661.2 examples/sec; 0.077 sec/batch)
2017-06-02 05:21:50.837252: step 111020, loss = 0.61 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:21:51.712994: step 111030, loss = 0.65 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:21:52.562326: step 111040, loss = 0.68 (1507.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:21:53.428156: step 111050, loss = 0.54 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:21:54.288955: step 111060, loss = 0.72 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:21:55.133439: step 111070, loss = 0.67 (1515.7 examples/sec; 0.084 sec/batch)
2017-06-02 05:21:55.968155: step 111080, loss = 0.65 (1533.4 examples/sec; 0.083 sec/batch)
2017-06-02 05:21:56.823671: step 111090, loss = 0.81 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:21:57.734521: step 111100, loss = 0.76 (1405.3 examples/sec; 0.091 sec/batch)
2017-06-02 05:21:58.501332: step 111110, loss = 0.60 (1669.2 examples/sec; 0.077 sec/batch)
2017-06-02 05:21:59.353828: step 111120, loss = 0.61 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:22:00.209240: step 111130, loss = 0.63 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:22:01.068166: step 111140, loss = 0.68 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:22:01.950690: step 111150, loss = 0.73 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:22:02.810183: step 111160, loss = 0.77 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:22:03.701991: step 111170, loss = 0.69 (1435.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:22:04.584618: step 111180, loss = 0.71 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:22:05.458950: step 111190, loss = 0.68 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:22:06.447541: step 111200, loss = 0.55 (1294.8 examples/sec; 0.099 sec/batch)
2017-06-02 05:22:07.202378: step 111210, loss = 0.51 (1695.8 examples/sec; 0.075 sec/batch)
2017-06-02 05:22:08.063436: step 111220, loss = 0.82 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:22:08.936365: step 111230, loss = 0.66 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:22:09.805451: step 111240, loss = 0.79 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:22:10.669128: step 111250, loss = 0.63 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:22:11.565823: step 111260, loss = 0.79 (1427.5 examples/sec; 0.090 sec/batch)
2017-06-02 05:22:12.431918: step 111270, loss = 0.80 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:22:13.285247: step 111280, loss = 0.57 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:22:14.147077: step 111290, loss = 0.91 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:22:15.142495: step 111300, loss = 0.75 (1285.9 examples/sec; 0.100 sec/batch)
2017-06-02 05:22:15.929798: step 111310, loss = 0.63 (1625.8 examples/sec; 0.079 sec/batch)
2017-06-02 05:22:16.785742: step 111320, loss = 0.62 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:22:17.641046: step 111330, loss = 0.56 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:22:18.488376: step 111340, loss = 0.67 (1510.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:22:19.391264: step 111350, loss = 0.54 (1417.7 examples/sec; 0.090 sec/batch)
2017-06-02 05:22:20.273719: step 111360, loss = 0.74 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:22:21.153980: step 111370, loss = 0.69 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:22:22.016712: step 111380, loss = 0.80 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:22:22.896295: step 111390, loss = 0.53 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:22:23.888653: step 111400, loss = 0.70 (1289.9 examples/sec; 0.099 sec/batch)
2017-06-02 05:22:24.616382: step 111410, loss = 0.59 (1758.9 examples/sec; 0.073 sec/batch)
2017-06-02 05:22:25.494269: step 111420, loss = 0.68 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:22:26.350424: step 111430, loss = 0.63 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:22:27.216383: step 111440, loss = 0.66 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:22:28.077354: step 111450, loss = 0.62 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:22:28.946388: step 111460, loss = 0.69 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:22:29.826995: step 111470, loss = 0.60 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:22:30.697608: step 111480, loss = 0.84 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:22:31.586062: step 111490, loss = 0.71 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:22:32.549168: step 111500, loss = 0.60 (1329.0 examples/sec; 0.096 sec/batch)
2017-06-02 05:22:33.299752: step 111510, loss = 0.74 (1705.4 examples/sec; 0.075 sec/batch)
2017-06-02 05:22:34.149045: step 111520, loss = 0.77 (1507.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:22:35.012331: step 111530, loss = 0.65 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:22:35.866180: step 111540, loss = 0.70 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:22:36.716866: step 111550, loss = 0.71 (1504.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:22:37.583560: step 111560, loss = 0.69 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:22:38.456792: step 111570, loss = 0.71 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:22:39.326058: step 111580, loss = 0.60 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:22:40.200632: step 111590, loss = 0.66 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:22:41.127404: step 111600, loss = 0.62 (1381.1 examples/sec; 0.093 sec/batch)
2017-06-02 05:22:41.915208: step 111610, loss = 0.63 (1624.8 examples/sec; 0.079 sec/batch)
2017-06-02 05:22:42.777141: step 111620, loss = 0.73 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:22:43.650564: step 111630, loss = 0.71 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:22:44.522427: step 111640, loss = 0.86 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:22:45.394673: step 111650, loss = 0.68 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:22:46.269943: step 111660, loss = 0.76 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:22:47.116885: step 111670, loss = 0.73 (1511.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:22:48.000840: step 111680, loss = 0.71 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:22:48.883097: step 111690, loss = 0.66 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:22:49.842392: step 111700, loss = 0.78 (1334.3 examples/sec; 0.096 sec/batch)
2017-06-02 05:22:50.609063: step 111710, loss = 0.71 (1669.6 examples/sec; 0.077 sec/batch)
2017-06-02 05:22:51.470423: step 111720, loss = 0.57 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:22:52.351489: step 111730, loss = 0.76 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:22:53.228918: step 111740, loss = 0.72 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:22:54.088499: step 111750, loss = 0.67 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:22:54.970175: step 111760, loss = 0.61 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:22:55.807506: step 111770, loss = 0.67 (1528.7 examples/sec; 0.084 sec/batch)
2017-06-02 05:22:56.670678: step 111780, loss = 0.63 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:22:57.555040: step 111790, loss = 0.79 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:22:58.505506: step 111800, loss = 0.62 (1346.7 examples/sec; 0.095 sec/batch)
2017-06-02 05:22:59.281471: step 111810, loss = 0.69 (1649.6 examples/sec; 0.078 sec/batch)
2017-06-02 05:23:00.180644: step 111820, loss = 0.65 (1423.6 examples/sec; 0.090 sec/batch)
2017-06-02 05:23:01.066853: step 111830, loss = 0.66 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:23:01.934595: step 111840, loss = 0.77 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:23:02.793755: step 111850, loss = 0.60 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:23:03.644110: step 111860, loss = 0.61 (1505.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:23:04.538346: step 111870, loss = 0.53 (1431.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:23:05.403878: step 111880, loss = 0.74 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:23:06.282990: step 111890, loss = 0.55 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:23:07.231659: step 111900, loss = 0.73 (1349.3 examples/sec; 0.095 sec/batch)
2017-06-02 05:23:07.996364: step 111910, loss = 0.70 (1673.8 examples/sec; 0.076 sec/batch)
2017-06-02 05:23:08.872642: step 111920, loss = 0.67 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:23:09.750961: step 111930, loss = 0.80 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:23:10.628712: step 111940, loss = 0.80 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:23:11.515600: step 111950, loss = 0.63 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:23:12.380468: step 111960, loss = 0.65 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:23:13.252352: step 111970, loss = 0.83 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:23:14.128079: step 111980, loss = 0.65 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:23:14.984986: step 111990, loss = 0.72 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:23:15.944247: step 112000, loss = 0.65 (1334.4 examples/sec; 0.096 sec/batch)
2017-06-02 05:23:16.713896: step 112010, loss = 0.85 (1663.1 examples/sec; 0.077 sec/batch)
2017-06-02 05:23:17.565405: step 112020, loss = 0.59 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:23:18.439957: step 112030, loss = 0.52 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:23:19.309539: step 112040, loss = 0.59 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:23:20.187943: step 112050, loss = 0.66 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:23:21.041666: step 112060, loss = 0.73 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:23:21.909280: step 112070, loss = 0.63 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:23:22.758610: step 112080, loss = 0.72 (1507.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:23:23.622121: step 112090, loss = 0.66 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:23:24.583006: step 112100, loss = 0.75 (1332.1 examples/sec; 0.096 sec/batch)
2017-06-02 05:23:25.341843: step 112110, loss = 0.63 (1686.8 examples/sec; 0.076 sec/batch)
2017-06-02 05:23:26.182617: step 112120, loss = 0.77 (1522.4 examples/sec; 0.084 sec/batch)
2017-06-02 05:23:27.051507: step 112130, loss = 0.63 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:23:27.891694: step 112140, loss = 0.79 (1523.5 examples/sec; 0.084 sec/batch)
2017-06-02 05:23:28.778892: step 112150, loss = 0.58 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:23:29.634794: step 112160, loss = 0.74 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:23:30.491737: step 112170, loss = 0.76 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:23:31.366787: step 112180, loss = 0.64 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:23:32.251753: step 112190, loss = 0.69 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:23:33.233303: step 112200, loss = 0.71 (1304.1 examples/sec; 0.098 sec/batch)
2017-06-02 05:23:33.995609: step 112210, loss = 0.67 (1679.1 examples/sec; 0.076 sec/batch)
2017-06-02 05:23:34.863656: step 112220, loss = 0.68 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:23:35.745421: step 112230, loss = 0.67 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:23:36.631129: step 112240, loss = 0.74 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:23:37.501218: step 112250, loss = 0.65 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:23:38.396242: step 112260, loss = 0.73 (1430.2 examples/sec; 0.090 sec/batch)
2017-06-02 05:23:39.296408: step 112270, loss = 0.84 (1421.9 examples/sec; 0.090 sec/batch)
2017-06-02 05:23:40.177145: step 112280, loss = 0.87 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:23:41.041009: step 112290, loss = 0.70 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:23:42.035103: step 112300, loss = 0.81 (1287.6 examples/sec; 0.099 sec/batch)
2017-06-02 05:23:42.822202: step 112310, loss = 0.55 (1626.2 examples/sec; 0.079 sec/batch)
2017-06-02 05:23:43.685984: step 112320, loss = 0.73 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:23:44.560531: step 112330, loss = 0.68 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:23:45.428504: step 112340, loss = 0.83 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:23:46.299503: step 112350, loss = 0.87 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:23:47.176861: step 112360, loss = 0.60 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:23:48.070225: step 112370, loss = 0.77 (1432.8 examples/sec; 0.089 sec/batch)
2017-06-02 05:23:48.944666: step 112380, loss = 0.61 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:23:49.821970: step 112390, loss = 0.65 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:23:50.777072: step 112400, loss = 0.76 (1340.2 examples/sec; 0.096 sec/batch)
2017-06-02 05:23:51.535648: step 112410, loss = 0.72 (1687.4 examples/sec; 0.076 sec/batch)
2017-06-02 05:23:52.418291: step 112420, loss = 0.70 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:23:53.275139: step 112430, loss = 0.78 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:23:54.138956: step 112440, loss = 0.69 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:23:55.025124: step 112450, loss = 0.70 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:23:55.893218: step 112460, loss = 0.75 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:23:56.775339: step 112470, loss = 0.84 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:23:57.647281: step 112480, loss = 0.86 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:23:58.515742: step 112490, loss = 0.80 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:23:59.549747: step 112500, loss = 0.63 (1237.9 examples/sec; 0.103 sec/batch)
2017-06-02 05:24:00.266064: step 112510, loss = 0.68 (1787.0 examples/sec; 0.072 sec/batch)
2017-06-02 05:24:01.134293: step 112520, loss = 0.69 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:24:01.981626: step 112530, loss = 0.64 (1510.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:24:02.863903: step 112540, loss = 0.66 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:24:03.711017: step 112550, loss = 0.68 (1511.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:24:04.573952: step 112560, loss = 0.66 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:24:05.432798: step 112570, loss = 0.81 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:24:06.280638: step 112580, loss = 0.76 (1509.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:24:07.147599: step 112590, loss = 0.76 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:24:08.101928: step 112600, loss = 0.65 (1341.2 examples/sec; 0.095 sec/batch)
2017-06-02 05:24:08.886111: step 112610, loss = 0.70 (1632.3 examples/sec; 0.078 sec/batch)
2017-06-02 05:24:09.762120: step 112620, loss = 0.62 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:24:10.625076: step 112630, loss = 0.63 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:24:11.487266: step 112640, loss = 0.70 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:24:12.358147: step 112650, loss = 0.72 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:24:13.224578: step 112660, loss = 0.81 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:24:14.076650: step 112670, loss = 0.76 (1502.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:24:14.961755: step 112680, loss = 0.62 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:24:15.832309: step 112690, loss = 0.64 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:24:16.809263: step 112700, loss = 0.62 (1310.2 examples/sec; 0.098 sec/batch)
2017-06-02 05:24:17.590524: step 112710, loss = 0.73 (1638.4 examples/sec; 0.078 sec/batch)
2017-06-02 05:24:18.460500: step 112720, loss = 0.65 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:24:19.329674: step 112730, loss = 0.59 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:24:20.195234: step 112740, loss = 0.62 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:24:21.056531: step 112750, loss = 0.61 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:24:21.925555: step 112760, loss = 0.74 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:24:22.775973: step 112770, loss = 0.68 (1505.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:24:23.625910: step 112780, loss = 0.56 (1506.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:24:24.512156: step 112790, loss = 0.64 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:24:25.478606: step 112800, loss = 0.68 (1324.4 examples/sec; 0.097 sec/batch)
2017-06-02 05:24:26.268595: step 112810, loss = 0.69 (1620.3 examples/sec; 0.079 sec/batch)
2017-06-02 05:24:27.108249: step 112820, loss = 0.71 (1524.4 examples/sec; 0.084 sec/batch)
2017-06-02 05:24:27.993645: step 112830, loss = 0.53 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:24:28.858175: step 112840, loss = 0.73 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:24:29.712072: step 112850, loss = 0.82 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:24:30.590018: step 112860, loss = 0.68 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:24:31.444123: step 112870, loss = 0.70 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:24:32.306957: step 112880, loss = 0.56 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:24:33.167991: step 112890, loss = 0.68 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:24:34.124785: step 112900, loss = 0.88 (1337.8 examples/sec; 0.096 sec/batch)
2017-06-02 05:24:34.905977: step 112910, loss = 0.81 (1638.5 examples/sec; 0.078 sec/batch)
2017-06-02 05:24:35.784752: step 112920, loss = 0.67 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:24:36.662479: step 112930, loss = 0.55 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:24:37.525034: step 112940, loss = 0.67 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:24:38.387108: step 112950, loss = 0.69 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:24:39.253801: step 112960, loss = 0.74 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:24:40.123168: step 112970, loss = 0.76 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:24:40.998403: step 112980, loss = 0.62 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:24:41.847584: step 112990, loss = 0.72 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:24:42.786558: step 113000, loss = 0.78 (1363.2 examples/sec; 0.094 sec/batch)
2017-06-02 05:24:43.551763: step 113010, loss = 0.92 (1672.8 examples/sec; 0.077 sec/batch)
2017-06-02 05:24:44.409417: step 113020, loss = 0.75 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:24:45.253066: step 113030, loss = 0.67 (1517.2 examples/sec; 0.084 sec/batch)
2017-06-02 05:24:46.106451: step 113040, loss = 0.72 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:24:46.950431: step 113050, loss = 0.51 (1516.6 examples/sec; 0.084 sec/batch)
2017-06-02 05:24:47.787642: step 113060, loss = 0.67 (1528.9 examples/sec; 0.084 sec/batch)
2017-06-02 05:24:48.636243: step 113070, loss = 0.72 (1508.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:24:49.516520: step 113080, loss = 0.82 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:24:50.370291: step 113090, loss = 0.55 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:24:51.348981: step 113100, loss = 0.67 (1307.8 examples/sec; 0.098 sec/batch)
2017-06-02 05:24:52.116997: step 113110, loss = 0.77 (1666.7 examples/sec; 0.077 sec/batch)
2017-06-02 05:24:53.009199: step 113120, loss = 0.67 (1434.6 examples/sec; 0.089 sec/batch)
2017-06-02 05:24:53.850711: step 113130, loss = 0.72 (1521.1 examples/sec; 0.084 sec/batch)
2017-06-02 05:24:54.705941: step 113140, loss = 0.56 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:24:55.561923: step 113150, loss = 0.63 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:24:56.418937: step 113160, loss = 0.85 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:24:57.270476: step 113170, loss = 0.83 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:24:58.118835: step 113180, loss = 0.54 (1508.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:24:58.995138: step 113190, loss = 0.74 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:24:59.953721: step 113200, loss = 0.67 (1335.3 examples/sec; 0.096 sec/batch)
2017-06-02 05:25:00.715603: step 113210, loss = 0.67 (1680.1 examples/sec; 0.076 sec/batch)
2017-06-02 05:25:01.591140: step 113220, loss = 0.72 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:25:02.466296: step 113230, loss = 0.83 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:25:03.361714: step 113240, loss = 0.68 (1429.5 examples/sec; 0.090 sec/batch)
2017-06-02 05:25:04.244385: step 113250, loss = 0.74 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:25:05.147671: step 113260, loss = 0.82 (1417.0 examples/sec; 0.090 sec/batch)
2017-06-02 05:25:06.023717: step 113270, loss = 0.66 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:25:06.871378: step 113280, loss = 0.67 (1510.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:25:07.761276: step 113290, loss = 0.52 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:25:08.725661: step 113300, loss = 0.69 (1327.3 examples/sec; 0.096 sec/batch)
2017-06-02 05:25:09.493032: step 113310, loss = 0.64 (1668.1 examples/sec; 0.077 sec/batch)
2017-06-02 05:25:10.341354: step 113320, loss = 0.75 (1508.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:25:11.215566: step 113330, loss = 0.62 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:25:12.090571: step 113340, loss = 0.82 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:25:12.953267: step 113350, loss = 0.75 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:25:13.837445: step 113360, loss = 0.68 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:25:14.690444: step 113370, loss = 0.71 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:25:15.569368: step 113380, loss = 0.70 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:25:16.456441: step 113390, loss = 0.77 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 05:25:17.397043: step 113400, loss = 0.72 (1360.8 examples/sec; 0.094 sec/batch)
2017-06-02 05:25:18.177578: step 113410, loss = 0.78 (1639.9 examples/sec; 0.078 sec/batch)
2017-06-02 05:25:19.021941: step 113420, loss = 0.65 (1515.9 examples/sec; 0.084 sec/batch)
2017-06-02 05:25:19.878109: step 113430, loss = 0.85 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:25:20.755769: step 113440, loss = 0.62 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:25:21.608484: step 113450, loss = 0.71 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:25:22.476311: step 113460, loss = 0.64 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:25:23.339100: step 113470, loss = 0.70 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:25:24.214144: step 113480, loss = 0.77 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:25:25.103137: step 113490, loss = 0.74 (1439.8 examples/sec; 0.089 sec/batch)
2017-06-02 05:25:26.057272: step 113500, loss = 0.81 (1341.5 examples/sec; 0.095 sec/batch)
2017-06-02 05:25:26.856953: step 113510, loss = 0.67 (1600.6 examples/sec; 0.080 sec/batch)
2017-06-02 05:25:27.738292: step 113520, loss = 0.76 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:25:28.636765: step 113530, loss = 0.68 (1424.6 examples/sec; 0.090 sec/batch)
2017-06-02 05:25:29.479032: step 113540, loss = 0.76 (1519.7 examples/sec; 0.084 sec/batch)
2017-06-02 05:25:30.333133: step 113550, loss = 0.73 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:25:31.208352: step 113560, loss = 0.83 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:25:32.087101: step 113570, loss = 0.67 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:25:32.987253: step 113580, loss = 0.85 (1421.9 examples/sec; 0.090 sec/batch)
2017-06-02 05:25:33.863058: step 113590, loss = 0.72 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:25:34.826299: step 113600, loss = 0.64 (1328.8 examples/sec; 0.096 sec/batch)
2017-06-02 05:25:35.610889: step 113610, loss = 0.77 (1631.4 examples/sec; 0.078 sec/batch)
2017-06-02 05:25:36.490329: step 113620, loss = 0.82 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:25:37.330954: step 113630, loss = 0.73 (1523.5 examples/sec; 0.084 sec/batch)
2017-06-02 05:25:38.205753: step 113640, loss = 0.63 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:25:39.086233: step 113650, loss = 0.60 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:25:39.943344: step 113660, loss = 0.65 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:25:40.799371: step 113670, loss = 0.77 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:25:41.668704: step 113680, loss = 0.64 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:25:42.536081: step 113690, loss = 0.77 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:25:43.503002: step 113700, loss = 0.70 (1323.8 examples/sec; 0.097 sec/batch)
2017-06-02 05:25:44.277021: step 113710, loss = 0.60 (1653.7 examples/sec; 0.077 sec/batch)
2017-06-02 05:25:45.144502: step 113720, loss = 0.55 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:25:46.028547: step 113730, loss = 0.79 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:25:46.901265: step 113740, loss = 0.65 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:25:47.774808: step 113750, loss = 0.64 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:25:48.657355: step 113760, loss = 0.57 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:25:49.529307: step 113770, loss = 0.79 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:25:50.400566: step 113780, loss = 0.81 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:25:51.283597: step 113790, loss = 0.82 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:25:52.249519: step 113800, loss = 0.54 (1325.2 examples/sec; 0.097 sec/batch)
2017-06-02 05:25:53.035535: step 113810, loss = 0.81 (1628.5 examples/sec; 0.079 sec/batch)
2017-06-02 05:25:53.919650: step 113820, loss = 0.71 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:25:54.790871: step 113830, loss = 0.60 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:25:55.645965: step 113840, loss = 0.76 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:25:56.501541: step 113850, loss = 0.80 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:25:57.359325: step 113860, loss = 0.73 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:25:58.197039: step 113870, loss = 0.68 (1528.0 examples/sec; 0.084 sec/batch)
2017-06-02 05:25:59.041845: step 113880, loss = 0.78 (1515.1 examples/sec; 0.084 sec/batch)
2017-06-02 05:25:59.919917: step 113890, loss = 0.82 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:26:00.902089: step 113900, loss = 0.71 (1303.2 examples/sec; 0.098 sec/batch)
2017-06-02 05:26:01.678115: step 113910, loss = 0.67 (1649.4 examples/sec; 0.078 sec/batch)
2017-06-02 05:26:02.533050: step 113920, loss = 0.64 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:26:03.408725: step 113930, loss = 0.79 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:26:04.315387: step 113940, loss = 0.64 (1411.8 examples/sec; 0.091 sec/batch)
2017-06-02 05:26:05.193498: step 113950, loss = 0.65 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:26:06.049527: step 113960, loss = 0.72 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:26:06.917919: step 113970, loss = 0.73 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:26:07.803503: step 113980, loss = 0.64 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:26:08.676876: step 113990, loss = 0.70 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:26:09.630727: step 114000, loss = 0.65 (1341.9 examples/sec; 0.095 sec/batch)
2017-06-02 05:26:10.397073: step 114010, loss = 0.57 (1670.3 examples/sec; 0.077 sec/batch)
2017-06-02 05:26:11.277274: step 114020, loss = 0.64 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:26:12.163897: step 114030, loss = 0.75 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:26:13.027982: step 114040, loss = 0.64 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:26:13.888239: step 114050, loss = 0.63 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:26:14.756418: step 114060, loss = 0.64 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:26:15.627628: step 114070, loss = 0.69 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:26:16.504782: step 114080, loss = 0.71 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:26:17.387805: step 114090, loss = 0.72 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:26:18.395901: step 114100, loss = 0.68 (1269.7 examples/sec; 0.101 sec/batch)
2017-06-02 05:26:19.114310: step 114110, loss = 0.71 (1781.7 examples/sec; 0.072 sec/batch)
2017-06-02 05:26:19.957522: step 114120, loss = 0.62 (1518.0 examples/sec; 0.084 sec/batch)
2017-06-02 05:26:20.806291: step 114130, loss = 0.65 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:26:21.657765: step 114140, loss = 0.66 (1503.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:26:22.512724: step 114150, loss = 0.74 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:26:23.382809: step 114160, loss = 0.74 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:26:24.263123: step 114170, loss = 0.65 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:26:25.105384: step 114180, loss = 0.67 (1519.7 examples/sec; 0.084 sec/batch)
2017-06-02 05:26:25.966448: step 114190, loss = 0.81 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:26:26.927569: step 114200, loss = 0.72 (1331.8 examples/sec; 0.096 sec/batch)
2017-06-02 05:26:27.703860: step 114210, loss = 0.62 (1648.9 examples/sec; 0.078 sec/batch)
2017-06-02 05:26:28.555103: step 114220, loss = 0.60 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:26:29.406704: step 114230, loss = 0.66 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:26:30.260278: step 114240, loss = 0.74 (1499.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:26:31.124637: step 114250, loss = 0.60 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:26:31.980088: step 114260, loss = 0.71 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:26:32.836490: step 114270, loss = 0.62 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:26:33.715436: step 114280, loss = 0.65 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:26:34.583819: step 114290, loss = 0.75 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:26:35.572563: step 114300, loss = 0.58 (1294.6 examples/sec; 0.099 sec/batch)
2017-06-02 05:26:36.339389: step 114310, loss = 0.85 (1669.2 examples/sec; 0.077 sec/batch)
2017-06-02 05:26:37.212555: step 114320, loss = 0.81 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:26:38.090106: step 114330, loss = 0.83 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:26:38.965223: step 114340, loss = 0.69 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:26:39.824512: step 114350, loss = 0.66 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:26:40.681214: step 114360, loss = 0.76 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:26:41.538356: step 114370, loss = 0.64 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:26:42.393844: step 114380, loss = 0.74 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:26:43.278870: step 114390, loss = 0.61 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:26:44.238137: step 114400, loss = 0.72 (1334.4 examples/sec; 0.096 sec/batch)
2017-06-02 05:26:44.993881: step 114410, loss = 0.72 (1693.7 examples/sec; 0.076 sec/batch)
2017-06-02 05:26:45.844186: step 114420, loss = 0.82 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:26:46.715378: step 114430, loss = 0.85 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:26:47.565853: step 114440, loss = 0.64 (1505.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:26:48.445597: step 114450, loss = 0.72 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:26:49.345094: step 114460, loss = 0.67 (1423.0 examples/sec; 0.090 sec/batch)
2017-06-02 05:26:50.194696: step 114470, loss = 0.89 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:26:51.077570: step 114480, loss = 0.76 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:26:51.933110: step 114490, loss = 0.80 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:26:52.906684: step 114500, loss = 0.68 (1314.7 examples/sec; 0.097 sec/batch)
2017-06-02 05:26:53.667445: step 114510, loss = 0.78 (1682.6 examples/sec; 0.076 sec/batch)
2017-06-02 05:26:54.546428: step 114520, loss = 0.69 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:26:55.418555: step 114530, loss = 0.78 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:26:56.270776: step 114540, loss = 0.64 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:26:57.156980: step 114550, loss = 0.69 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:26:58.035545: step 114560, loss = 0.71 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:26:58.921088: step 114570, loss = 0.63 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:26:59.747658: step 114580, loss = 0.64 (1548.6 examples/sec; 0.083 sec/batch)
2017-06-02 05:27:00.631709: step 114590, loss = 0.72 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:27:01.626664: step 114600, loss = 0.56 (1286.5 examples/sec; 0.099 sec/batch)
2017-06-02 05:27:02.387048: step 114610, loss = 0.68 (1683.4 examples/sec; 0.076 sec/batch)
2017-06-02 05:27:03.257262: step 114620, loss = 0.71 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:27:04.122142: step 114630, loss = 0.79 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:27:04.996937: step 114640, loss = 0.82 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:27:05.871943: step 114650, loss = 0.61 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:27:06.765681: step 114660, loss = 0.69 (1432.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:27:07.637217: step 114670, loss = 0.61 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:27:08.524745: step 114680, loss = 0.72 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:27:09.384760: step 114690, loss = 0.82 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:27:10.376012: step 114700, loss = 0.58 (1291.3 examples/sec; 0.099 sec/batch)
2017-06-02 05:27:11.106779: step 114710, loss = 0.72 (1751.6 examples/sec; 0.073 sec/batch)
2017-06-02 05:27:11.951521: step 114720, loss = 0.62 (1515.3 examples/sec; 0.084 sec/batch)
2017-06-02 05:27:12.799307: step 114730, loss = 0.74 (1509.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:27:13.660936: step 114740, loss = 0.68 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:27:14.531546: step 114750, loss = 0.63 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:27:15.389903: step 114760, loss = 0.53 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:27:16.273050: step 114770, loss = 0.86 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:27:17.138662: step 114780, loss = 0.54 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:27:18.009131: step 114790, loss = 0.80 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:27:18.970377: step 114800, loss = 0.54 (1331.6 examples/sec; 0.096 sec/batch)
2017-06-02 05:27:19.756894: step 114810, loss = 0.84 (1627.4 examples/sec; 0.079 sec/batch)
2017-06-02 05:27:20.632475: step 114820, loss = 0.78 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:27:21.534570: step 114830, loss = 0.80 (1418.9 examples/sec; 0.090 sec/batch)
2017-06-02 05:27:22.405232: step 114840, loss = 0.57 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:27:23.285235: step 114850, loss = 0.64 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:27:24.164954: step 114860, loss = 0.82 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:27:25.044682: step 114870, loss = 0.76 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:27:25.910987: step 114880, loss = 0.67 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:27:26.759458: step 114890, loss = 0.61 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:27:27.723455: step 114900, loss = 0.67 (1327.8 examples/sec; 0.096 sec/batch)
2017-06-02 05:27:28.537486: step 114910, loss = 0.68 (1572.4 examples/sec; 0.081 sec/batch)
2017-06-02 05:27:29.415644: step 114920, loss = 0.81 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:27:30.314924: step 114930, loss = 0.81 (1423.2 examples/sec; 0.090 sec/batch)
2017-06-02 05:27:31.190047: step 114940, loss = 0.64 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:27:32.050044: step 114950, loss = 0.71 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:27:32.940971: step 114960, loss = 0.69 (1436.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:27:33.832486: step 114970, loss = 0.79 (1435.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:27:34.706112: step 114980, loss = 0.67 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:27:35.577134: step 114990, loss = 0.61 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:27:36.552376: step 115000, loss = 0.71 (1312.5 examples/sec; 0.098 sec/batch)
2017-06-02 05:27:37.332366: step 115010, loss = 0.84 (1641.1 examples/sec; 0.078 sec/batch)
2017-06-02 05:27:38.214062: step 115020, loss = 0.66 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:27:39.105382: step 115030, loss = 0.67 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 05:27:39.990227: step 115040, loss = 0.69 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:27:40.858599: step 115050, loss = 0.67 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:27:41.713310: step 115060, loss = 0.80 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:27:42.573133: step 115070, loss = 0.67 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:27:43.451857: step 115080, loss = 0.71 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:27:44.315620: step 115090, loss = 0.66 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:27:45.301962: step 115100, loss = 0.54 (1297.7 examples/sec; 0.099 sec/batch)
2017-06-02 05:27:46.083201: step 115110, loss = 0.65 (1638.4 examples/sec; 0.078 sec/batch)
2017-06-02 05:27:46.934986: step 115120, loss = 0.80 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:27:47.797298: step 115130, loss = 0.76 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:27:48.668781: step 115140, loss = 0.70 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:27:49.520900: step 115150, loss = 0.88 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:27:50.375127: step 115160, loss = 0.58 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:27:51.232026: step 115170, loss = 0.61 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:27:52.091349: step 115180, loss = 0.67 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:27:52.961113: step 115190, loss = 0.77 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:27:53.933335: step 115200, loss = 0.66 (1316.5 examples/sec; 0.097 sec/batch)
2017-06-02 05:27:54.631522: step 115210, loss = 0.76 (1833.3 examples/sec; 0.070 sec/batch)
2017-06-02 05:27:55.488021: step 115220, loss = 0.68 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:27:56.360457: step 115230, loss = 0.69 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:27:57.225878: step 115240, loss = 0.68 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:27:58.090572: step 115250, loss = 0.67 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:27:58.966315: step 115260, loss = 0.65 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:27:59.849623: step 115270, loss = 0.64 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:28:00.699539: step 115280, loss = 0.59 (1506.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:28:01.553631: step 115290, loss = 0.70 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:28:02.518320: step 115300, loss = 0.75 (1326.8 examples/sec; 0.096 sec/batch)
2017-06-02 05:28:03.294660: step 115310, loss = 0.63 (1648.8 examples/sec; 0.078 sec/batch)
2017-06-02 05:28:04.164158: step 115320, loss = 0.71 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:28:05.021424: step 115330, loss = 0.78 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:28:05.888213: step 115340, loss = 0.71 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:28:06.749722: step 115350, loss = 0.62 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:28:07.613191: step 115360, loss = 0.73 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:28:08.497274: step 115370, loss = 0.84 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:28:09.357319: step 115380, loss = 0.71 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:28:10.219293: step 115390, loss = 0.90 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:28:11.181935: step 115400, loss = 0.77 (1329.7 examples/sec; 0.096 sec/batch)
2017-06-02 05:28:11.932215: step 115410, loss = 0.72 (1706.1 examples/sec; 0.075 sec/batch)
2017-06-02 05:28:12.801713: step 115420, loss = 0.58 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:28:13.667372: step 115430, loss = 0.65 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:28:14.569533: step 115440, loss = 0.64 (1418.8 examples/sec; 0.090 sec/batch)
2017-06-02 05:28:15.426635: step 115450, loss = 0.65 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:28:16.305067: step 115460, loss = 0.66 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:28:17.180341: step 115470, loss = 0.71 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:28:18.062590: step 115480, loss = 0.63 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:28:18.922396: step 115490, loss = 0.74 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:28:19.872242: step 115500, loss = 0.60 (1347.6 examples/sec; 0.095 sec/batch)
2017-06-02 05:28:20.643028: step 115510, loss = 0.75 (1660.7 examples/sec; 0.077 sec/batch)
2017-06-02 05:28:21.504254: step 115520, loss = 0.82 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:28:22.383545: step 115530, loss = 0.65 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:28:23.264858: step 115540, loss = 0.70 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:28:24.146364: step 115550, loss = 0.58 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:28:25.036940: step 115560, loss = 0.69 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:28:25.906948: step 115570, loss = 0.69 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:28:26.787919: step 115580, loss = 0.65 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:28:27.671640: step 115590, loss = 0.65 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:28:28.646602: step 115600, loss = 0.62 (1312.9 examples/sec; 0.097 sec/batch)
2017-06-02 05:28:29.435159: step 115610, loss = 0.68 (1623.2 examples/sec; 0.079 sec/batch)
2017-06-02 05:28:30.280395: step 115620, loss = 0.57 (1514.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:28:31.167864: step 115630, loss = 0.72 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:28:32.039201: step 115640, loss = 0.80 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:28:32.896955: step 115650, loss = 0.54 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:28:33.791686: step 115660, loss = 0.58 (1430.6 examples/sec; 0.089 sec/batch)
2017-06-02 05:28:34.660026: step 115670, loss = 0.54 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:28:35.520973: step 115680, loss = 0.63 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:28:36.395648: step 115690, loss = 0.66 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:28:37.434332: step 115700, loss = 0.75 (1232.3 examples/sec; 0.104 sec/batch)
2017-06-02 05:28:38.147108: step 115710, loss = 0.79 (1795.8 examples/sec; 0.071 sec/batch)
2017-06-02 05:28:39.003634: step 115720, loss = 0.86 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:28:39.873513: step 115730, loss = 0.63 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:28:40.698844: step 115740, loss = 0.68 (1550.9 examples/sec; 0.083 sec/batch)
2017-06-02 05:28:41.561798: step 115750, loss = 0.63 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:28:42.435578: step 115760, loss = 0.72 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:28:43.311096: step 115770, loss = 0.66 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:28:44.178167: step 115780, loss = 0.72 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:28:45.046506: step 115790, loss = 0.56 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:28:46.006660: step 115800, loss = 0.66 (1333.1 examples/sec; 0.096 sec/batch)
2017-06-02 05:28:46.740268: step 115810, loss = 0.68 (1744.8 examples/sec; 0.073 sec/batch)
2017-06-02 05:28:47.617579: step 115820, loss = 0.59 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:28:48.501452: step 115830, loss = 0.65 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:28:49.368173: step 115840, loss = 0.76 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:28:50.218683: step 115850, loss = 0.80 (1505.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:28:51.078070: step 115860, loss = 0.55 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:28:51.943239: step 115870, loss = 0.76 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:28:52.806595: step 115880, loss = 0.78 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:28:53.686839: step 115890, loss = 0.71 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:28:54.605131: step 115900, loss = 0.64 (1393.9 examples/sec; 0.092 sec/batch)
2017-06-02 05:28:55.381199: step 115910, loss = 0.60 (1649.3 examples/sec; 0.078 sec/batch)
2017-06-02 05:28:56.249432: step 115920, loss = 0.64 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:28:57.116379: step 115930, loss = 0.58 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:28:57.986204: step 115940, loss = 0.64 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:28:58.838956: step 115950, loss = 0.69 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:28:59.691445: step 115960, loss = 0.58 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:29:00.557997: step 115970, loss = 0.71 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:29:01.431804: step 115980, loss = 0.64 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:29:02.299866: step 115990, loss = 0.76 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:29:03.268650: step 116000, loss = 0.64 (1321.2 examples/sec; 0.097 sec/batch)
2017-06-02 05:29:04.049190: step 116010, loss = 0.72 (1639.9 examples/sec; 0.078 sec/batch)
2017-06-02 05:29:04.919784: step 116020, loss = 0.67 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:29:05.795823: step 116030, loss = 0.79 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:29:06.679796: step 116040, loss = 0.74 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:29:07.573288: step 116050, loss = 0.61 (1432.6 examples/sec; 0.089 sec/batch)
2017-06-02 05:29:08.440278: step 116060, loss = 0.68 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:29:09.301033: step 116070, loss = 0.70 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:29:10.159944: step 116080, loss = 0.68 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:29:11.030847: step 116090, loss = 0.58 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:29:11.995978: step 116100, loss = 0.65 (1326.2 examples/sec; 0.097 sec/batch)
2017-06-02 05:29:12.768790: step 116110, loss = 0.70 (1656.3 examples/sec; 0.077 sec/batch)
2017-06-02 05:29:13.654028: step 116120, loss = 0.70 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 05:29:14.531158: step 116130, loss = 0.58 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:29:15.400188: step 116140, loss = 0.66 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:29:16.283202: step 116150, loss = 0.62 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:29:17.150335: step 116160, loss = 0.57 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:29:18.020689: step 116170, loss = 0.62 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:29:18.898042: step 116180, loss = 0.74 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:29:19.757696: step 116190, loss = 0.67 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:29:20.755563: step 116200, loss = 0.65 (1282.7 examples/sec; 0.100 sec/batch)
2017-06-02 05:29:21.456083: step 116210, loss = 0.75 (1827.2 examples/sec; 0.070 sec/batch)
2017-06-02 05:29:22.301895: step 116220, loss = 0.66 (1513.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:29:23.126658: step 116230, loss = 0.62 (1551.9 examples/sec; 0.082 sec/batch)
2017-06-02 05:29:23.995656: step 116240, loss = 0.61 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:29:24.848293: step 116250, loss = 0.55 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:29:25.725764: step 116260, loss = 0.70 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:29:26.594122: step 116270, loss = 0.83 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:29:27.460991: step 116280, loss = 0.57 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:29:28.315565: step 116290, loss = 0.80 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:29:29.274523: step 116300, loss = 0.68 (1334.8 examples/sec; 0.096 sec/batch)
2017-06-02 05:29:30.031871: step 116310, loss = 0.72 (1690.1 examples/sec; 0.076 sec/batch)
2017-06-02 05:29:30.908081: step 116320, loss = 0.65 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:29:31.746001: step 116330, loss = 0.74 (1527.6 examples/sec; 0.084 sec/batch)
2017-06-02 05:29:32.597919: step 116340, loss = 0.74 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:29:33.455316: step 116350, loss = 0.69 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:29:34.297338: step 116360, loss = 0.81 (1520.2 examples/sec; 0.084 sec/batch)
2017-06-02 05:29:35.147873: step 116370, loss = 0.61 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:29:36.012445: step 116380, loss = 0.63 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:29:36.867956: step 116390, loss = 0.70 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:29:37.794967: step 116400, loss = 0.64 (1380.8 examples/sec; 0.093 sec/batch)
2017-06-02 05:29:38.557395: step 116410, loss = 0.56 (1678.8 examples/sec; 0.076 sec/batch)
2017-06-02 05:29:39.409759: step 116420, loss = 0.71 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:29:40.269790: step 116430, loss = 0.51 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:29:41.116664: step 116440, loss = 0.67 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:29:41.970770: step 116450, loss = 0.72 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:29:42.830996: step 116460, loss = 0.64 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:29:43.695914: step 116470, loss = 0.70 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:29:44.568963: step 116480, loss = 0.62 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:29:45.438444: step 116490, loss = 0.74 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:29:46.411078: step 116500, loss = 0.77 (1316.0 examples/sec; 0.097 sec/batch)
2017-06-02 05:29:47.195319: step 116510, loss = 0.84 (1632.2 examples/sec; 0.078 sec/batch)
2017-06-02 05:29:48.038381: step 116520, loss = 0.80 (1518.3 examples/sec; 0.084 sec/batch)
2017-06-02 05:29:48.902579: step 116530, loss = 0.87 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:29:49.771424: step 116540, loss = 0.64 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:29:50.643936: step 116550, loss = 0.59 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:29:51.628144: step 116560, loss = 0.61 (1300.5 examples/sec; 0.098 sec/batch)
2017-06-02 05:29:52.481247: step 116570, loss = 0.61 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:29:53.330112: step 116580, loss = 0.69 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:29:54.203982: step 116590, loss = 0.69 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:29:55.143884: step 116600, loss = 0.87 (1361.8 examples/sec; 0.094 sec/batch)
2017-06-02 05:29:55.905249: step 116610, loss = 0.57 (1681.2 examples/sec; 0.076 sec/batch)
2017-06-02 05:29:56.772221: step 116620, loss = 0.74 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:29:57.634489: step 116630, loss = 0.81 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:29:58.505495: step 116640, loss = 0.64 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:29:59.379423: step 116650, loss = 0.62 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:30:00.231879: step 116660, loss = 0.64 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:30:01.096517: step 116670, loss = 0.67 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:30:01.949968: step 116680, loss = 0.67 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:30:02.822114: step 116690, loss = 0.76 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:30:03.791390: step 116700, loss = 0.52 (1320.6 examples/sec; 0.097 sec/batch)
2017-06-02 05:30:04.564494: step 116710, loss = 0.82 (1655.7 examples/sec; 0.077 sec/batch)
2017-06-02 05:30:05.442083: step 116720, loss = 0.64 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:30:06.307740: step 116730, loss = 0.63 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:30:07.176702: step 116740, loss = 0.74 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:30:08.049799: step 116750, loss = 0.77 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:30:08.916611: step 116760, loss = 0.66 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:30:09.793059: step 116770, loss = 0.66 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:30:10.652518: step 116780, loss = 0.66 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:30:11.514225: step 116790, loss = 0.72 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:30:12.491676: step 116800, loss = 0.70 (1309.5 examples/sec; 0.098 sec/batch)
2017-06-02 05:30:13.249929: step 116810, loss = 0.79 (1688.1 examples/sec; 0.076 sec/batch)
2017-06-02 05:30:14.114151: step 116820, loss = 0.84 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:30:15.004850: step 116830, loss = 0.71 (1437.1 examples/sec; 0.089 sec/batch)
2017-06-02 05:30:15.898538: step 116840, loss = 0.67 (1432.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:30:16.758826: step 116850, loss = 0.75 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:30:17.640713: step 116860, loss = 0.60 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:30:18.496703: step 116870, loss = 0.61 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:30:19.366677: step 116880, loss = 0.73 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:30:20.219800: step 116890, loss = 0.82 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:30:21.169138: step 116900, loss = 0.83 (1348.3 examples/sec; 0.095 sec/batch)
2017-06-02 05:30:21.925394: step 116910, loss = 0.60 (1692.6 examples/sec; 0.076 sec/batch)
2017-06-02 05:30:22.787234: step 116920, loss = 0.68 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:30:23.666525: step 116930, loss = 0.77 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:30:24.543517: step 116940, loss = 0.69 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:30:25.404784: step 116950, loss = 0.62 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:30:26.271324: step 116960, loss = 0.75 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:30:27.135436: step 116970, loss = 0.73 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:30:27.989890: step 116980, loss = 0.66 (1498.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:30:28.849458: step 116990, loss = 0.80 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:30:29.813042: step 117000, loss = 0.83 (1328.4 examples/sec; 0.096 sec/batch)
2017-06-02 05:30:30.561892: step 117010, loss = 0.70 (1709.3 examples/sec; 0.075 sec/batch)
2017-06-02 05:30:31.405178: step 117020, loss = 0.62 (1517.9 examples/sec; 0.084 sec/batch)
2017-06-02 05:30:32.263198: step 117030, loss = 0.69 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:30:33.103958: step 117040, loss = 0.79 (1522.5 examples/sec; 0.084 sec/batch)
2017-06-02 05:30:33.957647: step 117050, loss = 0.78 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:30:34.824119: step 117060, loss = 0.73 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:30:35.687442: step 117070, loss = 0.72 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:30:36.547117: step 117080, loss = 0.76 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:30:37.419078: step 117090, loss = 0.70 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:30:38.379717: step 117100, loss = 0.73 (1332.5 examples/sec; 0.096 sec/batch)
2017-06-02 05:30:39.141943: step 117110, loss = 0.70 (1679.3 examples/sec; 0.076 sec/batch)
2017-06-02 05:30:40.004971: step 117120, loss = 0.66 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:30:40.887552: step 117130, loss = 0.75 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:30:41.781307: step 117140, loss = 0.81 (1432.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:30:42.637875: step 117150, loss = 0.62 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:30:43.538002: step 117160, loss = 0.60 (1422.0 examples/sec; 0.090 sec/batch)
2017-06-02 05:30:44.398202: step 117170, loss = 0.71 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:30:45.247957: step 117180, loss = 0.86 (1506.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:30:46.115246: step 117190, loss = 0.55 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:30:47.089103: step 117200, loss = 0.67 (1314.4 examples/sec; 0.097 sec/batch)
2017-06-02 05:30:47.879775: step 117210, loss = 0.70 (1618.9 examples/sec; 0.079 sec/batch)
2017-06-02 05:30:48.762547: step 117220, loss = 0.70 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:30:49.620219: step 117230, loss = 0.58 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:30:50.454941: step 117240, loss = 0.60 (1533.4 examples/sec; 0.083 sec/batch)
2017-06-02 05:30:51.320569: step 117250, loss = 0.61 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:30:52.180727: step 117260, loss = 0.75 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:30:53.039116: step 117270, loss = 0.56 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:30:53.888018: step 117280, loss = 0.66 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:30:54.756117: step 117290, loss = 0.67 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:30:55.711493: step 117300, loss = 0.68 (1339.8 examples/sec; 0.096 sec/batch)
2017-06-02 05:30:56.493696: step 117310, loss = 0.54 (1636.4 examples/sec; 0.078 sec/batch)
2017-06-02 05:30:57.364062: step 117320, loss = 0.70 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:30:58.238001: step 117330, loss = 0.72 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:30:59.094521: step 117340, loss = 0.81 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:30:59.976427: step 117350, loss = 0.76 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:31:00.830804: step 117360, loss = 0.65 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:31:01.701960: step 117370, loss = 0.61 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:31:02.573366: step 117380, loss = 0.61 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:31:03.433944: step 117390, loss = 0.76 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:31:04.409873: step 117400, loss = 0.69 (1311.6 examples/sec; 0.098 sec/batch)
2017-06-02 05:31:05.211237: step 117410, loss = 0.71 (1597.3 examples/sec; 0.080 sec/batch)
2017-06-02 05:31:06.061525: step 117420, loss = 0.80 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:31:06.917610: step 117430, loss = 0.79 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:31:07.786778: step 117440, loss = 0.80 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:31:08.658637: step 117450, loss = 0.61 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:31:09.508296: step 117460, loss = 0.71 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:31:10.372439: step 117470, loss = 0.70 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:31:11.250097: step 117480, loss = 0.74 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:31:12.114964: step 117490, loss = 0.68 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:31:13.084799: step 117500, loss = 0.72 (1319.8 examples/sec; 0.097 sec/batch)
2017-06-02 05:31:13.863580: step 117510, loss = 0.77 (1643.6 examples/sec; 0.078 sec/batch)
2017-06-02 05:31:14.726341: step 117520, loss = 0.60 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:31:15.596142: step 117530, loss = 0.60 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:31:16.479184: step 117540, loss = 0.70 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:31:17.338123: step 117550, loss = 0.72 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:31:18.211193: step 117560, loss = 0.62 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:31:19.085474: step 117570, loss = 0.67 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:31:19.952337: step 117580, loss = 0.72 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:31:20.811782: step 117590, loss = 0.64 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:31:21.766168: step 117600, loss = 0.73 (1341.2 examples/sec; 0.095 sec/batch)
2017-06-02 05:31:22.504694: step 117610, loss = 0.79 (1733.2 examples/sec; 0.074 sec/batch)
2017-06-02 05:31:23.371269: step 117620, loss = 0.73 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:31:24.233746: step 117630, loss = 0.62 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:31:25.100752: step 117640, loss = 0.70 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:31:25.943280: step 117650, loss = 0.62 (1519.2 examples/sec; 0.084 sec/batch)
2017-06-02 05:31:26.820823: step 117660, loss = 0.63 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:31:27.683672: step 117670, loss = 0.73 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:31:28.527916: step 117680, loss = 0.60 (1516.2 examples/sec; 0.084 sec/batch)
2017-06-02 05:31:29.375450: step 117690, loss = 0.76 (1510.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:31:30.302368: step 117700, loss = 0.64 (1380.9 examples/sec; 0.093 sec/batch)
2017-06-02 05:31:31.054636: step 117710, loss = 0.74 (1701.5 examples/sec; 0.075 sec/batch)
2017-06-02 05:31:31.922983: step 117720, loss = 0.78 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:31:32.790414: step 117730, loss = 0.76 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:31:33.653589: step 117740, loss = 0.71 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:31:34.509938: step 117750, loss = 0.56 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:31:35.354650: step 117760, loss = 0.63 (1515.3 examples/sec; 0.084 sec/batch)
2017-06-02 05:31:36.208316: step 117770, loss = 0.71 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:31:37.060034: step 117780, loss = 0.68 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:31:37.896771: step 117790, loss = 0.72 (1529.8 examples/sec; 0.084 sec/batch)
2017-06-02 05:31:38.836403: step 117800, loss = 0.71 (1362.2 examples/sec; 0.094 sec/batch)
2017-06-02 05:31:39.595848: step 117810, loss = 0.63 (1685.5 examples/sec; 0.076 sec/batch)
2017-06-02 05:31:40.477363: step 117820, loss = 0.73 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:31:41.349893: step 117830, loss = 0.65 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:31:42.194137: step 117840, loss = 0.64 (1516.2 examples/sec; 0.084 sec/batch)
2017-06-02 05:31:43.046869: step 117850, loss = 0.78 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:31:43.903973: step 117860, loss = 0.68 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:31:44.779059: step 117870, loss = 0.69 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:31:45.654610: step 117880, loss = 0.69 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:31:46.505763: step 117890, loss = 0.69 (1503.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:31:47.447585: step 117900, loss = 0.62 (1359.1 examples/sec; 0.094 sec/batch)
2017-06-02 05:31:48.233692: step 117910, loss = 0.59 (1628.3 examples/sec; 0.079 sec/batch)
2017-06-02 05:31:49.089663: step 117920, loss = 0.57 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:31:49.961713: step 117930, loss = 0.65 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:31:50.792785: step 117940, loss = 0.59 (1540.2 examples/sec; 0.083 sec/batch)
2017-06-02 05:31:51.654953: step 117950, loss = 0.72 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:31:52.526383: step 117960, loss = 0.57 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:31:53.386544: step 117970, loss = 0.70 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:31:54.264617: step 117980, loss = 0.70 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:31:55.121885: step 117990, loss = 0.73 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:31:56.099979: step 118000, loss = 0.65 (1308.7 examples/sec; 0.098 sec/batch)
2017-06-02 05:31:56.905995: step 118010, loss = 0.75 (1588.1 examples/sec; 0.081 sec/batch)
2017-06-02 05:31:57.794786: step 118020, loss = 0.63 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:31:58.685334: step 118030, loss = 0.64 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:31:59.548959: step 118040, loss = 0.67 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:32:00.425263: step 118050, loss = 0.62 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:32:01.299543: step 118060, loss = 0.66 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:32:02.175350: step 118070, loss = 0.74 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:32:03.052112: step 118080, loss = 0.70 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:32:03.932196: step 118090, loss = 0.62 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:32:04.914197: step 118100, loss = 0.56 (1303.5 examples/sec; 0.098 sec/batch)
2017-06-02 05:32:05.685505: step 118110, loss = 0.70 (1659.5 examples/sec; 0.077 sec/batch)
2017-06-02 05:32:06.537670: step 118120, loss = 0.69 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:32:07.388302: step 118130, loss = 0.76 (1504.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:32:08.242841: step 118140, loss = 0.70 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:32:09.104584: step 118150, loss = 0.71 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:32:09.951352: step 118160, loss = 0.69 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:32:10.835785: step 118170, loss = 0.75 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:32:11.699168: step 118180, loss = 0.68 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:32:12.582834: step 118190, loss = 0.74 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:32:13.522806: step 118200, loss = 0.72 (1361.7 examples/sec; 0.094 sec/batch)
2017-06-02 05:32:14.292879: step 118210, loss = 0.70 (1662.2 examples/sec; 0.077 sec/batch)
2017-06-02 05:32:15.172835: step 118220, loss = 0.77 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:32:16.046477: step 118230, loss = 0.53 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:32:16.890035: step 118240, loss = 0.74 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 05:32:17.761786: step 118250, loss = 0.72 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:32:18.614793: step 118260, loss = 0.62 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:32:19.481698: step 118270, loss = 0.76 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:32:20.365616: step 118280, loss = 0.83 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:32:21.217171: step 118290, loss = 0.73 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:32:22.162725: step 118300, loss = 0.75 (1353.7 examples/sec; 0.095 sec/batch)
2017-06-02 05:32:22.913576: step 118310, loss = 0.72 (1704.7 examples/sec; 0.075 sec/batch)
2017-06-02 05:32:23.751544: step 118320, loss = 0.64 (1527.5 examples/sec; 0.084 sec/batch)
2017-06-02 05:32:24.638623: step 118330, loss = 0.76 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 05:32:25.491628: step 118340, loss = 0.66 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:32:26.358659: step 118350, loss = 0.70 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:32:27.223341: step 118360, loss = 0.58 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:32:28.067690: step 118370, loss = 0.67 (1515.9 examples/sec; 0.084 sec/batch)
2017-06-02 05:32:28.920335: step 118380, loss = 0.64 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:32:29.778838: step 118390, loss = 0.62 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:32:30.713490: step 118400, loss = 0.71 (1369.5 examples/sec; 0.093 sec/batch)
2017-06-02 05:32:31.474854: step 118410, loss = 0.75 (1681.2 examples/sec; 0.076 sec/batch)
2017-06-02 05:32:32.329654: step 118420, loss = 0.66 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:32:33.176532: step 118430, loss = 0.60 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:32:34.011685: step 118440, loss = 0.63 (1532.7 examples/sec; 0.084 sec/batch)
2017-06-02 05:32:34.869896: step 118450, loss = 0.63 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:32:35.721542: step 118460, loss = 0.69 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:32:36.575914: step 118470, loss = 0.74 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:32:37.451361: step 118480, loss = 0.72 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:32:38.299329: step 118490, loss = 0.72 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:32:39.271627: step 118500, loss = 0.61 (1316.5 examples/sec; 0.097 sec/batch)
2017-06-02 05:32:40.023901: step 118510, loss = 0.51 (1701.5 examples/sec; 0.075 sec/batch)
2017-06-02 05:32:40.884298: step 118520, loss = 0.70 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:32:41.754406: step 118530, loss = 0.88 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:32:42.624474: step 118540, loss = 0.70 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:32:43.479815: step 118550, loss = 0.81 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:32:44.328227: step 118560, loss = 0.64 (1508.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:32:45.193710: step 118570, loss = 0.69 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:32:46.072390: step 118580, loss = 0.77 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:32:46.902204: step 118590, loss = 0.63 (1542.5 examples/sec; 0.083 sec/batch)
2017-06-02 05:32:47.901302: step 118600, loss = 0.66 (1281.1 examples/sec; 0.100 sec/batch)
2017-06-02 05:32:48.630722: step 118610, loss = 0.77 (1754.8 examples/sec; 0.073 sec/batch)
2017-06-02 05:32:49.485794: step 118620, loss = 0.70 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:32:50.340965: step 118630, loss = 0.67 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:32:51.225905: step 118640, loss = 0.71 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:32:52.099666: step 118650, loss = 0.65 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:32:52.973603: step 118660, loss = 0.80 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:32:53.839570: step 118670, loss = 0.62 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:32:54.711813: step 118680, loss = 0.68 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:32:55.580819: step 118690, loss = 0.59 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:32:56.538597: step 118700, loss = 0.62 (1336.4 examples/sec; 0.096 sec/batch)
2017-06-02 05:32:57.301846: step 118710, loss = 0.81 (1677.0 examples/sec; 0.076 sec/batch)
2017-06-02 05:32:58.179053: step 118720, loss = 0.79 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:32:59.061048: step 118730, loss = 0.73 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:32:59.916416: step 118740, loss = 0.55 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:33:00.759644: step 118750, loss = 0.58 (1518.0 examples/sec; 0.084 sec/batch)
2017-06-02 05:33:01.615297: step 118760, loss = 0.81 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:33:02.491439: step 118770, loss = 0.92 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:33:03.361895: step 118780, loss = 0.75 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:33:04.218081: step 118790, loss = 0.79 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:33:05.183629: step 118800, loss = 0.83 (1325.7 examples/sec; 0.097 sec/batch)
2017-06-02 05:33:05.973041: step 118810, loss = 0.69 (1621.5 examples/sec; 0.079 sec/batch)
2017-06-02 05:33:06.828030: step 118820, loss = 0.61 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:33:07.683511: step 118830, loss = 0.60 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:33:08.564709: step 118840, loss = 0.72 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:33:09.416683: step 118850, loss = 0.73 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:33:10.295500: step 118860, loss = 0.65 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:33:11.184061: step 118870, loss = 0.85 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 05:33:12.033267: step 118880, loss = 0.65 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:33:12.885372: step 118890, loss = 0.70 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:33:13.846665: step 118900, loss = 0.82 (1331.5 examples/sec; 0.096 sec/batch)
2017-06-02 05:33:14.611043: step 118910, loss = 0.70 (1674.6 examples/sec; 0.076 sec/batch)
2017-06-02 05:33:15.478472: step 118920, loss = 0.60 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:33:16.373765: step 118930, loss = 0.77 (1429.7 examples/sec; 0.090 sec/batch)
2017-06-02 05:33:17.253696: step 118940, loss = 0.60 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:33:18.094278: step 118950, loss = 0.68 (1522.8 examples/sec; 0.084 sec/batch)
2017-06-02 05:33:18.979911: step 118960, loss = 0.70 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:33:19.841363: step 118970, loss = 0.69 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:33:20.699246: step 118980, loss = 0.68 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:33:21.569223: step 118990, loss = 0.65 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:33:22.529541: step 119000, loss = 0.77 (1332.9 examples/sec; 0.096 sec/batch)
2017-06-02 05:33:23.277066: step 119010, loss = 0.72 (1712.3 examples/sec; 0.075 sec/batch)
2017-06-02 05:33:24.132195: step 119020, loss = 0.65 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:33:24.987088: step 119030, loss = 0.75 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:33:25.849071: step 119040, loss = 0.78 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:33:26.713327: step 119050, loss = 0.87 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:33:27.569861: step 119060, loss = 0.78 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:33:28.436217: step 119070, loss = 0.82 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:33:29.317249: step 119080, loss = 0.62 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:33:30.212647: step 119090, loss = 0.70 (1429.5 examples/sec; 0.090 sec/batch)
2017-06-02 05:33:31.184304: step 119100, loss = 0.76 (1317.3 examples/sec; 0.097 sec/batch)
2017-06-02 05:33:31.956632: step 119110, loss = 0.64 (1657.3 examples/sec; 0.077 sec/batch)
2017-06-02 05:33:32.845894: step 119120, loss = 0.77 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:33:33.724229: step 119130, loss = 0.64 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:33:34.591102: step 119140, loss = 0.73 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:33:35.451337: step 119150, loss = 0.76 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:33:36.322677: step 119160, loss = 0.61 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:33:37.178558: step 119170, loss = 0.67 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:33:38.026111: step 119180, loss = 0.81 (1510.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:33:38.861433: step 119190, loss = 0.72 (1532.3 examples/sec; 0.084 sec/batch)
2017-06-02 05:33:39.848445: step 119200, loss = 0.62 (1296.9 examples/sec; 0.099 sec/batch)
2017-06-02 05:33:40.642742: step 119210, loss = 0.63 (1611.5 examples/sec; 0.079 sec/batch)
2017-06-02 05:33:41.488943: step 119220, loss = 0.67 (1512.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:33:42.345469: step 119230, loss = 0.72 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:33:43.205157: step 119240, loss = 0.78 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:33:44.053825: step 119250, loss = 0.52 (1508.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:33:44.907091: step 119260, loss = 0.73 (1500.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:33:45.786385: step 119270, loss = 0.63 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:33:46.644328: step 119280, loss = 0.77 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:33:47.504605: step 119290, loss = 0.66 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:33:48.461595: step 119300, loss = 0.73 (1337.5 examples/sec; 0.096 sec/batch)
2017-06-02 05:33:49.213404: step 119310, loss = 0.88 (1702.6 examples/sec; 0.075 sec/batch)
2017-06-02 05:33:50.086852: step 119320, loss = 0.64 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:33:50.945196: step 119330, loss = 0.64 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:33:51.815652: step 119340, loss = 0.74 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:33:52.700623: step 119350, loss = 0.63 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:33:53.562226: step 119360, loss = 0.74 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:33:54.413443: step 119370, loss = 0.63 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:33:55.262242: step 119380, loss = 0.59 (1508.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:33:56.117390: step 119390, loss = 0.69 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:33:57.077237: step 119400, loss = 0.65 (1333.5 examples/sec; 0.096 sec/batch)
2017-06-02 05:33:57.864523: step 119410, loss = 0.74 (1625.8 examples/sec; 0.079 sec/batch)
2017-06-02 05:33:58.708496: step 119420, loss = 0.54 (1516.6 examples/sec; 0.084 sec/batch)
2017-06-02 05:33:59.557714: step 119430, loss = 0.64 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:34:00.424612: step 119440, loss = 0.55 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:34:01.293587: step 119450, loss = 0.87 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:34:02.164951: step 119460, loss = 0.74 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:34:03.040362: step 119470, loss = 0.63 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:34:03.861313: step 119480, loss = 0.82 (1559.2 examples/sec; 0.082 sec/batch)
2017-06-02 05:34:04.737222: step 119490, loss = 0.65 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:34:05.714443: step 119500, loss = 0.69 (1309.8 examples/sec; 0.098 sec/batch)
2017-06-02 05:34:06.465133: step 119510, loss = 0.66 (1705.1 examples/sec; 0.075 sec/batch)
2017-06-02 05:34:07.336505: step 119520, loss = 0.68 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:34:08.180913: step 119530, loss = 0.71 (1515.9 examples/sec; 0.084 sec/batch)
2017-06-02 05:34:09.019608: step 119540, loss = 0.76 (1526.2 examples/sec; 0.084 sec/batch)
2017-06-02 05:34:09.895225: step 119550, loss = 0.66 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:34:10.735100: step 119560, loss = 0.63 (1524.0 examples/sec; 0.084 sec/batch)
2017-06-02 05:34:11.597590: step 119570, loss = 0.78 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:34:12.492178: step 119580, loss = 0.89 (1430.8 examples/sec; 0.089 sec/batch)
2017-06-02 05:34:13.347859: step 119590, loss = 0.74 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:34:14.322307: step 119600, loss = 0.63 (1313.6 examples/sec; 0.097 sec/batch)
2017-06-02 05:34:15.100688: step 119610, loss = 0.67 (1644.4 examples/sec; 0.078 sec/batch)
2017-06-02 05:34:15.979008: step 119620, loss = 0.72 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:34:16.839098: step 119630, loss = 0.67 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:34:17.685013: step 119640, loss = 0.66 (1513.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:34:18.526793: step 119650, loss = 0.72 (1520.6 examples/sec; 0.084 sec/batch)
2017-06-02 05:34:19.394579: step 119660, loss = 0.64 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:34:20.243137: step 119670, loss = 0.73 (1508.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:34:21.096283: step 119680, loss = 0.73 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:34:21.942896: step 119690, loss = 0.63 (1511.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:34:22.919815: step 119700, loss = 0.58 (1310.2 examples/sec; 0.098 sec/batch)
2017-06-02 05:34:23.681210: step 119710, loss = 0.64 (1681.1 examples/sec; 0.076 sec/batch)
2017-06-02 05:34:24.571010: step 119720, loss = 0.72 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 05:34:25.414243: step 119730, loss = 0.73 (1518.0 examples/sec; 0.084 sec/batch)
2017-06-02 05:34:26.263759: step 119740, loss = 0.66 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:34:27.151302: step 119750, loss = 0.93 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:34:28.022230: step 119760, loss = 0.75 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:34:28.911170: step 119770, loss = 0.78 (1439.9 examples/sec; 0.089 sec/batch)
2017-06-02 05:34:29.808712: step 119780, loss = 0.60 (1426.1 examples/sec; 0.090 sec/batch)
2017-06-02 05:34:30.672250: step 119790, loss = 0.66 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:34:31.624758: step 119800, loss = 0.67 (1343.8 examples/sec; 0.095 sec/batch)
2017-06-02 05:34:32.398018: step 119810, loss = 0.93 (1655.3 examples/sec; 0.077 sec/batch)
2017-06-02 05:34:33.271307: step 119820, loss = 0.67 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:34:34.126144: step 119830, loss = 0.59 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:34:35.020641: step 119840, loss = 0.73 (1431.0 examples/sec; 0.089 sec/batch)
2017-06-02 05:34:35.879791: step 119850, loss = 0.67 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:34:36.735995: step 119860, loss = 0.69 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:34:37.595030: step 119870, loss = 0.65 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:34:38.473749: step 119880, loss = 0.69 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:34:39.344991: step 119890, loss = 0.61 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:34:40.312428: step 119900, loss = 0.63 (1323.1 examples/sec; 0.097 sec/batch)
2017-06-02 05:34:41.075873: step 119910, loss = 0.76 (1676.6 examples/sec; 0.076 sec/batch)
2017-06-02 05:34:41.949357: step 119920, loss = 0.77 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:34:42.831215: step 119930, loss = 0.55 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:34:43.705020: step 119940, loss = 0.80 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:34:44.554846: step 119950, loss = 0.74 (1506.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:34:45.406173: step 119960, loss = 0.67 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:34:46.269467: step 119970, loss = 0.67 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:34:47.132777: step 119980, loss = 0.73 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:34:47.998371: step 119990, loss = 0.67 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:34:48.933015: step 120000, loss = 0.78 (1369.5 examples/sec; 0.093 sec/batch)
2017-06-02 05:34:49.716518: step 120010, loss = 0.70 (1633.7 examples/sec; 0.078 sec/batch)
2017-06-02 05:34:50.564560: step 120020, loss = 0.60 (1509.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:34:51.431050: step 120030, loss = 0.67 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:34:52.302336: step 120040, loss = 0.62 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:34:53.162504: step 120050, loss = 0.67 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:34:54.025361: step 120060, loss = 0.67 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:34:54.909966: step 120070, loss = 0.61 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:34:55.782152: step 120080, loss = 0.78 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:34:56.648963: step 120090, loss = 0.75 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:34:57.636670: step 120100, loss = 0.67 (1295.9 examples/sec; 0.099 sec/batch)
2017-06-02 05:34:58.362292: step 120110, loss = 0.61 (1764.0 examples/sec; 0.073 sec/batch)
2017-06-02 05:34:59.238369: step 120120, loss = 0.67 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:35:00.119234: step 120130, loss = 0.86 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:35:00.997854: step 120140, loss = 0.69 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:35:01.855094: step 120150, loss = 0.72 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:35:02.711045: step 120160, loss = 0.79 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:35:03.560465: step 120170, loss = 0.76 (1506.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:35:04.421790: step 120180, loss = 0.84 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:35:05.275151: step 120190, loss = 0.76 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:35:06.224032: step 120200, loss = 0.64 (1349.0 examples/sec; 0.095 sec/batch)
2017-06-02 05:35:06.992905: step 120210, loss = 0.57 (1664.8 examples/sec; 0.077 sec/batch)
2017-06-02 05:35:07.852964: step 120220, loss = 0.97 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:35:08.699371: step 120230, loss = 0.67 (1512.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:35:09.572933: step 120240, loss = 0.64 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:35:10.417404: step 120250, loss = 0.53 (1515.8 examples/sec; 0.084 sec/batch)
2017-06-02 05:35:11.276331: step 120260, loss = 0.80 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:35:12.135388: step 120270, loss = 0.59 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:35:13.019003: step 120280, loss = 0.67 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:35:13.893707: step 120290, loss = 0.66 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:35:14.906887: step 120300, loss = 0.77 (1263.3 examples/sec; 0.101 sec/batch)
2017-06-02 05:35:15.630519: step 120310, loss = 0.82 (1768.9 examples/sec; 0.072 sec/batch)
2017-06-02 05:35:16.494701: step 120320, loss = 0.80 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:35:17.368692: step 120330, loss = 0.79 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:35:18.235364: step 120340, loss = 0.77 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:35:19.089140: step 120350, loss = 0.62 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:35:19.951804: step 120360, loss = 0.71 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:35:20.823994: step 120370, loss = 0.60 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:35:21.681344: step 120380, loss = 0.66 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:35:22.563156: step 120390, loss = 0.60 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:35:23.534831: step 120400, loss = 0.64 (1317.3 examples/sec; 0.097 sec/batch)
2017-06-02 05:35:24.309606: step 120410, loss = 0.67 (1652.1 examples/sec; 0.077 sec/batch)
2017-06-02 05:35:25.187970: step 120420, loss = 0.57 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:35:26.057566: step 120430, loss = 0.67 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:35:26.924670: step 120440, loss = 0.66 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:35:27.815432: step 120450, loss = 0.57 (1437.0 examples/sec; 0.089 sec/batch)
2017-06-02 05:35:28.701495: step 120460, loss = 0.67 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 05:35:29.564205: step 120470, loss = 0.80 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:35:30.408182: step 120480, loss = 0.68 (1516.6 examples/sec; 0.084 sec/batch)
2017-06-02 05:35:31.271520: step 120490, loss = 0.60 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:35:32.267026: step 120500, loss = 0.64 (1285.8 examples/sec; 0.100 sec/batch)
2017-06-02 05:35:33.026656: step 120510, loss = 0.72 (1685.0 examples/sec; 0.076 sec/batch)
2017-06-02 05:35:33.875483: step 120520, loss = 0.59 (1508.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:35:34.773561: step 120530, loss = 0.60 (1425.3 examples/sec; 0.090 sec/batch)
2017-06-02 05:35:35.629556: step 120540, loss = 0.79 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:35:36.505188: step 120550, loss = 0.51 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:35:37.380147: step 120560, loss = 0.71 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:35:38.266939: step 120570, loss = 0.83 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:35:39.123553: step 120580, loss = 0.67 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:35:39.993774: step 120590, loss = 0.70 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:35:40.942726: step 120600, loss = 0.71 (1348.8 examples/sec; 0.095 sec/batch)
2017-06-02 05:35:41.712355: step 120610, loss = 0.72 (1663.1 examples/sec; 0.077 sec/batch)
2017-06-02 05:35:42.549355: step 120620, loss = 0.63 (1529.3 examples/sec; 0.084 sec/batch)
2017-06-02 05:35:43.428772: step 120630, loss = 0.78 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:35:44.275196: step 120640, loss = 0.74 (1512.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:35:45.135267: step 120650, loss = 0.80 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:35:46.004135: step 120660, loss = 0.65 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:35:46.889379: step 120670, loss = 0.58 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 05:35:47.739226: step 120680, loss = 0.60 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:35:48.622980: step 120690, loss = 0.67 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:35:49.579155: step 120700, loss = 0.66 (1338.6 examples/sec; 0.096 sec/batch)
2017-06-02 05:35:50.349804: step 120710, loss = 0.63 (1660.9 examples/sec; 0.077 sec/batch)
2017-06-02 05:35:51.222041: step 120720, loss = 0.74 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:35:52.072021: step 120730, loss = 0.59 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:35:52.910775: step 120740, loss = 0.68 (1526.1 examples/sec; 0.084 sec/batch)
2017-06-02 05:35:53.783018: step 120750, loss = 0.76 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:35:54.653499: step 120760, loss = 0.74 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:35:55.504459: step 120770, loss = 0.63 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:35:56.382378: step 120780, loss = 0.73 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:35:57.270414: step 120790, loss = 0.62 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:35:58.218849: step 120800, loss = 0.71 (1349.6 examples/sec; 0.095 sec/batch)
2017-06-02 05:35:59.003983: step 120810, loss = 0.66 (1630.3 examples/sec; 0.079 sec/batch)
2017-06-02 05:35:59.854707: step 120820, loss = 0.64 (1504.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:36:00.726463: step 120830, loss = 0.60 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:36:01.600781: step 120840, loss = 0.85 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:36:02.479690: step 120850, loss = 0.69 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:36:03.346539: step 120860, loss = 0.67 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:36:04.210877: step 120870, loss = 0.68 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:36:05.080016: step 120880, loss = 0.77 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:36:05.969404: step 120890, loss = 0.73 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:36:06.949021: step 120900, loss = 0.78 (1306.6 examples/sec; 0.098 sec/batch)
2017-06-02 05:36:07.741270: step 120910, loss = 0.62 (1615.7 examples/sec; 0.079 sec/batch)
2017-06-02 05:36:08.602005: step 120920, loss = 0.68 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:36:09.477821: step 120930, loss = 0.74 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:36:10.349763: step 120940, loss = 0.68 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:36:11.224189: step 120950, loss = 0.79 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:36:12.071638: step 120960, loss = 0.60 (1510.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:36:12.956313: step 120970, loss = 0.70 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:36:13.784680: step 120980, loss = 0.66 (1545.2 examples/sec; 0.083 sec/batch)
2017-06-02 05:36:14.662359: step 120990, loss = 0.73 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:36:15.610687: step 121000, loss = 0.68 (1349.7 examples/sec; 0.095 sec/batch)
2017-06-02 05:36:16.383703: step 121010, loss = 0.61 (1655.9 examples/sec; 0.077 sec/batch)
2017-06-02 05:36:17.269883: step 121020, loss = 0.58 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:36:18.148372: step 121030, loss = 0.72 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:36:19.021942: step 121040, loss = 0.82 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:36:19.907716: step 121050, loss = 0.83 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 05:36:20.756768: step 121060, loss = 0.66 (1507.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:36:21.607677: step 121070, loss = 0.79 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:36:22.477394: step 121080, loss = 0.61 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:36:23.364659: step 121090, loss = 0.77 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 05:36:24.327664: step 121100, loss = 0.71 (1329.2 examples/sec; 0.096 sec/batch)
2017-06-02 05:36:25.084976: step 121110, loss = 0.68 (1690.2 examples/sec; 0.076 sec/batch)
2017-06-02 05:36:25.970934: step 121120, loss = 0.68 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:36:26.832647: step 121130, loss = 0.76 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:36:27.685976: step 121140, loss = 0.73 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:36:28.538947: step 121150, loss = 0.73 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:36:29.405831: step 121160, loss = 0.68 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:36:30.253306: step 121170, loss = 0.73 (1510.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:36:31.100715: step 121180, loss = 0.59 (1510.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:36:31.966579: step 121190, loss = 0.69 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:36:32.915408: step 121200, loss = 0.78 (1349.0 examples/sec; 0.095 sec/batch)
2017-06-02 05:36:33.690428: step 121210, loss = 0.64 (1651.6 examples/sec; 0.078 sec/batch)
2017-06-02 05:36:34.536514: step 121220, loss = 0.55 (1512.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:36:35.411498: step 121230, loss = 0.56 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:36:36.275000: step 121240, loss = 0.65 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:36:37.134636: step 121250, loss = 0.65 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:36:37.976084: step 121260, loss = 0.71 (1521.2 examples/sec; 0.084 sec/batch)
2017-06-02 05:36:38.835125: step 121270, loss = 0.75 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:36:39.708026: step 121280, loss = 0.69 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:36:40.564068: step 121290, loss = 0.63 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:36:41.532101: step 121300, loss = 0.69 (1322.3 examples/sec; 0.097 sec/batch)
2017-06-02 05:36:42.286490: step 121310, loss = 0.73 (1696.7 examples/sec; 0.075 sec/batch)
2017-06-02 05:36:43.144080: step 121320, loss = 0.65 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:36:44.022552: step 121330, loss = 0.59 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:36:44.890970: step 121340, loss = 0.58 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:36:45.756533: step 121350, loss = 0.73 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:36:46.641620: step 121360, loss = 0.65 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:36:47.513303: step 121370, loss = 0.79 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:36:48.373492: step 121380, loss = 0.79 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:36:49.215331: step 121390, loss = 0.65 (1520.5 examples/sec; 0.084 sec/batch)
2017-06-02 05:36:50.163875: step 121400, loss = 0.69 (1349.4 examples/sec; 0.095 sec/batch)
2017-06-02 05:36:50.925292: step 121410, loss = 0.86 (1681.1 examples/sec; 0.076 sec/batch)
2017-06-02 05:36:51.804192: step 121420, loss = 0.78 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:36:52.659677: step 121430, loss = 0.66 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:36:53.538288: step 121440, loss = 0.70 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:36:54.414600: step 121450, loss = 0.80 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:36:55.272490: step 121460, loss = 0.76 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:36:56.163312: step 121470, loss = 0.63 (1436.9 examples/sec; 0.089 sec/batch)
2017-06-02 05:36:57.032146: step 121480, loss = 0.72 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:36:57.880935: step 121490, loss = 0.59 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:36:58.823969: step 121500, loss = 0.74 (1357.3 examples/sec; 0.094 sec/batch)
2017-06-02 05:36:59.605738: step 121510, loss = 0.88 (1637.3 examples/sec; 0.078 sec/batch)
2017-06-02 05:37:00.454387: step 121520, loss = 0.70 (1508.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:37:01.313406: step 121530, loss = 0.59 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:37:02.173132: step 121540, loss = 0.60 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:37:03.032691: step 121550, loss = 0.79 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:37:03.910142: step 121560, loss = 0.74 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:37:04.762222: step 121570, loss = 0.76 (1502.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:37:05.627509: step 121580, loss = 0.56 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:37:06.475609: step 121590, loss = 0.62 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:37:07.438829: step 121600, loss = 0.73 (1328.9 examples/sec; 0.096 sec/batch)
2017-06-02 05:37:08.213359: step 121610, loss = 0.86 (1652.6 examples/sec; 0.077 sec/batch)
2017-06-02 05:37:09.103319: step 121620, loss = 0.79 (1438.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:37:09.963264: step 121630, loss = 0.64 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:37:10.854357: step 121640, loss = 0.58 (1436.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:37:11.720668: step 121650, loss = 0.61 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:37:12.579797: step 121660, loss = 0.66 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:37:13.448163: step 121670, loss = 0.62 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:37:14.332647: step 121680, loss = 0.62 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:37:15.184334: step 121690, loss = 0.78 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:37:16.187148: step 121700, loss = 0.66 (1276.4 examples/sec; 0.100 sec/batch)
2017-06-02 05:37:16.927290: step 121710, loss = 0.79 (1729.4 examples/sec; 0.074 sec/batch)
2017-06-02 05:37:17.800453: step 121720, loss = 0.63 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:37:18.652224: step 121730, loss = 0.83 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:37:19.524770: step 121740, loss = 0.71 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:37:20.370308: step 121750, loss = 0.75 (1513.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:37:21.225658: step 121760, loss = 0.68 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:37:22.074794: step 121770, loss = 0.66 (1507.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:37:22.929086: step 121780, loss = 0.66 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:37:23.800895: step 121790, loss = 0.77 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:37:24.786439: step 121800, loss = 0.76 (1298.8 examples/sec; 0.099 sec/batch)
2017-06-02 05:37:25.583219: step 121810, loss = 0.71 (1606.5 examples/sec; 0.080 sec/batch)
2017-06-02 05:37:26.481933: step 121820, loss = 0.65 (1424.3 examples/sec; 0.090 sec/batch)
2017-06-02 05:37:27.357024: step 121830, loss = 0.81 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:37:28.235040: step 121840, loss = 0.59 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:37:29.126379: step 121850, loss = 0.77 (1436.0 examples/sec; 0.089 sec/batch)
2017-06-02 05:37:29.997803: step 121860, loss = 0.71 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:37:30.870555: step 121870, loss = 0.67 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:37:31.741514: step 121880, loss = 0.66 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:37:32.624899: step 121890, loss = 0.82 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:37:33.587988: step 121900, loss = 0.79 (1329.0 examples/sec; 0.096 sec/batch)
2017-06-02 05:37:34.371856: step 121910, loss = 0.67 (1632.9 examples/sec; 0.078 sec/batch)
2017-06-02 05:37:35.258795: step 121920, loss = 0.62 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:37:36.128100: step 121930, loss = 0.74 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:37:37.006118: step 121940, loss = 0.77 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:37:37.884194: step 121950, loss = 0.68 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:37:38.778960: step 121960, loss = 0.71 (1430.5 examples/sec; 0.089 sec/batch)
2017-06-02 05:37:39.660864: step 121970, loss = 0.65 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:37:40.539615: step 121980, loss = 0.73 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:37:41.396442: step 121990, loss = 0.76 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:37:42.366332: step 122000, loss = 0.77 (1319.7 examples/sec; 0.097 sec/batch)
2017-06-02 05:37:43.150091: step 122010, loss = 0.65 (1633.2 examples/sec; 0.078 sec/batch)
2017-06-02 05:37:44.055142: step 122020, loss = 0.62 (1414.3 examples/sec; 0.091 sec/batch)
2017-06-02 05:37:44.921348: step 122030, loss = 0.53 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:37:45.784137: step 122040, loss = 0.71 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:37:46.638675: step 122050, loss = 0.71 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:37:47.507869: step 122060, loss = 0.72 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:37:48.372823: step 122070, loss = 0.69 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:37:49.243245: step 122080, loss = 0.66 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:37:50.070447: step 122090, loss = 0.65 (1547.4 examples/sec; 0.083 sec/batch)
2017-06-02 05:37:51.102836: step 122100, loss = 0.68 (1239.8 examples/sec; 0.103 sec/batch)
2017-06-02 05:37:51.824670: step 122110, loss = 0.62 (1773.3 examples/sec; 0.072 sec/batch)
2017-06-02 05:37:52.673831: step 122120, loss = 0.79 (1507.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:37:53.538727: step 122130, loss = 0.61 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:37:54.387970: step 122140, loss = 0.59 (1507.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:37:55.240325: step 122150, loss = 0.69 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:37:56.084660: step 122160, loss = 0.68 (1516.0 examples/sec; 0.084 sec/batch)
2017-06-02 05:37:56.952954: step 122170, loss = 0.88 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:37:57.790095: step 122180, loss = 0.69 (1529.0 examples/sec; 0.084 sec/batch)
2017-06-02 05:37:58.652177: step 122190, loss = 0.64 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:37:59.605704: step 122200, loss = 0.75 (1342.4 examples/sec; 0.095 sec/batch)
2017-06-02 05:38:00.362092: step 122210, loss = 0.74 (1692.3 examples/sec; 0.076 sec/batch)
2017-06-02 05:38:01.217874: step 122220, loss = 0.76 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:38:02.056831: step 122230, loss = 0.76 (1525.7 examples/sec; 0.084 sec/batch)
2017-06-02 05:38:02.905555: step 122240, loss = 0.64 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:38:03.775062: step 122250, loss = 0.69 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:38:04.617608: step 122260, loss = 0.56 (1519.2 examples/sec; 0.084 sec/batch)
2017-06-02 05:38:05.487337: step 122270, loss = 0.52 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:38:06.344691: step 122280, loss = 0.85 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:38:07.181814: step 122290, loss = 0.67 (1529.0 examples/sec; 0.084 sec/batch)
2017-06-02 05:38:08.164378: step 122300, loss = 0.74 (1302.7 examples/sec; 0.098 sec/batch)
2017-06-02 05:38:08.927384: step 122310, loss = 0.63 (1677.6 examples/sec; 0.076 sec/batch)
2017-06-02 05:38:09.778783: step 122320, loss = 0.67 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:38:10.626697: step 122330, loss = 0.75 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:38:11.479263: step 122340, loss = 0.72 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:38:12.315834: step 122350, loss = 0.64 (1530.1 examples/sec; 0.084 sec/batch)
2017-06-02 05:38:13.158599: step 122360, loss = 0.65 (1518.8 examples/sec; 0.084 sec/batch)
2017-06-02 05:38:14.050975: step 122370, loss = 0.69 (1434.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:38:14.917914: step 122380, loss = 0.69 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:38:15.783174: step 122390, loss = 0.71 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:38:16.781618: step 122400, loss = 0.66 (1282.0 examples/sec; 0.100 sec/batch)
2017-06-02 05:38:17.508428: step 122410, loss = 0.59 (1761.1 examples/sec; 0.073 sec/batch)
2017-06-02 05:38:18.376542: step 122420, loss = 0.63 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:38:19.241477: step 122430, loss = 0.62 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:38:20.095585: step 122440, loss = 0.60 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:38:20.940435: step 122450, loss = 0.79 (1515.1 examples/sec; 0.084 sec/batch)
2017-06-02 05:38:21.795783: step 122460, loss = 0.68 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:38:22.671181: step 122470, loss = 0.84 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:38:23.549616: step 122480, loss = 0.73 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:38:24.410467: step 122490, loss = 0.72 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:38:25.390105: step 122500, loss = 0.70 (1306.6 examples/sec; 0.098 sec/batch)
2017-06-02 05:38:26.132540: step 122510, loss = 0.67 (1724.1 examples/sec; 0.074 sec/batch)
2017-06-02 05:38:26.994305: step 122520, loss = 0.78 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:38:27.881403: step 122530, loss = 0.68 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 05:38:28.736559: step 122540, loss = 0.62 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:38:29.584981: step 122550, loss = 0.67 (1508.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:38:30.433552: step 122560, loss = 0.73 (1508.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:38:31.294425: step 122570, loss = 0.60 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:38:32.165617: step 122580, loss = 0.71 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:38:33.022015: step 122590, loss = 0.86 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:38:33.963682: step 122600, loss = 0.64 (1359.3 examples/sec; 0.094 sec/batch)
2017-06-02 05:38:34.756616: step 122610, loss = 0.76 (1614.3 examples/sec; 0.079 sec/batch)
2017-06-02 05:38:35.627106: step 122620, loss = 0.61 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:38:36.503338: step 122630, loss = 0.72 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:38:37.381818: step 122640, loss = 0.68 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:38:38.257041: step 122650, loss = 0.63 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:38:39.121379: step 122660, loss = 0.66 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:38:39.969401: step 122670, loss = 0.70 (1509.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:38:40.835422: step 122680, loss = 0.55 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:38:41.710542: step 122690, loss = 0.79 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:38:42.692097: step 122700, loss = 0.70 (1304.0 examples/sec; 0.098 sec/batch)
2017-06-02 05:38:43.473695: step 122710, loss = 0.69 (1637.7 examples/sec; 0.078 sec/batch)
2017-06-02 05:38:44.342954: step 122720, loss = 0.63 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:38:45.167688: step 122730, loss = 0.56 (1552.0 examples/sec; 0.082 sec/batch)
2017-06-02 05:38:46.027861: step 122740, loss = 0.84 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:38:46.891548: step 122750, loss = 0.75 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:38:47.755135: step 122760, loss = 0.69 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:38:48.642805: step 122770, loss = 0.68 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 05:38:49.511148: step 122780, loss = 0.77 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:38:50.369379: step 122790, loss = 0.64 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:38:51.323702: step 122800, loss = 0.65 (1341.3 examples/sec; 0.095 sec/batch)
2017-06-02 05:38:52.114307: step 122810, loss = 0.77 (1619.0 examples/sec; 0.079 sec/batch)
2017-06-02 05:38:52.947176: step 122820, loss = 0.85 (1536.9 examples/sec; 0.083 sec/batch)
2017-06-02 05:38:53.824799: step 122830, loss = 0.74 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:38:54.683427: step 122840, loss = 0.65 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:38:55.534314: step 122850, loss = 0.79 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:38:56.392162: step 122860, loss = 0.52 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:38:57.255692: step 122870, loss = 0.56 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:38:58.111404: step 122880, loss = 0.60 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:38:58.962979: step 122890, loss = 0.64 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:38:59.917567: step 122900, loss = 0.71 (1340.9 examples/sec; 0.095 sec/batch)
2017-06-02 05:39:00.670397: step 122910, loss = 0.52 (1700.3 examples/sec; 0.075 sec/batch)
2017-06-02 05:39:01.546017: step 122920, loss = 0.72 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:39:02.411088: step 122930, loss = 0.64 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:39:03.264951: step 122940, loss = 0.76 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:39:04.127796: step 122950, loss = 0.66 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:39:05.001669: step 122960, loss = 0.57 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:39:05.879344: step 122970, loss = 0.78 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:39:06.754454: step 122980, loss = 0.73 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:39:07.627726: step 122990, loss = 0.69 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:39:08.560137: step 123000, loss = 0.69 (1372.8 examples/sec; 0.093 sec/batch)
2017-06-02 05:39:09.342764: step 123010, loss = 0.69 (1635.5 examples/sec; 0.078 sec/batch)
2017-06-02 05:39:10.219745: step 123020, loss = 0.93 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:39:11.092485: step 123030, loss = 0.65 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:39:11.970626: step 123040, loss = 0.58 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:39:12.859655: step 123050, loss = 0.69 (1439.8 examples/sec; 0.089 sec/batch)
2017-06-02 05:39:13.727568: step 123060, loss = 0.65 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:39:14.610318: step 123070, loss = 0.78 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:39:15.477316: step 123080, loss = 0.61 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:39:16.340496: step 123090, loss = 0.68 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:39:17.317885: step 123100, loss = 0.65 (1309.6 examples/sec; 0.098 sec/batch)
2017-06-02 05:39:18.091403: step 123110, loss = 0.65 (1654.8 examples/sec; 0.077 sec/batch)
2017-06-02 05:39:18.963083: step 123120, loss = 0.68 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:39:19.866258: step 123130, loss = 0.57 (1417.2 examples/sec; 0.090 sec/batch)
2017-06-02 05:39:20.746419: step 123140, loss = 0.70 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:39:21.628746: step 123150, loss = 0.69 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:39:22.511407: step 123160, loss = 0.68 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:39:23.392529: step 123170, loss = 0.77 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:39:24.273444: step 123180, loss = 0.62 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:39:25.168473: step 123190, loss = 0.66 (1430.1 examples/sec; 0.090 sec/batch)
2017-06-02 05:39:26.137931: step 123200, loss = 0.64 (1320.3 examples/sec; 0.097 sec/batch)
2017-06-02 05:39:26.913639: step 123210, loss = 0.70 (1650.1 examples/sec; 0.078 sec/batch)
2017-06-02 05:39:27.764092: step 123220, loss = 0.88 (1505.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:39:28.622776: step 123230, loss = 0.64 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:39:29.493707: step 123240, loss = 0.74 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:39:30.356819: step 123250, loss = 0.72 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:39:31.245231: step 123260, loss = 0.72 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 05:39:32.111374: step 123270, loss = 0.67 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:39:32.968167: step 123280, loss = 0.75 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:39:33.821652: step 123290, loss = 0.66 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:39:34.783822: step 123300, loss = 0.56 (1330.3 examples/sec; 0.096 sec/batch)
2017-06-02 05:39:35.568289: step 123310, loss = 0.64 (1631.7 examples/sec; 0.078 sec/batch)
2017-06-02 05:39:36.433662: step 123320, loss = 0.68 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:39:37.281448: step 123330, loss = 0.68 (1509.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:39:38.142146: step 123340, loss = 0.84 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:39:39.024517: step 123350, loss = 0.70 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:39:39.907442: step 123360, loss = 0.95 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:39:40.787900: step 123370, loss = 0.77 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:39:41.653236: step 123380, loss = 0.75 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:39:42.509247: step 123390, loss = 0.71 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:39:43.498582: step 123400, loss = 0.63 (1293.8 examples/sec; 0.099 sec/batch)
2017-06-02 05:39:44.249146: step 123410, loss = 0.57 (1705.4 examples/sec; 0.075 sec/batch)
2017-06-02 05:39:45.119017: step 123420, loss = 0.58 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:39:46.004818: step 123430, loss = 0.78 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 05:39:46.854998: step 123440, loss = 0.74 (1505.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:39:47.720262: step 123450, loss = 0.56 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:39:48.575994: step 123460, loss = 0.65 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:39:49.435311: step 123470, loss = 0.88 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:39:50.325265: step 123480, loss = 0.60 (1438.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:39:51.180556: step 123490, loss = 0.77 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:39:52.184122: step 123500, loss = 0.61 (1275.5 examples/sec; 0.100 sec/batch)
2017-06-02 05:39:52.946629: step 123510, loss = 0.63 (1678.7 examples/sec; 0.076 sec/batch)
2017-06-02 05:39:53.829197: step 123520, loss = 0.67 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:39:54.677820: step 123530, loss = 0.61 (1508.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:39:55.520305: step 123540, loss = 0.72 (1519.3 examples/sec; 0.084 sec/batch)
2017-06-02 05:39:56.416281: step 123550, loss = 0.66 (1428.6 examples/sec; 0.090 sec/batch)
2017-06-02 05:39:57.289148: step 123560, loss = 0.62 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:39:58.165324: step 123570, loss = 0.66 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:39:59.029441: step 123580, loss = 0.77 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:39:59.927041: step 123590, loss = 0.64 (1426.0 examples/sec; 0.090 sec/batch)
2017-06-02 05:40:00.902955: step 123600, loss = 0.78 (1311.6 examples/sec; 0.098 sec/batch)
2017-06-02 05:40:01.682313: step 123610, loss = 0.59 (1642.4 examples/sec; 0.078 sec/batch)
2017-06-02 05:40:02.535569: step 123620, loss = 0.73 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:40:03.408956: step 123630, loss = 0.61 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:40:04.288632: step 123640, loss = 0.71 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:40:05.147592: step 123650, loss = 0.70 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:40:06.028547: step 123660, loss = 0.74 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:40:06.893885: step 123670, loss = 0.70 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:40:07.745116: step 123680, loss = 0.62 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:40:08.631158: step 123690, loss = 0.64 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 05:40:09.565487: step 123700, loss = 0.66 (1370.0 examples/sec; 0.093 sec/batch)
2017-06-02 05:40:10.306020: step 123710, loss = 0.52 (1728.5 examples/sec; 0.074 sec/batch)
2017-06-02 05:40:11.164522: step 123720, loss = 0.66 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:40:12.064445: step 123730, loss = 0.79 (1422.3 examples/sec; 0.090 sec/batch)
2017-06-02 05:40:12.924229: step 123740, loss = 0.61 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:40:13.786305: step 123750, loss = 0.76 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:40:14.640987: step 123760, loss = 0.71 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:40:15.506395: step 123770, loss = 0.69 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:40:16.361536: step 123780, loss = 0.68 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:40:17.218419: step 123790, loss = 0.64 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:40:18.180580: step 123800, loss = 0.69 (1330.3 examples/sec; 0.096 sec/batch)
2017-06-02 05:40:18.970569: step 123810, loss = 0.65 (1620.3 examples/sec; 0.079 sec/batch)
2017-06-02 05:40:19.843190: step 123820, loss = 0.70 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:40:20.732579: step 123830, loss = 0.65 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:40:21.598753: step 123840, loss = 0.72 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:40:22.476606: step 123850, loss = 0.73 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:40:23.346996: step 123860, loss = 0.57 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:40:24.192637: step 123870, loss = 0.63 (1513.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:40:25.046337: step 123880, loss = 0.84 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:40:25.925279: step 123890, loss = 0.66 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:40:26.890535: step 123900, loss = 0.70 (1326.1 examples/sec; 0.097 sec/batch)
2017-06-02 05:40:27.666744: step 123910, loss = 0.83 (1649.1 examples/sec; 0.078 sec/batch)
2017-06-02 05:40:28.534023: step 123920, loss = 0.77 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:40:29.397815: step 123930, loss = 0.60 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:40:30.230437: step 123940, loss = 0.70 (1537.3 examples/sec; 0.083 sec/batch)
2017-06-02 05:40:31.079576: step 123950, loss = 0.79 (1507.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:40:31.941094: step 123960, loss = 0.62 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:40:32.818366: step 123970, loss = 0.75 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:40:33.696911: step 123980, loss = 0.84 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:40:34.564363: step 123990, loss = 0.75 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:40:35.573273: step 124000, loss = 0.62 (1268.7 examples/sec; 0.101 sec/batch)
2017-06-02 05:40:36.308368: step 124010, loss = 0.75 (1741.3 examples/sec; 0.074 sec/batch)
2017-06-02 05:40:37.170510: step 124020, loss = 0.63 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:40:38.045597: step 124030, loss = 0.62 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:40:38.923405: step 124040, loss = 0.63 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:40:39.799960: step 124050, loss = 0.74 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:40:40.679659: step 124060, loss = 0.63 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:40:41.546702: step 124070, loss = 0.76 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:40:42.433489: step 124080, loss = 0.72 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:40:43.303463: step 124090, loss = 0.72 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:40:44.270399: step 124100, loss = 0.67 (1323.8 examples/sec; 0.097 sec/batch)
2017-06-02 05:40:45.016085: step 124110, loss = 0.65 (1716.5 examples/sec; 0.075 sec/batch)
2017-06-02 05:40:45.863078: step 124120, loss = 0.74 (1511.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:40:46.719647: step 124130, loss = 0.69 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:40:47.563516: step 124140, loss = 0.64 (1516.9 examples/sec; 0.084 sec/batch)
2017-06-02 05:40:48.393570: step 124150, loss = 0.65 (1542.1 examples/sec; 0.083 sec/batch)
2017-06-02 05:40:49.259896: step 124160, loss = 0.67 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:40:50.135293: step 124170, loss = 0.74 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:40:50.995030: step 124180, loss = 0.60 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:40:51.854967: step 124190, loss = 0.75 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:40:52.803596: step 124200, loss = 0.80 (1349.3 examples/sec; 0.095 sec/batch)
2017-06-02 05:40:53.589103: step 124210, loss = 0.64 (1629.5 examples/sec; 0.079 sec/batch)
2017-06-02 05:40:54.454466: step 124220, loss = 0.69 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:40:55.314610: step 124230, loss = 0.65 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:40:56.161611: step 124240, loss = 0.77 (1511.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:40:57.021177: step 124250, loss = 0.57 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:40:57.873302: step 124260, loss = 0.72 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:40:58.731049: step 124270, loss = 0.75 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:40:59.598699: step 124280, loss = 0.69 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:41:00.441992: step 124290, loss = 0.66 (1517.9 examples/sec; 0.084 sec/batch)
2017-06-02 05:41:01.397890: step 124300, loss = 0.61 (1339.1 examples/sec; 0.096 sec/batch)
2017-06-02 05:41:02.189300: step 124310, loss = 0.61 (1617.4 examples/sec; 0.079 sec/batch)
2017-06-02 05:41:03.054242: step 124320, loss = 0.77 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:41:03.923795: step 124330, loss = 0.66 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:41:04.813956: step 124340, loss = 0.70 (1437.9 examples/sec; 0.089 sec/batch)
2017-06-02 05:41:05.680622: step 124350, loss = 0.74 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:41:06.533628: step 124360, loss = 0.70 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:41:07.394714: step 124370, loss = 0.75 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:41:08.284769: step 124380, loss = 0.63 (1438.1 examples/sec; 0.089 sec/batch)
2017-06-02 05:41:09.148508: step 124390, loss = 0.68 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:41:10.110428: step 124400, loss = 0.71 (1330.7 examples/sec; 0.096 sec/batch)
2017-06-02 05:41:10.915132: step 124410, loss = 0.79 (1590.7 examples/sec; 0.080 sec/batch)
2017-06-02 05:41:11.779124: step 124420, loss = 0.61 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:41:12.661986: step 124430, loss = 0.71 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:41:13.546904: step 124440, loss = 0.87 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:41:14.431104: step 124450, loss = 0.72 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:41:15.305107: step 124460, loss = 0.61 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:41:16.183144: step 124470, loss = 0.68 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:41:17.037209: step 124480, loss = 0.57 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:41:17.919173: step 124490, loss = 0.68 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:41:18.893373: step 124500, loss = 0.66 (1313.9 examples/sec; 0.097 sec/batch)
2017-06-02 05:41:19.690141: step 124510, loss = 0.59 (1606.5 examples/sec; 0.080 sec/batch)
2017-06-02 05:41:20.559703: step 124520, loss = 0.94 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:41:21.468562: step 124530, loss = 0.65 (1408.4 examples/sec; 0.091 sec/batch)
2017-06-02 05:41:22.331435: step 124540, loss = 0.66 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:41:23.221743: step 124550, loss = 0.74 (1437.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:41:24.112508: step 124560, loss = 0.64 (1437.0 examples/sec; 0.089 sec/batch)
2017-06-02 05:41:24.971481: step 124570, loss = 0.71 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:41:25.859222: step 124580, loss = 0.66 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 05:41:26.719218: step 124590, loss = 0.63 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:41:27.687638: step 124600, loss = 0.73 (1321.7 examples/sec; 0.097 sec/batch)
2017-06-02 05:41:28.483892: step 124610, loss = 0.71 (1607.5 examples/sec; 0.080 sec/batch)
2017-06-02 05:41:29.353922: step 124620, loss = 0.68 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:41:30.209210: step 124630, loss = 0.64 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:41:31.069549: step 124640, loss = 0.62 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:41:31.939598: step 124650, loss = 0.63 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:41:32.782131: step 124660, loss = 0.65 (1519.2 examples/sec; 0.084 sec/batch)
2017-06-02 05:41:33.669923: step 124670, loss = 0.65 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 05:41:34.539655: step 124680, loss = 0.58 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:41:35.398799: step 124690, loss = 0.67 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:41:36.384006: step 124700, loss = 0.78 (1299.2 examples/sec; 0.099 sec/batch)
2017-06-02 05:41:37.178817: step 124710, loss = 0.73 (1610.5 examples/sec; 0.079 sec/batch)
2017-06-02 05:41:38.040842: step 124720, loss = 0.67 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:41:38.907187: step 124730, loss = 0.69 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:41:39.765838: step 124740, loss = 0.81 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:41:40.604300: step 124750, loss = 0.62 (1526.6 examples/sec; 0.084 sec/batch)
2017-06-02 05:41:41.455213: step 124760, loss = 0.60 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:41:42.317071: step 124770, loss = 0.60 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:41:43.185187: step 124780, loss = 0.62 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:41:44.075226: step 124790, loss = 0.60 (1438.1 examples/sec; 0.089 sec/batch)
2017-06-02 05:41:45.046568: step 124800, loss = 0.85 (1317.8 examples/sec; 0.097 sec/batch)
2017-06-02 05:41:45.826991: step 124810, loss = 0.60 (1640.1 examples/sec; 0.078 sec/batch)
2017-06-02 05:41:46.705813: step 124820, loss = 0.78 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:41:47.591198: step 124830, loss = 0.58 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:41:48.472446: step 124840, loss = 0.63 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:41:49.316872: step 124850, loss = 0.70 (1515.9 examples/sec; 0.084 sec/batch)
2017-06-02 05:41:50.181785: step 124860, loss = 0.67 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:41:51.056326: step 124870, loss = 0.66 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:41:51.918984: step 124880, loss = 0.72 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:41:52.790661: step 124890, loss = 0.73 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:41:53.812482: step 124900, loss = 0.80 (1252.7 examples/sec; 0.102 sec/batch)
2017-06-02 05:41:54.527499: step 124910, loss = 0.68 (1790.2 examples/sec; 0.072 sec/batch)
2017-06-02 05:41:55.366631: step 124920, loss = 0.66 (1525.4 examples/sec; 0.084 sec/batch)
2017-06-02 05:41:56.235827: step 124930, loss = 0.60 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:41:57.110585: step 124940, loss = 0.78 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:41:57.991879: step 124950, loss = 0.66 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:41:58.874755: step 124960, loss = 0.76 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:41:59.740391: step 124970, loss = 0.57 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:42:00.635260: step 124980, loss = 0.58 (1430.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:42:01.531870: step 124990, loss = 0.74 (1427.6 examples/sec; 0.090 sec/batch)
2017-06-02 05:42:02.505424: step 125000, loss = 0.69 (1314.8 examples/sec; 0.097 sec/batch)
2017-06-02 05:42:03.282439: step 125010, loss = 0.62 (1647.4 examples/sec; 0.078 sec/batch)
2017-06-02 05:42:04.131764: step 125020, loss = 0.69 (1507.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:42:05.006639: step 125030, loss = 0.82 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:42:05.881474: step 125040, loss = 0.69 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:42:06.746337: step 125050, loss = 0.67 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:42:07.586427: step 125060, loss = 0.76 (1523.6 examples/sec; 0.084 sec/batch)
2017-06-02 05:42:08.431273: step 125070, loss = 0.76 (1515.1 examples/sec; 0.084 sec/batch)
2017-06-02 05:42:09.313274: step 125080, loss = 0.73 (1451.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:42:10.201977: step 125090, loss = 0.73 (1440.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:42:11.157084: step 125100, loss = 0.71 (1340.2 examples/sec; 0.096 sec/batch)
2017-06-02 05:42:11.923969: step 125110, loss = 0.73 (1669.1 examples/sec; 0.077 sec/batch)
2017-06-02 05:42:12.795406: step 125120, loss = 0.74 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:42:13.683622: step 125130, loss = 0.84 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 05:42:14.550347: step 125140, loss = 0.54 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:42:15.419988: step 125150, loss = 0.52 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:42:16.284182: step 125160, loss = 0.64 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:42:17.140986: step 125170, loss = 0.64 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:42:17.997392: step 125180, loss = 0.65 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:42:18.823669: step 125190, loss = 0.63 (1549.1 examples/sec; 0.083 sec/batch)
2017-06-02 05:42:19.782613: step 125200, loss = 0.63 (1334.8 examples/sec; 0.096 sec/batch)
2017-06-02 05:42:20.551638: step 125210, loss = 0.72 (1664.5 examples/sec; 0.077 sec/batch)
2017-06-02 05:42:21.436200: step 125220, loss = 0.75 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:42:22.296951: step 125230, loss = 0.62 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:42:23.170561: step 125240, loss = 0.72 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:42:24.019666: step 125250, loss = 0.68 (1507.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:42:24.888590: step 125260, loss = 0.75 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:42:25.764399: step 125270, loss = 0.66 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:42:26.620867: step 125280, loss = 0.66 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:42:27.488419: step 125290, loss = 0.83 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:42:28.421978: step 125300, loss = 0.78 (1371.1 examples/sec; 0.093 sec/batch)
2017-06-02 05:42:29.186830: step 125310, loss = 0.55 (1673.5 examples/sec; 0.076 sec/batch)
2017-06-02 05:42:30.029959: step 125320, loss = 0.62 (1518.2 examples/sec; 0.084 sec/batch)
2017-06-02 05:42:30.880664: step 125330, loss = 0.67 (1504.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:42:31.738588: step 125340, loss = 0.70 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:42:32.580962: step 125350, loss = 0.74 (1519.5 examples/sec; 0.084 sec/batch)
2017-06-02 05:42:33.432633: step 125360, loss = 0.79 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:42:34.283165: step 125370, loss = 0.77 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:42:35.142010: step 125380, loss = 0.60 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:42:36.003016: step 125390, loss = 0.68 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:42:37.023321: step 125400, loss = 0.67 (1254.5 examples/sec; 0.102 sec/batch)
2017-06-02 05:42:37.739316: step 125410, loss = 0.70 (1787.7 examples/sec; 0.072 sec/batch)
2017-06-02 05:42:38.598740: step 125420, loss = 0.71 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:42:39.462659: step 125430, loss = 0.62 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:42:40.318606: step 125440, loss = 0.54 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:42:41.185167: step 125450, loss = 0.78 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:42:42.060041: step 125460, loss = 0.74 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:42:42.915023: step 125470, loss = 0.76 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:42:43.767563: step 125480, loss = 0.71 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:42:44.624191: step 125490, loss = 0.74 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:42:45.583779: step 125500, loss = 0.80 (1333.9 examples/sec; 0.096 sec/batch)
2017-06-02 05:42:46.339212: step 125510, loss = 0.73 (1694.4 examples/sec; 0.076 sec/batch)
2017-06-02 05:42:47.216034: step 125520, loss = 0.73 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:42:48.076756: step 125530, loss = 0.56 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:42:48.915068: step 125540, loss = 0.62 (1526.8 examples/sec; 0.084 sec/batch)
2017-06-02 05:42:49.759326: step 125550, loss = 0.64 (1516.1 examples/sec; 0.084 sec/batch)
2017-06-02 05:42:50.625123: step 125560, loss = 0.54 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:42:51.498304: step 125570, loss = 0.67 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:42:52.380197: step 125580, loss = 0.73 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:42:53.237055: step 125590, loss = 0.69 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:42:54.215225: step 125600, loss = 0.65 (1308.6 examples/sec; 0.098 sec/batch)
2017-06-02 05:42:54.984031: step 125610, loss = 0.81 (1664.9 examples/sec; 0.077 sec/batch)
2017-06-02 05:42:55.833531: step 125620, loss = 0.83 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:42:56.703749: step 125630, loss = 0.70 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:42:57.557177: step 125640, loss = 0.73 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:42:58.440797: step 125650, loss = 0.73 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:42:59.327120: step 125660, loss = 0.81 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 05:43:00.206626: step 125670, loss = 0.79 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:43:01.108004: step 125680, loss = 0.82 (1420.1 examples/sec; 0.090 sec/batch)
2017-06-02 05:43:01.985393: step 125690, loss = 0.66 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:43:02.997740: step 125700, loss = 0.69 (1264.4 examples/sec; 0.101 sec/batch)
2017-06-02 05:43:03.775910: step 125710, loss = 0.59 (1644.9 examples/sec; 0.078 sec/batch)
2017-06-02 05:43:04.652039: step 125720, loss = 0.63 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:43:05.543506: step 125730, loss = 0.82 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 05:43:06.408814: step 125740, loss = 0.64 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:43:07.307501: step 125750, loss = 0.65 (1424.3 examples/sec; 0.090 sec/batch)
2017-06-02 05:43:08.153640: step 125760, loss = 0.64 (1512.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:43:09.028421: step 125770, loss = 0.72 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:43:09.924937: step 125780, loss = 0.76 (1427.8 examples/sec; 0.090 sec/batch)
2017-06-02 05:43:10.788817: step 125790, loss = 0.72 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:43:11.751103: step 125800, loss = 0.63 (1330.2 examples/sec; 0.096 sec/batch)
2017-06-02 05:43:12.557600: step 125810, loss = 0.67 (1587.1 examples/sec; 0.081 sec/batch)
2017-06-02 05:43:13.456223: step 125820, loss = 0.61 (1424.4 examples/sec; 0.090 sec/batch)
2017-06-02 05:43:14.333591: step 125830, loss = 0.79 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:43:15.218574: step 125840, loss = 0.69 (1446.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:43:16.075671: step 125850, loss = 0.70 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:43:16.923643: step 125860, loss = 0.74 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:43:17.778613: step 125870, loss = 0.66 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:43:18.616337: step 125880, loss = 0.62 (1527.9 examples/sec; 0.084 sec/batch)
2017-06-02 05:43:19.484309: step 125890, loss = 0.54 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:43:20.459766: step 125900, loss = 0.67 (1312.2 examples/sec; 0.098 sec/batch)
2017-06-02 05:43:21.234683: step 125910, loss = 0.70 (1651.8 examples/sec; 0.077 sec/batch)
2017-06-02 05:43:22.109498: step 125920, loss = 0.53 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:43:22.993641: step 125930, loss = 0.65 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:43:23.853448: step 125940, loss = 0.76 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:43:24.723661: step 125950, loss = 0.74 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:43:25.610098: step 125960, loss = 0.69 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 05:43:26.462895: step 125970, loss = 0.63 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:43:27.305695: step 125980, loss = 0.64 (1518.7 examples/sec; 0.084 sec/batch)
2017-06-02 05:43:28.153792: step 125990, loss = 0.88 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:43:29.115017: step 126000, loss = 0.73 (1331.6 examples/sec; 0.096 sec/batch)
2017-06-02 05:43:29.878846: step 126010, loss = 0.72 (1675.8 examples/sec; 0.076 sec/batch)
2017-06-02 05:43:30.753052: step 126020, loss = 0.76 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:43:31.622441: step 126030, loss = 0.74 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:43:32.514831: step 126040, loss = 0.74 (1434.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:43:33.389953: step 126050, loss = 0.86 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:43:34.263600: step 126060, loss = 0.64 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:43:35.128828: step 126070, loss = 0.69 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:43:36.003999: step 126080, loss = 0.70 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:43:36.864212: step 126090, loss = 0.68 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:43:37.814559: step 126100, loss = 0.69 (1346.9 examples/sec; 0.095 sec/batch)
2017-06-02 05:43:38.591566: step 126110, loss = 0.62 (1647.3 examples/sec; 0.078 sec/batch)
2017-06-02 05:43:39.461080: step 126120, loss = 0.75 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:43:40.284882: step 126130, loss = 0.49 (1553.8 examples/sec; 0.082 sec/batch)
2017-06-02 05:43:41.132657: step 126140, loss = 0.79 (1509.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:43:42.014584: step 126150, loss = 0.68 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:43:42.872534: step 126160, loss = 0.75 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:43:43.738012: step 126170, loss = 0.66 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:43:44.578831: step 126180, loss = 0.77 (1522.4 examples/sec; 0.084 sec/batch)
2017-06-02 05:43:45.439400: step 126190, loss = 0.62 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:43:46.395127: step 126200, loss = 0.63 (1339.3 examples/sec; 0.096 sec/batch)
2017-06-02 05:43:47.185254: step 126210, loss = 0.73 (1620.0 examples/sec; 0.079 sec/batch)
2017-06-02 05:43:48.039464: step 126220, loss = 0.68 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:43:48.923106: step 126230, loss = 0.69 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:43:49.799461: step 126240, loss = 0.61 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:43:50.649154: step 126250, loss = 0.77 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:43:51.511719: step 126260, loss = 0.71 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:43:52.367472: step 126270, loss = 0.67 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:43:53.242973: step 126280, loss = 0.68 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:43:54.137069: step 126290, loss = 0.82 (1431.6 examples/sec; 0.089 sec/batch)
2017-06-02 05:43:55.107637: step 126300, loss = 0.71 (1318.8 examples/sec; 0.097 sec/batch)
2017-06-02 05:43:55.894673: step 126310, loss = 0.60 (1626.4 examples/sec; 0.079 sec/batch)
2017-06-02 05:43:56.779344: step 126320, loss = 0.74 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:43:57.660119: step 126330, loss = 0.62 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:43:58.533250: step 126340, loss = 0.74 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:43:59.413702: step 126350, loss = 0.80 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:44:00.280467: step 126360, loss = 0.71 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:44:01.148366: step 126370, loss = 0.66 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:44:02.009829: step 126380, loss = 0.71 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:44:02.862985: step 126390, loss = 0.58 (1500.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:44:03.833246: step 126400, loss = 0.63 (1319.2 examples/sec; 0.097 sec/batch)
2017-06-02 05:44:04.613611: step 126410, loss = 0.68 (1640.3 examples/sec; 0.078 sec/batch)
2017-06-02 05:44:05.483948: step 126420, loss = 0.59 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:44:06.348342: step 126430, loss = 0.65 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:44:07.196309: step 126440, loss = 0.72 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:44:08.049158: step 126450, loss = 0.77 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:44:08.904256: step 126460, loss = 0.55 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:44:09.757282: step 126470, loss = 0.85 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:44:10.604721: step 126480, loss = 0.80 (1510.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:44:11.465257: step 126490, loss = 0.63 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:44:12.415561: step 126500, loss = 0.76 (1346.9 examples/sec; 0.095 sec/batch)
2017-06-02 05:44:13.184130: step 126510, loss = 0.67 (1665.5 examples/sec; 0.077 sec/batch)
2017-06-02 05:44:14.052590: step 126520, loss = 0.68 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:44:14.928823: step 126530, loss = 0.58 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:44:15.795714: step 126540, loss = 0.64 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:44:16.654684: step 126550, loss = 0.71 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:44:17.531707: step 126560, loss = 0.72 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:44:18.390463: step 126570, loss = 0.69 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:44:19.250952: step 126580, loss = 0.69 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:44:20.156042: step 126590, loss = 0.67 (1414.2 examples/sec; 0.091 sec/batch)
2017-06-02 05:44:21.108006: step 126600, loss = 0.65 (1344.6 examples/sec; 0.095 sec/batch)
2017-06-02 05:44:21.895647: step 126610, loss = 0.65 (1625.1 examples/sec; 0.079 sec/batch)
2017-06-02 05:44:22.801552: step 126620, loss = 0.71 (1413.0 examples/sec; 0.091 sec/batch)
2017-06-02 05:44:23.667291: step 126630, loss = 0.70 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:44:24.511437: step 126640, loss = 0.77 (1516.3 examples/sec; 0.084 sec/batch)
2017-06-02 05:44:25.395212: step 126650, loss = 0.62 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:44:26.282682: step 126660, loss = 0.69 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:44:27.170563: step 126670, loss = 0.64 (1441.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:44:28.041785: step 126680, loss = 0.82 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:44:28.946921: step 126690, loss = 0.69 (1414.2 examples/sec; 0.091 sec/batch)
2017-06-02 05:44:29.903607: step 126700, loss = 0.60 (1337.9 examples/sec; 0.096 sec/batch)
2017-06-02 05:44:30.683630: step 126710, loss = 0.67 (1641.0 examples/sec; 0.078 sec/batch)
2017-06-02 05:44:31.547413: step 126720, loss = 0.74 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:44:32.410791: step 126730, loss = 0.55 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:44:33.288393: step 126740, loss = 0.66 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:44:34.149628: step 126750, loss = 0.85 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:44:35.011387: step 126760, loss = 0.65 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:44:35.883947: step 126770, loss = 0.73 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:44:36.775574: step 126780, loss = 0.62 (1435.6 examples/sec; 0.089 sec/batch)
2017-06-02 05:44:37.639304: step 126790, loss = 0.78 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:44:38.605433: step 126800, loss = 0.61 (1324.9 examples/sec; 0.097 sec/batch)
2017-06-02 05:44:39.373999: step 126810, loss = 0.67 (1665.4 examples/sec; 0.077 sec/batch)
2017-06-02 05:44:40.237293: step 126820, loss = 0.73 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:44:41.090174: step 126830, loss = 0.72 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:44:41.952200: step 126840, loss = 0.85 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:44:42.826977: step 126850, loss = 0.74 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:44:43.686408: step 126860, loss = 0.68 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:44:44.545575: step 126870, loss = 0.66 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:44:45.432830: step 126880, loss = 0.74 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:44:46.297584: step 126890, loss = 0.63 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:44:47.237142: step 126900, loss = 0.57 (1362.4 examples/sec; 0.094 sec/batch)
2017-06-02 05:44:48.009665: step 126910, loss = 0.70 (1656.9 examples/sec; 0.077 sec/batch)
2017-06-02 05:44:48.864166: step 126920, loss = 0.56 (1498.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:44:49.724380: step 126930, loss = 0.79 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:44:50.595374: step 126940, loss = 0.81 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:44:51.462603: step 126950, loss = 0.63 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:44:52.341874: step 126960, loss = 0.63 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:44:53.196793: step 126970, loss = 0.71 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:44:54.073269: step 126980, loss = 0.90 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:44:54.949468: step 126990, loss = 0.63 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:44:55.914397: step 127000, loss = 0.57 (1326.5 examples/sec; 0.096 sec/batch)
2017-06-02 05:44:56.699834: step 127010, loss = 0.59 (1629.7 examples/sec; 0.079 sec/batch)
2017-06-02 05:44:57.615086: step 127020, loss = 0.51 (1398.5 examples/sec; 0.092 sec/batch)
2017-06-02 05:44:58.485631: step 127030, loss = 0.78 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:44:59.344370: step 127040, loss = 0.76 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:45:00.214439: step 127050, loss = 0.66 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:45:01.093171: step 127060, loss = 0.66 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:45:01.943426: step 127070, loss = 0.67 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:45:02.815978: step 127080, loss = 0.64 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:45:03.703110: step 127090, loss = 0.77 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 05:45:04.643553: step 127100, loss = 0.67 (1361.1 examples/sec; 0.094 sec/batch)
2017-06-02 05:45:05.423102: step 127110, loss = 0.69 (1642.0 examples/sec; 0.078 sec/batch)
2017-06-02 05:45:06.271024: step 127120, loss = 0.77 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:45:07.132646: step 127130, loss = 0.89 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:45:07.983900: step 127140, loss = 0.75 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:45:08.859654: step 127150, loss = 0.53 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:45:09.720444: step 127160, loss = 0.60 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:45:10.579865: step 127170, loss = 0.56 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:45:11.435747: step 127180, loss = 0.82 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:45:12.287306: step 127190, loss = 0.76 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:45:13.268530: step 127200, loss = 0.72 (1304.5 examples/sec; 0.098 sec/batch)
2017-06-02 05:45:14.040176: step 127210, loss = 0.64 (1658.8 examples/sec; 0.077 sec/batch)
2017-06-02 05:45:14.901174: step 127220, loss = 0.54 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:45:15.786834: step 127230, loss = 0.59 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:45:16.670027: step 127240, loss = 0.66 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:45:17.572185: step 127250, loss = 0.86 (1418.8 examples/sec; 0.090 sec/batch)
2017-06-02 05:45:18.454071: step 127260, loss = 0.69 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:45:19.322539: step 127270, loss = 0.69 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:45:20.209406: step 127280, loss = 0.66 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:45:21.086904: step 127290, loss = 0.81 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:45:22.033984: step 127300, loss = 0.61 (1351.5 examples/sec; 0.095 sec/batch)
2017-06-02 05:45:22.802324: step 127310, loss = 0.82 (1665.9 examples/sec; 0.077 sec/batch)
2017-06-02 05:45:23.664622: step 127320, loss = 0.87 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:45:24.543659: step 127330, loss = 0.68 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:45:25.398571: step 127340, loss = 0.53 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:45:26.268560: step 127350, loss = 0.69 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:45:27.148240: step 127360, loss = 0.66 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:45:27.987500: step 127370, loss = 0.65 (1525.2 examples/sec; 0.084 sec/batch)
2017-06-02 05:45:28.868569: step 127380, loss = 0.64 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:45:29.731364: step 127390, loss = 0.73 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:45:30.706495: step 127400, loss = 0.47 (1312.6 examples/sec; 0.098 sec/batch)
2017-06-02 05:45:31.477803: step 127410, loss = 0.73 (1659.5 examples/sec; 0.077 sec/batch)
2017-06-02 05:45:32.346540: step 127420, loss = 0.69 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:45:33.210266: step 127430, loss = 0.68 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:45:34.061216: step 127440, loss = 0.65 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:45:34.919138: step 127450, loss = 0.70 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:45:35.804060: step 127460, loss = 0.61 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:45:36.679140: step 127470, loss = 0.68 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:45:37.566598: step 127480, loss = 0.59 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:45:38.418128: step 127490, loss = 0.57 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:45:39.400048: step 127500, loss = 0.63 (1303.6 examples/sec; 0.098 sec/batch)
2017-06-02 05:45:40.182338: step 127510, loss = 0.73 (1636.2 examples/sec; 0.078 sec/batch)
2017-06-02 05:45:41.056539: step 127520, loss = 0.83 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:45:41.928941: step 127530, loss = 0.78 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:45:42.779152: step 127540, loss = 0.64 (1505.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:45:43.624086: step 127550, loss = 0.62 (1514.9 examples/sec; 0.084 sec/batch)
2017-06-02 05:45:44.520853: step 127560, loss = 0.64 (1427.4 examples/sec; 0.090 sec/batch)
2017-06-02 05:45:45.399624: step 127570, loss = 0.72 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:45:46.257098: step 127580, loss = 0.59 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:45:47.105722: step 127590, loss = 0.89 (1508.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:45:48.060778: step 127600, loss = 0.66 (1340.2 examples/sec; 0.096 sec/batch)
2017-06-02 05:45:48.821994: step 127610, loss = 0.84 (1681.5 examples/sec; 0.076 sec/batch)
2017-06-02 05:45:49.688491: step 127620, loss = 0.57 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:45:50.549086: step 127630, loss = 0.63 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:45:51.418623: step 127640, loss = 0.70 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:45:52.284946: step 127650, loss = 0.56 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:45:53.174855: step 127660, loss = 0.56 (1438.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:45:54.016771: step 127670, loss = 0.59 (1520.4 examples/sec; 0.084 sec/batch)
2017-06-02 05:45:54.873607: step 127680, loss = 0.63 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:45:55.743054: step 127690, loss = 0.70 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:45:56.764370: step 127700, loss = 0.70 (1253.3 examples/sec; 0.102 sec/batch)
2017-06-02 05:45:57.481028: step 127710, loss = 0.63 (1786.1 examples/sec; 0.072 sec/batch)
2017-06-02 05:45:58.319303: step 127720, loss = 0.58 (1527.0 examples/sec; 0.084 sec/batch)
2017-06-02 05:45:59.168914: step 127730, loss = 0.61 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:46:00.036176: step 127740, loss = 0.70 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:46:00.896184: step 127750, loss = 0.85 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:46:01.770234: step 127760, loss = 0.77 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:46:02.599724: step 127770, loss = 0.69 (1543.1 examples/sec; 0.083 sec/batch)
2017-06-02 05:46:03.471230: step 127780, loss = 0.63 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:46:04.338654: step 127790, loss = 0.69 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:46:05.282015: step 127800, loss = 0.72 (1356.9 examples/sec; 0.094 sec/batch)
2017-06-02 05:46:06.036876: step 127810, loss = 0.72 (1695.7 examples/sec; 0.075 sec/batch)
2017-06-02 05:46:06.919706: step 127820, loss = 0.88 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:46:07.788593: step 127830, loss = 0.69 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:46:08.629795: step 127840, loss = 0.72 (1521.6 examples/sec; 0.084 sec/batch)
2017-06-02 05:46:09.492504: step 127850, loss = 0.69 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:46:10.346785: step 127860, loss = 0.60 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:46:11.216896: step 127870, loss = 0.86 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:46:12.068531: step 127880, loss = 0.79 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:46:12.939785: step 127890, loss = 0.67 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:46:13.898693: step 127900, loss = 0.68 (1334.8 examples/sec; 0.096 sec/batch)
2017-06-02 05:46:14.655210: step 127910, loss = 0.60 (1692.0 examples/sec; 0.076 sec/batch)
2017-06-02 05:46:15.533095: step 127920, loss = 0.57 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:46:16.392598: step 127930, loss = 0.83 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:46:17.271263: step 127940, loss = 0.61 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:46:18.150758: step 127950, loss = 0.56 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:46:19.030232: step 127960, loss = 0.76 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:46:19.900187: step 127970, loss = 0.65 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:46:20.774351: step 127980, loss = 0.61 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:46:21.648378: step 127990, loss = 0.68 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:46:22.612964: step 128000, loss = 0.64 (1327.0 examples/sec; 0.096 sec/batch)
2017-06-02 05:46:23.359925: step 128010, loss = 0.70 (1713.6 examples/sec; 0.075 sec/batch)
2017-06-02 05:46:24.247672: step 128020, loss = 0.48 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 05:46:25.121053: step 128030, loss = 0.64 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:46:25.992145: step 128040, loss = 0.73 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:46:26.849421: step 128050, loss = 0.91 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:46:27.695541: step 128060, loss = 0.68 (1512.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:46:28.571602: step 128070, loss = 0.60 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:46:29.460061: step 128080, loss = 0.79 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:46:30.316149: step 128090, loss = 0.87 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:46:31.261123: step 128100, loss = 0.77 (1354.5 examples/sec; 0.094 sec/batch)
2017-06-02 05:46:32.007744: step 128110, loss = 0.70 (1714.4 examples/sec; 0.075 sec/batch)
2017-06-02 05:46:32.894247: step 128120, loss = 0.60 (1443.9 examples/sec; 0.089 sec/batch)
2017-06-02 05:46:33.764652: step 128130, loss = 0.76 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:46:34.636909: step 128140, loss = 0.80 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:46:35.504705: step 128150, loss = 0.53 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:46:36.363007: step 128160, loss = 0.86 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:46:37.211129: step 128170, loss = 0.74 (1509.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:46:38.080123: step 128180, loss = 0.63 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:46:38.931885: step 128190, loss = 0.67 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:46:39.891853: step 128200, loss = 0.70 (1333.4 examples/sec; 0.096 sec/batch)
2017-06-02 05:46:40.661691: step 128210, loss = 0.63 (1662.7 examples/sec; 0.077 sec/batch)
2017-06-02 05:46:41.520709: step 128220, loss = 0.74 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:46:42.406361: step 128230, loss = 0.72 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:46:43.291704: step 128240, loss = 0.52 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 05:46:44.150950: step 128250, loss = 0.72 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:46:45.010556: step 128260, loss = 0.76 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:46:45.839666: step 128270, loss = 0.74 (1543.9 examples/sec; 0.083 sec/batch)
2017-06-02 05:46:46.695259: step 128280, loss = 0.70 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:46:47.556022: step 128290, loss = 0.64 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:46:48.492150: step 128300, loss = 0.56 (1367.3 examples/sec; 0.094 sec/batch)
2017-06-02 05:46:49.276050: step 128310, loss = 0.70 (1632.9 examples/sec; 0.078 sec/batch)
2017-06-02 05:46:50.137917: step 128320, loss = 0.81 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:46:50.982468: step 128330, loss = 0.67 (1515.6 examples/sec; 0.084 sec/batch)
2017-06-02 05:46:51.848386: step 128340, loss = 0.77 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:46:52.696990: step 128350, loss = 0.55 (1508.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:46:53.555262: step 128360, loss = 0.76 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:46:54.402321: step 128370, loss = 0.57 (1511.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:46:55.244130: step 128380, loss = 0.59 (1520.5 examples/sec; 0.084 sec/batch)
2017-06-02 05:46:56.096717: step 128390, loss = 0.72 (1501.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:46:57.053196: step 128400, loss = 0.67 (1338.2 examples/sec; 0.096 sec/batch)
2017-06-02 05:46:57.832930: step 128410, loss = 0.63 (1641.6 examples/sec; 0.078 sec/batch)
2017-06-02 05:46:58.700653: step 128420, loss = 0.57 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:46:59.550726: step 128430, loss = 0.65 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:47:00.394051: step 128440, loss = 0.69 (1517.8 examples/sec; 0.084 sec/batch)
2017-06-02 05:47:01.253638: step 128450, loss = 0.72 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:47:02.098336: step 128460, loss = 0.57 (1515.3 examples/sec; 0.084 sec/batch)
2017-06-02 05:47:02.935138: step 128470, loss = 0.67 (1529.6 examples/sec; 0.084 sec/batch)
2017-06-02 05:47:03.811399: step 128480, loss = 0.70 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:47:04.668431: step 128490, loss = 0.72 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:47:05.665948: step 128500, loss = 0.70 (1283.2 examples/sec; 0.100 sec/batch)
2017-06-02 05:47:06.383931: step 128510, loss = 0.63 (1782.8 examples/sec; 0.072 sec/batch)
2017-06-02 05:47:07.256000: step 128520, loss = 0.72 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:47:08.137251: step 128530, loss = 0.68 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:47:09.005928: step 128540, loss = 0.72 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:47:09.878787: step 128550, loss = 0.61 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:47:10.729644: step 128560, loss = 0.69 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:47:11.591751: step 128570, loss = 0.72 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:47:12.415933: step 128580, loss = 0.73 (1553.1 examples/sec; 0.082 sec/batch)
2017-06-02 05:47:13.301509: step 128590, loss = 0.65 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:47:14.273461: step 128600, loss = 0.60 (1316.9 examples/sec; 0.097 sec/batch)
2017-06-02 05:47:15.035261: step 128610, loss = 0.62 (1680.2 examples/sec; 0.076 sec/batch)
2017-06-02 05:47:15.889512: step 128620, loss = 0.72 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:47:16.739195: step 128630, loss = 0.59 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:47:17.609464: step 128640, loss = 0.71 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:47:18.476841: step 128650, loss = 0.62 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:47:19.361429: step 128660, loss = 0.64 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:47:20.225494: step 128670, loss = 0.76 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:47:21.098786: step 128680, loss = 0.65 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:47:21.981207: step 128690, loss = 0.70 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:47:22.954438: step 128700, loss = 0.65 (1315.2 examples/sec; 0.097 sec/batch)
2017-06-02 05:47:23.743067: step 128710, loss = 0.68 (1623.0 examples/sec; 0.079 sec/batch)
2017-06-02 05:47:24.619299: step 128720, loss = 0.59 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:47:25.486106: step 128730, loss = 0.81 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:47:26.337296: step 128740, loss = 0.55 (1503.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:47:27.229417: step 128750, loss = 0.61 (1434.8 examples/sec; 0.089 sec/batch)
2017-06-02 05:47:28.091211: step 128760, loss = 0.64 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:47:28.949873: step 128770, loss = 0.67 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:47:29.812606: step 128780, loss = 0.56 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:47:30.676483: step 128790, loss = 0.63 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:47:31.646944: step 128800, loss = 0.83 (1319.0 examples/sec; 0.097 sec/batch)
2017-06-02 05:47:32.443823: step 128810, loss = 0.79 (1606.3 examples/sec; 0.080 sec/batch)
2017-06-02 05:47:33.330868: step 128820, loss = 0.65 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 05:47:34.214835: step 128830, loss = 0.78 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:47:35.091785: step 128840, loss = 0.67 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:47:35.931202: step 128850, loss = 0.68 (1524.9 examples/sec; 0.084 sec/batch)
2017-06-02 05:47:36.782382: step 128860, loss = 0.67 (1503.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:47:37.649521: step 128870, loss = 0.61 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:47:38.507677: step 128880, loss = 0.71 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:47:39.379376: step 128890, loss = 0.64 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:47:40.321175: step 128900, loss = 0.60 (1359.1 examples/sec; 0.094 sec/batch)
2017-06-02 05:47:41.100501: step 128910, loss = 0.74 (1642.5 examples/sec; 0.078 sec/batch)
2017-06-02 05:47:41.982862: step 128920, loss = 0.63 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:47:42.875102: step 128930, loss = 0.60 (1434.6 examples/sec; 0.089 sec/batch)
2017-06-02 05:47:43.756689: step 128940, loss = 0.72 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:47:44.626017: step 128950, loss = 0.58 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:47:45.478445: step 128960, loss = 0.63 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:47:46.332159: step 128970, loss = 0.69 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:47:47.196335: step 128980, loss = 0.72 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:47:48.062460: step 128990, loss = 0.69 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:47:49.038793: step 129000, loss = 0.73 (1311.0 examples/sec; 0.098 sec/batch)
2017-06-02 05:47:49.814827: step 129010, loss = 0.71 (1649.4 examples/sec; 0.078 sec/batch)
2017-06-02 05:47:50.676868: step 129020, loss = 0.73 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:47:51.527201: step 129030, loss = 0.62 (1505.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:47:52.390203: step 129040, loss = 0.69 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:47:53.246260: step 129050, loss = 0.65 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:47:54.080593: step 129060, loss = 0.57 (1534.2 examples/sec; 0.083 sec/batch)
2017-06-02 05:47:54.944797: step 129070, loss = 0.61 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:47:55.810469: step 129080, loss = 0.69 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:47:56.655677: step 129090, loss = 0.62 (1514.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:47:57.635320: step 129100, loss = 0.58 (1306.6 examples/sec; 0.098 sec/batch)
2017-06-02 05:47:58.409902: step 129110, loss = 0.61 (1652.5 examples/sec; 0.077 sec/batch)
2017-06-02 05:47:59.274264: step 129120, loss = 0.55 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:48:00.119537: step 129130, loss = 0.64 (1514.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:48:00.998284: step 129140, loss = 0.66 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:48:01.835962: step 129150, loss = 0.74 (1528.9 examples/sec; 0.084 sec/batch)
2017-06-02 05:48:02.705655: step 129160, loss = 0.71 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:48:03.554908: step 129170, loss = 0.55 (1507.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:48:04.423997: step 129180, loss = 0.80 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:48:05.292992: step 129190, loss = 0.62 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:48:06.291024: step 129200, loss = 0.62 (1282.5 examples/sec; 0.100 sec/batch)
2017-06-02 05:48:07.037042: step 129210, loss = 0.68 (1715.8 examples/sec; 0.075 sec/batch)
2017-06-02 05:48:07.922243: step 129220, loss = 0.69 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 05:48:08.800374: step 129230, loss = 0.73 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:48:09.666453: step 129240, loss = 0.64 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:48:10.532722: step 129250, loss = 0.63 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:48:11.404385: step 129260, loss = 0.53 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:48:12.269304: step 129270, loss = 0.61 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:48:13.103473: step 129280, loss = 0.82 (1534.4 examples/sec; 0.083 sec/batch)
2017-06-02 05:48:13.966889: step 129290, loss = 0.63 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:48:14.946363: step 129300, loss = 0.68 (1306.8 examples/sec; 0.098 sec/batch)
2017-06-02 05:48:15.734420: step 129310, loss = 0.72 (1624.3 examples/sec; 0.079 sec/batch)
2017-06-02 05:48:16.601774: step 129320, loss = 0.65 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:48:17.474584: step 129330, loss = 0.68 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:48:18.333762: step 129340, loss = 0.68 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:48:19.205662: step 129350, loss = 0.61 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:48:20.054931: step 129360, loss = 0.70 (1507.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:48:20.921133: step 129370, loss = 0.56 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:48:21.799201: step 129380, loss = 0.77 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:48:22.688513: step 129390, loss = 0.65 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:48:23.664602: step 129400, loss = 0.72 (1311.4 examples/sec; 0.098 sec/batch)
2017-06-02 05:48:24.429127: step 129410, loss = 0.81 (1674.3 examples/sec; 0.076 sec/batch)
2017-06-02 05:48:25.300355: step 129420, loss = 0.73 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:48:26.133445: step 129430, loss = 0.59 (1536.5 examples/sec; 0.083 sec/batch)
2017-06-02 05:48:26.992548: step 129440, loss = 0.71 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:48:27.826218: step 129450, loss = 0.58 (1535.4 examples/sec; 0.083 sec/batch)
2017-06-02 05:48:28.685019: step 129460, loss = 0.77 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:48:29.553784: step 129470, loss = 0.70 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:48:30.431984: step 129480, loss = 0.70 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:48:31.312959: step 129490, loss = 0.81 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:48:32.283141: step 129500, loss = 0.77 (1319.3 examples/sec; 0.097 sec/batch)
2017-06-02 05:48:33.076819: step 129510, loss = 0.82 (1612.8 examples/sec; 0.079 sec/batch)
2017-06-02 05:48:33.958677: step 129520, loss = 0.78 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:48:34.823514: step 129530, loss = 0.67 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:48:35.653936: step 129540, loss = 0.82 (1541.4 examples/sec; 0.083 sec/batch)
2017-06-02 05:48:36.502675: step 129550, loss = 0.72 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:48:37.380235: step 129560, loss = 0.62 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:48:38.225323: step 129570, loss = 0.83 (1514.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:48:39.085757: step 129580, loss = 0.68 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:48:39.948106: step 129590, loss = 0.66 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:48:40.929873: step 129600, loss = 0.73 (1303.8 examples/sec; 0.098 sec/batch)
2017-06-02 05:48:41.682501: step 129610, loss = 0.81 (1700.8 examples/sec; 0.075 sec/batch)
2017-06-02 05:48:42.528725: step 129620, loss = 0.73 (1512.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:48:43.403101: step 129630, loss = 0.66 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:48:44.264003: step 129640, loss = 0.67 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:48:45.127601: step 129650, loss = 0.64 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:48:45.998619: step 129660, loss = 0.61 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:48:46.878144: step 129670, loss = 0.61 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:48:47.719801: step 129680, loss = 0.56 (1520.8 examples/sec; 0.084 sec/batch)
2017-06-02 05:48:48.572172: step 129690, loss = 0.89 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:48:49.564004: step 129700, loss = 0.66 (1290.5 examples/sec; 0.099 sec/batch)
2017-06-02 05:48:50.296927: step 129710, loss = 0.63 (1746.4 examples/sec; 0.073 sec/batch)
2017-06-02 05:48:51.159792: step 129720, loss = 0.59 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:48:52.038340: step 129730, loss = 0.72 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:48:52.885513: step 129740, loss = 0.70 (1510.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:48:53.757975: step 129750, loss = 0.73 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:48:54.608903: step 129760, loss = 0.65 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:48:55.488480: step 129770, loss = 0.76 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:48:56.337511: step 129780, loss = 0.64 (1507.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:48:57.178723: step 129790, loss = 0.60 (1521.6 examples/sec; 0.084 sec/batch)
2017-06-02 05:48:58.183302: step 129800, loss = 0.78 (1274.1 examples/sec; 0.100 sec/batch)
2017-06-02 05:48:58.936413: step 129810, loss = 0.63 (1699.6 examples/sec; 0.075 sec/batch)
2017-06-02 05:48:59.824410: step 129820, loss = 0.73 (1441.5 examples/sec; 0.089 sec/batch)
2017-06-02 05:49:00.669473: step 129830, loss = 0.67 (1514.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:49:01.537917: step 129840, loss = 0.69 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:49:02.408395: step 129850, loss = 0.64 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:49:03.274147: step 129860, loss = 0.68 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:49:04.146264: step 129870, loss = 0.60 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:49:04.988182: step 129880, loss = 0.57 (1520.3 examples/sec; 0.084 sec/batch)
2017-06-02 05:49:05.856854: step 129890, loss = 0.71 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:49:06.819561: step 129900, loss = 0.62 (1329.6 examples/sec; 0.096 sec/batch)
2017-06-02 05:49:07.603952: step 129910, loss = 0.68 (1631.8 examples/sec; 0.078 sec/batch)
2017-06-02 05:49:08.460272: step 129920, loss = 0.62 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:49:09.313723: step 129930, loss = 0.79 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:49:10.183655: step 129940, loss = 0.67 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:49:11.038193: step 129950, loss = 0.73 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:49:11.886410: step 129960, loss = 0.72 (1509.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:49:12.736294: step 129970, loss = 0.63 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:49:13.597997: step 129980, loss = 0.83 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:49:14.450056: step 129990, loss = 0.54 (1502.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:49:15.406939: step 130000, loss = 0.64 (1337.7 examples/sec; 0.096 sec/batch)
2017-06-02 05:49:16.189791: step 130010, loss = 0.63 (1635.1 examples/sec; 0.078 sec/batch)
2017-06-02 05:49:17.048728: step 130020, loss = 0.70 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:49:17.946873: step 130030, loss = 0.64 (1425.2 examples/sec; 0.090 sec/batch)
2017-06-02 05:49:18.822402: step 130040, loss = 0.70 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:49:19.695446: step 130050, loss = 0.68 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:49:20.573333: step 130060, loss = 0.70 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:49:21.444675: step 130070, loss = 0.76 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:49:22.292488: step 130080, loss = 0.68 (1509.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:49:23.165637: step 130090, loss = 0.62 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:49:24.123205: step 130100, loss = 0.62 (1336.7 examples/sec; 0.096 sec/batch)
2017-06-02 05:49:24.886855: step 130110, loss = 0.58 (1676.2 examples/sec; 0.076 sec/batch)
2017-06-02 05:49:25.743078: step 130120, loss = 0.79 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:49:26.613276: step 130130, loss = 0.65 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:49:27.467434: step 130140, loss = 0.64 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:49:28.348294: step 130150, loss = 0.62 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:49:29.212850: step 130160, loss = 0.69 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:49:30.067839: step 130170, loss = 0.76 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:49:30.913813: step 130180, loss = 0.79 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:49:31.770997: step 130190, loss = 0.57 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:49:32.722522: step 130200, loss = 0.65 (1345.2 examples/sec; 0.095 sec/batch)
2017-06-02 05:49:33.496894: step 130210, loss = 0.66 (1652.9 examples/sec; 0.077 sec/batch)
2017-06-02 05:49:34.399912: step 130220, loss = 0.49 (1417.5 examples/sec; 0.090 sec/batch)
2017-06-02 05:49:35.268514: step 130230, loss = 0.66 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:49:36.125037: step 130240, loss = 0.66 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:49:36.983837: step 130250, loss = 0.57 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:49:37.833688: step 130260, loss = 0.93 (1506.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:49:38.704807: step 130270, loss = 0.77 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:49:39.593389: step 130280, loss = 0.73 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 05:49:40.482382: step 130290, loss = 0.61 (1439.8 examples/sec; 0.089 sec/batch)
2017-06-02 05:49:41.467271: step 130300, loss = 0.72 (1299.6 examples/sec; 0.098 sec/batch)
2017-06-02 05:49:42.235067: step 130310, loss = 0.72 (1667.1 examples/sec; 0.077 sec/batch)
2017-06-02 05:49:43.087690: step 130320, loss = 0.81 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:49:43.932059: step 130330, loss = 0.69 (1515.9 examples/sec; 0.084 sec/batch)
2017-06-02 05:49:44.803565: step 130340, loss = 0.58 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:49:45.674345: step 130350, loss = 0.70 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:49:46.525934: step 130360, loss = 0.78 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:49:47.376029: step 130370, loss = 0.81 (1505.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:49:48.235252: step 130380, loss = 0.72 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:49:49.109279: step 130390, loss = 0.66 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:49:50.074473: step 130400, loss = 0.53 (1326.2 examples/sec; 0.097 sec/batch)
2017-06-02 05:49:50.835209: step 130410, loss = 0.64 (1682.6 examples/sec; 0.076 sec/batch)
2017-06-02 05:49:51.797192: step 130420, loss = 0.66 (1330.6 examples/sec; 0.096 sec/batch)
2017-06-02 05:49:52.669766: step 130430, loss = 0.63 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:49:53.515304: step 130440, loss = 0.74 (1513.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:49:54.378619: step 130450, loss = 0.58 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:49:55.230559: step 130460, loss = 0.68 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:49:56.089466: step 130470, loss = 0.76 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:49:56.926001: step 130480, loss = 0.75 (1530.1 examples/sec; 0.084 sec/batch)
2017-06-02 05:49:57.770133: step 130490, loss = 0.70 (1516.4 examples/sec; 0.084 sec/batch)
2017-06-02 05:49:58.713486: step 130500, loss = 0.72 (1356.8 examples/sec; 0.094 sec/batch)
2017-06-02 05:49:59.453261: step 130510, loss = 0.63 (1730.3 examples/sec; 0.074 sec/batch)
2017-06-02 05:50:00.332507: step 130520, loss = 0.57 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:50:01.194505: step 130530, loss = 0.70 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:50:02.051505: step 130540, loss = 0.62 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:50:02.916043: step 130550, loss = 0.72 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:50:03.773499: step 130560, loss = 0.73 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:50:04.619266: step 130570, loss = 0.65 (1513.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:50:05.492577: step 130580, loss = 0.70 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:50:06.349123: step 130590, loss = 0.60 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:50:07.336205: step 130600, loss = 0.57 (1296.8 examples/sec; 0.099 sec/batch)
2017-06-02 05:50:08.125310: step 130610, loss = 0.64 (1622.1 examples/sec; 0.079 sec/batch)
2017-06-02 05:50:08.999316: step 130620, loss = 0.61 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:50:09.876016: step 130630, loss = 0.67 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:50:10.762648: step 130640, loss = 0.67 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:50:11.650656: step 130650, loss = 0.53 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:50:12.509277: step 130660, loss = 0.80 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:50:13.371977: step 130670, loss = 0.81 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:50:14.225570: step 130680, loss = 0.59 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:50:15.086250: step 130690, loss = 0.77 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:50:16.044259: step 130700, loss = 0.66 (1336.1 examples/sec; 0.096 sec/batch)
2017-06-02 05:50:16.826274: step 130710, loss = 0.69 (1636.8 examples/sec; 0.078 sec/batch)
2017-06-02 05:50:17.698204: step 130720, loss = 0.71 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:50:18.577959: step 130730, loss = 0.79 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:50:19.439632: step 130740, loss = 0.64 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:50:20.308145: step 130750, loss = 0.75 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:50:21.170564: step 130760, loss = 0.73 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:50:22.032685: step 130770, loss = 0.73 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:50:22.903625: step 130780, loss = 0.55 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:50:23.777477: step 130790, loss = 0.62 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:50:24.743105: step 130800, loss = 0.56 (1325.6 examples/sec; 0.097 sec/batch)
2017-06-02 05:50:25.541042: step 130810, loss = 0.74 (1604.1 examples/sec; 0.080 sec/batch)
2017-06-02 05:50:26.398808: step 130820, loss = 0.79 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:50:27.266594: step 130830, loss = 0.61 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:50:28.129685: step 130840, loss = 0.60 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:50:28.988073: step 130850, loss = 0.60 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:50:29.844351: step 130860, loss = 0.60 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:50:30.688464: step 130870, loss = 0.70 (1516.4 examples/sec; 0.084 sec/batch)
2017-06-02 05:50:31.561783: step 130880, loss = 0.71 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:50:32.437281: step 130890, loss = 0.55 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:50:33.401245: step 130900, loss = 0.60 (1327.8 examples/sec; 0.096 sec/batch)
2017-06-02 05:50:34.179930: step 130910, loss = 0.74 (1643.8 examples/sec; 0.078 sec/batch)
2017-06-02 05:50:35.059301: step 130920, loss = 0.65 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:50:35.936715: step 130930, loss = 0.64 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:50:36.810236: step 130940, loss = 0.65 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:50:37.661822: step 130950, loss = 0.62 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:50:38.514485: step 130960, loss = 0.61 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:50:39.353949: step 130970, loss = 0.61 (1524.8 examples/sec; 0.084 sec/batch)
2017-06-02 05:50:40.220475: step 130980, loss = 0.84 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:50:41.096711: step 130990, loss = 0.71 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:50:42.103999: step 131000, loss = 0.64 (1270.7 examples/sec; 0.101 sec/batch)
2017-06-02 05:50:42.841975: step 131010, loss = 0.82 (1734.5 examples/sec; 0.074 sec/batch)
2017-06-02 05:50:43.708197: step 131020, loss = 0.65 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:50:44.615305: step 131030, loss = 0.82 (1411.1 examples/sec; 0.091 sec/batch)
2017-06-02 05:50:45.472721: step 131040, loss = 0.78 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:50:46.341628: step 131050, loss = 0.81 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:50:47.190921: step 131060, loss = 0.66 (1507.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:50:48.052250: step 131070, loss = 0.59 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:50:48.913211: step 131080, loss = 0.77 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:50:49.781988: step 131090, loss = 0.66 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:50:50.735901: step 131100, loss = 0.78 (1341.8 examples/sec; 0.095 sec/batch)
2017-06-02 05:50:51.499717: step 131110, loss = 0.84 (1675.8 examples/sec; 0.076 sec/batch)
2017-06-02 05:50:52.374930: step 131120, loss = 0.62 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:50:53.258313: step 131130, loss = 0.79 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:50:54.143241: step 131140, loss = 0.81 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:50:55.017381: step 131150, loss = 0.57 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:50:55.895338: step 131160, loss = 0.62 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:50:56.762893: step 131170, loss = 0.69 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:50:57.629505: step 131180, loss = 0.87 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:50:58.510791: step 131190, loss = 0.80 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:50:59.492460: step 131200, loss = 0.76 (1303.9 examples/sec; 0.098 sec/batch)
2017-06-02 05:51:00.251814: step 131210, loss = 0.63 (1685.6 examples/sec; 0.076 sec/batch)
2017-06-02 05:51:01.132704: step 131220, loss = 0.75 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:51:01.997920: step 131230, loss = 0.67 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:51:02.883693: step 131240, loss = 0.54 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 05:51:03.758561: step 131250, loss = 0.71 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:51:04.624885: step 131260, loss = 0.63 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:51:05.491450: step 131270, loss = 0.59 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:51:06.375818: step 131280, loss = 0.66 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:51:07.229315: step 131290, loss = 0.78 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:51:08.185114: step 131300, loss = 0.72 (1339.2 examples/sec; 0.096 sec/batch)
2017-06-02 05:51:08.961149: step 131310, loss = 0.68 (1649.4 examples/sec; 0.078 sec/batch)
2017-06-02 05:51:09.835656: step 131320, loss = 0.52 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:51:10.715003: step 131330, loss = 0.65 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:51:11.595537: step 131340, loss = 0.62 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:51:12.449451: step 131350, loss = 0.73 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:51:13.294662: step 131360, loss = 0.71 (1514.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:51:14.142225: step 131370, loss = 0.70 (1510.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:51:14.998813: step 131380, loss = 0.62 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:51:15.869615: step 131390, loss = 0.71 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:51:16.838024: step 131400, loss = 0.64 (1321.7 examples/sec; 0.097 sec/batch)
2017-06-02 05:51:17.607855: step 131410, loss = 0.65 (1662.7 examples/sec; 0.077 sec/batch)
2017-06-02 05:51:18.449746: step 131420, loss = 0.73 (1520.4 examples/sec; 0.084 sec/batch)
2017-06-02 05:51:19.311468: step 131430, loss = 0.81 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:51:20.183045: step 131440, loss = 0.82 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:51:21.041264: step 131450, loss = 0.65 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:51:21.899213: step 131460, loss = 0.64 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:51:22.762501: step 131470, loss = 0.60 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:51:23.639361: step 131480, loss = 0.75 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:51:24.507076: step 131490, loss = 0.71 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:51:25.506208: step 131500, loss = 0.59 (1281.1 examples/sec; 0.100 sec/batch)
2017-06-02 05:51:26.285137: step 131510, loss = 0.73 (1643.3 examples/sec; 0.078 sec/batch)
2017-06-02 05:51:27.155588: step 131520, loss = 0.66 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:51:28.022853: step 131530, loss = 0.73 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:51:28.887852: step 131540, loss = 0.72 (1479.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:51:29.761208: step 131550, loss = 0.69 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:51:30.635482: step 131560, loss = 0.58 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:51:31.512296: step 131570, loss = 0.81 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:51:32.375267: step 131580, loss = 0.62 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:51:33.236109: step 131590, loss = 0.60 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:51:34.187918: step 131600, loss = 0.72 (1344.8 examples/sec; 0.095 sec/batch)
2017-06-02 05:51:34.964948: step 131610, loss = 0.63 (1647.3 examples/sec; 0.078 sec/batch)
2017-06-02 05:51:35.842542: step 131620, loss = 0.69 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:51:36.715936: step 131630, loss = 0.62 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:51:37.579723: step 131640, loss = 0.61 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:51:38.448162: step 131650, loss = 0.58 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:51:39.308401: step 131660, loss = 0.78 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:51:40.175899: step 131670, loss = 0.59 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:51:41.035392: step 131680, loss = 0.70 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:51:41.917666: step 131690, loss = 0.60 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:51:42.922823: step 131700, loss = 0.66 (1273.4 examples/sec; 0.101 sec/batch)
2017-06-02 05:51:43.651094: step 131710, loss = 0.71 (1757.6 examples/sec; 0.073 sec/batch)
2017-06-02 05:51:44.507278: step 131720, loss = 0.62 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:51:45.396374: step 131730, loss = 0.67 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:51:46.272857: step 131740, loss = 0.58 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:51:47.160561: step 131750, loss = 0.61 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 05:51:48.018295: step 131760, loss = 0.70 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:51:48.899193: step 131770, loss = 0.92 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:51:49.762207: step 131780, loss = 0.63 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:51:50.640394: step 131790, loss = 0.72 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:51:51.601062: step 131800, loss = 0.68 (1332.4 examples/sec; 0.096 sec/batch)
2017-06-02 05:51:52.392751: step 131810, loss = 0.72 (1616.8 examples/sec; 0.079 sec/batch)
2017-06-02 05:51:53.270400: step 131820, loss = 0.72 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:51:54.145428: step 131830, loss = 0.63 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:51:55.028420: step 131840, loss = 0.81 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:51:55.899362: step 131850, loss = 0.76 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:51:56.775020: step 131860, loss = 0.63 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:51:57.615207: step 131870, loss = 0.69 (1523.5 examples/sec; 0.084 sec/batch)
2017-06-02 05:51:58.463231: step 131880, loss = 0.56 (1509.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:51:59.342172: step 131890, loss = 0.74 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:52:00.302066: step 131900, loss = 0.74 (1333.5 examples/sec; 0.096 sec/batch)
2017-06-02 05:52:01.073307: step 131910, loss = 0.51 (1659.7 examples/sec; 0.077 sec/batch)
2017-06-02 05:52:01.929681: step 131920, loss = 0.60 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:52:02.788998: step 131930, loss = 0.72 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:52:03.650582: step 131940, loss = 0.59 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:52:04.504223: step 131950, loss = 0.71 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:52:05.341568: step 131960, loss = 0.60 (1528.7 examples/sec; 0.084 sec/batch)
2017-06-02 05:52:06.186047: step 131970, loss = 0.63 (1515.7 examples/sec; 0.084 sec/batch)
2017-06-02 05:52:07.046781: step 131980, loss = 0.67 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:52:07.920570: step 131990, loss = 0.84 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:52:08.916249: step 132000, loss = 0.57 (1285.6 examples/sec; 0.100 sec/batch)
2017-06-02 05:52:09.674278: step 132010, loss = 0.68 (1688.6 examples/sec; 0.076 sec/batch)
2017-06-02 05:52:10.525506: step 132020, loss = 0.65 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:52:11.408927: step 132030, loss = 0.61 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:52:12.289403: step 132040, loss = 0.74 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:52:13.144819: step 132050, loss = 0.83 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:52:14.024413: step 132060, loss = 0.59 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:52:14.899724: step 132070, loss = 0.70 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:52:15.746340: step 132080, loss = 0.79 (1511.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:52:16.608632: step 132090, loss = 0.69 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:52:17.579850: step 132100, loss = 0.74 (1317.9 examples/sec; 0.097 sec/batch)
2017-06-02 05:52:18.364433: step 132110, loss = 0.68 (1631.4 examples/sec; 0.078 sec/batch)
2017-06-02 05:52:19.242976: step 132120, loss = 0.64 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:52:20.115664: step 132130, loss = 0.66 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:52:20.985145: step 132140, loss = 0.69 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:52:21.840891: step 132150, loss = 0.67 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:52:22.706024: step 132160, loss = 0.74 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:52:23.574343: step 132170, loss = 0.69 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:52:24.436974: step 132180, loss = 0.67 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:52:25.307618: step 132190, loss = 0.76 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:52:26.266820: step 132200, loss = 0.60 (1334.4 examples/sec; 0.096 sec/batch)
2017-06-02 05:52:27.036621: step 132210, loss = 0.65 (1662.8 examples/sec; 0.077 sec/batch)
2017-06-02 05:52:27.894884: step 132220, loss = 0.80 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:52:28.755689: step 132230, loss = 0.79 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:52:29.616642: step 132240, loss = 0.72 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:52:30.481704: step 132250, loss = 0.87 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:52:31.370728: step 132260, loss = 0.57 (1439.8 examples/sec; 0.089 sec/batch)
2017-06-02 05:52:32.208976: step 132270, loss = 0.80 (1527.0 examples/sec; 0.084 sec/batch)
2017-06-02 05:52:33.085287: step 132280, loss = 0.72 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:52:33.937790: step 132290, loss = 0.74 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:52:34.876019: step 132300, loss = 0.66 (1364.3 examples/sec; 0.094 sec/batch)
2017-06-02 05:52:35.643579: step 132310, loss = 0.79 (1667.7 examples/sec; 0.077 sec/batch)
2017-06-02 05:52:36.495480: step 132320, loss = 0.87 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:52:37.358067: step 132330, loss = 0.60 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:52:38.217697: step 132340, loss = 0.62 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:52:39.106613: step 132350, loss = 0.68 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 05:52:39.952647: step 132360, loss = 0.74 (1512.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:52:40.821689: step 132370, loss = 0.62 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:52:41.706356: step 132380, loss = 0.60 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:52:42.569432: step 132390, loss = 0.63 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:52:43.581381: step 132400, loss = 0.65 (1264.9 examples/sec; 0.101 sec/batch)
2017-06-02 05:52:44.286254: step 132410, loss = 0.66 (1816.0 examples/sec; 0.070 sec/batch)
2017-06-02 05:52:45.162074: step 132420, loss = 0.57 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:52:46.030877: step 132430, loss = 0.67 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:52:46.869601: step 132440, loss = 0.66 (1526.1 examples/sec; 0.084 sec/batch)
2017-06-02 05:52:47.742778: step 132450, loss = 0.77 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:52:48.623197: step 132460, loss = 0.78 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:52:49.506210: step 132470, loss = 0.64 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:52:50.398614: step 132480, loss = 0.65 (1434.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:52:51.254636: step 132490, loss = 0.64 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:52:52.251090: step 132500, loss = 0.74 (1284.6 examples/sec; 0.100 sec/batch)
2017-06-02 05:52:53.016771: step 132510, loss = 0.61 (1671.7 examples/sec; 0.077 sec/batch)
2017-06-02 05:52:53.870113: step 132520, loss = 0.82 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:52:54.742223: step 132530, loss = 0.65 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:52:55.632494: step 132540, loss = 0.83 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 05:52:56.486974: step 132550, loss = 0.65 (1498.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:52:57.346359: step 132560, loss = 0.54 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:52:58.227681: step 132570, loss = 0.74 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:52:59.101829: step 132580, loss = 0.59 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:52:59.968456: step 132590, loss = 0.52 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:53:00.932473: step 132600, loss = 0.74 (1327.8 examples/sec; 0.096 sec/batch)
2017-06-02 05:53:01.732062: step 132610, loss = 0.66 (1600.8 examples/sec; 0.080 sec/batch)
2017-06-02 05:53:02.601928: step 132620, loss = 0.67 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:53:03.454958: step 132630, loss = 0.72 (1500.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:53:04.341769: step 132640, loss = 0.65 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:53:05.231106: step 132650, loss = 0.78 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:53:06.093983: step 132660, loss = 0.64 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:53:06.987100: step 132670, loss = 0.76 (1433.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:53:07.858711: step 132680, loss = 0.58 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:53:08.747229: step 132690, loss = 0.64 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 05:53:09.765463: step 132700, loss = 0.55 (1257.1 examples/sec; 0.102 sec/batch)
2017-06-02 05:53:10.509278: step 132710, loss = 0.58 (1720.8 examples/sec; 0.074 sec/batch)
2017-06-02 05:53:11.415514: step 132720, loss = 0.64 (1412.4 examples/sec; 0.091 sec/batch)
2017-06-02 05:53:12.296588: step 132730, loss = 0.78 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:53:13.155581: step 132740, loss = 0.66 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:53:14.019675: step 132750, loss = 0.72 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:53:14.886284: step 132760, loss = 0.79 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:53:15.763300: step 132770, loss = 0.65 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:53:16.625327: step 132780, loss = 0.52 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:53:17.500378: step 132790, loss = 0.65 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:53:18.447739: step 132800, loss = 0.78 (1351.1 examples/sec; 0.095 sec/batch)
2017-06-02 05:53:19.208019: step 132810, loss = 0.65 (1683.6 examples/sec; 0.076 sec/batch)
2017-06-02 05:53:20.028784: step 132820, loss = 0.59 (1559.5 examples/sec; 0.082 sec/batch)
2017-06-02 05:53:20.905835: step 132830, loss = 0.81 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:53:21.789709: step 132840, loss = 0.65 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:53:22.667383: step 132850, loss = 0.58 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:53:23.556381: step 132860, loss = 0.68 (1439.8 examples/sec; 0.089 sec/batch)
2017-06-02 05:53:24.423024: step 132870, loss = 0.64 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:53:25.302301: step 132880, loss = 0.63 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:53:26.149249: step 132890, loss = 0.68 (1511.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:53:27.089863: step 132900, loss = 0.58 (1360.8 examples/sec; 0.094 sec/batch)
2017-06-02 05:53:27.860714: step 132910, loss = 0.66 (1660.5 examples/sec; 0.077 sec/batch)
2017-06-02 05:53:28.716528: step 132920, loss = 0.77 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:53:29.570013: step 132930, loss = 0.63 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:53:30.449468: step 132940, loss = 0.81 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:53:31.318685: step 132950, loss = 0.63 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:53:32.188615: step 132960, loss = 0.60 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:53:33.035380: step 132970, loss = 0.52 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:53:33.897169: step 132980, loss = 0.92 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:53:34.700956: step 132990, loss = 0.85 (1592.5 examples/sec; 0.080 sec/batch)
2017-06-02 05:53:35.660216: step 133000, loss = 0.74 (1334.4 examples/sec; 0.096 sec/batch)
2017-06-02 05:53:36.420506: step 133010, loss = 0.57 (1683.6 examples/sec; 0.076 sec/batch)
2017-06-02 05:53:37.264897: step 133020, loss = 0.75 (1515.9 examples/sec; 0.084 sec/batch)
2017-06-02 05:53:38.119022: step 133030, loss = 0.67 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:53:38.974319: step 133040, loss = 0.61 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:53:39.847713: step 133050, loss = 0.81 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:53:40.719589: step 133060, loss = 0.69 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:53:41.572078: step 133070, loss = 0.83 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:53:42.441979: step 133080, loss = 0.76 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:53:43.322136: step 133090, loss = 0.65 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:53:44.267999: step 133100, loss = 0.63 (1353.3 examples/sec; 0.095 sec/batch)
2017-06-02 05:53:45.025444: step 133110, loss = 0.61 (1689.9 examples/sec; 0.076 sec/batch)
2017-06-02 05:53:45.866440: step 133120, loss = 0.57 (1522.0 examples/sec; 0.084 sec/batch)
2017-06-02 05:53:46.711315: step 133130, loss = 0.75 (1515.0 examples/sec; 0.084 sec/batch)
2017-06-02 05:53:47.587379: step 133140, loss = 0.69 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:53:48.433855: step 133150, loss = 0.67 (1512.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:53:49.303078: step 133160, loss = 0.76 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:53:50.150717: step 133170, loss = 0.61 (1510.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:53:51.016407: step 133180, loss = 0.63 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:53:51.863644: step 133190, loss = 0.63 (1510.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:53:52.833149: step 133200, loss = 0.72 (1320.3 examples/sec; 0.097 sec/batch)
2017-06-02 05:53:53.596512: step 133210, loss = 0.63 (1676.8 examples/sec; 0.076 sec/batch)
2017-06-02 05:53:54.463972: step 133220, loss = 0.83 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:53:55.319201: step 133230, loss = 0.47 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:53:56.184399: step 133240, loss = 0.75 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:53:57.047331: step 133250, loss = 0.60 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:53:57.910471: step 133260, loss = 0.69 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:53:58.774920: step 133270, loss = 0.74 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:53:59.645677: step 133280, loss = 0.75 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:54:00.508316: step 133290, loss = 0.88 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:54:01.475535: step 133300, loss = 0.73 (1323.4 examples/sec; 0.097 sec/batch)
2017-06-02 05:54:02.221287: step 133310, loss = 0.71 (1716.4 examples/sec; 0.075 sec/batch)
2017-06-02 05:54:03.076385: step 133320, loss = 0.78 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:54:03.973804: step 133330, loss = 0.73 (1426.3 examples/sec; 0.090 sec/batch)
2017-06-02 05:54:04.806128: step 133340, loss = 0.66 (1537.9 examples/sec; 0.083 sec/batch)
2017-06-02 05:54:05.654064: step 133350, loss = 0.69 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:54:06.509959: step 133360, loss = 0.73 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:54:07.384025: step 133370, loss = 0.87 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:54:08.235296: step 133380, loss = 0.72 (1503.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:54:09.109895: step 133390, loss = 0.60 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:54:10.060882: step 133400, loss = 0.77 (1346.0 examples/sec; 0.095 sec/batch)
2017-06-02 05:54:10.826035: step 133410, loss = 0.77 (1672.9 examples/sec; 0.077 sec/batch)
2017-06-02 05:54:11.719222: step 133420, loss = 0.63 (1433.1 examples/sec; 0.089 sec/batch)
2017-06-02 05:54:12.591316: step 133430, loss = 0.70 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:54:13.470053: step 133440, loss = 0.74 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:54:14.345298: step 133450, loss = 0.66 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:54:15.190328: step 133460, loss = 0.69 (1514.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:54:16.051201: step 133470, loss = 0.67 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:54:16.918696: step 133480, loss = 0.71 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:54:17.782397: step 133490, loss = 0.76 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:54:18.757440: step 133500, loss = 0.61 (1312.8 examples/sec; 0.098 sec/batch)
2017-06-02 05:54:19.542766: step 133510, loss = 0.62 (1629.9 examples/sec; 0.079 sec/batch)
2017-06-02 05:54:20.410636: step 133520, loss = 0.63 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:54:21.273186: step 133530, loss = 0.71 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:54:22.167356: step 133540, loss = 0.56 (1431.5 examples/sec; 0.089 sec/batch)
2017-06-02 05:54:23.050406: step 133550, loss = 0.61 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:54:23.925487: step 133560, loss = 0.62 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:54:24.775774: step 133570, loss = 0.60 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:54:25.641614: step 133580, loss = 0.77 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:54:26.494493: step 133590, loss = 0.68 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:54:27.436015: step 133600, loss = 0.71 (1359.5 examples/sec; 0.094 sec/batch)
2017-06-02 05:54:28.212968: step 133610, loss = 0.62 (1647.4 examples/sec; 0.078 sec/batch)
2017-06-02 05:54:29.084081: step 133620, loss = 0.55 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:54:29.967722: step 133630, loss = 0.68 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:54:30.866693: step 133640, loss = 0.64 (1423.8 examples/sec; 0.090 sec/batch)
2017-06-02 05:54:31.750744: step 133650, loss = 0.64 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:54:32.625944: step 133660, loss = 0.53 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:54:33.510756: step 133670, loss = 0.60 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:54:34.373859: step 133680, loss = 0.63 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:54:35.262001: step 133690, loss = 0.67 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:54:36.250607: step 133700, loss = 0.74 (1294.7 examples/sec; 0.099 sec/batch)
2017-06-02 05:54:37.045308: step 133710, loss = 0.52 (1610.7 examples/sec; 0.079 sec/batch)
2017-06-02 05:54:37.926184: step 133720, loss = 0.69 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:54:38.814295: step 133730, loss = 0.66 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:54:39.702349: step 133740, loss = 0.71 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:54:40.580459: step 133750, loss = 0.85 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:54:41.447706: step 133760, loss = 0.82 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:54:42.301116: step 133770, loss = 0.73 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:54:43.180454: step 133780, loss = 0.61 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:54:44.062121: step 133790, loss = 0.69 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:54:45.053884: step 133800, loss = 0.63 (1290.6 examples/sec; 0.099 sec/batch)
2017-06-02 05:54:45.851317: step 133810, loss = 0.64 (1605.2 examples/sec; 0.080 sec/batch)
2017-06-02 05:54:46.730859: step 133820, loss = 0.66 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:54:47.598906: step 133830, loss = 0.64 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:54:48.496846: step 133840, loss = 0.75 (1425.5 examples/sec; 0.090 sec/batch)
2017-06-02 05:54:49.356435: step 133850, loss = 0.70 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:54:50.234281: step 133860, loss = 0.63 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:54:51.112305: step 133870, loss = 0.50 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:54:52.006202: step 133880, loss = 0.65 (1431.9 examples/sec; 0.089 sec/batch)
2017-06-02 05:54:52.897712: step 133890, loss = 0.56 (1435.8 examples/sec; 0.089 sec/batch)
2017-06-02 05:54:53.901904: step 133900, loss = 0.68 (1274.7 examples/sec; 0.100 sec/batch)
2017-06-02 05:54:54.696225: step 133910, loss = 0.66 (1611.5 examples/sec; 0.079 sec/batch)
2017-06-02 05:54:55.566680: step 133920, loss = 0.78 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:54:56.458029: step 133930, loss = 0.67 (1436.0 examples/sec; 0.089 sec/batch)
2017-06-02 05:54:57.337271: step 133940, loss = 0.76 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:54:58.209239: step 133950, loss = 0.64 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:54:59.096478: step 133960, loss = 0.70 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 05:54:59.979401: step 133970, loss = 0.71 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:55:00.840564: step 133980, loss = 0.73 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:55:01.709067: step 133990, loss = 0.58 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:55:02.675657: step 134000, loss = 0.71 (1324.2 examples/sec; 0.097 sec/batch)
2017-06-02 05:55:03.468659: step 134010, loss = 0.82 (1614.1 examples/sec; 0.079 sec/batch)
2017-06-02 05:55:04.327227: step 134020, loss = 0.55 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:55:05.190665: step 134030, loss = 0.71 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:55:06.076228: step 134040, loss = 0.60 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:55:06.946221: step 134050, loss = 0.73 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:55:07.821055: step 134060, loss = 0.78 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:55:08.689984: step 134070, loss = 0.72 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:55:09.531251: step 134080, loss = 0.65 (1521.5 examples/sec; 0.084 sec/batch)
2017-06-02 05:55:10.381954: step 134090, loss = 0.74 (1504.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:55:11.350394: step 134100, loss = 0.60 (1321.7 examples/sec; 0.097 sec/batch)
2017-06-02 05:55:12.119901: step 134110, loss = 0.66 (1663.4 examples/sec; 0.077 sec/batch)
2017-06-02 05:55:12.988119: step 134120, loss = 0.67 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:55:13.866340: step 134130, loss = 0.54 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:55:14.767174: step 134140, loss = 0.63 (1420.9 examples/sec; 0.090 sec/batch)
2017-06-02 05:55:15.615201: step 134150, loss = 0.83 (1509.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:55:16.486268: step 134160, loss = 0.79 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:55:17.361792: step 134170, loss = 0.71 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:55:18.213965: step 134180, loss = 0.63 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:55:19.086527: step 134190, loss = 0.69 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:55:20.046525: step 134200, loss = 0.63 (1333.3 examples/sec; 0.096 sec/batch)
2017-06-02 05:55:20.826501: step 134210, loss = 0.73 (1641.1 examples/sec; 0.078 sec/batch)
2017-06-02 05:55:21.723147: step 134220, loss = 0.77 (1427.6 examples/sec; 0.090 sec/batch)
2017-06-02 05:55:22.607295: step 134230, loss = 0.87 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:55:23.456938: step 134240, loss = 0.77 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:55:24.312940: step 134250, loss = 0.67 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:55:25.155287: step 134260, loss = 0.68 (1519.6 examples/sec; 0.084 sec/batch)
2017-06-02 05:55:26.003637: step 134270, loss = 0.83 (1508.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:55:26.861815: step 134280, loss = 0.86 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:55:27.717816: step 134290, loss = 0.60 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:55:28.680627: step 134300, loss = 0.73 (1329.4 examples/sec; 0.096 sec/batch)
2017-06-02 05:55:29.448231: step 134310, loss = 0.61 (1667.5 examples/sec; 0.077 sec/batch)
2017-06-02 05:55:30.336846: step 134320, loss = 0.58 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:55:31.202932: step 134330, loss = 0.61 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:55:32.075277: step 134340, loss = 0.77 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:55:32.934332: step 134350, loss = 0.56 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:55:33.794556: step 134360, loss = 0.77 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:55:34.631378: step 134370, loss = 0.61 (1529.6 examples/sec; 0.084 sec/batch)
2017-06-02 05:55:35.523245: step 134380, loss = 0.69 (1435.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:55:36.358094: step 134390, loss = 0.68 (1533.2 examples/sec; 0.083 sec/batch)
2017-06-02 05:55:37.303912: step 134400, loss = 0.66 (1353.3 examples/sec; 0.095 sec/batch)
2017-06-02 05:55:38.095595: step 134410, loss = 0.61 (1616.8 examples/sec; 0.079 sec/batch)
2017-06-02 05:55:38.947799: step 134420, loss = 0.65 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:55:39.822615: step 134430, loss = 0.73 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:55:40.685836: step 134440, loss = 0.66 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:55:41.554721: step 134450, loss = 0.60 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:55:42.416512: step 134460, loss = 0.75 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:55:43.289175: step 134470, loss = 0.87 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:55:44.168454: step 134480, loss = 0.58 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:55:45.026389: step 134490, loss = 0.68 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:55:45.964116: step 134500, loss = 0.66 (1365.0 examples/sec; 0.094 sec/batch)
2017-06-02 05:55:46.730325: step 134510, loss = 0.58 (1670.6 examples/sec; 0.077 sec/batch)
2017-06-02 05:55:47.609225: step 134520, loss = 0.77 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:55:48.451100: step 134530, loss = 0.70 (1520.4 examples/sec; 0.084 sec/batch)
2017-06-02 05:55:49.341480: step 134540, loss = 0.77 (1437.6 examples/sec; 0.089 sec/batch)
2017-06-02 05:55:50.218162: step 134550, loss = 0.67 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:55:51.099099: step 134560, loss = 0.65 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:55:51.985983: step 134570, loss = 0.58 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:55:52.871160: step 134580, loss = 0.58 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 05:55:53.755753: step 134590, loss = 0.58 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:55:54.734132: step 134600, loss = 0.71 (1308.3 examples/sec; 0.098 sec/batch)
2017-06-02 05:55:55.507234: step 134610, loss = 0.63 (1655.7 examples/sec; 0.077 sec/batch)
2017-06-02 05:55:56.404750: step 134620, loss = 0.60 (1426.1 examples/sec; 0.090 sec/batch)
2017-06-02 05:55:57.269960: step 134630, loss = 0.77 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:55:58.142582: step 134640, loss = 0.61 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:55:59.008201: step 134650, loss = 0.50 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:55:59.870078: step 134660, loss = 0.67 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:56:00.715665: step 134670, loss = 0.71 (1513.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:56:01.577404: step 134680, loss = 0.64 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:56:02.439159: step 134690, loss = 0.77 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:56:03.408878: step 134700, loss = 0.64 (1320.0 examples/sec; 0.097 sec/batch)
2017-06-02 05:56:04.181196: step 134710, loss = 0.81 (1657.3 examples/sec; 0.077 sec/batch)
2017-06-02 05:56:05.047967: step 134720, loss = 0.63 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:56:05.899285: step 134730, loss = 0.63 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:56:06.738939: step 134740, loss = 0.62 (1524.4 examples/sec; 0.084 sec/batch)
2017-06-02 05:56:07.638208: step 134750, loss = 0.69 (1423.4 examples/sec; 0.090 sec/batch)
2017-06-02 05:56:08.530255: step 134760, loss = 0.68 (1434.9 examples/sec; 0.089 sec/batch)
2017-06-02 05:56:09.423414: step 134770, loss = 0.68 (1433.1 examples/sec; 0.089 sec/batch)
2017-06-02 05:56:10.268047: step 134780, loss = 0.64 (1515.4 examples/sec; 0.084 sec/batch)
2017-06-02 05:56:11.150690: step 134790, loss = 0.72 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:56:12.178277: step 134800, loss = 0.63 (1245.6 examples/sec; 0.103 sec/batch)
2017-06-02 05:56:12.885083: step 134810, loss = 0.52 (1811.0 examples/sec; 0.071 sec/batch)
2017-06-02 05:56:13.753788: step 134820, loss = 0.64 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:56:14.610125: step 134830, loss = 0.70 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:56:15.487827: step 134840, loss = 0.59 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:56:16.379566: step 134850, loss = 0.82 (1435.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:56:17.273881: step 134860, loss = 0.56 (1431.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:56:18.154017: step 134870, loss = 0.63 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:56:19.030944: step 134880, loss = 0.57 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:56:19.912758: step 134890, loss = 0.58 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:56:20.890881: step 134900, loss = 0.74 (1308.6 examples/sec; 0.098 sec/batch)
2017-06-02 05:56:21.677311: step 134910, loss = 0.64 (1627.6 examples/sec; 0.079 sec/batch)
2017-06-02 05:56:22.551047: step 134920, loss = 0.67 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:56:23.415950: step 134930, loss = 0.67 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:56:24.284077: step 134940, loss = 0.65 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:56:25.178062: step 134950, loss = 0.64 (1431.8 examples/sec; 0.089 sec/batch)
2017-06-02 05:56:26.050061: step 134960, loss = 0.77 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:56:26.934678: step 134970, loss = 0.60 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:56:27.807130: step 134980, loss = 0.72 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:56:28.703912: step 134990, loss = 0.55 (1427.3 examples/sec; 0.090 sec/batch)
2017-06-02 05:56:29.695700: step 135000, loss = 0.58 (1290.6 examples/sec; 0.099 sec/batch)
2017-06-02 05:56:30.451044: step 135010, loss = 0.74 (1694.6 examples/sec; 0.076 sec/batch)
2017-06-02 05:56:31.328182: step 135020, loss = 0.63 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:56:32.190048: step 135030, loss = 0.68 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:56:33.085305: step 135040, loss = 0.63 (1429.7 examples/sec; 0.090 sec/batch)
2017-06-02 05:56:33.945790: step 135050, loss = 0.54 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:56:34.808381: step 135060, loss = 0.69 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:56:35.665795: step 135070, loss = 0.71 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:56:36.507359: step 135080, loss = 0.84 (1521.0 examples/sec; 0.084 sec/batch)
2017-06-02 05:56:37.363140: step 135090, loss = 0.72 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:56:38.347605: step 135100, loss = 0.64 (1300.2 examples/sec; 0.098 sec/batch)
2017-06-02 05:56:39.131153: step 135110, loss = 0.63 (1633.6 examples/sec; 0.078 sec/batch)
2017-06-02 05:56:40.036225: step 135120, loss = 0.60 (1414.3 examples/sec; 0.091 sec/batch)
2017-06-02 05:56:40.919110: step 135130, loss = 0.68 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:56:41.790791: step 135140, loss = 0.81 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:56:42.678071: step 135150, loss = 0.74 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 05:56:43.568879: step 135160, loss = 0.93 (1436.9 examples/sec; 0.089 sec/batch)
2017-06-02 05:56:44.452938: step 135170, loss = 0.88 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:56:45.348455: step 135180, loss = 0.73 (1429.4 examples/sec; 0.090 sec/batch)
2017-06-02 05:56:46.207153: step 135190, loss = 0.78 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:56:47.218149: step 135200, loss = 0.64 (1266.1 examples/sec; 0.101 sec/batch)
2017-06-02 05:56:47.985758: step 135210, loss = 0.66 (1667.6 examples/sec; 0.077 sec/batch)
2017-06-02 05:56:48.850757: step 135220, loss = 0.89 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:56:49.726160: step 135230, loss = 0.73 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:56:50.579682: step 135240, loss = 0.67 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 05:56:51.444870: step 135250, loss = 0.62 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:56:52.321857: step 135260, loss = 0.76 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:56:53.201724: step 135270, loss = 0.78 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:56:54.060353: step 135280, loss = 0.54 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:56:54.931409: step 135290, loss = 0.81 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:56:55.899443: step 135300, loss = 0.62 (1322.3 examples/sec; 0.097 sec/batch)
2017-06-02 05:56:56.679920: step 135310, loss = 0.63 (1640.1 examples/sec; 0.078 sec/batch)
2017-06-02 05:56:57.562213: step 135320, loss = 0.58 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:56:58.430075: step 135330, loss = 0.65 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:56:59.313841: step 135340, loss = 0.78 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:57:00.175781: step 135350, loss = 0.66 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:57:01.046267: step 135360, loss = 0.75 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:57:01.904777: step 135370, loss = 0.70 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:57:02.735385: step 135380, loss = 0.61 (1541.0 examples/sec; 0.083 sec/batch)
2017-06-02 05:57:03.604359: step 135390, loss = 0.62 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:57:04.544261: step 135400, loss = 0.72 (1361.8 examples/sec; 0.094 sec/batch)
2017-06-02 05:57:05.312887: step 135410, loss = 0.69 (1665.3 examples/sec; 0.077 sec/batch)
2017-06-02 05:57:06.170639: step 135420, loss = 0.74 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:57:07.022849: step 135430, loss = 0.66 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:57:07.866054: step 135440, loss = 0.69 (1518.0 examples/sec; 0.084 sec/batch)
2017-06-02 05:57:08.726182: step 135450, loss = 0.81 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 05:57:09.604336: step 135460, loss = 0.67 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:57:10.464507: step 135470, loss = 0.75 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:57:11.329455: step 135480, loss = 0.71 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:57:12.193496: step 135490, loss = 0.86 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:57:13.156801: step 135500, loss = 0.63 (1328.8 examples/sec; 0.096 sec/batch)
2017-06-02 05:57:13.922555: step 135510, loss = 0.76 (1671.6 examples/sec; 0.077 sec/batch)
2017-06-02 05:57:14.788308: step 135520, loss = 0.77 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:57:15.645246: step 135530, loss = 0.72 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:57:16.496317: step 135540, loss = 0.64 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 05:57:17.367006: step 135550, loss = 0.69 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:57:18.228013: step 135560, loss = 0.66 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:57:19.116068: step 135570, loss = 0.80 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:57:20.003006: step 135580, loss = 0.51 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:57:20.860131: step 135590, loss = 0.69 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:57:21.796648: step 135600, loss = 0.65 (1366.8 examples/sec; 0.094 sec/batch)
2017-06-02 05:57:22.587800: step 135610, loss = 0.60 (1617.9 examples/sec; 0.079 sec/batch)
2017-06-02 05:57:23.464146: step 135620, loss = 0.57 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:57:24.324598: step 135630, loss = 0.67 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 05:57:25.176920: step 135640, loss = 0.77 (1501.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:57:26.041251: step 135650, loss = 0.60 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:57:26.938901: step 135660, loss = 0.63 (1426.0 examples/sec; 0.090 sec/batch)
2017-06-02 05:57:27.817291: step 135670, loss = 0.77 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:57:28.671682: step 135680, loss = 0.68 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:57:29.562208: step 135690, loss = 0.61 (1437.4 examples/sec; 0.089 sec/batch)
2017-06-02 05:57:30.518843: step 135700, loss = 1.03 (1338.0 examples/sec; 0.096 sec/batch)
2017-06-02 05:57:31.282661: step 135710, loss = 0.69 (1675.8 examples/sec; 0.076 sec/batch)
2017-06-02 05:57:32.154618: step 135720, loss = 0.51 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:57:33.015116: step 135730, loss = 0.74 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:57:33.837920: step 135740, loss = 0.65 (1555.7 examples/sec; 0.082 sec/batch)
2017-06-02 05:57:34.694418: step 135750, loss = 0.74 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:57:35.544846: step 135760, loss = 0.74 (1505.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:57:36.398595: step 135770, loss = 0.62 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 05:57:37.287789: step 135780, loss = 0.60 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 05:57:38.159430: step 135790, loss = 0.71 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 05:57:39.163352: step 135800, loss = 0.72 (1275.0 examples/sec; 0.100 sec/batch)
2017-06-02 05:57:39.912798: step 135810, loss = 0.53 (1708.0 examples/sec; 0.075 sec/batch)
2017-06-02 05:57:40.773710: step 135820, loss = 0.64 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:57:41.656625: step 135830, loss = 0.61 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:57:42.499993: step 135840, loss = 0.60 (1517.7 examples/sec; 0.084 sec/batch)
2017-06-02 05:57:43.360322: step 135850, loss = 0.79 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:57:44.244800: step 135860, loss = 0.79 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:57:45.111588: step 135870, loss = 0.88 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:57:45.977645: step 135880, loss = 0.69 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:57:46.843054: step 135890, loss = 0.72 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:57:47.774785: step 135900, loss = 0.72 (1373.8 examples/sec; 0.093 sec/batch)
2017-06-02 05:57:48.511281: step 135910, loss = 0.68 (1738.0 examples/sec; 0.074 sec/batch)
2017-06-02 05:57:49.364081: step 135920, loss = 0.65 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 05:57:50.235937: step 135930, loss = 0.78 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:57:51.121533: step 135940, loss = 0.77 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:57:51.953701: step 135950, loss = 0.66 (1538.1 examples/sec; 0.083 sec/batch)
2017-06-02 05:57:52.802067: step 135960, loss = 0.81 (1508.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:57:53.684186: step 135970, loss = 0.58 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:57:54.542438: step 135980, loss = 0.70 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:57:55.381932: step 135990, loss = 0.65 (1524.7 examples/sec; 0.084 sec/batch)
2017-06-02 05:57:56.336487: step 136000, loss = 0.80 (1340.9 examples/sec; 0.095 sec/batch)
2017-06-02 05:57:57.113186: step 136010, loss = 0.58 (1648.0 examples/sec; 0.078 sec/batch)
2017-06-02 05:57:57.984358: step 136020, loss = 0.68 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:57:58.830832: step 136030, loss = 0.76 (1512.2 examples/sec; 0.085 sec/batch)
2017-06-02 05:57:59.704321: step 136040, loss = 0.63 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:58:00.568070: step 136050, loss = 0.57 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:58:01.444710: step 136060, loss = 0.68 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:58:02.294339: step 136070, loss = 0.71 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 05:58:03.157022: step 136080, loss = 0.71 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:58:04.036010: step 136090, loss = 0.74 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:58:05.020019: step 136100, loss = 0.57 (1300.8 examples/sec; 0.098 sec/batch)
2017-06-02 05:58:05.792436: step 136110, loss = 0.65 (1657.1 examples/sec; 0.077 sec/batch)
2017-06-02 05:58:06.655619: step 136120, loss = 0.57 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:58:07.531530: step 136130, loss = 0.79 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:58:08.406048: step 136140, loss = 0.62 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:58:09.293997: step 136150, loss = 0.74 (1441.5 examples/sec; 0.089 sec/batch)
2017-06-02 05:58:10.192858: step 136160, loss = 0.81 (1424.0 examples/sec; 0.090 sec/batch)
2017-06-02 05:58:11.073712: step 136170, loss = 0.67 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:58:11.962424: step 136180, loss = 0.66 (1440.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:58:12.844536: step 136190, loss = 0.66 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:58:13.783328: step 136200, loss = 0.73 (1363.5 examples/sec; 0.094 sec/batch)
2017-06-02 05:58:14.583566: step 136210, loss = 0.72 (1599.5 examples/sec; 0.080 sec/batch)
2017-06-02 05:58:15.434991: step 136220, loss = 0.64 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 05:58:16.307611: step 136230, loss = 0.55 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:58:17.175233: step 136240, loss = 0.67 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:58:18.041949: step 136250, loss = 0.61 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:58:18.903034: step 136260, loss = 0.58 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:58:19.767765: step 136270, loss = 0.58 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:58:20.637525: step 136280, loss = 0.57 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 05:58:21.497585: step 136290, loss = 0.81 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:58:22.464585: step 136300, loss = 0.61 (1323.7 examples/sec; 0.097 sec/batch)
2017-06-02 05:58:23.204830: step 136310, loss = 0.92 (1729.2 examples/sec; 0.074 sec/batch)
2017-06-02 05:58:24.071358: step 136320, loss = 0.68 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:58:24.925795: step 136330, loss = 0.82 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:58:25.792843: step 136340, loss = 0.77 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:58:26.682268: step 136350, loss = 0.62 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 05:58:27.565561: step 136360, loss = 0.72 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:58:28.430490: step 136370, loss = 0.61 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:58:29.300554: step 136380, loss = 0.51 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:58:30.146679: step 136390, loss = 0.52 (1512.8 examples/sec; 0.085 sec/batch)
2017-06-02 05:58:31.112158: step 136400, loss = 0.77 (1325.8 examples/sec; 0.097 sec/batch)
2017-06-02 05:58:31.900817: step 136410, loss = 0.61 (1623.0 examples/sec; 0.079 sec/batch)
2017-06-02 05:58:32.790560: step 136420, loss = 0.76 (1438.6 examples/sec; 0.089 sec/batch)
2017-06-02 05:58:33.662045: step 136430, loss = 0.72 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:58:34.523675: step 136440, loss = 0.56 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:58:35.391945: step 136450, loss = 0.88 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:58:36.257805: step 136460, loss = 0.67 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 05:58:37.142843: step 136470, loss = 0.72 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:58:38.043447: step 136480, loss = 0.70 (1421.3 examples/sec; 0.090 sec/batch)
2017-06-02 05:58:38.907424: step 136490, loss = 0.65 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:58:39.857031: step 136500, loss = 0.71 (1347.9 examples/sec; 0.095 sec/batch)
2017-06-02 05:58:40.648336: step 136510, loss = 0.77 (1617.6 examples/sec; 0.079 sec/batch)
2017-06-02 05:58:41.515490: step 136520, loss = 0.71 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:58:42.388727: step 136530, loss = 0.76 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:58:43.250082: step 136540, loss = 0.81 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:58:44.119525: step 136550, loss = 0.68 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 05:58:44.967684: step 136560, loss = 0.67 (1509.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:58:45.862267: step 136570, loss = 0.75 (1430.8 examples/sec; 0.089 sec/batch)
2017-06-02 05:58:46.737323: step 136580, loss = 0.85 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:58:47.608156: step 136590, loss = 0.63 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:58:48.570516: step 136600, loss = 0.75 (1330.1 examples/sec; 0.096 sec/batch)
2017-06-02 05:58:49.318997: step 136610, loss = 0.55 (1710.1 examples/sec; 0.075 sec/batch)
2017-06-02 05:58:50.209242: step 136620, loss = 0.62 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 05:58:51.095165: step 136630, loss = 0.68 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 05:58:51.981555: step 136640, loss = 0.72 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 05:58:52.860228: step 136650, loss = 0.59 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:58:53.729843: step 136660, loss = 0.73 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 05:58:54.610809: step 136670, loss = 0.60 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:58:55.492231: step 136680, loss = 0.85 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:58:56.380409: step 136690, loss = 0.76 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 05:58:57.353143: step 136700, loss = 0.79 (1315.9 examples/sec; 0.097 sec/batch)
2017-06-02 05:58:58.135331: step 136710, loss = 0.54 (1636.4 examples/sec; 0.078 sec/batch)
2017-06-02 05:58:59.025292: step 136720, loss = 0.58 (1438.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:58:59.888877: step 136730, loss = 0.56 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:59:00.771617: step 136740, loss = 0.81 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 05:59:01.654142: step 136750, loss = 0.67 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:59:02.544156: step 136760, loss = 0.66 (1438.1 examples/sec; 0.089 sec/batch)
2017-06-02 05:59:03.417050: step 136770, loss = 0.66 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:59:04.293453: step 136780, loss = 0.63 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 05:59:05.161230: step 136790, loss = 0.75 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:59:06.164079: step 136800, loss = 0.51 (1276.4 examples/sec; 0.100 sec/batch)
2017-06-02 05:59:06.907447: step 136810, loss = 0.64 (1721.9 examples/sec; 0.074 sec/batch)
2017-06-02 05:59:07.749217: step 136820, loss = 0.70 (1520.6 examples/sec; 0.084 sec/batch)
2017-06-02 05:59:08.609021: step 136830, loss = 0.55 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:59:09.462621: step 136840, loss = 0.64 (1499.6 examples/sec; 0.085 sec/batch)
2017-06-02 05:59:10.358239: step 136850, loss = 0.64 (1429.2 examples/sec; 0.090 sec/batch)
2017-06-02 05:59:11.227557: step 136860, loss = 0.56 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:59:12.088352: step 136870, loss = 0.67 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:59:12.950575: step 136880, loss = 0.57 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 05:59:13.814922: step 136890, loss = 0.47 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 05:59:14.773443: step 136900, loss = 0.71 (1335.4 examples/sec; 0.096 sec/batch)
2017-06-02 05:59:15.565700: step 136910, loss = 0.54 (1615.6 examples/sec; 0.079 sec/batch)
2017-06-02 05:59:16.412206: step 136920, loss = 0.52 (1512.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:59:17.267586: step 136930, loss = 0.53 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 05:59:18.117463: step 136940, loss = 0.58 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 05:59:19.008194: step 136950, loss = 0.67 (1437.0 examples/sec; 0.089 sec/batch)
2017-06-02 05:59:19.893674: step 136960, loss = 0.63 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 05:59:20.726187: step 136970, loss = 0.64 (1537.5 examples/sec; 0.083 sec/batch)
2017-06-02 05:59:21.592587: step 136980, loss = 0.67 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 05:59:22.458749: step 136990, loss = 0.54 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:59:23.432863: step 137000, loss = 0.56 (1314.0 examples/sec; 0.097 sec/batch)
2017-06-02 05:59:24.220010: step 137010, loss = 0.65 (1626.1 examples/sec; 0.079 sec/batch)
2017-06-02 05:59:25.098184: step 137020, loss = 0.56 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 05:59:25.976613: step 137030, loss = 0.57 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 05:59:26.856079: step 137040, loss = 0.54 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 05:59:27.737421: step 137050, loss = 0.59 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:59:28.610670: step 137060, loss = 0.66 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:59:29.471937: step 137070, loss = 0.46 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 05:59:30.349298: step 137080, loss = 0.72 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 05:59:31.226817: step 137090, loss = 0.55 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 05:59:32.191380: step 137100, loss = 0.56 (1327.0 examples/sec; 0.096 sec/batch)
2017-06-02 05:59:32.983702: step 137110, loss = 0.47 (1615.5 examples/sec; 0.079 sec/batch)
2017-06-02 05:59:33.865995: step 137120, loss = 0.58 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:59:34.736917: step 137130, loss = 0.52 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:59:35.604878: step 137140, loss = 0.55 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 05:59:36.482645: step 137150, loss = 0.60 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 05:59:37.373325: step 137160, loss = 0.60 (1437.1 examples/sec; 0.089 sec/batch)
2017-06-02 05:59:38.260386: step 137170, loss = 0.55 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 05:59:39.154877: step 137180, loss = 0.50 (1431.0 examples/sec; 0.089 sec/batch)
2017-06-02 05:59:40.039565: step 137190, loss = 0.52 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 05:59:41.012129: step 137200, loss = 0.50 (1316.1 examples/sec; 0.097 sec/batch)
2017-06-02 05:59:41.789058: step 137210, loss = 0.49 (1647.5 examples/sec; 0.078 sec/batch)
2017-06-02 05:59:42.683524: step 137220, loss = 0.47 (1431.0 examples/sec; 0.089 sec/batch)
2017-06-02 05:59:43.557349: step 137230, loss = 0.61 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 05:59:44.445437: step 137240, loss = 0.64 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 05:59:45.323758: step 137250, loss = 0.69 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 05:59:46.192141: step 137260, loss = 0.57 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:59:47.033931: step 137270, loss = 0.58 (1520.6 examples/sec; 0.084 sec/batch)
2017-06-02 05:59:47.892591: step 137280, loss = 0.49 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 05:59:48.751201: step 137290, loss = 0.55 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:59:49.718734: step 137300, loss = 0.47 (1322.9 examples/sec; 0.097 sec/batch)
2017-06-02 05:59:50.500779: step 137310, loss = 0.47 (1636.7 examples/sec; 0.078 sec/batch)
2017-06-02 05:59:51.461596: step 137320, loss = 0.58 (1332.2 examples/sec; 0.096 sec/batch)
2017-06-02 05:59:52.284186: step 137330, loss = 0.57 (1556.1 examples/sec; 0.082 sec/batch)
2017-06-02 05:59:53.144809: step 137340, loss = 0.52 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 05:59:54.002157: step 137350, loss = 0.64 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 05:59:54.865382: step 137360, loss = 0.63 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 05:59:55.739651: step 137370, loss = 0.56 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 05:59:56.611508: step 137380, loss = 0.58 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 05:59:57.454171: step 137390, loss = 0.62 (1519.0 examples/sec; 0.084 sec/batch)
2017-06-02 05:59:58.403576: step 137400, loss = 0.55 (1348.2 examples/sec; 0.095 sec/batch)
2017-06-02 05:59:59.238149: step 137410, loss = 0.57 (1533.7 examples/sec; 0.083 sec/batch)
2017-06-02 06:00:00.107833: step 137420, loss = 0.62 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:00:01.006814: step 137430, loss = 0.57 (1423.8 examples/sec; 0.090 sec/batch)
2017-06-02 06:00:01.879533: step 137440, loss = 0.51 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:00:02.767446: step 137450, loss = 0.50 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 06:00:03.633623: step 137460, loss = 0.60 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:00:04.485697: step 137470, loss = 0.63 (1502.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:00:05.327390: step 137480, loss = 0.56 (1520.7 examples/sec; 0.084 sec/batch)
2017-06-02 06:00:06.213047: step 137490, loss = 0.57 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 06:00:07.204065: step 137500, loss = 0.52 (1291.6 examples/sec; 0.099 sec/batch)
2017-06-02 06:00:08.014185: step 137510, loss = 0.50 (1580.0 examples/sec; 0.081 sec/batch)
2017-06-02 06:00:08.873277: step 137520, loss = 0.51 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:00:09.767963: step 137530, loss = 0.49 (1430.7 examples/sec; 0.089 sec/batch)
2017-06-02 06:00:10.655330: step 137540, loss = 0.59 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 06:00:11.531202: step 137550, loss = 0.47 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:00:12.426518: step 137560, loss = 0.49 (1429.7 examples/sec; 0.090 sec/batch)
2017-06-02 06:00:13.307251: step 137570, loss = 0.51 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:00:14.190501: step 137580, loss = 0.48 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:00:15.073999: step 137590, loss = 0.52 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:00:16.086487: step 137600, loss = 0.49 (1265.7 examples/sec; 0.101 sec/batch)
2017-06-02 06:00:16.813515: step 137610, loss = 0.50 (1757.7 examples/sec; 0.073 sec/batch)
2017-06-02 06:00:17.684417: step 137620, loss = 0.53 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:00:18.524026: step 137630, loss = 0.53 (1524.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:00:19.389152: step 137640, loss = 0.51 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:00:20.249963: step 137650, loss = 0.51 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:00:21.141502: step 137660, loss = 0.44 (1435.7 examples/sec; 0.089 sec/batch)
2017-06-02 06:00:22.024104: step 137670, loss = 0.66 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:00:22.910029: step 137680, loss = 0.70 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 06:00:23.785958: step 137690, loss = 0.54 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:00:24.741538: step 137700, loss = 0.65 (1339.5 examples/sec; 0.096 sec/batch)
2017-06-02 06:00:25.517776: step 137710, loss = 0.39 (1648.9 examples/sec; 0.078 sec/batch)
2017-06-02 06:00:26.404844: step 137720, loss = 0.66 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:00:27.262345: step 137730, loss = 0.51 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:00:28.127374: step 137740, loss = 0.56 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:00:28.988118: step 137750, loss = 0.49 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:00:29.856778: step 137760, loss = 0.44 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:00:30.732999: step 137770, loss = 0.56 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:00:31.624267: step 137780, loss = 0.52 (1436.2 examples/sec; 0.089 sec/batch)
2017-06-02 06:00:32.509104: step 137790, loss = 0.53 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:00:33.479773: step 137800, loss = 0.46 (1318.7 examples/sec; 0.097 sec/batch)
2017-06-02 06:00:34.249552: step 137810, loss = 0.39 (1662.8 examples/sec; 0.077 sec/batch)
2017-06-02 06:00:35.105755: step 137820, loss = 0.47 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:00:35.981389: step 137830, loss = 0.46 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:00:36.851529: step 137840, loss = 0.39 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:00:37.727467: step 137850, loss = 0.50 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:00:38.618771: step 137860, loss = 0.55 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 06:00:39.470845: step 137870, loss = 0.59 (1502.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:00:40.353582: step 137880, loss = 0.44 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:00:41.243833: step 137890, loss = 0.50 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 06:00:42.197807: step 137900, loss = 0.48 (1341.8 examples/sec; 0.095 sec/batch)
2017-06-02 06:00:42.971922: step 137910, loss = 0.48 (1653.5 examples/sec; 0.077 sec/batch)
2017-06-02 06:00:43.814958: step 137920, loss = 0.55 (1518.3 examples/sec; 0.084 sec/batch)
2017-06-02 06:00:44.659217: step 137930, loss = 0.51 (1516.1 examples/sec; 0.084 sec/batch)
2017-06-02 06:00:45.556390: step 137940, loss = 0.56 (1426.7 examples/sec; 0.090 sec/batch)
2017-06-02 06:00:46.437030: step 137950, loss = 0.49 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:00:47.289691: step 137960, loss = 0.46 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:00:48.156574: step 137970, loss = 0.58 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:00:49.038223: step 137980, loss = 0.50 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:00:49.907867: step 137990, loss = 0.44 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:00:50.866520: step 138000, loss = 0.52 (1335.2 examples/sec; 0.096 sec/batch)
2017-06-02 06:00:51.641974: step 138010, loss = 0.39 (1650.6 examples/sec; 0.078 sec/batch)
2017-06-02 06:00:52.507620: step 138020, loss = 0.52 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:00:53.378557: step 138030, loss = 0.64 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:00:54.245110: step 138040, loss = 0.64 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:00:55.134286: step 138050, loss = 0.51 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 06:00:56.009070: step 138060, loss = 0.69 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:00:56.887827: step 138070, loss = 0.61 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:00:57.767682: step 138080, loss = 0.65 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:00:58.626842: step 138090, loss = 0.45 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:00:59.598389: step 138100, loss = 0.50 (1317.5 examples/sec; 0.097 sec/batch)
2017-06-02 06:01:00.356459: step 138110, loss = 0.56 (1688.5 examples/sec; 0.076 sec/batch)
2017-06-02 06:01:01.216627: step 138120, loss = 0.62 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:01:02.114438: step 138130, loss = 0.62 (1425.7 examples/sec; 0.090 sec/batch)
2017-06-02 06:01:02.995298: step 138140, loss = 0.51 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:01:03.896940: step 138150, loss = 0.55 (1419.6 examples/sec; 0.090 sec/batch)
2017-06-02 06:01:04.790586: step 138160, loss = 0.53 (1432.3 examples/sec; 0.089 sec/batch)
2017-06-02 06:01:05.645682: step 138170, loss = 0.50 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:01:06.532804: step 138180, loss = 0.45 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 06:01:07.407684: step 138190, loss = 0.77 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:01:08.361825: step 138200, loss = 0.42 (1341.5 examples/sec; 0.095 sec/batch)
2017-06-02 06:01:09.148591: step 138210, loss = 0.53 (1626.9 examples/sec; 0.079 sec/batch)
2017-06-02 06:01:10.035667: step 138220, loss = 0.52 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:01:10.895712: step 138230, loss = 0.48 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:01:11.755993: step 138240, loss = 0.53 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:01:12.609837: step 138250, loss = 0.41 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:01:13.491490: step 138260, loss = 0.53 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:01:14.362923: step 138270, loss = 0.52 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:01:15.212755: step 138280, loss = 0.51 (1506.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:01:16.097795: step 138290, loss = 0.66 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 06:01:17.039717: step 138300, loss = 0.59 (1358.9 examples/sec; 0.094 sec/batch)
2017-06-02 06:01:17.832039: step 138310, loss = 0.58 (1615.5 examples/sec; 0.079 sec/batch)
2017-06-02 06:01:18.704482: step 138320, loss = 0.64 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:01:19.564541: step 138330, loss = 0.53 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:01:20.445200: step 138340, loss = 0.45 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:01:21.316078: step 138350, loss = 0.51 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:01:22.196747: step 138360, loss = 0.53 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:01:23.063784: step 138370, loss = 0.47 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:01:23.943509: step 138380, loss = 0.41 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:01:24.813844: step 138390, loss = 0.51 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:01:25.771183: step 138400, loss = 0.50 (1337.0 examples/sec; 0.096 sec/batch)
2017-06-02 06:01:26.519664: step 138410, loss = 0.39 (1710.1 examples/sec; 0.075 sec/batch)
2017-06-02 06:01:27.388996: step 138420, loss = 0.50 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:01:28.249085: step 138430, loss = 0.50 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:01:29.098130: step 138440, loss = 0.52 (1507.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:01:29.962800: step 138450, loss = 0.57 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:01:30.823951: step 138460, loss = 0.44 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:01:31.692478: step 138470, loss = 0.56 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:01:32.550230: step 138480, loss = 0.46 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:01:33.399910: step 138490, loss = 0.54 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:01:34.421597: step 138500, loss = 0.45 (1252.8 examples/sec; 0.102 sec/batch)
2017-06-02 06:01:35.135289: step 138510, loss = 0.46 (1793.5 examples/sec; 0.071 sec/batch)
2017-06-02 06:01:35.971508: step 138520, loss = 0.57 (1530.7 examples/sec; 0.084 sec/batch)
2017-06-02 06:01:36.820967: step 138530, loss = 0.51 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:01:37.675780: step 138540, loss = 0.62 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:01:38.543109: step 138550, loss = 0.51 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:01:39.412369: step 138560, loss = 0.44 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:01:40.273465: step 138570, loss = 0.62 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:01:41.137528: step 138580, loss = 0.59 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:01:41.993500: step 138590, loss = 0.48 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:01:42.989095: step 138600, loss = 0.45 (1285.7 examples/sec; 0.100 sec/batch)
2017-06-02 06:01:43.761763: step 138610, loss = 0.49 (1656.6 examples/sec; 0.077 sec/batch)
2017-06-02 06:01:44.644405: step 138620, loss = 0.42 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:01:45.519789: step 138630, loss = 0.55 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:01:46.389360: step 138640, loss = 0.53 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:01:47.253829: step 138650, loss = 0.44 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:01:48.120189: step 138660, loss = 0.49 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:01:48.981096: step 138670, loss = 0.45 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:01:49.868253: step 138680, loss = 0.56 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 06:01:50.719896: step 138690, loss = 0.51 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:01:51.702820: step 138700, loss = 0.46 (1302.2 examples/sec; 0.098 sec/batch)
2017-06-02 06:01:52.491244: step 138710, loss = 0.69 (1623.5 examples/sec; 0.079 sec/batch)
2017-06-02 06:01:53.368672: step 138720, loss = 0.54 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:01:54.248374: step 138730, loss = 0.56 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:01:55.143501: step 138740, loss = 0.57 (1430.0 examples/sec; 0.090 sec/batch)
2017-06-02 06:01:56.026146: step 138750, loss = 0.49 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:01:56.898619: step 138760, loss = 0.54 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:01:57.797694: step 138770, loss = 0.59 (1423.7 examples/sec; 0.090 sec/batch)
2017-06-02 06:01:58.654160: step 138780, loss = 0.55 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:01:59.524437: step 138790, loss = 0.56 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:02:00.536777: step 138800, loss = 0.53 (1264.4 examples/sec; 0.101 sec/batch)
2017-06-02 06:02:01.269467: step 138810, loss = 0.63 (1747.0 examples/sec; 0.073 sec/batch)
2017-06-02 06:02:02.126755: step 138820, loss = 0.53 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:02:02.995424: step 138830, loss = 0.53 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:02:03.876909: step 138840, loss = 0.63 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:02:04.734929: step 138850, loss = 0.49 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:02:05.558186: step 138860, loss = 0.53 (1554.8 examples/sec; 0.082 sec/batch)
2017-06-02 06:02:06.402756: step 138870, loss = 0.48 (1515.6 examples/sec; 0.084 sec/batch)
2017-06-02 06:02:07.280578: step 138880, loss = 0.46 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:02:08.152864: step 138890, loss = 0.47 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:02:09.127623: step 138900, loss = 0.55 (1313.1 examples/sec; 0.097 sec/batch)
2017-06-02 06:02:09.877675: step 138910, loss = 0.42 (1706.6 examples/sec; 0.075 sec/batch)
2017-06-02 06:02:10.740136: step 138920, loss = 0.65 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:02:11.618402: step 138930, loss = 0.48 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:02:12.500411: step 138940, loss = 0.57 (1451.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:02:13.383374: step 138950, loss = 0.51 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:02:14.249188: step 138960, loss = 0.53 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:02:15.147344: step 138970, loss = 0.43 (1425.1 examples/sec; 0.090 sec/batch)
2017-06-02 06:02:15.986019: step 138980, loss = 0.45 (1526.2 examples/sec; 0.084 sec/batch)
2017-06-02 06:02:16.837778: step 138990, loss = 0.61 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:02:17.786982: step 139000, loss = 0.44 (1348.5 examples/sec; 0.095 sec/batch)
2017-06-02 06:02:18.553097: step 139010, loss = 0.41 (1670.8 examples/sec; 0.077 sec/batch)
2017-06-02 06:02:19.419681: step 139020, loss = 0.71 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:02:20.299914: step 139030, loss = 0.41 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:02:21.168986: step 139040, loss = 0.44 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:02:22.014467: step 139050, loss = 0.46 (1514.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:02:22.874387: step 139060, loss = 0.42 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:02:23.745104: step 139070, loss = 0.48 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:02:24.640629: step 139080, loss = 0.51 (1429.3 examples/sec; 0.090 sec/batch)
2017-06-02 06:02:25.497883: step 139090, loss = 0.51 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:02:26.454350: step 139100, loss = 0.44 (1338.5 examples/sec; 0.096 sec/batch)
2017-06-02 06:02:27.241310: step 139110, loss = 0.34 (1626.2 examples/sec; 0.079 sec/batch)
2017-06-02 06:02:28.095674: step 139120, loss = 0.42 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:02:28.937012: step 139130, loss = 0.44 (1521.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:02:29.769291: step 139140, loss = 0.49 (1537.9 examples/sec; 0.083 sec/batch)
2017-06-02 06:02:30.643828: step 139150, loss = 0.43 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:02:31.522496: step 139160, loss = 0.51 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:02:32.411254: step 139170, loss = 0.49 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 06:02:33.285670: step 139180, loss = 0.48 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:02:34.153082: step 139190, loss = 0.44 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:02:35.121552: step 139200, loss = 0.46 (1321.7 examples/sec; 0.097 sec/batch)
2017-06-02 06:02:35.892347: step 139210, loss = 0.53 (1660.6 examples/sec; 0.077 sec/batch)
2017-06-02 06:02:36.755789: step 139220, loss = 0.44 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:02:37.658718: step 139230, loss = 0.60 (1417.6 examples/sec; 0.090 sec/batch)
2017-06-02 06:02:38.547419: step 139240, loss = 0.66 (1440.3 examples/sec; 0.089 sec/batch)
2017-06-02 06:02:39.420713: step 139250, loss = 0.51 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:02:40.293595: step 139260, loss = 0.51 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:02:41.176424: step 139270, loss = 0.54 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:02:42.044320: step 139280, loss = 0.56 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:02:42.908315: step 139290, loss = 0.37 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:02:43.879443: step 139300, loss = 0.46 (1318.0 examples/sec; 0.097 sec/batch)
2017-06-02 06:02:44.668663: step 139310, loss = 0.52 (1621.9 examples/sec; 0.079 sec/batch)
2017-06-02 06:02:45.536020: step 139320, loss = 0.52 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:02:46.392689: step 139330, loss = 0.48 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:02:47.245295: step 139340, loss = 0.50 (1501.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:02:48.104775: step 139350, loss = 0.48 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:02:48.941522: step 139360, loss = 0.48 (1529.7 examples/sec; 0.084 sec/batch)
2017-06-02 06:02:49.829686: step 139370, loss = 0.52 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 06:02:50.710390: step 139380, loss = 0.51 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:02:51.547941: step 139390, loss = 0.45 (1528.3 examples/sec; 0.084 sec/batch)
2017-06-02 06:02:52.519723: step 139400, loss = 0.56 (1317.2 examples/sec; 0.097 sec/batch)
2017-06-02 06:02:53.292935: step 139410, loss = 0.47 (1655.4 examples/sec; 0.077 sec/batch)
2017-06-02 06:02:54.143680: step 139420, loss = 0.56 (1504.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:02:55.015140: step 139430, loss = 0.45 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:02:55.853956: step 139440, loss = 0.50 (1526.0 examples/sec; 0.084 sec/batch)
2017-06-02 06:02:56.700050: step 139450, loss = 0.46 (1512.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:02:57.567467: step 139460, loss = 0.54 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:02:58.409149: step 139470, loss = 0.43 (1520.8 examples/sec; 0.084 sec/batch)
2017-06-02 06:02:59.261048: step 139480, loss = 0.49 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:03:00.139012: step 139490, loss = 0.52 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:03:01.101968: step 139500, loss = 0.40 (1329.2 examples/sec; 0.096 sec/batch)
2017-06-02 06:03:01.878666: step 139510, loss = 0.53 (1648.0 examples/sec; 0.078 sec/batch)
2017-06-02 06:03:02.762993: step 139520, loss = 0.51 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:03:03.613115: step 139530, loss = 0.45 (1505.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:03:04.503659: step 139540, loss = 0.46 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 06:03:05.369274: step 139550, loss = 0.38 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:03:06.242354: step 139560, loss = 0.43 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:03:07.095068: step 139570, loss = 0.43 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:03:07.943536: step 139580, loss = 0.57 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:03:08.843803: step 139590, loss = 0.46 (1421.8 examples/sec; 0.090 sec/batch)
2017-06-02 06:03:09.810933: step 139600, loss = 0.48 (1323.5 examples/sec; 0.097 sec/batch)
2017-06-02 06:03:10.595947: step 139610, loss = 0.45 (1630.5 examples/sec; 0.079 sec/batch)
2017-06-02 06:03:11.487536: step 139620, loss = 0.47 (1435.6 examples/sec; 0.089 sec/batch)
2017-06-02 06:03:12.386378: step 139630, loss = 0.54 (1424.1 examples/sec; 0.090 sec/batch)
2017-06-02 06:03:13.259027: step 139640, loss = 0.61 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:03:14.140891: step 139650, loss = 0.53 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:03:15.030214: step 139660, loss = 0.42 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 06:03:15.899411: step 139670, loss = 0.52 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:03:16.771450: step 139680, loss = 0.43 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:03:17.640550: step 139690, loss = 0.47 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:03:18.586017: step 139700, loss = 0.44 (1353.8 examples/sec; 0.095 sec/batch)
2017-06-02 06:03:19.348670: step 139710, loss = 0.44 (1678.4 examples/sec; 0.076 sec/batch)
2017-06-02 06:03:20.246722: step 139720, loss = 0.45 (1425.3 examples/sec; 0.090 sec/batch)
2017-06-02 06:03:21.122499: step 139730, loss = 0.53 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:03:21.987581: step 139740, loss = 0.43 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:03:22.884832: step 139750, loss = 0.51 (1426.6 examples/sec; 0.090 sec/batch)
2017-06-02 06:03:23.774909: step 139760, loss = 0.44 (1438.1 examples/sec; 0.089 sec/batch)
2017-06-02 06:03:24.646325: step 139770, loss = 0.43 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:03:25.512530: step 139780, loss = 0.43 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:03:26.403959: step 139790, loss = 0.43 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 06:03:27.384356: step 139800, loss = 0.58 (1305.6 examples/sec; 0.098 sec/batch)
2017-06-02 06:03:28.169606: step 139810, loss = 0.41 (1630.1 examples/sec; 0.079 sec/batch)
2017-06-02 06:03:29.028593: step 139820, loss = 0.49 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:03:29.865498: step 139830, loss = 0.41 (1529.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:03:30.731464: step 139840, loss = 0.48 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:03:31.587944: step 139850, loss = 0.38 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:03:32.454262: step 139860, loss = 0.52 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:03:33.329315: step 139870, loss = 0.48 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:03:34.181764: step 139880, loss = 0.55 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:03:35.035248: step 139890, loss = 0.43 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:03:36.006825: step 139900, loss = 0.49 (1317.5 examples/sec; 0.097 sec/batch)
2017-06-02 06:03:36.794220: step 139910, loss = 0.38 (1625.6 examples/sec; 0.079 sec/batch)
2017-06-02 06:03:37.627424: step 139920, loss = 0.41 (1536.2 examples/sec; 0.083 sec/batch)
2017-06-02 06:03:38.499653: step 139930, loss = 0.51 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:03:39.369455: step 139940, loss = 0.48 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:03:40.251282: step 139950, loss = 0.55 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:03:41.088077: step 139960, loss = 0.60 (1529.7 examples/sec; 0.084 sec/batch)
2017-06-02 06:03:41.957802: step 139970, loss = 0.41 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:03:42.819100: step 139980, loss = 0.47 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:03:43.696520: step 139990, loss = 0.43 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:03:44.662354: step 140000, loss = 0.52 (1325.3 examples/sec; 0.097 sec/batch)
2017-06-02 06:03:45.443037: step 140010, loss = 0.57 (1639.6 examples/sec; 0.078 sec/batch)
2017-06-02 06:03:46.315668: step 140020, loss = 0.59 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:03:47.168881: step 140030, loss = 0.45 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:03:48.024417: step 140040, loss = 0.39 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:03:48.906851: step 140050, loss = 0.47 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:03:49.778850: step 140060, loss = 0.45 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:03:50.653811: step 140070, loss = 0.52 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:03:51.538123: step 140080, loss = 0.46 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:03:52.421631: step 140090, loss = 0.53 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:03:53.391212: step 140100, loss = 0.43 (1320.2 examples/sec; 0.097 sec/batch)
2017-06-02 06:03:54.144700: step 140110, loss = 0.54 (1698.8 examples/sec; 0.075 sec/batch)
2017-06-02 06:03:55.038067: step 140120, loss = 0.42 (1432.8 examples/sec; 0.089 sec/batch)
2017-06-02 06:03:55.896386: step 140130, loss = 0.53 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:03:56.787736: step 140140, loss = 0.47 (1436.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:03:57.639049: step 140150, loss = 0.53 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:03:58.482301: step 140160, loss = 0.33 (1518.0 examples/sec; 0.084 sec/batch)
2017-06-02 06:03:59.336611: step 140170, loss = 0.47 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:04:00.211768: step 140180, loss = 0.47 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:04:01.080620: step 140190, loss = 0.43 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:04:02.050424: step 140200, loss = 0.45 (1319.8 examples/sec; 0.097 sec/batch)
2017-06-02 06:04:02.827279: step 140210, loss = 0.44 (1647.7 examples/sec; 0.078 sec/batch)
2017-06-02 06:04:03.714073: step 140220, loss = 0.55 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 06:04:04.594696: step 140230, loss = 0.39 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:04:05.470054: step 140240, loss = 0.44 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:04:06.340463: step 140250, loss = 0.41 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:04:07.213598: step 140260, loss = 0.51 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:04:08.087791: step 140270, loss = 0.40 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:04:08.952121: step 140280, loss = 0.44 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:04:09.851210: step 140290, loss = 0.47 (1423.7 examples/sec; 0.090 sec/batch)
2017-06-02 06:04:10.798549: step 140300, loss = 0.46 (1351.1 examples/sec; 0.095 sec/batch)
2017-06-02 06:04:11.569468: step 140310, loss = 0.44 (1660.4 examples/sec; 0.077 sec/batch)
2017-06-02 06:04:12.444396: step 140320, loss = 0.42 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:04:13.316377: step 140330, loss = 0.45 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:04:14.167202: step 140340, loss = 0.50 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:04:15.029741: step 140350, loss = 0.54 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:04:15.882331: step 140360, loss = 0.48 (1501.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:04:16.729247: step 140370, loss = 0.42 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:04:17.618156: step 140380, loss = 0.46 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:04:18.468720: step 140390, loss = 0.45 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:04:19.436578: step 140400, loss = 0.56 (1322.5 examples/sec; 0.097 sec/batch)
2017-06-02 06:04:20.235327: step 140410, loss = 0.55 (1602.5 examples/sec; 0.080 sec/batch)
2017-06-02 06:04:21.119389: step 140420, loss = 0.47 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:04:22.000657: step 140430, loss = 0.42 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:04:22.843380: step 140440, loss = 0.47 (1518.9 examples/sec; 0.084 sec/batch)
2017-06-02 06:04:23.706408: step 140450, loss = 0.53 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:04:24.578625: step 140460, loss = 0.33 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:04:25.452043: step 140470, loss = 0.43 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:04:26.313015: step 140480, loss = 0.44 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:04:27.171257: step 140490, loss = 0.48 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:04:28.130068: step 140500, loss = 0.50 (1335.0 examples/sec; 0.096 sec/batch)
2017-06-02 06:04:28.898140: step 140510, loss = 0.41 (1666.5 examples/sec; 0.077 sec/batch)
2017-06-02 06:04:29.790855: step 140520, loss = 0.48 (1433.8 examples/sec; 0.089 sec/batch)
2017-06-02 06:04:30.659419: step 140530, loss = 0.46 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:04:31.538904: step 140540, loss = 0.40 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:04:32.401025: step 140550, loss = 0.34 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:04:33.260015: step 140560, loss = 0.55 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:04:34.108070: step 140570, loss = 0.45 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:04:34.975318: step 140580, loss = 0.49 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:04:35.826327: step 140590, loss = 0.44 (1504.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:04:36.774572: step 140600, loss = 0.68 (1349.9 examples/sec; 0.095 sec/batch)
2017-06-02 06:04:37.574863: step 140610, loss = 0.42 (1599.4 examples/sec; 0.080 sec/batch)
2017-06-02 06:04:38.444074: step 140620, loss = 0.58 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:04:39.298110: step 140630, loss = 0.41 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:04:40.172669: step 140640, loss = 0.36 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:04:41.007940: step 140650, loss = 0.44 (1532.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:04:41.867506: step 140660, loss = 0.41 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:04:42.723173: step 140670, loss = 0.41 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:04:43.610391: step 140680, loss = 0.42 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 06:04:44.475005: step 140690, loss = 0.48 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:04:45.479043: step 140700, loss = 0.46 (1274.9 examples/sec; 0.100 sec/batch)
2017-06-02 06:04:46.264377: step 140710, loss = 0.58 (1629.9 examples/sec; 0.079 sec/batch)
2017-06-02 06:04:47.125519: step 140720, loss = 0.57 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:04:47.996694: step 140730, loss = 0.43 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:04:48.879069: step 140740, loss = 0.43 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:04:49.753997: step 140750, loss = 0.48 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:04:50.625958: step 140760, loss = 0.56 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:04:51.491358: step 140770, loss = 0.47 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:04:52.344065: step 140780, loss = 0.43 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:04:53.212399: step 140790, loss = 0.62 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:04:54.158454: step 140800, loss = 0.38 (1353.0 examples/sec; 0.095 sec/batch)
2017-06-02 06:04:54.931521: step 140810, loss = 0.40 (1655.8 examples/sec; 0.077 sec/batch)
2017-06-02 06:04:55.784798: step 140820, loss = 0.51 (1500.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:04:56.644442: step 140830, loss = 0.43 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:04:57.506441: step 140840, loss = 0.49 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:04:58.354662: step 140850, loss = 0.33 (1509.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:04:59.235259: step 140860, loss = 0.48 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:05:00.101833: step 140870, loss = 0.44 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:05:00.941365: step 140880, loss = 0.45 (1524.7 examples/sec; 0.084 sec/batch)
2017-06-02 06:05:01.829624: step 140890, loss = 0.43 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:05:02.774767: step 140900, loss = 0.47 (1354.3 examples/sec; 0.095 sec/batch)
2017-06-02 06:05:03.553274: step 140910, loss = 0.42 (1644.2 examples/sec; 0.078 sec/batch)
2017-06-02 06:05:04.406146: step 140920, loss = 0.43 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:05:05.287191: step 140930, loss = 0.43 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:05:06.150510: step 140940, loss = 0.39 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:05:07.037971: step 140950, loss = 0.55 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 06:05:07.928175: step 140960, loss = 0.41 (1437.9 examples/sec; 0.089 sec/batch)
2017-06-02 06:05:08.814015: step 140970, loss = 0.50 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:05:09.679965: step 140980, loss = 0.44 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:05:10.554455: step 140990, loss = 0.44 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:05:11.531582: step 141000, loss = 0.57 (1310.0 examples/sec; 0.098 sec/batch)
2017-06-02 06:05:12.260024: step 141010, loss = 0.52 (1757.2 examples/sec; 0.073 sec/batch)
2017-06-02 06:05:13.116033: step 141020, loss = 0.47 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:05:13.988043: step 141030, loss = 0.35 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:05:14.848181: step 141040, loss = 0.40 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:05:15.735682: step 141050, loss = 0.44 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 06:05:16.623297: step 141060, loss = 0.55 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 06:05:17.501182: step 141070, loss = 0.57 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:05:18.371885: step 141080, loss = 0.45 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:05:19.256499: step 141090, loss = 0.53 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:05:20.238933: step 141100, loss = 0.44 (1302.9 examples/sec; 0.098 sec/batch)
2017-06-02 06:05:21.021034: step 141110, loss = 0.40 (1636.6 examples/sec; 0.078 sec/batch)
2017-06-02 06:05:21.885974: step 141120, loss = 0.42 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:05:22.763164: step 141130, loss = 0.40 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:05:23.600914: step 141140, loss = 0.49 (1527.9 examples/sec; 0.084 sec/batch)
2017-06-02 06:05:24.476280: step 141150, loss = 0.54 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:05:25.333414: step 141160, loss = 0.50 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:05:26.208091: step 141170, loss = 0.54 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:05:27.086903: step 141180, loss = 0.45 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:05:27.936177: step 141190, loss = 0.60 (1507.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:05:28.906902: step 141200, loss = 0.41 (1318.6 examples/sec; 0.097 sec/batch)
2017-06-02 06:05:29.675715: step 141210, loss = 0.57 (1664.9 examples/sec; 0.077 sec/batch)
2017-06-02 06:05:30.518048: step 141220, loss = 0.43 (1519.6 examples/sec; 0.084 sec/batch)
2017-06-02 06:05:31.374944: step 141230, loss = 0.35 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:05:32.212090: step 141240, loss = 0.37 (1529.0 examples/sec; 0.084 sec/batch)
2017-06-02 06:05:33.039033: step 141250, loss = 0.40 (1547.9 examples/sec; 0.083 sec/batch)
2017-06-02 06:05:33.919313: step 141260, loss = 0.40 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:05:34.792521: step 141270, loss = 0.45 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:05:35.673020: step 141280, loss = 0.43 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:05:36.549345: step 141290, loss = 0.54 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:05:37.557856: step 141300, loss = 0.43 (1269.2 examples/sec; 0.101 sec/batch)
2017-06-02 06:05:38.292034: step 141310, loss = 0.47 (1743.4 examples/sec; 0.073 sec/batch)
2017-06-02 06:05:39.160777: step 141320, loss = 0.44 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:05:39.998236: step 141330, loss = 0.41 (1528.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:05:40.878798: step 141340, loss = 0.51 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:05:41.751312: step 141350, loss = 0.45 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:05:42.623771: step 141360, loss = 0.49 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:05:43.512294: step 141370, loss = 0.52 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 06:05:44.413536: step 141380, loss = 0.56 (1420.3 examples/sec; 0.090 sec/batch)
2017-06-02 06:05:45.282902: step 141390, loss = 0.65 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:05:46.293423: step 141400, loss = 0.51 (1266.7 examples/sec; 0.101 sec/batch)
2017-06-02 06:05:47.006291: step 141410, loss = 0.43 (1795.6 examples/sec; 0.071 sec/batch)
2017-06-02 06:05:47.889052: step 141420, loss = 0.42 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:05:48.762455: step 141430, loss = 0.44 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:05:49.638889: step 141440, loss = 0.45 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:05:50.487393: step 141450, loss = 0.43 (1508.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:05:51.355410: step 141460, loss = 0.49 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:05:52.236718: step 141470, loss = 0.41 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:05:53.103822: step 141480, loss = 0.46 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:05:53.970913: step 141490, loss = 0.47 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:05:54.953835: step 141500, loss = 0.35 (1302.2 examples/sec; 0.098 sec/batch)
2017-06-02 06:05:55.676609: step 141510, loss = 0.43 (1771.0 examples/sec; 0.072 sec/batch)
2017-06-02 06:05:56.549683: step 141520, loss = 0.46 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:05:57.440460: step 141530, loss = 0.58 (1437.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:05:58.308178: step 141540, loss = 0.51 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:05:59.181573: step 141550, loss = 0.47 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:06:00.051285: step 141560, loss = 0.45 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:06:00.911073: step 141570, loss = 0.43 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:06:01.767803: step 141580, loss = 0.37 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:06:02.618364: step 141590, loss = 0.47 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:06:03.560176: step 141600, loss = 0.53 (1359.1 examples/sec; 0.094 sec/batch)
2017-06-02 06:06:04.333631: step 141610, loss = 0.34 (1654.9 examples/sec; 0.077 sec/batch)
2017-06-02 06:06:05.195998: step 141620, loss = 0.42 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:06:06.041985: step 141630, loss = 0.43 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:06:06.898181: step 141640, loss = 0.39 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:06:07.772247: step 141650, loss = 0.39 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:06:08.629147: step 141660, loss = 0.38 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:06:09.503231: step 141670, loss = 0.47 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:06:10.368949: step 141680, loss = 0.53 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:06:11.239777: step 141690, loss = 0.60 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:06:12.170870: step 141700, loss = 0.39 (1374.7 examples/sec; 0.093 sec/batch)
2017-06-02 06:06:12.925443: step 141710, loss = 0.41 (1696.3 examples/sec; 0.075 sec/batch)
2017-06-02 06:06:13.765647: step 141720, loss = 0.48 (1523.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:06:14.601074: step 141730, loss = 0.46 (1532.1 examples/sec; 0.084 sec/batch)
2017-06-02 06:06:15.443078: step 141740, loss = 0.43 (1520.2 examples/sec; 0.084 sec/batch)
2017-06-02 06:06:16.258643: step 141750, loss = 0.43 (1569.5 examples/sec; 0.082 sec/batch)
2017-06-02 06:06:17.122546: step 141760, loss = 0.46 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:06:17.988277: step 141770, loss = 0.40 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:06:18.846839: step 141780, loss = 0.42 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:06:19.711553: step 141790, loss = 0.37 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:06:20.673850: step 141800, loss = 0.38 (1330.1 examples/sec; 0.096 sec/batch)
2017-06-02 06:06:21.435161: step 141810, loss = 0.47 (1681.3 examples/sec; 0.076 sec/batch)
2017-06-02 06:06:22.271289: step 141820, loss = 0.41 (1530.9 examples/sec; 0.084 sec/batch)
2017-06-02 06:06:23.125277: step 141830, loss = 0.53 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:06:23.975547: step 141840, loss = 0.34 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:06:24.824993: step 141850, loss = 0.41 (1506.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:06:25.688496: step 141860, loss = 0.38 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:06:26.522143: step 141870, loss = 0.38 (1535.4 examples/sec; 0.083 sec/batch)
2017-06-02 06:06:27.356885: step 141880, loss = 0.41 (1533.4 examples/sec; 0.083 sec/batch)
2017-06-02 06:06:28.201483: step 141890, loss = 0.50 (1515.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:06:29.152091: step 141900, loss = 0.49 (1346.5 examples/sec; 0.095 sec/batch)
2017-06-02 06:06:29.907930: step 141910, loss = 0.46 (1693.5 examples/sec; 0.076 sec/batch)
2017-06-02 06:06:30.762472: step 141920, loss = 0.44 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:06:31.596322: step 141930, loss = 0.40 (1535.0 examples/sec; 0.083 sec/batch)
2017-06-02 06:06:32.452305: step 141940, loss = 0.40 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:06:33.312785: step 141950, loss = 0.38 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:06:34.168647: step 141960, loss = 0.45 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:06:35.026832: step 141970, loss = 0.46 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:06:35.891034: step 141980, loss = 0.36 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:06:36.736802: step 141990, loss = 0.40 (1513.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:06:37.655855: step 142000, loss = 0.51 (1392.7 examples/sec; 0.092 sec/batch)
2017-06-02 06:06:38.431784: step 142010, loss = 0.48 (1649.6 examples/sec; 0.078 sec/batch)
2017-06-02 06:06:39.292151: step 142020, loss = 0.44 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:06:40.152391: step 142030, loss = 0.33 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:06:41.016053: step 142040, loss = 0.43 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:06:41.867405: step 142050, loss = 0.44 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:06:42.724797: step 142060, loss = 0.40 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:06:43.572168: step 142070, loss = 0.38 (1510.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:06:44.430710: step 142080, loss = 0.42 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:06:45.281663: step 142090, loss = 0.45 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:06:46.233295: step 142100, loss = 0.37 (1345.0 examples/sec; 0.095 sec/batch)
2017-06-02 06:06:46.975833: step 142110, loss = 0.46 (1723.8 examples/sec; 0.074 sec/batch)
2017-06-02 06:06:47.836401: step 142120, loss = 0.50 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:06:48.680486: step 142130, loss = 0.45 (1516.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:06:49.548266: step 142140, loss = 0.42 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:06:50.398855: step 142150, loss = 0.44 (1504.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:06:51.241692: step 142160, loss = 0.51 (1518.7 examples/sec; 0.084 sec/batch)
2017-06-02 06:06:52.094088: step 142170, loss = 0.55 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:06:52.929616: step 142180, loss = 0.39 (1532.0 examples/sec; 0.084 sec/batch)
2017-06-02 06:06:53.800368: step 142190, loss = 0.53 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:06:54.760462: step 142200, loss = 0.38 (1333.2 examples/sec; 0.096 sec/batch)
2017-06-02 06:06:55.516677: step 142210, loss = 0.43 (1692.6 examples/sec; 0.076 sec/batch)
2017-06-02 06:06:56.372781: step 142220, loss = 0.52 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:06:57.250044: step 142230, loss = 0.44 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:06:58.126574: step 142240, loss = 0.38 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:06:59.002758: step 142250, loss = 0.45 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:06:59.874588: step 142260, loss = 0.34 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:07:00.733862: step 142270, loss = 0.35 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:07:01.621765: step 142280, loss = 0.40 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 06:07:02.494829: step 142290, loss = 0.41 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:07:03.437087: step 142300, loss = 0.39 (1358.4 examples/sec; 0.094 sec/batch)
2017-06-02 06:07:04.193429: step 142310, loss = 0.38 (1692.4 examples/sec; 0.076 sec/batch)
2017-06-02 06:07:05.057506: step 142320, loss = 0.45 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:07:05.920942: step 142330, loss = 0.42 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:07:06.781254: step 142340, loss = 0.53 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:07:07.657023: step 142350, loss = 0.44 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:07:08.527467: step 142360, loss = 0.42 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:07:09.399306: step 142370, loss = 0.49 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:07:10.280189: step 142380, loss = 0.31 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:07:11.130975: step 142390, loss = 0.47 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:07:12.070069: step 142400, loss = 0.48 (1363.0 examples/sec; 0.094 sec/batch)
2017-06-02 06:07:12.852883: step 142410, loss = 0.39 (1635.1 examples/sec; 0.078 sec/batch)
2017-06-02 06:07:13.721033: step 142420, loss = 0.40 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:07:14.585808: step 142430, loss = 0.42 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:07:15.453796: step 142440, loss = 0.45 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:07:16.328913: step 142450, loss = 0.44 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:07:17.203477: step 142460, loss = 0.43 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:07:18.097322: step 142470, loss = 0.47 (1432.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:07:18.944659: step 142480, loss = 0.37 (1510.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:07:19.820910: step 142490, loss = 0.41 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:07:20.797984: step 142500, loss = 0.43 (1310.0 examples/sec; 0.098 sec/batch)
2017-06-02 06:07:21.558532: step 142510, loss = 0.43 (1683.0 examples/sec; 0.076 sec/batch)
2017-06-02 06:07:22.449590: step 142520, loss = 0.47 (1436.5 examples/sec; 0.089 sec/batch)
2017-06-02 06:07:23.316658: step 142530, loss = 0.48 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:07:24.176285: step 142540, loss = 0.39 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:07:25.024350: step 142550, loss = 0.37 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:07:25.902529: step 142560, loss = 0.42 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:07:26.751211: step 142570, loss = 0.43 (1508.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:07:27.618445: step 142580, loss = 0.37 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:07:28.500910: step 142590, loss = 0.44 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:07:29.475479: step 142600, loss = 0.40 (1313.4 examples/sec; 0.097 sec/batch)
2017-06-02 06:07:30.249899: step 142610, loss = 0.39 (1652.9 examples/sec; 0.077 sec/batch)
2017-06-02 06:07:31.135430: step 142620, loss = 0.42 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 06:07:31.996964: step 142630, loss = 0.62 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:07:32.848392: step 142640, loss = 0.51 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:07:33.705521: step 142650, loss = 0.49 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:07:34.585788: step 142660, loss = 0.49 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:07:35.453746: step 142670, loss = 0.36 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:07:36.300751: step 142680, loss = 0.40 (1511.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:07:37.170533: step 142690, loss = 0.41 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:07:38.129708: step 142700, loss = 0.48 (1334.5 examples/sec; 0.096 sec/batch)
2017-06-02 06:07:38.912501: step 142710, loss = 0.39 (1635.2 examples/sec; 0.078 sec/batch)
2017-06-02 06:07:39.743125: step 142720, loss = 0.43 (1541.0 examples/sec; 0.083 sec/batch)
2017-06-02 06:07:40.644295: step 142730, loss = 0.42 (1420.4 examples/sec; 0.090 sec/batch)
2017-06-02 06:07:41.529241: step 142740, loss = 0.39 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:07:42.381996: step 142750, loss = 0.36 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:07:43.247500: step 142760, loss = 0.49 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:07:44.121330: step 142770, loss = 0.43 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:07:44.978408: step 142780, loss = 0.42 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:07:45.867181: step 142790, loss = 0.35 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 06:07:46.834084: step 142800, loss = 0.49 (1323.8 examples/sec; 0.097 sec/batch)
2017-06-02 06:07:47.561984: step 142810, loss = 0.39 (1758.5 examples/sec; 0.073 sec/batch)
2017-06-02 06:07:48.415547: step 142820, loss = 0.42 (1499.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:07:49.284609: step 142830, loss = 0.34 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:07:50.160101: step 142840, loss = 0.41 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:07:51.032096: step 142850, loss = 0.50 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:07:51.879746: step 142860, loss = 0.51 (1510.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:07:52.730917: step 142870, loss = 0.43 (1503.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:07:53.566350: step 142880, loss = 0.42 (1532.1 examples/sec; 0.084 sec/batch)
2017-06-02 06:07:54.443864: step 142890, loss = 0.41 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:07:55.392321: step 142900, loss = 0.45 (1349.6 examples/sec; 0.095 sec/batch)
2017-06-02 06:07:56.179019: step 142910, loss = 0.39 (1627.1 examples/sec; 0.079 sec/batch)
2017-06-02 06:07:57.030370: step 142920, loss = 0.37 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:07:57.908385: step 142930, loss = 0.43 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:07:58.746696: step 142940, loss = 0.40 (1526.9 examples/sec; 0.084 sec/batch)
2017-06-02 06:07:59.612013: step 142950, loss = 0.42 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:08:00.482296: step 142960, loss = 0.44 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:08:01.331190: step 142970, loss = 0.45 (1507.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:08:02.200776: step 142980, loss = 0.39 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:08:03.059509: step 142990, loss = 0.45 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:08:04.057113: step 143000, loss = 0.43 (1283.1 examples/sec; 0.100 sec/batch)
2017-06-02 06:08:04.847281: step 143010, loss = 0.45 (1619.9 examples/sec; 0.079 sec/batch)
2017-06-02 06:08:05.693074: step 143020, loss = 0.47 (1513.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:08:06.549487: step 143030, loss = 0.53 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:08:07.422117: step 143040, loss = 0.46 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:08:08.294291: step 143050, loss = 0.47 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:08:09.141013: step 143060, loss = 0.44 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:08:09.999013: step 143070, loss = 0.44 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:08:10.854551: step 143080, loss = 0.38 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:08:11.732913: step 143090, loss = 0.42 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:08:12.676791: step 143100, loss = 0.33 (1356.1 examples/sec; 0.094 sec/batch)
2017-06-02 06:08:13.447812: step 143110, loss = 0.43 (1660.2 examples/sec; 0.077 sec/batch)
2017-06-02 06:08:14.324308: step 143120, loss = 0.45 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:08:15.198868: step 143130, loss = 0.44 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:08:16.078093: step 143140, loss = 0.46 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:08:16.962942: step 143150, loss = 0.46 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:08:17.843860: step 143160, loss = 0.38 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:08:18.718924: step 143170, loss = 0.42 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:08:19.582717: step 143180, loss = 0.41 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:08:20.480127: step 143190, loss = 0.32 (1426.3 examples/sec; 0.090 sec/batch)
2017-06-02 06:08:21.479780: step 143200, loss = 0.31 (1280.4 examples/sec; 0.100 sec/batch)
2017-06-02 06:08:22.260742: step 143210, loss = 0.36 (1639.1 examples/sec; 0.078 sec/batch)
2017-06-02 06:08:23.126477: step 143220, loss = 0.45 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:08:23.995217: step 143230, loss = 0.42 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:08:24.844861: step 143240, loss = 0.37 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:08:25.704397: step 143250, loss = 0.42 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:08:26.576569: step 143260, loss = 0.45 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:08:27.440857: step 143270, loss = 0.42 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:08:28.293867: step 143280, loss = 0.45 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:08:29.141436: step 143290, loss = 0.40 (1510.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:08:30.096335: step 143300, loss = 0.57 (1340.4 examples/sec; 0.095 sec/batch)
2017-06-02 06:08:30.864205: step 143310, loss = 0.36 (1666.9 examples/sec; 0.077 sec/batch)
2017-06-02 06:08:31.736715: step 143320, loss = 0.55 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:08:32.605887: step 143330, loss = 0.36 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:08:33.448692: step 143340, loss = 0.44 (1518.7 examples/sec; 0.084 sec/batch)
2017-06-02 06:08:34.304385: step 143350, loss = 0.45 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:08:35.195743: step 143360, loss = 0.43 (1436.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:08:36.074401: step 143370, loss = 0.34 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:08:36.982374: step 143380, loss = 0.40 (1410.4 examples/sec; 0.091 sec/batch)
2017-06-02 06:08:37.859360: step 143390, loss = 0.42 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:08:38.852958: step 143400, loss = 0.45 (1288.2 examples/sec; 0.099 sec/batch)
2017-06-02 06:08:39.590397: step 143410, loss = 0.33 (1735.8 examples/sec; 0.074 sec/batch)
2017-06-02 06:08:40.447741: step 143420, loss = 0.47 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:08:41.331458: step 143430, loss = 0.41 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:08:42.185604: step 143440, loss = 0.51 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:08:43.047198: step 143450, loss = 0.27 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:08:43.897082: step 143460, loss = 0.34 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:08:44.771715: step 143470, loss = 0.38 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:08:45.643368: step 143480, loss = 0.47 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:08:46.498155: step 143490, loss = 0.47 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:08:47.477894: step 143500, loss = 0.46 (1306.5 examples/sec; 0.098 sec/batch)
2017-06-02 06:08:48.269569: step 143510, loss = 0.36 (1616.8 examples/sec; 0.079 sec/batch)
2017-06-02 06:08:49.129478: step 143520, loss = 0.37 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:08:49.965827: step 143530, loss = 0.35 (1530.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:08:50.817179: step 143540, loss = 0.32 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:08:51.678678: step 143550, loss = 0.44 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:08:52.541040: step 143560, loss = 0.40 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:08:53.415055: step 143570, loss = 0.40 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:08:54.260814: step 143580, loss = 0.38 (1513.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:08:55.121132: step 143590, loss = 0.40 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:08:56.104160: step 143600, loss = 0.42 (1302.1 examples/sec; 0.098 sec/batch)
2017-06-02 06:08:56.868490: step 143610, loss = 0.43 (1674.7 examples/sec; 0.076 sec/batch)
2017-06-02 06:08:57.706481: step 143620, loss = 0.44 (1527.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:08:58.559365: step 143630, loss = 0.43 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:08:59.425597: step 143640, loss = 0.44 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:09:00.275648: step 143650, loss = 0.38 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:09:01.110327: step 143660, loss = 0.38 (1533.5 examples/sec; 0.083 sec/batch)
2017-06-02 06:09:01.974914: step 143670, loss = 0.39 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:09:02.820952: step 143680, loss = 0.36 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:09:03.689942: step 143690, loss = 0.36 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:09:04.643565: step 143700, loss = 0.37 (1342.2 examples/sec; 0.095 sec/batch)
2017-06-02 06:09:05.418313: step 143710, loss = 0.46 (1652.2 examples/sec; 0.077 sec/batch)
2017-06-02 06:09:06.257631: step 143720, loss = 0.35 (1525.0 examples/sec; 0.084 sec/batch)
2017-06-02 06:09:07.116840: step 143730, loss = 0.42 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:09:07.989384: step 143740, loss = 0.38 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:09:08.852825: step 143750, loss = 0.47 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:09:09.718819: step 143760, loss = 0.36 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:09:10.587665: step 143770, loss = 0.37 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:09:11.457432: step 143780, loss = 0.38 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:09:12.289696: step 143790, loss = 0.38 (1538.0 examples/sec; 0.083 sec/batch)
2017-06-02 06:09:13.244336: step 143800, loss = 0.35 (1340.8 examples/sec; 0.095 sec/batch)
2017-06-02 06:09:14.030757: step 143810, loss = 0.50 (1627.7 examples/sec; 0.079 sec/batch)
2017-06-02 06:09:14.889164: step 143820, loss = 0.40 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:09:15.751930: step 143830, loss = 0.39 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:09:16.615059: step 143840, loss = 0.36 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:09:17.488434: step 143850, loss = 0.39 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:09:18.367660: step 143860, loss = 0.35 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:09:19.238860: step 143870, loss = 0.43 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:09:20.110968: step 143880, loss = 0.40 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:09:20.961090: step 143890, loss = 0.40 (1505.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:09:21.912214: step 143900, loss = 0.38 (1345.8 examples/sec; 0.095 sec/batch)
2017-06-02 06:09:22.699791: step 143910, loss = 0.35 (1625.2 examples/sec; 0.079 sec/batch)
2017-06-02 06:09:23.593133: step 143920, loss = 0.53 (1432.8 examples/sec; 0.089 sec/batch)
2017-06-02 06:09:24.469664: step 143930, loss = 0.38 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:09:25.342855: step 143940, loss = 0.38 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:09:26.203216: step 143950, loss = 0.27 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:09:27.079477: step 143960, loss = 0.44 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:09:27.955841: step 143970, loss = 0.47 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:09:28.829064: step 143980, loss = 0.43 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:09:29.701887: step 143990, loss = 0.36 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:09:30.704178: step 144000, loss = 0.28 (1277.1 examples/sec; 0.100 sec/batch)
2017-06-02 06:09:31.460563: step 144010, loss = 0.45 (1692.3 examples/sec; 0.076 sec/batch)
2017-06-02 06:09:32.336555: step 144020, loss = 0.40 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:09:33.203085: step 144030, loss = 0.41 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:09:34.068999: step 144040, loss = 0.37 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:09:34.913686: step 144050, loss = 0.34 (1515.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:09:35.787392: step 144060, loss = 0.33 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:09:36.648845: step 144070, loss = 0.43 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:09:37.526841: step 144080, loss = 0.39 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:09:38.393887: step 144090, loss = 0.46 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:09:39.347402: step 144100, loss = 0.43 (1342.4 examples/sec; 0.095 sec/batch)
2017-06-02 06:09:40.104674: step 144110, loss = 0.48 (1690.3 examples/sec; 0.076 sec/batch)
2017-06-02 06:09:40.968390: step 144120, loss = 0.41 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:09:41.832286: step 144130, loss = 0.44 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:09:42.674430: step 144140, loss = 0.32 (1519.9 examples/sec; 0.084 sec/batch)
2017-06-02 06:09:43.550061: step 144150, loss = 0.44 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:09:44.392634: step 144160, loss = 0.40 (1519.2 examples/sec; 0.084 sec/batch)
2017-06-02 06:09:45.226372: step 144170, loss = 0.38 (1535.3 examples/sec; 0.083 sec/batch)
2017-06-02 06:09:46.098939: step 144180, loss = 0.34 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:09:46.952194: step 144190, loss = 0.37 (1500.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:09:47.921000: step 144200, loss = 0.47 (1321.2 examples/sec; 0.097 sec/batch)
2017-06-02 06:09:48.691060: step 144210, loss = 0.43 (1662.2 examples/sec; 0.077 sec/batch)
2017-06-02 06:09:49.545213: step 144220, loss = 0.40 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:09:50.412825: step 144230, loss = 0.30 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:09:51.259630: step 144240, loss = 0.40 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:09:52.240316: step 144250, loss = 0.43 (1305.2 examples/sec; 0.098 sec/batch)
2017-06-02 06:09:53.100131: step 144260, loss = 0.40 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:09:53.931708: step 144270, loss = 0.37 (1539.2 examples/sec; 0.083 sec/batch)
2017-06-02 06:09:54.813354: step 144280, loss = 0.37 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:09:55.661774: step 144290, loss = 0.35 (1508.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:09:56.618171: step 144300, loss = 0.44 (1338.3 examples/sec; 0.096 sec/batch)
2017-06-02 06:09:57.392343: step 144310, loss = 0.49 (1653.4 examples/sec; 0.077 sec/batch)
2017-06-02 06:09:58.237061: step 144320, loss = 0.41 (1515.3 examples/sec; 0.084 sec/batch)
2017-06-02 06:09:59.107996: step 144330, loss = 0.33 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:09:59.951749: step 144340, loss = 0.38 (1517.1 examples/sec; 0.084 sec/batch)
2017-06-02 06:10:00.844040: step 144350, loss = 0.40 (1434.5 examples/sec; 0.089 sec/batch)
2017-06-02 06:10:01.717943: step 144360, loss = 0.32 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:10:02.553389: step 144370, loss = 0.43 (1532.1 examples/sec; 0.084 sec/batch)
2017-06-02 06:10:03.387776: step 144380, loss = 0.45 (1534.1 examples/sec; 0.083 sec/batch)
2017-06-02 06:10:04.238351: step 144390, loss = 0.42 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:10:05.232254: step 144400, loss = 0.37 (1287.9 examples/sec; 0.099 sec/batch)
2017-06-02 06:10:06.020239: step 144410, loss = 0.47 (1624.4 examples/sec; 0.079 sec/batch)
2017-06-02 06:10:06.883390: step 144420, loss = 0.44 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:10:07.737403: step 144430, loss = 0.35 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:10:08.619598: step 144440, loss = 0.51 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:10:09.483566: step 144450, loss = 0.40 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:10:10.348577: step 144460, loss = 0.42 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:10:11.203774: step 144470, loss = 0.46 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:10:12.049583: step 144480, loss = 0.50 (1513.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:10:12.902038: step 144490, loss = 0.39 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:10:13.838834: step 144500, loss = 0.31 (1366.4 examples/sec; 0.094 sec/batch)
2017-06-02 06:10:14.611381: step 144510, loss = 0.35 (1656.9 examples/sec; 0.077 sec/batch)
2017-06-02 06:10:15.497954: step 144520, loss = 0.40 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 06:10:16.373086: step 144530, loss = 0.43 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:10:17.251330: step 144540, loss = 0.32 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:10:18.114769: step 144550, loss = 0.35 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:10:18.999155: step 144560, loss = 0.37 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:10:19.892231: step 144570, loss = 0.33 (1433.3 examples/sec; 0.089 sec/batch)
2017-06-02 06:10:20.785996: step 144580, loss = 0.43 (1432.1 examples/sec; 0.089 sec/batch)
2017-06-02 06:10:21.676452: step 144590, loss = 0.43 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 06:10:22.646296: step 144600, loss = 0.45 (1319.8 examples/sec; 0.097 sec/batch)
2017-06-02 06:10:23.418593: step 144610, loss = 0.37 (1657.4 examples/sec; 0.077 sec/batch)
2017-06-02 06:10:24.285972: step 144620, loss = 0.39 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:10:25.172350: step 144630, loss = 0.38 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 06:10:26.031531: step 144640, loss = 0.39 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:10:26.918284: step 144650, loss = 0.40 (1443.5 examples/sec; 0.089 sec/batch)
2017-06-02 06:10:27.785777: step 144660, loss = 0.32 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:10:28.648026: step 144670, loss = 0.40 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:10:29.518984: step 144680, loss = 0.34 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:10:30.390802: step 144690, loss = 0.36 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:10:31.344979: step 144700, loss = 0.34 (1341.5 examples/sec; 0.095 sec/batch)
2017-06-02 06:10:32.118689: step 144710, loss = 0.49 (1654.4 examples/sec; 0.077 sec/batch)
2017-06-02 06:10:32.982425: step 144720, loss = 0.45 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:10:33.856180: step 144730, loss = 0.39 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:10:34.705214: step 144740, loss = 0.40 (1507.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:10:35.587123: step 144750, loss = 0.39 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:10:36.436700: step 144760, loss = 0.37 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:10:37.326481: step 144770, loss = 0.39 (1438.6 examples/sec; 0.089 sec/batch)
2017-06-02 06:10:38.177987: step 144780, loss = 0.34 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:10:39.015938: step 144790, loss = 0.36 (1527.6 examples/sec; 0.084 sec/batch)
2017-06-02 06:10:39.968066: step 144800, loss = 0.35 (1344.3 examples/sec; 0.095 sec/batch)
2017-06-02 06:10:40.753441: step 144810, loss = 0.36 (1629.8 examples/sec; 0.079 sec/batch)
2017-06-02 06:10:41.624409: step 144820, loss = 0.41 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:10:42.471035: step 144830, loss = 0.46 (1511.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:10:43.328754: step 144840, loss = 0.38 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:10:44.177607: step 144850, loss = 0.40 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:10:45.036853: step 144860, loss = 0.44 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:10:45.890606: step 144870, loss = 0.33 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:10:46.760531: step 144880, loss = 0.36 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:10:47.625911: step 144890, loss = 0.31 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:10:48.618705: step 144900, loss = 0.41 (1289.3 examples/sec; 0.099 sec/batch)
2017-06-02 06:10:49.319428: step 144910, loss = 0.41 (1826.7 examples/sec; 0.070 sec/batch)
2017-06-02 06:10:50.162653: step 144920, loss = 0.44 (1518.0 examples/sec; 0.084 sec/batch)
2017-06-02 06:10:51.014939: step 144930, loss = 0.41 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:10:51.875743: step 144940, loss = 0.35 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:10:52.750424: step 144950, loss = 0.49 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:10:53.628512: step 144960, loss = 0.46 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:10:54.492771: step 144970, loss = 0.35 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:10:55.334200: step 144980, loss = 0.39 (1521.2 examples/sec; 0.084 sec/batch)
2017-06-02 06:10:56.192316: step 144990, loss = 0.46 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:10:57.162216: step 145000, loss = 0.48 (1319.7 examples/sec; 0.097 sec/batch)
2017-06-02 06:10:57.928385: step 145010, loss = 0.35 (1670.6 examples/sec; 0.077 sec/batch)
2017-06-02 06:10:58.791784: step 145020, loss = 0.37 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:10:59.673803: step 145030, loss = 0.39 (1451.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:11:00.537126: step 145040, loss = 0.29 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:11:01.425472: step 145050, loss = 0.37 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 06:11:02.287658: step 145060, loss = 0.35 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:11:03.141045: step 145070, loss = 0.38 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:11:03.995526: step 145080, loss = 0.31 (1498.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:11:04.836887: step 145090, loss = 0.41 (1521.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:11:05.794801: step 145100, loss = 0.45 (1336.2 examples/sec; 0.096 sec/batch)
2017-06-02 06:11:06.543809: step 145110, loss = 0.37 (1708.9 examples/sec; 0.075 sec/batch)
2017-06-02 06:11:07.401388: step 145120, loss = 0.46 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:11:08.250267: step 145130, loss = 0.56 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:11:09.114292: step 145140, loss = 0.33 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:11:09.996019: step 145150, loss = 0.38 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:11:10.861761: step 145160, loss = 0.40 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:11:11.718865: step 145170, loss = 0.38 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:11:12.570333: step 145180, loss = 0.30 (1503.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:11:13.454357: step 145190, loss = 0.40 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:11:14.423315: step 145200, loss = 0.45 (1321.0 examples/sec; 0.097 sec/batch)
2017-06-02 06:11:15.164572: step 145210, loss = 0.41 (1726.8 examples/sec; 0.074 sec/batch)
2017-06-02 06:11:16.028743: step 145220, loss = 0.38 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:11:16.922079: step 145230, loss = 0.38 (1432.8 examples/sec; 0.089 sec/batch)
2017-06-02 06:11:17.796703: step 145240, loss = 0.31 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:11:18.672355: step 145250, loss = 0.34 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:11:19.546176: step 145260, loss = 0.42 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:11:20.401835: step 145270, loss = 0.35 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:11:21.251818: step 145280, loss = 0.45 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:11:22.118710: step 145290, loss = 0.36 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:11:23.084529: step 145300, loss = 0.37 (1325.3 examples/sec; 0.097 sec/batch)
2017-06-02 06:11:23.864566: step 145310, loss = 0.41 (1641.0 examples/sec; 0.078 sec/batch)
2017-06-02 06:11:24.745906: step 145320, loss = 0.28 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:11:25.625645: step 145330, loss = 0.34 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:11:26.478469: step 145340, loss = 0.39 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:11:27.346504: step 145350, loss = 0.33 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:11:28.220046: step 145360, loss = 0.39 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:11:29.060449: step 145370, loss = 0.47 (1523.1 examples/sec; 0.084 sec/batch)
2017-06-02 06:11:29.932369: step 145380, loss = 0.42 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:11:30.822363: step 145390, loss = 0.37 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 06:11:31.795570: step 145400, loss = 0.41 (1315.2 examples/sec; 0.097 sec/batch)
2017-06-02 06:11:32.565155: step 145410, loss = 0.37 (1663.3 examples/sec; 0.077 sec/batch)
2017-06-02 06:11:33.426561: step 145420, loss = 0.44 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:11:34.274488: step 145430, loss = 0.49 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:11:35.130037: step 145440, loss = 0.34 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:11:35.991052: step 145450, loss = 0.42 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:11:36.842681: step 145460, loss = 0.39 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:11:37.696399: step 145470, loss = 0.46 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:11:38.546945: step 145480, loss = 0.45 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:11:39.419221: step 145490, loss = 0.31 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:11:40.366579: step 145500, loss = 0.35 (1351.1 examples/sec; 0.095 sec/batch)
2017-06-02 06:11:41.131570: step 145510, loss = 0.41 (1673.2 examples/sec; 0.076 sec/batch)
2017-06-02 06:11:42.032488: step 145520, loss = 0.38 (1420.8 examples/sec; 0.090 sec/batch)
2017-06-02 06:11:42.926157: step 145530, loss = 0.43 (1432.3 examples/sec; 0.089 sec/batch)
2017-06-02 06:11:43.775913: step 145540, loss = 0.43 (1506.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:11:44.653076: step 145550, loss = 0.42 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:11:45.497837: step 145560, loss = 0.51 (1515.2 examples/sec; 0.084 sec/batch)
2017-06-02 06:11:46.340327: step 145570, loss = 0.37 (1519.3 examples/sec; 0.084 sec/batch)
2017-06-02 06:11:47.197103: step 145580, loss = 0.44 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:11:48.047913: step 145590, loss = 0.41 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:11:49.002595: step 145600, loss = 0.44 (1340.8 examples/sec; 0.095 sec/batch)
2017-06-02 06:11:49.774662: step 145610, loss = 0.44 (1657.9 examples/sec; 0.077 sec/batch)
2017-06-02 06:11:50.659607: step 145620, loss = 0.35 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:11:51.539089: step 145630, loss = 0.39 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:11:52.408627: step 145640, loss = 0.45 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:11:53.271998: step 145650, loss = 0.37 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:11:54.131006: step 145660, loss = 0.44 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:11:54.993669: step 145670, loss = 0.41 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:11:55.853488: step 145680, loss = 0.37 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:11:56.719293: step 145690, loss = 0.37 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:11:57.648676: step 145700, loss = 0.35 (1377.3 examples/sec; 0.093 sec/batch)
2017-06-02 06:11:58.427694: step 145710, loss = 0.47 (1643.1 examples/sec; 0.078 sec/batch)
2017-06-02 06:11:59.293900: step 145720, loss = 0.37 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:12:00.158515: step 145730, loss = 0.48 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:12:01.011482: step 145740, loss = 0.33 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:12:01.852592: step 145750, loss = 0.42 (1521.8 examples/sec; 0.084 sec/batch)
2017-06-02 06:12:02.704549: step 145760, loss = 0.34 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:12:03.579698: step 145770, loss = 0.36 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:12:04.441678: step 145780, loss = 0.47 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:12:05.294828: step 145790, loss = 0.48 (1500.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:12:06.239624: step 145800, loss = 0.37 (1354.8 examples/sec; 0.094 sec/batch)
2017-06-02 06:12:06.988017: step 145810, loss = 0.34 (1710.3 examples/sec; 0.075 sec/batch)
2017-06-02 06:12:07.837572: step 145820, loss = 0.39 (1506.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:12:08.695196: step 145830, loss = 0.36 (1492.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:12:09.575147: step 145840, loss = 0.29 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:12:10.414001: step 145850, loss = 0.38 (1525.9 examples/sec; 0.084 sec/batch)
2017-06-02 06:12:11.268185: step 145860, loss = 0.33 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:12:12.123030: step 145870, loss = 0.36 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:12:12.991164: step 145880, loss = 0.41 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:12:13.833252: step 145890, loss = 0.34 (1520.0 examples/sec; 0.084 sec/batch)
2017-06-02 06:12:14.830335: step 145900, loss = 0.40 (1283.7 examples/sec; 0.100 sec/batch)
2017-06-02 06:12:15.520548: step 145910, loss = 0.36 (1854.5 examples/sec; 0.069 sec/batch)
2017-06-02 06:12:16.386578: step 145920, loss = 0.42 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:12:17.247081: step 145930, loss = 0.39 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:12:18.097566: step 145940, loss = 0.33 (1505.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:12:18.952701: step 145950, loss = 0.43 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:12:19.814413: step 145960, loss = 0.43 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:12:20.679628: step 145970, loss = 0.43 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:12:21.536613: step 145980, loss = 0.36 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:12:22.416120: step 145990, loss = 0.38 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:12:23.387659: step 146000, loss = 0.30 (1317.5 examples/sec; 0.097 sec/batch)
2017-06-02 06:12:24.128894: step 146010, loss = 0.39 (1726.9 examples/sec; 0.074 sec/batch)
2017-06-02 06:12:24.990843: step 146020, loss = 0.35 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:12:25.834608: step 146030, loss = 0.39 (1517.0 examples/sec; 0.084 sec/batch)
2017-06-02 06:12:26.687583: step 146040, loss = 0.36 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:12:27.549674: step 146050, loss = 0.38 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:12:28.400560: step 146060, loss = 0.34 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:12:29.236651: step 146070, loss = 0.42 (1530.9 examples/sec; 0.084 sec/batch)
2017-06-02 06:12:30.107291: step 146080, loss = 0.52 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:12:30.967112: step 146090, loss = 0.48 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:12:31.946249: step 146100, loss = 0.38 (1307.3 examples/sec; 0.098 sec/batch)
2017-06-02 06:12:32.738355: step 146110, loss = 0.39 (1616.0 examples/sec; 0.079 sec/batch)
2017-06-02 06:12:33.618818: step 146120, loss = 0.40 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:12:34.477229: step 146130, loss = 0.40 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:12:35.334088: step 146140, loss = 0.32 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:12:36.191380: step 146150, loss = 0.31 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:12:37.046073: step 146160, loss = 0.30 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:12:37.918505: step 146170, loss = 0.36 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:12:38.775689: step 146180, loss = 0.32 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:12:39.623922: step 146190, loss = 0.37 (1509.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:12:40.568724: step 146200, loss = 0.40 (1354.8 examples/sec; 0.094 sec/batch)
2017-06-02 06:12:41.355984: step 146210, loss = 0.41 (1625.9 examples/sec; 0.079 sec/batch)
2017-06-02 06:12:42.231073: step 146220, loss = 0.49 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:12:43.081999: step 146230, loss = 0.34 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:12:43.959278: step 146240, loss = 0.51 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:12:44.853865: step 146250, loss = 0.38 (1430.8 examples/sec; 0.089 sec/batch)
2017-06-02 06:12:45.738200: step 146260, loss = 0.37 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:12:46.604842: step 146270, loss = 0.41 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:12:47.476814: step 146280, loss = 0.34 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:12:48.351682: step 146290, loss = 0.36 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:12:49.299356: step 146300, loss = 0.35 (1350.7 examples/sec; 0.095 sec/batch)
2017-06-02 06:12:50.074421: step 146310, loss = 0.35 (1651.5 examples/sec; 0.078 sec/batch)
2017-06-02 06:12:50.932339: step 146320, loss = 0.40 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:12:51.820995: step 146330, loss = 0.37 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 06:12:52.674511: step 146340, loss = 0.35 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:12:53.524218: step 146350, loss = 0.40 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:12:54.367907: step 146360, loss = 0.37 (1517.1 examples/sec; 0.084 sec/batch)
2017-06-02 06:12:55.237344: step 146370, loss = 0.32 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:12:56.098527: step 146380, loss = 0.37 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:12:56.963087: step 146390, loss = 0.37 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:12:57.916581: step 146400, loss = 0.35 (1342.4 examples/sec; 0.095 sec/batch)
2017-06-02 06:12:58.705743: step 146410, loss = 0.43 (1622.0 examples/sec; 0.079 sec/batch)
2017-06-02 06:12:59.599921: step 146420, loss = 0.42 (1431.5 examples/sec; 0.089 sec/batch)
2017-06-02 06:13:00.463221: step 146430, loss = 0.37 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:13:01.358342: step 146440, loss = 0.42 (1430.0 examples/sec; 0.090 sec/batch)
2017-06-02 06:13:02.238413: step 146450, loss = 0.33 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:13:03.074782: step 146460, loss = 0.49 (1530.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:13:03.952927: step 146470, loss = 0.38 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:13:04.801456: step 146480, loss = 0.38 (1508.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:13:05.652424: step 146490, loss = 0.32 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:13:06.613861: step 146500, loss = 0.36 (1331.3 examples/sec; 0.096 sec/batch)
2017-06-02 06:13:07.365081: step 146510, loss = 0.33 (1703.9 examples/sec; 0.075 sec/batch)
2017-06-02 06:13:08.243339: step 146520, loss = 0.40 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:13:09.103297: step 146530, loss = 0.42 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:13:10.006590: step 146540, loss = 0.33 (1417.0 examples/sec; 0.090 sec/batch)
2017-06-02 06:13:10.852108: step 146550, loss = 0.43 (1513.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:13:11.703896: step 146560, loss = 0.32 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:13:12.561329: step 146570, loss = 0.40 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:13:13.404167: step 146580, loss = 0.42 (1518.7 examples/sec; 0.084 sec/batch)
2017-06-02 06:13:14.276654: step 146590, loss = 0.41 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:13:15.259523: step 146600, loss = 0.28 (1302.3 examples/sec; 0.098 sec/batch)
2017-06-02 06:13:16.023592: step 146610, loss = 0.36 (1675.3 examples/sec; 0.076 sec/batch)
2017-06-02 06:13:16.900547: step 146620, loss = 0.36 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:13:17.787587: step 146630, loss = 0.32 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:13:18.662347: step 146640, loss = 0.30 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:13:19.549292: step 146650, loss = 0.36 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 06:13:20.403714: step 146660, loss = 0.34 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:13:21.279501: step 146670, loss = 0.43 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:13:22.125135: step 146680, loss = 0.40 (1513.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:13:22.978425: step 146690, loss = 0.39 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:13:23.979616: step 146700, loss = 0.38 (1278.5 examples/sec; 0.100 sec/batch)
2017-06-02 06:13:24.765742: step 146710, loss = 0.32 (1628.3 examples/sec; 0.079 sec/batch)
2017-06-02 06:13:25.611833: step 146720, loss = 0.30 (1512.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:13:26.472350: step 146730, loss = 0.41 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:13:27.344006: step 146740, loss = 0.45 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:13:28.229275: step 146750, loss = 0.34 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 06:13:29.084093: step 146760, loss = 0.40 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:13:29.935450: step 146770, loss = 0.41 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:13:30.811654: step 146780, loss = 0.36 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:13:31.668914: step 146790, loss = 0.38 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:13:32.687546: step 146800, loss = 0.40 (1256.6 examples/sec; 0.102 sec/batch)
2017-06-02 06:13:33.403196: step 146810, loss = 0.35 (1788.6 examples/sec; 0.072 sec/batch)
2017-06-02 06:13:34.256102: step 146820, loss = 0.29 (1500.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:13:35.124240: step 146830, loss = 0.33 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:13:35.985329: step 146840, loss = 0.37 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:13:36.849287: step 146850, loss = 0.37 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:13:37.711471: step 146860, loss = 0.40 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:13:38.583111: step 146870, loss = 0.37 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:13:39.433420: step 146880, loss = 0.35 (1505.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:13:40.301024: step 146890, loss = 0.38 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:13:41.259217: step 146900, loss = 0.39 (1335.9 examples/sec; 0.096 sec/batch)
2017-06-02 06:13:42.022382: step 146910, loss = 0.28 (1677.2 examples/sec; 0.076 sec/batch)
2017-06-02 06:13:42.885398: step 146920, loss = 0.38 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:13:43.750828: step 146930, loss = 0.38 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:13:44.609565: step 146940, loss = 0.30 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:13:45.473452: step 146950, loss = 0.38 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:13:46.324250: step 146960, loss = 0.25 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:13:47.196236: step 146970, loss = 0.29 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:13:48.069139: step 146980, loss = 0.39 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:13:48.932038: step 146990, loss = 0.42 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:13:49.923241: step 147000, loss = 0.31 (1291.4 examples/sec; 0.099 sec/batch)
2017-06-02 06:13:50.669695: step 147010, loss = 0.33 (1714.8 examples/sec; 0.075 sec/batch)
2017-06-02 06:13:51.527760: step 147020, loss = 0.42 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:13:52.411628: step 147030, loss = 0.30 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:13:53.276486: step 147040, loss = 0.42 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:13:54.126671: step 147050, loss = 0.36 (1505.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:13:54.981563: step 147060, loss = 0.48 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:13:55.860909: step 147070, loss = 0.46 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:13:56.731489: step 147080, loss = 0.33 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:13:57.602834: step 147090, loss = 0.35 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:13:58.587576: step 147100, loss = 0.41 (1299.8 examples/sec; 0.098 sec/batch)
2017-06-02 06:13:59.345912: step 147110, loss = 0.44 (1687.9 examples/sec; 0.076 sec/batch)
2017-06-02 06:14:00.219946: step 147120, loss = 0.41 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:14:01.079811: step 147130, loss = 0.41 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:14:01.948131: step 147140, loss = 0.36 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:14:02.806414: step 147150, loss = 0.61 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:14:03.687234: step 147160, loss = 0.38 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:14:04.542844: step 147170, loss = 0.41 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:14:05.396962: step 147180, loss = 0.33 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:14:06.260140: step 147190, loss = 0.29 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:14:07.241001: step 147200, loss = 0.26 (1305.0 examples/sec; 0.098 sec/batch)
2017-06-02 06:14:08.029821: step 147210, loss = 0.35 (1622.7 examples/sec; 0.079 sec/batch)
2017-06-02 06:14:08.894892: step 147220, loss = 0.29 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:14:09.758172: step 147230, loss = 0.37 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:14:10.635249: step 147240, loss = 0.35 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:14:11.498625: step 147250, loss = 0.31 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:14:12.361276: step 147260, loss = 0.34 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:14:13.220561: step 147270, loss = 0.39 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:14:14.060822: step 147280, loss = 0.44 (1523.3 examples/sec; 0.084 sec/batch)
2017-06-02 06:14:14.941540: step 147290, loss = 0.31 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:14:15.888867: step 147300, loss = 0.32 (1351.2 examples/sec; 0.095 sec/batch)
2017-06-02 06:14:16.639085: step 147310, loss = 0.44 (1706.2 examples/sec; 0.075 sec/batch)
2017-06-02 06:14:17.469760: step 147320, loss = 0.48 (1541.0 examples/sec; 0.083 sec/batch)
2017-06-02 06:14:18.354869: step 147330, loss = 0.46 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 06:14:19.199930: step 147340, loss = 0.41 (1514.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:14:20.093718: step 147350, loss = 0.31 (1432.1 examples/sec; 0.089 sec/batch)
2017-06-02 06:14:20.960047: step 147360, loss = 0.41 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:14:21.837312: step 147370, loss = 0.32 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:14:22.706186: step 147380, loss = 0.33 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:14:23.564170: step 147390, loss = 0.35 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:14:24.577844: step 147400, loss = 0.33 (1262.7 examples/sec; 0.101 sec/batch)
2017-06-02 06:14:25.308074: step 147410, loss = 0.45 (1752.9 examples/sec; 0.073 sec/batch)
2017-06-02 06:14:26.203541: step 147420, loss = 0.39 (1429.4 examples/sec; 0.090 sec/batch)
2017-06-02 06:14:27.064065: step 147430, loss = 0.50 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:14:27.953012: step 147440, loss = 0.34 (1439.9 examples/sec; 0.089 sec/batch)
2017-06-02 06:14:28.840269: step 147450, loss = 0.36 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 06:14:29.707169: step 147460, loss = 0.43 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:14:30.584311: step 147470, loss = 0.33 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:14:31.433191: step 147480, loss = 0.38 (1507.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:14:32.292671: step 147490, loss = 0.35 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:14:33.247086: step 147500, loss = 0.42 (1341.1 examples/sec; 0.095 sec/batch)
2017-06-02 06:14:34.036469: step 147510, loss = 0.40 (1621.5 examples/sec; 0.079 sec/batch)
2017-06-02 06:14:34.865874: step 147520, loss = 0.38 (1543.3 examples/sec; 0.083 sec/batch)
2017-06-02 06:14:35.739727: step 147530, loss = 0.37 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:14:36.597288: step 147540, loss = 0.50 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:14:37.441341: step 147550, loss = 0.28 (1516.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:14:38.284933: step 147560, loss = 0.35 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:14:39.175825: step 147570, loss = 0.40 (1436.7 examples/sec; 0.089 sec/batch)
2017-06-02 06:14:40.048031: step 147580, loss = 0.38 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:14:40.916861: step 147590, loss = 0.41 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:14:41.869173: step 147600, loss = 0.35 (1344.1 examples/sec; 0.095 sec/batch)
2017-06-02 06:14:42.638843: step 147610, loss = 0.42 (1663.1 examples/sec; 0.077 sec/batch)
2017-06-02 06:14:43.481702: step 147620, loss = 0.33 (1518.6 examples/sec; 0.084 sec/batch)
2017-06-02 06:14:44.329973: step 147630, loss = 0.36 (1508.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:14:45.217091: step 147640, loss = 0.34 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 06:14:46.054500: step 147650, loss = 0.36 (1528.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:14:46.929753: step 147660, loss = 0.32 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:14:47.777863: step 147670, loss = 0.35 (1509.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:14:48.622060: step 147680, loss = 0.34 (1516.2 examples/sec; 0.084 sec/batch)
2017-06-02 06:14:49.471251: step 147690, loss = 0.29 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:14:50.430366: step 147700, loss = 0.43 (1334.6 examples/sec; 0.096 sec/batch)
2017-06-02 06:14:51.183746: step 147710, loss = 0.33 (1699.0 examples/sec; 0.075 sec/batch)
2017-06-02 06:14:52.047346: step 147720, loss = 0.32 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:14:52.909380: step 147730, loss = 0.42 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:14:53.759208: step 147740, loss = 0.32 (1506.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:14:54.611916: step 147750, loss = 0.35 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:14:55.471562: step 147760, loss = 0.37 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:14:56.320769: step 147770, loss = 0.40 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:14:57.176342: step 147780, loss = 0.28 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:14:58.028635: step 147790, loss = 0.34 (1501.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:14:58.957158: step 147800, loss = 0.41 (1378.5 examples/sec; 0.093 sec/batch)
2017-06-02 06:14:59.731583: step 147810, loss = 0.36 (1652.9 examples/sec; 0.077 sec/batch)
2017-06-02 06:15:00.589831: step 147820, loss = 0.32 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:15:01.440517: step 147830, loss = 0.32 (1504.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:15:02.276390: step 147840, loss = 0.42 (1531.3 examples/sec; 0.084 sec/batch)
2017-06-02 06:15:03.112379: step 147850, loss = 0.30 (1531.1 examples/sec; 0.084 sec/batch)
2017-06-02 06:15:03.981392: step 147860, loss = 0.34 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:15:04.822823: step 147870, loss = 0.42 (1521.2 examples/sec; 0.084 sec/batch)
2017-06-02 06:15:05.655823: step 147880, loss = 0.45 (1536.6 examples/sec; 0.083 sec/batch)
2017-06-02 06:15:06.514713: step 147890, loss = 0.38 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:15:07.453511: step 147900, loss = 0.33 (1363.4 examples/sec; 0.094 sec/batch)
2017-06-02 06:15:08.211658: step 147910, loss = 0.35 (1688.4 examples/sec; 0.076 sec/batch)
2017-06-02 06:15:09.055221: step 147920, loss = 0.37 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:15:09.928631: step 147930, loss = 0.43 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:15:10.785430: step 147940, loss = 0.32 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:15:11.629192: step 147950, loss = 0.38 (1517.0 examples/sec; 0.084 sec/batch)
2017-06-02 06:15:12.495298: step 147960, loss = 0.37 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:15:13.382542: step 147970, loss = 0.35 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 06:15:14.260121: step 147980, loss = 0.38 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:15:15.121147: step 147990, loss = 0.35 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:15:16.082715: step 148000, loss = 0.32 (1331.2 examples/sec; 0.096 sec/batch)
2017-06-02 06:15:16.869694: step 148010, loss = 0.41 (1626.5 examples/sec; 0.079 sec/batch)
2017-06-02 06:15:17.728471: step 148020, loss = 0.40 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:15:18.600746: step 148030, loss = 0.43 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:15:19.477016: step 148040, loss = 0.33 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:15:20.334469: step 148050, loss = 0.48 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:15:21.169648: step 148060, loss = 0.41 (1532.6 examples/sec; 0.084 sec/batch)
2017-06-02 06:15:22.038140: step 148070, loss = 0.32 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:15:22.901057: step 148080, loss = 0.48 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:15:23.776875: step 148090, loss = 0.34 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:15:24.738241: step 148100, loss = 0.41 (1331.4 examples/sec; 0.096 sec/batch)
2017-06-02 06:15:25.500586: step 148110, loss = 0.45 (1679.0 examples/sec; 0.076 sec/batch)
2017-06-02 06:15:26.349119: step 148120, loss = 0.33 (1508.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:15:27.212479: step 148130, loss = 0.34 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:15:28.052553: step 148140, loss = 0.35 (1523.7 examples/sec; 0.084 sec/batch)
2017-06-02 06:15:28.908395: step 148150, loss = 0.40 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:15:29.761773: step 148160, loss = 0.46 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:15:30.598107: step 148170, loss = 0.37 (1530.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:15:31.468167: step 148180, loss = 0.34 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:15:32.315283: step 148190, loss = 0.31 (1511.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:15:33.282827: step 148200, loss = 0.32 (1322.9 examples/sec; 0.097 sec/batch)
2017-06-02 06:15:34.037230: step 148210, loss = 0.28 (1696.7 examples/sec; 0.075 sec/batch)
2017-06-02 06:15:34.875391: step 148220, loss = 0.35 (1527.2 examples/sec; 0.084 sec/batch)
2017-06-02 06:15:35.757366: step 148230, loss = 0.35 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:15:36.619647: step 148240, loss = 0.41 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:15:37.482749: step 148250, loss = 0.32 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:15:38.333179: step 148260, loss = 0.33 (1505.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:15:39.207542: step 148270, loss = 0.33 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:15:40.058926: step 148280, loss = 0.49 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:15:40.939181: step 148290, loss = 0.37 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:15:41.917679: step 148300, loss = 0.34 (1308.1 examples/sec; 0.098 sec/batch)
2017-06-02 06:15:42.650902: step 148310, loss = 0.27 (1745.7 examples/sec; 0.073 sec/batch)
2017-06-02 06:15:43.514529: step 148320, loss = 0.35 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:15:44.372030: step 148330, loss = 0.41 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:15:45.225003: step 148340, loss = 0.44 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:15:46.079931: step 148350, loss = 0.38 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:15:46.940702: step 148360, loss = 0.47 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:15:47.813779: step 148370, loss = 0.36 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:15:48.642602: step 148380, loss = 0.31 (1544.3 examples/sec; 0.083 sec/batch)
2017-06-02 06:15:49.520381: step 148390, loss = 0.34 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:15:50.478672: step 148400, loss = 0.32 (1335.7 examples/sec; 0.096 sec/batch)
2017-06-02 06:15:51.239601: step 148410, loss = 0.34 (1682.2 examples/sec; 0.076 sec/batch)
2017-06-02 06:15:52.100257: step 148420, loss = 0.43 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:15:52.949727: step 148430, loss = 0.40 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:15:53.823962: step 148440, loss = 0.34 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:15:54.674708: step 148450, loss = 0.26 (1504.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:15:55.542099: step 148460, loss = 0.37 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:15:56.407406: step 148470, loss = 0.35 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:15:57.284007: step 148480, loss = 0.33 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:15:58.145113: step 148490, loss = 0.36 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:15:59.146562: step 148500, loss = 0.44 (1278.1 examples/sec; 0.100 sec/batch)
2017-06-02 06:15:59.856228: step 148510, loss = 0.38 (1803.7 examples/sec; 0.071 sec/batch)
2017-06-02 06:16:00.735163: step 148520, loss = 0.30 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:16:01.599895: step 148530, loss = 0.26 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:16:02.475093: step 148540, loss = 0.38 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:16:03.341698: step 148550, loss = 0.37 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:16:04.224953: step 148560, loss = 0.25 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:16:05.081677: step 148570, loss = 0.35 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:16:05.943055: step 148580, loss = 0.35 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:16:06.807772: step 148590, loss = 0.37 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:16:07.792265: step 148600, loss = 0.46 (1300.2 examples/sec; 0.098 sec/batch)
2017-06-02 06:16:08.543217: step 148610, loss = 0.30 (1704.5 examples/sec; 0.075 sec/batch)
2017-06-02 06:16:09.395474: step 148620, loss = 0.27 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:16:10.281129: step 148630, loss = 0.33 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 06:16:11.162241: step 148640, loss = 0.40 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:16:12.013276: step 148650, loss = 0.36 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:16:12.887866: step 148660, loss = 0.42 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:16:13.782827: step 148670, loss = 0.39 (1430.2 examples/sec; 0.089 sec/batch)
2017-06-02 06:16:14.642650: step 148680, loss = 0.45 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:16:15.500542: step 148690, loss = 0.32 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:16:16.493092: step 148700, loss = 0.45 (1289.6 examples/sec; 0.099 sec/batch)
2017-06-02 06:16:17.267301: step 148710, loss = 0.36 (1653.3 examples/sec; 0.077 sec/batch)
2017-06-02 06:16:18.136956: step 148720, loss = 0.52 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:16:18.988527: step 148730, loss = 0.32 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:16:19.848533: step 148740, loss = 0.30 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:16:20.714061: step 148750, loss = 0.30 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:16:21.576689: step 148760, loss = 0.46 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:16:22.445241: step 148770, loss = 0.38 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:16:23.297112: step 148780, loss = 0.46 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:16:24.133966: step 148790, loss = 0.31 (1529.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:16:25.110373: step 148800, loss = 0.32 (1310.9 examples/sec; 0.098 sec/batch)
2017-06-02 06:16:25.881644: step 148810, loss = 0.28 (1659.6 examples/sec; 0.077 sec/batch)
2017-06-02 06:16:26.745581: step 148820, loss = 0.43 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:16:27.624289: step 148830, loss = 0.29 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:16:28.484662: step 148840, loss = 0.45 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:16:29.349158: step 148850, loss = 0.45 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:16:30.222534: step 148860, loss = 0.26 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:16:31.080259: step 148870, loss = 0.37 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:16:31.912324: step 148880, loss = 0.43 (1538.4 examples/sec; 0.083 sec/batch)
2017-06-02 06:16:32.774844: step 148890, loss = 0.29 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:16:33.731097: step 148900, loss = 0.32 (1338.5 examples/sec; 0.096 sec/batch)
2017-06-02 06:16:34.512979: step 148910, loss = 0.31 (1637.1 examples/sec; 0.078 sec/batch)
2017-06-02 06:16:35.422270: step 148920, loss = 0.38 (1407.7 examples/sec; 0.091 sec/batch)
2017-06-02 06:16:36.261218: step 148930, loss = 0.35 (1525.7 examples/sec; 0.084 sec/batch)
2017-06-02 06:16:37.135747: step 148940, loss = 0.32 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:16:37.982442: step 148950, loss = 0.32 (1511.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:16:38.867969: step 148960, loss = 0.29 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 06:16:39.741569: step 148970, loss = 0.32 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:16:40.595038: step 148980, loss = 0.36 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:16:41.476542: step 148990, loss = 0.28 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:16:42.419399: step 149000, loss = 0.38 (1357.5 examples/sec; 0.094 sec/batch)
2017-06-02 06:16:43.182384: step 149010, loss = 0.29 (1677.6 examples/sec; 0.076 sec/batch)
2017-06-02 06:16:44.043244: step 149020, loss = 0.32 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:16:44.905747: step 149030, loss = 0.27 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:16:45.756382: step 149040, loss = 0.44 (1504.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:16:46.651249: step 149050, loss = 0.38 (1430.4 examples/sec; 0.089 sec/batch)
2017-06-02 06:16:47.500316: step 149060, loss = 0.40 (1507.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:16:48.373444: step 149070, loss = 0.27 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:16:49.224665: step 149080, loss = 0.26 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:16:50.088926: step 149090, loss = 0.48 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:16:51.103214: step 149100, loss = 0.25 (1262.0 examples/sec; 0.101 sec/batch)
2017-06-02 06:16:51.789522: step 149110, loss = 0.32 (1865.0 examples/sec; 0.069 sec/batch)
2017-06-02 06:16:52.643344: step 149120, loss = 0.38 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:16:53.511537: step 149130, loss = 0.24 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:16:54.378386: step 149140, loss = 0.40 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:16:55.250911: step 149150, loss = 0.42 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:16:56.118110: step 149160, loss = 0.27 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:16:56.977089: step 149170, loss = 0.33 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:16:57.831699: step 149180, loss = 0.34 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:16:58.723907: step 149190, loss = 0.30 (1434.6 examples/sec; 0.089 sec/batch)
2017-06-02 06:16:59.700375: step 149200, loss = 0.35 (1310.8 examples/sec; 0.098 sec/batch)
2017-06-02 06:17:00.482076: step 149210, loss = 0.24 (1637.5 examples/sec; 0.078 sec/batch)
2017-06-02 06:17:01.368051: step 149220, loss = 0.39 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 06:17:02.243508: step 149230, loss = 0.38 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:17:03.115221: step 149240, loss = 0.37 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:17:03.995292: step 149250, loss = 0.35 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:17:04.863450: step 149260, loss = 0.48 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:17:05.725623: step 149270, loss = 0.40 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:17:06.610723: step 149280, loss = 0.38 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 06:17:07.496337: step 149290, loss = 0.31 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 06:17:08.470033: step 149300, loss = 0.36 (1314.6 examples/sec; 0.097 sec/batch)
2017-06-02 06:17:09.219124: step 149310, loss = 0.31 (1708.8 examples/sec; 0.075 sec/batch)
2017-06-02 06:17:10.074835: step 149320, loss = 0.40 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:17:10.965820: step 149330, loss = 0.39 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 06:17:11.858424: step 149340, loss = 0.33 (1434.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:17:12.746038: step 149350, loss = 0.38 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 06:17:13.625653: step 149360, loss = 0.39 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:17:14.492084: step 149370, loss = 0.32 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:17:15.348449: step 149380, loss = 0.33 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:17:16.239728: step 149390, loss = 0.31 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 06:17:17.212859: step 149400, loss = 0.33 (1315.3 examples/sec; 0.097 sec/batch)
2017-06-02 06:17:18.018347: step 149410, loss = 0.37 (1589.1 examples/sec; 0.081 sec/batch)
2017-06-02 06:17:18.919394: step 149420, loss = 0.38 (1420.6 examples/sec; 0.090 sec/batch)
2017-06-02 06:17:19.786161: step 149430, loss = 0.43 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:17:20.654447: step 149440, loss = 0.37 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:17:21.530554: step 149450, loss = 0.36 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:17:22.425741: step 149460, loss = 0.34 (1429.9 examples/sec; 0.090 sec/batch)
2017-06-02 06:17:23.319047: step 149470, loss = 0.40 (1432.9 examples/sec; 0.089 sec/batch)
2017-06-02 06:17:24.186921: step 149480, loss = 0.32 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:17:25.067238: step 149490, loss = 0.28 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:17:26.056192: step 149500, loss = 0.44 (1294.3 examples/sec; 0.099 sec/batch)
2017-06-02 06:17:26.801347: step 149510, loss = 0.32 (1717.8 examples/sec; 0.075 sec/batch)
2017-06-02 06:17:27.686751: step 149520, loss = 0.42 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 06:17:28.555475: step 149530, loss = 0.41 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:17:29.436990: step 149540, loss = 0.45 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:17:30.303791: step 149550, loss = 0.30 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:17:31.188790: step 149560, loss = 0.28 (1446.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:17:32.077677: step 149570, loss = 0.39 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:17:32.945155: step 149580, loss = 0.30 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:17:33.829152: step 149590, loss = 0.37 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:17:34.763261: step 149600, loss = 0.32 (1370.3 examples/sec; 0.093 sec/batch)
2017-06-02 06:17:35.516363: step 149610, loss = 0.37 (1699.7 examples/sec; 0.075 sec/batch)
2017-06-02 06:17:36.392409: step 149620, loss = 0.28 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:17:37.253687: step 149630, loss = 0.36 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:17:38.123292: step 149640, loss = 0.30 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:17:38.983635: step 149650, loss = 0.33 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:17:39.841484: step 149660, loss = 0.37 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:17:40.717347: step 149670, loss = 0.33 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:17:41.612007: step 149680, loss = 0.24 (1430.7 examples/sec; 0.089 sec/batch)
2017-06-02 06:17:42.483334: step 149690, loss = 0.34 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:17:43.464324: step 149700, loss = 0.31 (1304.8 examples/sec; 0.098 sec/batch)
2017-06-02 06:17:44.241960: step 149710, loss = 0.26 (1646.0 examples/sec; 0.078 sec/batch)
2017-06-02 06:17:45.083800: step 149720, loss = 0.36 (1520.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:17:45.938225: step 149730, loss = 0.25 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:17:46.787603: step 149740, loss = 0.37 (1507.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:17:47.653805: step 149750, loss = 0.38 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:17:48.528558: step 149760, loss = 0.31 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:17:49.372439: step 149770, loss = 0.34 (1516.8 examples/sec; 0.084 sec/batch)
2017-06-02 06:17:50.218128: step 149780, loss = 0.27 (1513.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:17:51.079364: step 149790, loss = 0.40 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:17:52.041753: step 149800, loss = 0.39 (1330.0 examples/sec; 0.096 sec/batch)
2017-06-02 06:17:52.820866: step 149810, loss = 0.34 (1642.9 examples/sec; 0.078 sec/batch)
2017-06-02 06:17:53.694810: step 149820, loss = 0.29 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:17:54.578880: step 149830, loss = 0.36 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:17:55.452232: step 149840, loss = 0.36 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:17:56.309916: step 149850, loss = 0.32 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:17:57.191486: step 149860, loss = 0.33 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:17:58.064174: step 149870, loss = 0.41 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:17:58.925007: step 149880, loss = 0.37 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:17:59.781403: step 149890, loss = 0.42 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:18:00.747378: step 149900, loss = 0.30 (1325.1 examples/sec; 0.097 sec/batch)
2017-06-02 06:18:01.531913: step 149910, loss = 0.34 (1631.5 examples/sec; 0.078 sec/batch)
2017-06-02 06:18:02.417745: step 149920, loss = 0.34 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:18:03.276896: step 149930, loss = 0.39 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:18:04.144797: step 149940, loss = 0.33 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:18:04.977118: step 149950, loss = 0.32 (1537.9 examples/sec; 0.083 sec/batch)
2017-06-02 06:18:05.836332: step 149960, loss = 0.50 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:18:06.706992: step 149970, loss = 0.28 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:18:07.575470: step 149980, loss = 0.38 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:18:08.441707: step 149990, loss = 0.36 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:18:09.405299: step 150000, loss = 0.31 (1328.4 examples/sec; 0.096 sec/batch)
2017-06-02 06:18:10.157260: step 150010, loss = 0.38 (1702.2 examples/sec; 0.075 sec/batch)
2017-06-02 06:18:11.008849: step 150020, loss = 0.35 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:18:11.891769: step 150030, loss = 0.39 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:18:12.787900: step 150040, loss = 0.33 (1428.4 examples/sec; 0.090 sec/batch)
2017-06-02 06:18:13.644666: step 150050, loss = 0.36 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:18:14.506337: step 150060, loss = 0.32 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:18:15.343439: step 150070, loss = 0.38 (1529.1 examples/sec; 0.084 sec/batch)
2017-06-02 06:18:16.226458: step 150080, loss = 0.34 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:18:17.100653: step 150090, loss = 0.41 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:18:18.070241: step 150100, loss = 0.40 (1320.1 examples/sec; 0.097 sec/batch)
2017-06-02 06:18:18.856233: step 150110, loss = 0.40 (1628.5 examples/sec; 0.079 sec/batch)
2017-06-02 06:18:19.715993: step 150120, loss = 0.33 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:18:20.570395: step 150130, loss = 0.39 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:18:21.443599: step 150140, loss = 0.30 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:18:22.317970: step 150150, loss = 0.33 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:18:23.195226: step 150160, loss = 0.30 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:18:24.071554: step 150170, loss = 0.35 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:18:24.942596: step 150180, loss = 0.34 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:18:25.813472: step 150190, loss = 0.58 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:18:26.809158: step 150200, loss = 0.32 (1285.5 examples/sec; 0.100 sec/batch)
2017-06-02 06:18:27.578051: step 150210, loss = 0.30 (1664.7 examples/sec; 0.077 sec/batch)
2017-06-02 06:18:28.440012: step 150220, loss = 0.32 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:18:29.282598: step 150230, loss = 0.37 (1519.1 examples/sec; 0.084 sec/batch)
2017-06-02 06:18:30.137016: step 150240, loss = 0.36 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:18:31.007035: step 150250, loss = 0.37 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:18:31.879965: step 150260, loss = 0.36 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:18:32.752913: step 150270, loss = 0.28 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:18:33.631457: step 150280, loss = 0.32 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:18:34.495768: step 150290, loss = 0.33 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:18:35.456054: step 150300, loss = 0.35 (1332.9 examples/sec; 0.096 sec/batch)
2017-06-02 06:18:36.220598: step 150310, loss = 0.38 (1674.2 examples/sec; 0.076 sec/batch)
2017-06-02 06:18:37.075474: step 150320, loss = 0.31 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:18:37.964969: step 150330, loss = 0.40 (1439.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:18:38.827194: step 150340, loss = 0.34 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:18:39.713178: step 150350, loss = 0.33 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 06:18:40.579670: step 150360, loss = 0.31 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:18:41.439776: step 150370, loss = 0.39 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:18:42.320580: step 150380, loss = 0.41 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:18:43.177526: step 150390, loss = 0.38 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:18:44.163219: step 150400, loss = 0.40 (1298.6 examples/sec; 0.099 sec/batch)
2017-06-02 06:18:44.945897: step 150410, loss = 0.31 (1635.4 examples/sec; 0.078 sec/batch)
2017-06-02 06:18:45.797082: step 150420, loss = 0.27 (1503.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:18:46.670348: step 150430, loss = 0.22 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:18:47.533059: step 150440, loss = 0.39 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:18:48.403528: step 150450, loss = 0.43 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:18:49.286885: step 150460, loss = 0.28 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:18:50.149599: step 150470, loss = 0.26 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:18:51.013329: step 150480, loss = 0.36 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:18:51.872814: step 150490, loss = 0.36 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:18:52.856657: step 150500, loss = 0.33 (1301.0 examples/sec; 0.098 sec/batch)
2017-06-02 06:18:53.607653: step 150510, loss = 0.31 (1704.4 examples/sec; 0.075 sec/batch)
2017-06-02 06:18:54.461943: step 150520, loss = 0.47 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:18:55.322129: step 150530, loss = 0.38 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:18:56.185412: step 150540, loss = 0.27 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:18:57.069776: step 150550, loss = 0.39 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:18:57.916069: step 150560, loss = 0.34 (1512.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:18:58.787222: step 150570, loss = 0.35 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:18:59.669095: step 150580, loss = 0.40 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:19:00.563546: step 150590, loss = 0.36 (1431.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:19:01.550867: step 150600, loss = 0.31 (1296.4 examples/sec; 0.099 sec/batch)
2017-06-02 06:19:02.359756: step 150610, loss = 0.33 (1582.5 examples/sec; 0.081 sec/batch)
2017-06-02 06:19:03.242866: step 150620, loss = 0.31 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:19:04.095717: step 150630, loss = 0.28 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:19:04.959169: step 150640, loss = 0.32 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:19:05.822272: step 150650, loss = 0.33 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:19:06.673768: step 150660, loss = 0.38 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:19:07.545519: step 150670, loss = 0.33 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:19:08.408288: step 150680, loss = 0.47 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:19:09.247956: step 150690, loss = 0.29 (1524.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:19:10.263988: step 150700, loss = 0.39 (1259.8 examples/sec; 0.102 sec/batch)
2017-06-02 06:19:10.982868: step 150710, loss = 0.32 (1780.6 examples/sec; 0.072 sec/batch)
2017-06-02 06:19:11.846001: step 150720, loss = 0.27 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:19:12.684881: step 150730, loss = 0.32 (1525.8 examples/sec; 0.084 sec/batch)
2017-06-02 06:19:13.547668: step 150740, loss = 0.34 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:19:14.393071: step 150750, loss = 0.29 (1514.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:19:15.268870: step 150760, loss = 0.31 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:19:16.096558: step 150770, loss = 0.23 (1546.5 examples/sec; 0.083 sec/batch)
2017-06-02 06:19:16.970804: step 150780, loss = 0.42 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:19:17.828645: step 150790, loss = 0.24 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:19:18.788818: step 150800, loss = 0.38 (1333.1 examples/sec; 0.096 sec/batch)
2017-06-02 06:19:19.561248: step 150810, loss = 0.32 (1657.1 examples/sec; 0.077 sec/batch)
2017-06-02 06:19:20.406078: step 150820, loss = 0.38 (1515.1 examples/sec; 0.084 sec/batch)
2017-06-02 06:19:21.256193: step 150830, loss = 0.35 (1505.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:19:22.121395: step 150840, loss = 0.39 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:19:22.981846: step 150850, loss = 0.39 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:19:23.831351: step 150860, loss = 0.36 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:19:24.697522: step 150870, loss = 0.45 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:19:25.581101: step 150880, loss = 0.32 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:19:26.445266: step 150890, loss = 0.39 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:19:27.387086: step 150900, loss = 0.35 (1359.1 examples/sec; 0.094 sec/batch)
2017-06-02 06:19:28.181913: step 150910, loss = 0.36 (1610.4 examples/sec; 0.079 sec/batch)
2017-06-02 06:19:29.081576: step 150920, loss = 0.34 (1422.7 examples/sec; 0.090 sec/batch)
2017-06-02 06:19:29.968835: step 150930, loss = 0.36 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 06:19:30.865394: step 150940, loss = 0.29 (1427.7 examples/sec; 0.090 sec/batch)
2017-06-02 06:19:31.748297: step 150950, loss = 0.25 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:19:32.626928: step 150960, loss = 0.34 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:19:33.490515: step 150970, loss = 0.30 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:19:34.376475: step 150980, loss = 0.36 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 06:19:35.252318: step 150990, loss = 0.29 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:19:36.291923: step 151000, loss = 0.38 (1231.2 examples/sec; 0.104 sec/batch)
2017-06-02 06:19:37.030999: step 151010, loss = 0.39 (1731.9 examples/sec; 0.074 sec/batch)
2017-06-02 06:19:37.898091: step 151020, loss = 0.28 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:19:38.788975: step 151030, loss = 0.27 (1436.7 examples/sec; 0.089 sec/batch)
2017-06-02 06:19:39.667325: step 151040, loss = 0.40 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:19:40.513064: step 151050, loss = 0.33 (1513.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:19:41.358946: step 151060, loss = 0.24 (1513.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:19:42.219005: step 151070, loss = 0.38 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:19:43.088655: step 151080, loss = 0.31 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:19:43.940863: step 151090, loss = 0.39 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:19:44.922763: step 151100, loss = 0.31 (1303.6 examples/sec; 0.098 sec/batch)
2017-06-02 06:19:45.722665: step 151110, loss = 0.32 (1600.2 examples/sec; 0.080 sec/batch)
2017-06-02 06:19:46.591485: step 151120, loss = 0.40 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:19:47.472164: step 151130, loss = 0.42 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:19:48.355189: step 151140, loss = 0.32 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:19:49.237537: step 151150, loss = 0.40 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:19:50.105690: step 151160, loss = 0.36 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:19:50.991965: step 151170, loss = 0.27 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 06:19:51.943425: step 151180, loss = 0.39 (1345.3 examples/sec; 0.095 sec/batch)
2017-06-02 06:19:52.828817: step 151190, loss = 0.37 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 06:19:53.809498: step 151200, loss = 0.29 (1305.2 examples/sec; 0.098 sec/batch)
2017-06-02 06:19:54.594187: step 151210, loss = 0.37 (1631.2 examples/sec; 0.078 sec/batch)
2017-06-02 06:19:55.426100: step 151220, loss = 0.32 (1538.6 examples/sec; 0.083 sec/batch)
2017-06-02 06:19:56.306164: step 151230, loss = 0.30 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:19:57.173336: step 151240, loss = 0.33 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:19:58.026202: step 151250, loss = 0.29 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:19:58.875988: step 151260, loss = 0.39 (1506.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:19:59.737852: step 151270, loss = 0.34 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:20:00.596634: step 151280, loss = 0.31 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:20:01.465306: step 151290, loss = 0.31 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:20:02.411418: step 151300, loss = 0.34 (1352.9 examples/sec; 0.095 sec/batch)
2017-06-02 06:20:03.153332: step 151310, loss = 0.27 (1725.3 examples/sec; 0.074 sec/batch)
2017-06-02 06:20:04.015023: step 151320, loss = 0.36 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:20:04.871313: step 151330, loss = 0.41 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:20:05.733553: step 151340, loss = 0.39 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:20:06.588250: step 151350, loss = 0.39 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:20:07.452598: step 151360, loss = 0.48 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:20:08.328526: step 151370, loss = 0.44 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:20:09.201668: step 151380, loss = 0.41 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:20:10.079924: step 151390, loss = 0.31 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:20:11.019361: step 151400, loss = 0.30 (1362.5 examples/sec; 0.094 sec/batch)
2017-06-02 06:20:11.770640: step 151410, loss = 0.44 (1703.8 examples/sec; 0.075 sec/batch)
2017-06-02 06:20:12.628518: step 151420, loss = 0.42 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:20:13.489011: step 151430, loss = 0.40 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:20:14.337923: step 151440, loss = 0.35 (1507.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:20:15.207320: step 151450, loss = 0.33 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:20:16.064606: step 151460, loss = 0.38 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:20:16.901759: step 151470, loss = 0.35 (1529.0 examples/sec; 0.084 sec/batch)
2017-06-02 06:20:17.771836: step 151480, loss = 0.32 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:20:18.628533: step 151490, loss = 0.45 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:20:19.622119: step 151500, loss = 0.30 (1288.3 examples/sec; 0.099 sec/batch)
2017-06-02 06:20:20.370776: step 151510, loss = 0.30 (1709.7 examples/sec; 0.075 sec/batch)
2017-06-02 06:20:21.213656: step 151520, loss = 0.42 (1518.6 examples/sec; 0.084 sec/batch)
2017-06-02 06:20:22.071795: step 151530, loss = 0.46 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:20:22.961961: step 151540, loss = 0.37 (1437.9 examples/sec; 0.089 sec/batch)
2017-06-02 06:20:23.832428: step 151550, loss = 0.30 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:20:24.704364: step 151560, loss = 0.32 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:20:25.570785: step 151570, loss = 0.26 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:20:26.407937: step 151580, loss = 0.42 (1529.0 examples/sec; 0.084 sec/batch)
2017-06-02 06:20:27.292789: step 151590, loss = 0.38 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:20:28.257405: step 151600, loss = 0.32 (1327.0 examples/sec; 0.096 sec/batch)
2017-06-02 06:20:29.020588: step 151610, loss = 0.43 (1677.2 examples/sec; 0.076 sec/batch)
2017-06-02 06:20:29.876760: step 151620, loss = 0.31 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:20:30.742773: step 151630, loss = 0.36 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:20:31.634576: step 151640, loss = 0.32 (1435.3 examples/sec; 0.089 sec/batch)
2017-06-02 06:20:32.482665: step 151650, loss = 0.39 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:20:33.347988: step 151660, loss = 0.36 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:20:34.216112: step 151670, loss = 0.37 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:20:35.081585: step 151680, loss = 0.33 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:20:35.931703: step 151690, loss = 0.39 (1505.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:20:36.946955: step 151700, loss = 0.31 (1260.8 examples/sec; 0.102 sec/batch)
2017-06-02 06:20:37.647846: step 151710, loss = 0.30 (1826.3 examples/sec; 0.070 sec/batch)
2017-06-02 06:20:38.512971: step 151720, loss = 0.30 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:20:39.406182: step 151730, loss = 0.37 (1433.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:20:40.264262: step 151740, loss = 0.36 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:20:41.128332: step 151750, loss = 0.27 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:20:41.969919: step 151760, loss = 0.25 (1520.9 examples/sec; 0.084 sec/batch)
2017-06-02 06:20:42.820884: step 151770, loss = 0.34 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:20:43.666666: step 151780, loss = 0.30 (1513.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:20:44.522086: step 151790, loss = 0.31 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:20:45.543324: step 151800, loss = 0.22 (1253.4 examples/sec; 0.102 sec/batch)
2017-06-02 06:20:46.246222: step 151810, loss = 0.33 (1821.1 examples/sec; 0.070 sec/batch)
2017-06-02 06:20:47.100368: step 151820, loss = 0.35 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:20:47.973349: step 151830, loss = 0.43 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:20:48.817436: step 151840, loss = 0.31 (1516.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:20:49.686051: step 151850, loss = 0.37 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:20:50.542446: step 151860, loss = 0.44 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:20:51.415357: step 151870, loss = 0.29 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:20:52.284985: step 151880, loss = 0.32 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:20:53.143589: step 151890, loss = 0.43 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:20:54.146504: step 151900, loss = 0.28 (1276.3 examples/sec; 0.100 sec/batch)
2017-06-02 06:20:54.855634: step 151910, loss = 0.39 (1805.0 examples/sec; 0.071 sec/batch)
2017-06-02 06:20:55.734142: step 151920, loss = 0.30 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:20:56.604165: step 151930, loss = 0.34 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:20:57.456571: step 151940, loss = 0.33 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:20:58.299764: step 151950, loss = 0.41 (1518.0 examples/sec; 0.084 sec/batch)
2017-06-02 06:20:59.189642: step 151960, loss = 0.35 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 06:21:00.034516: step 151970, loss = 0.31 (1515.0 examples/sec; 0.084 sec/batch)
2017-06-02 06:21:00.876232: step 151980, loss = 0.29 (1520.7 examples/sec; 0.084 sec/batch)
2017-06-02 06:21:01.744471: step 151990, loss = 0.37 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:21:02.690843: step 152000, loss = 0.31 (1352.5 examples/sec; 0.095 sec/batch)
2017-06-02 06:21:03.454043: step 152010, loss = 0.33 (1677.2 examples/sec; 0.076 sec/batch)
2017-06-02 06:21:04.324229: step 152020, loss = 0.29 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:21:05.167704: step 152030, loss = 0.35 (1517.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:21:05.998392: step 152040, loss = 0.34 (1540.9 examples/sec; 0.083 sec/batch)
2017-06-02 06:21:06.883303: step 152050, loss = 0.35 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:21:07.738489: step 152060, loss = 0.20 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:21:08.601674: step 152070, loss = 0.39 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:21:09.462805: step 152080, loss = 0.46 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:21:10.336364: step 152090, loss = 0.38 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:21:11.292718: step 152100, loss = 0.43 (1338.4 examples/sec; 0.096 sec/batch)
2017-06-02 06:21:12.055444: step 152110, loss = 0.39 (1678.2 examples/sec; 0.076 sec/batch)
2017-06-02 06:21:12.937019: step 152120, loss = 0.33 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:21:13.811227: step 152130, loss = 0.39 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:21:14.659336: step 152140, loss = 0.42 (1509.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:21:15.513644: step 152150, loss = 0.47 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:21:16.355632: step 152160, loss = 0.38 (1520.2 examples/sec; 0.084 sec/batch)
2017-06-02 06:21:17.206628: step 152170, loss = 0.36 (1504.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:21:18.074015: step 152180, loss = 0.42 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:21:18.949264: step 152190, loss = 0.34 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:21:19.927359: step 152200, loss = 0.33 (1308.7 examples/sec; 0.098 sec/batch)
2017-06-02 06:21:20.647059: step 152210, loss = 0.44 (1778.6 examples/sec; 0.072 sec/batch)
2017-06-02 06:21:21.500180: step 152220, loss = 0.27 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:21:22.361604: step 152230, loss = 0.31 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:21:23.226808: step 152240, loss = 0.31 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:21:24.127190: step 152250, loss = 0.36 (1421.6 examples/sec; 0.090 sec/batch)
2017-06-02 06:21:24.986282: step 152260, loss = 0.38 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:21:25.861854: step 152270, loss = 0.24 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:21:26.734458: step 152280, loss = 0.32 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:21:27.602315: step 152290, loss = 0.34 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:21:28.564370: step 152300, loss = 0.32 (1330.5 examples/sec; 0.096 sec/batch)
2017-06-02 06:21:29.324832: step 152310, loss = 0.40 (1683.2 examples/sec; 0.076 sec/batch)
2017-06-02 06:21:30.154518: step 152320, loss = 0.32 (1542.8 examples/sec; 0.083 sec/batch)
2017-06-02 06:21:31.009982: step 152330, loss = 0.38 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:21:31.893974: step 152340, loss = 0.39 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:21:32.737474: step 152350, loss = 0.34 (1517.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:21:33.570957: step 152360, loss = 0.28 (1535.7 examples/sec; 0.083 sec/batch)
2017-06-02 06:21:34.411250: step 152370, loss = 0.31 (1523.3 examples/sec; 0.084 sec/batch)
2017-06-02 06:21:35.265057: step 152380, loss = 0.37 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:21:36.103544: step 152390, loss = 0.34 (1526.6 examples/sec; 0.084 sec/batch)
2017-06-02 06:21:37.059658: step 152400, loss = 0.42 (1338.7 examples/sec; 0.096 sec/batch)
2017-06-02 06:21:37.824073: step 152410, loss = 0.29 (1674.5 examples/sec; 0.076 sec/batch)
2017-06-02 06:21:38.678078: step 152420, loss = 0.30 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:21:39.527031: step 152430, loss = 0.35 (1507.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:21:40.355289: step 152440, loss = 0.33 (1545.4 examples/sec; 0.083 sec/batch)
2017-06-02 06:21:41.188985: step 152450, loss = 0.28 (1535.3 examples/sec; 0.083 sec/batch)
2017-06-02 06:21:42.041338: step 152460, loss = 0.32 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:21:42.897090: step 152470, loss = 0.28 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:21:43.768538: step 152480, loss = 0.30 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:21:44.618579: step 152490, loss = 0.35 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:21:45.595334: step 152500, loss = 0.37 (1310.4 examples/sec; 0.098 sec/batch)
2017-06-02 06:21:46.367287: step 152510, loss = 0.27 (1658.1 examples/sec; 0.077 sec/batch)
2017-06-02 06:21:47.234069: step 152520, loss = 0.35 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:21:48.099940: step 152530, loss = 0.36 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:21:48.946631: step 152540, loss = 0.28 (1511.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:21:49.807834: step 152550, loss = 0.25 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:21:50.666459: step 152560, loss = 0.35 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:21:51.519967: step 152570, loss = 0.31 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:21:52.385907: step 152580, loss = 0.36 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:21:53.241414: step 152590, loss = 0.26 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:21:54.225355: step 152600, loss = 0.28 (1300.9 examples/sec; 0.098 sec/batch)
2017-06-02 06:21:54.996404: step 152610, loss = 0.29 (1660.1 examples/sec; 0.077 sec/batch)
2017-06-02 06:21:55.830934: step 152620, loss = 0.33 (1533.8 examples/sec; 0.083 sec/batch)
2017-06-02 06:21:56.696284: step 152630, loss = 0.26 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:21:57.546648: step 152640, loss = 0.39 (1505.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:21:58.401292: step 152650, loss = 0.32 (1497.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:21:59.288648: step 152660, loss = 0.48 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 06:22:00.143363: step 152670, loss = 0.39 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:22:01.007644: step 152680, loss = 0.44 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:22:01.873688: step 152690, loss = 0.42 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:22:02.828672: step 152700, loss = 0.35 (1340.3 examples/sec; 0.095 sec/batch)
2017-06-02 06:22:03.596202: step 152710, loss = 0.36 (1667.7 examples/sec; 0.077 sec/batch)
2017-06-02 06:22:04.431268: step 152720, loss = 0.32 (1532.8 examples/sec; 0.084 sec/batch)
2017-06-02 06:22:05.293963: step 152730, loss = 0.35 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:22:06.152331: step 152740, loss = 0.29 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:22:06.973142: step 152750, loss = 0.36 (1559.4 examples/sec; 0.082 sec/batch)
2017-06-02 06:22:07.840697: step 152760, loss = 0.45 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:22:08.716397: step 152770, loss = 0.36 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:22:09.588328: step 152780, loss = 0.28 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:22:10.433649: step 152790, loss = 0.26 (1514.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:22:11.377235: step 152800, loss = 0.28 (1356.5 examples/sec; 0.094 sec/batch)
2017-06-02 06:22:12.132818: step 152810, loss = 0.25 (1694.1 examples/sec; 0.076 sec/batch)
2017-06-02 06:22:13.022620: step 152820, loss = 0.23 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 06:22:13.886242: step 152830, loss = 0.32 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:22:14.734941: step 152840, loss = 0.35 (1508.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:22:15.588709: step 152850, loss = 0.31 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:22:16.431502: step 152860, loss = 0.34 (1518.8 examples/sec; 0.084 sec/batch)
2017-06-02 06:22:17.289317: step 152870, loss = 0.32 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:22:18.132372: step 152880, loss = 0.36 (1518.3 examples/sec; 0.084 sec/batch)
2017-06-02 06:22:18.971353: step 152890, loss = 0.27 (1525.7 examples/sec; 0.084 sec/batch)
2017-06-02 06:22:19.955831: step 152900, loss = 0.46 (1300.2 examples/sec; 0.098 sec/batch)
2017-06-02 06:22:20.732301: step 152910, loss = 0.34 (1648.5 examples/sec; 0.078 sec/batch)
2017-06-02 06:22:21.586106: step 152920, loss = 0.25 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:22:22.416114: step 152930, loss = 0.36 (1542.2 examples/sec; 0.083 sec/batch)
2017-06-02 06:22:23.254608: step 152940, loss = 0.27 (1526.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:22:24.124154: step 152950, loss = 0.40 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:22:24.975302: step 152960, loss = 0.36 (1503.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:22:25.855995: step 152970, loss = 0.25 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:22:26.698932: step 152980, loss = 0.40 (1518.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:22:27.567973: step 152990, loss = 0.31 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:22:28.522586: step 153000, loss = 0.31 (1340.8 examples/sec; 0.095 sec/batch)
2017-06-02 06:22:29.292575: step 153010, loss = 0.36 (1662.4 examples/sec; 0.077 sec/batch)
2017-06-02 06:22:30.181416: step 153020, loss = 0.27 (1440.1 examples/sec; 0.089 sec/batch)
2017-06-02 06:22:31.032288: step 153030, loss = 0.34 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:22:31.877238: step 153040, loss = 0.37 (1514.9 examples/sec; 0.084 sec/batch)
2017-06-02 06:22:32.735696: step 153050, loss = 0.32 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:22:33.579313: step 153060, loss = 0.30 (1517.3 examples/sec; 0.084 sec/batch)
2017-06-02 06:22:34.388762: step 153070, loss = 0.37 (1581.3 examples/sec; 0.081 sec/batch)
2017-06-02 06:22:35.251967: step 153080, loss = 0.25 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:22:36.126883: step 153090, loss = 0.34 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:22:37.155565: step 153100, loss = 0.33 (1244.3 examples/sec; 0.103 sec/batch)
2017-06-02 06:22:37.853713: step 153110, loss = 0.41 (1833.4 examples/sec; 0.070 sec/batch)
2017-06-02 06:22:38.701622: step 153120, loss = 0.32 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:22:39.557086: step 153130, loss = 0.29 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:22:40.419115: step 153140, loss = 0.37 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:22:41.281690: step 153150, loss = 0.26 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:22:42.125512: step 153160, loss = 0.30 (1516.9 examples/sec; 0.084 sec/batch)
2017-06-02 06:22:42.975955: step 153170, loss = 0.32 (1505.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:22:43.844580: step 153180, loss = 0.32 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:22:44.722415: step 153190, loss = 0.38 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:22:45.675038: step 153200, loss = 0.31 (1343.7 examples/sec; 0.095 sec/batch)
2017-06-02 06:22:46.437234: step 153210, loss = 0.45 (1679.3 examples/sec; 0.076 sec/batch)
2017-06-02 06:22:47.288498: step 153220, loss = 0.35 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:22:48.173807: step 153230, loss = 0.30 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 06:22:49.054269: step 153240, loss = 0.35 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:22:49.914926: step 153250, loss = 0.29 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:22:50.761892: step 153260, loss = 0.33 (1511.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:22:51.647763: step 153270, loss = 0.30 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 06:22:52.519604: step 153280, loss = 0.20 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:22:53.393547: step 153290, loss = 0.33 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:22:54.359525: step 153300, loss = 0.45 (1325.1 examples/sec; 0.097 sec/batch)
2017-06-02 06:22:55.141163: step 153310, loss = 0.35 (1637.6 examples/sec; 0.078 sec/batch)
2017-06-02 06:22:56.017083: step 153320, loss = 0.36 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:22:56.887642: step 153330, loss = 0.24 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:22:57.747204: step 153340, loss = 0.34 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:22:58.629983: step 153350, loss = 0.27 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:22:59.520432: step 153360, loss = 0.46 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 06:23:00.381573: step 153370, loss = 0.34 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:23:01.246874: step 153380, loss = 0.32 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:23:02.125342: step 153390, loss = 0.34 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:23:03.102513: step 153400, loss = 0.30 (1309.9 examples/sec; 0.098 sec/batch)
2017-06-02 06:23:03.870209: step 153410, loss = 0.37 (1667.3 examples/sec; 0.077 sec/batch)
2017-06-02 06:23:04.740281: step 153420, loss = 0.34 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:23:05.578268: step 153430, loss = 0.43 (1527.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:23:06.439893: step 153440, loss = 0.35 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:23:07.301292: step 153450, loss = 0.29 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:23:08.175847: step 153460, loss = 0.32 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:23:09.024312: step 153470, loss = 0.37 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:23:09.884548: step 153480, loss = 0.34 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:23:10.743033: step 153490, loss = 0.34 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:23:11.713380: step 153500, loss = 0.28 (1319.1 examples/sec; 0.097 sec/batch)
2017-06-02 06:23:12.481926: step 153510, loss = 0.48 (1665.5 examples/sec; 0.077 sec/batch)
2017-06-02 06:23:13.310886: step 153520, loss = 0.28 (1544.1 examples/sec; 0.083 sec/batch)
2017-06-02 06:23:14.157273: step 153530, loss = 0.41 (1512.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:23:15.024274: step 153540, loss = 0.34 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:23:15.902258: step 153550, loss = 0.32 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:23:16.740694: step 153560, loss = 0.37 (1526.7 examples/sec; 0.084 sec/batch)
2017-06-02 06:23:17.606687: step 153570, loss = 0.38 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:23:18.457150: step 153580, loss = 0.30 (1505.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:23:19.309581: step 153590, loss = 0.40 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:23:20.271944: step 153600, loss = 0.35 (1330.1 examples/sec; 0.096 sec/batch)
2017-06-02 06:23:21.036454: step 153610, loss = 0.37 (1674.3 examples/sec; 0.076 sec/batch)
2017-06-02 06:23:21.886074: step 153620, loss = 0.32 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:23:22.755568: step 153630, loss = 0.32 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:23:23.612412: step 153640, loss = 0.34 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:23:24.469895: step 153650, loss = 0.37 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:23:25.314829: step 153660, loss = 0.31 (1514.9 examples/sec; 0.084 sec/batch)
2017-06-02 06:23:26.173406: step 153670, loss = 0.36 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:23:26.981929: step 153680, loss = 0.36 (1583.1 examples/sec; 0.081 sec/batch)
2017-06-02 06:23:27.876716: step 153690, loss = 0.23 (1430.5 examples/sec; 0.089 sec/batch)
2017-06-02 06:23:28.835450: step 153700, loss = 0.41 (1335.1 examples/sec; 0.096 sec/batch)
2017-06-02 06:23:29.599482: step 153710, loss = 0.29 (1675.3 examples/sec; 0.076 sec/batch)
2017-06-02 06:23:30.462446: step 153720, loss = 0.42 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:23:31.326701: step 153730, loss = 0.36 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:23:32.182413: step 153740, loss = 0.31 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:23:33.036214: step 153750, loss = 0.30 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:23:33.886482: step 153760, loss = 0.33 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:23:34.703505: step 153770, loss = 0.38 (1566.7 examples/sec; 0.082 sec/batch)
2017-06-02 06:23:35.574434: step 153780, loss = 0.33 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:23:36.466636: step 153790, loss = 0.37 (1434.6 examples/sec; 0.089 sec/batch)
2017-06-02 06:23:37.423509: step 153800, loss = 0.32 (1337.7 examples/sec; 0.096 sec/batch)
2017-06-02 06:23:38.201785: step 153810, loss = 0.27 (1644.7 examples/sec; 0.078 sec/batch)
2017-06-02 06:23:39.047015: step 153820, loss = 0.33 (1514.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:23:39.885898: step 153830, loss = 0.33 (1525.8 examples/sec; 0.084 sec/batch)
2017-06-02 06:23:40.756361: step 153840, loss = 0.25 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:23:41.621473: step 153850, loss = 0.39 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:23:42.481355: step 153860, loss = 0.26 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:23:43.335930: step 153870, loss = 0.41 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:23:44.171202: step 153880, loss = 0.48 (1532.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:23:45.051728: step 153890, loss = 0.38 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:23:46.017301: step 153900, loss = 0.28 (1325.6 examples/sec; 0.097 sec/batch)
2017-06-02 06:23:46.818406: step 153910, loss = 0.27 (1597.8 examples/sec; 0.080 sec/batch)
2017-06-02 06:23:47.683728: step 153920, loss = 0.21 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:23:48.556559: step 153930, loss = 0.30 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:23:49.409341: step 153940, loss = 0.33 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:23:50.270988: step 153950, loss = 0.39 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:23:51.130112: step 153960, loss = 0.36 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:23:51.989140: step 153970, loss = 0.25 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:23:52.841446: step 153980, loss = 0.32 (1501.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:23:53.681923: step 153990, loss = 0.32 (1522.9 examples/sec; 0.084 sec/batch)
2017-06-02 06:23:54.605678: step 154000, loss = 0.41 (1385.6 examples/sec; 0.092 sec/batch)
2017-06-02 06:23:55.365135: step 154010, loss = 0.39 (1685.5 examples/sec; 0.076 sec/batch)
2017-06-02 06:23:56.242370: step 154020, loss = 0.41 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:23:57.129997: step 154030, loss = 0.36 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:23:57.992804: step 154040, loss = 0.29 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:23:58.844528: step 154050, loss = 0.40 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:23:59.701888: step 154060, loss = 0.29 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:24:00.566092: step 154070, loss = 0.31 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:24:01.435377: step 154080, loss = 0.29 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:24:02.283902: step 154090, loss = 0.35 (1508.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:24:03.250153: step 154100, loss = 0.29 (1324.7 examples/sec; 0.097 sec/batch)
2017-06-02 06:24:04.015780: step 154110, loss = 0.37 (1671.8 examples/sec; 0.077 sec/batch)
2017-06-02 06:24:04.882061: step 154120, loss = 0.26 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:24:05.730532: step 154130, loss = 0.34 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:24:06.592130: step 154140, loss = 0.39 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:24:07.460317: step 154150, loss = 0.32 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:24:08.329421: step 154160, loss = 0.32 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:24:09.192158: step 154170, loss = 0.41 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:24:10.076868: step 154180, loss = 0.36 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:24:10.967766: step 154190, loss = 0.25 (1436.7 examples/sec; 0.089 sec/batch)
2017-06-02 06:24:11.943689: step 154200, loss = 0.23 (1311.6 examples/sec; 0.098 sec/batch)
2017-06-02 06:24:12.709748: step 154210, loss = 0.39 (1670.9 examples/sec; 0.077 sec/batch)
2017-06-02 06:24:13.553797: step 154220, loss = 0.31 (1516.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:24:14.438417: step 154230, loss = 0.25 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:24:15.305116: step 154240, loss = 0.28 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:24:16.185933: step 154250, loss = 0.29 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:24:17.058803: step 154260, loss = 0.26 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:24:17.927715: step 154270, loss = 0.25 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:24:18.790485: step 154280, loss = 0.37 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:24:19.659889: step 154290, loss = 0.27 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:24:20.678676: step 154300, loss = 0.31 (1256.4 examples/sec; 0.102 sec/batch)
2017-06-02 06:24:21.379710: step 154310, loss = 0.35 (1825.9 examples/sec; 0.070 sec/batch)
2017-06-02 06:24:22.245028: step 154320, loss = 0.34 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:24:23.109780: step 154330, loss = 0.38 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:24:23.952376: step 154340, loss = 0.32 (1519.1 examples/sec; 0.084 sec/batch)
2017-06-02 06:24:24.803777: step 154350, loss = 0.36 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:24:25.635469: step 154360, loss = 0.38 (1539.0 examples/sec; 0.083 sec/batch)
2017-06-02 06:24:26.502523: step 154370, loss = 0.32 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:24:27.381123: step 154380, loss = 0.26 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:24:28.238433: step 154390, loss = 0.35 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:24:29.210196: step 154400, loss = 0.27 (1317.2 examples/sec; 0.097 sec/batch)
2017-06-02 06:24:29.980453: step 154410, loss = 0.37 (1661.8 examples/sec; 0.077 sec/batch)
2017-06-02 06:24:30.867592: step 154420, loss = 0.29 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 06:24:31.775407: step 154430, loss = 0.36 (1410.0 examples/sec; 0.091 sec/batch)
2017-06-02 06:24:32.633810: step 154440, loss = 0.36 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:24:33.526640: step 154450, loss = 0.36 (1433.6 examples/sec; 0.089 sec/batch)
2017-06-02 06:24:34.390745: step 154460, loss = 0.40 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:24:35.266160: step 154470, loss = 0.48 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:24:36.120910: step 154480, loss = 0.34 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:24:36.986903: step 154490, loss = 0.41 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:24:37.940415: step 154500, loss = 0.35 (1342.4 examples/sec; 0.095 sec/batch)
2017-06-02 06:24:38.694887: step 154510, loss = 0.28 (1696.6 examples/sec; 0.075 sec/batch)
2017-06-02 06:24:39.550191: step 154520, loss = 0.27 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:24:40.445710: step 154530, loss = 0.33 (1429.3 examples/sec; 0.090 sec/batch)
2017-06-02 06:24:41.313586: step 154540, loss = 0.35 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:24:42.164396: step 154550, loss = 0.36 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:24:43.045528: step 154560, loss = 0.32 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:24:43.926622: step 154570, loss = 0.34 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:24:44.818857: step 154580, loss = 0.38 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 06:24:45.721673: step 154590, loss = 0.40 (1417.7 examples/sec; 0.090 sec/batch)
2017-06-02 06:24:46.688063: step 154600, loss = 0.32 (1324.5 examples/sec; 0.097 sec/batch)
2017-06-02 06:24:47.447940: step 154610, loss = 0.31 (1684.5 examples/sec; 0.076 sec/batch)
2017-06-02 06:24:48.313294: step 154620, loss = 0.29 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:24:49.188492: step 154630, loss = 0.28 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:24:50.050169: step 154640, loss = 0.30 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:24:50.933781: step 154650, loss = 0.27 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:24:51.814980: step 154660, loss = 0.27 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:24:52.680841: step 154670, loss = 0.27 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:24:53.542896: step 154680, loss = 0.40 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:24:54.400822: step 154690, loss = 0.29 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:24:55.394146: step 154700, loss = 0.28 (1288.6 examples/sec; 0.099 sec/batch)
2017-06-02 06:24:56.154351: step 154710, loss = 0.34 (1683.8 examples/sec; 0.076 sec/batch)
2017-06-02 06:24:57.023315: step 154720, loss = 0.41 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:24:57.874527: step 154730, loss = 0.38 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:24:58.764103: step 154740, loss = 0.33 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 06:24:59.622534: step 154750, loss = 0.34 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:25:00.494038: step 154760, loss = 0.34 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:25:01.332667: step 154770, loss = 0.38 (1526.3 examples/sec; 0.084 sec/batch)
2017-06-02 06:25:02.189285: step 154780, loss = 0.29 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:25:03.035849: step 154790, loss = 0.33 (1512.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:25:04.005607: step 154800, loss = 0.24 (1319.9 examples/sec; 0.097 sec/batch)
2017-06-02 06:25:04.781681: step 154810, loss = 0.28 (1649.4 examples/sec; 0.078 sec/batch)
2017-06-02 06:25:05.662682: step 154820, loss = 0.30 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:25:06.543483: step 154830, loss = 0.22 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:25:07.403516: step 154840, loss = 0.34 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:25:08.272829: step 154850, loss = 0.26 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:25:09.146602: step 154860, loss = 0.37 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:25:10.031942: step 154870, loss = 0.34 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 06:25:10.909704: step 154880, loss = 0.41 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:25:11.772496: step 154890, loss = 0.37 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:25:12.743469: step 154900, loss = 0.46 (1318.3 examples/sec; 0.097 sec/batch)
2017-06-02 06:25:13.521645: step 154910, loss = 0.30 (1644.9 examples/sec; 0.078 sec/batch)
2017-06-02 06:25:14.393310: step 154920, loss = 0.39 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:25:15.278923: step 154930, loss = 0.27 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 06:25:16.153251: step 154940, loss = 0.42 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:25:17.011813: step 154950, loss = 0.31 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:25:17.861628: step 154960, loss = 0.26 (1506.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:25:18.710386: step 154970, loss = 0.33 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:25:19.549020: step 154980, loss = 0.25 (1526.3 examples/sec; 0.084 sec/batch)
2017-06-02 06:25:20.381696: step 154990, loss = 0.36 (1537.2 examples/sec; 0.083 sec/batch)
2017-06-02 06:25:21.321994: step 155000, loss = 0.33 (1361.3 examples/sec; 0.094 sec/batch)
2017-06-02 06:25:22.104498: step 155010, loss = 0.32 (1635.8 examples/sec; 0.078 sec/batch)
2017-06-02 06:25:22.965391: step 155020, loss = 0.36 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:25:23.812007: step 155030, loss = 0.41 (1511.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:25:24.657377: step 155040, loss = 0.26 (1514.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:25:25.528696: step 155050, loss = 0.35 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:25:26.386437: step 155060, loss = 0.23 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:25:27.291536: step 155070, loss = 0.36 (1414.2 examples/sec; 0.091 sec/batch)
2017-06-02 06:25:28.154912: step 155080, loss = 0.25 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:25:29.022584: step 155090, loss = 0.29 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:25:29.992178: step 155100, loss = 0.28 (1320.1 examples/sec; 0.097 sec/batch)
2017-06-02 06:25:30.763398: step 155110, loss = 0.30 (1659.7 examples/sec; 0.077 sec/batch)
2017-06-02 06:25:31.630049: step 155120, loss = 0.36 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:25:32.501715: step 155130, loss = 0.35 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:25:33.371945: step 155140, loss = 0.32 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:25:34.218105: step 155150, loss = 0.30 (1512.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:25:35.050874: step 155160, loss = 0.33 (1537.0 examples/sec; 0.083 sec/batch)
2017-06-02 06:25:35.897492: step 155170, loss = 0.42 (1511.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:25:36.764727: step 155180, loss = 0.27 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:25:37.645316: step 155190, loss = 0.33 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:25:38.623373: step 155200, loss = 0.27 (1308.7 examples/sec; 0.098 sec/batch)
2017-06-02 06:25:39.407785: step 155210, loss = 0.35 (1631.8 examples/sec; 0.078 sec/batch)
2017-06-02 06:25:40.289374: step 155220, loss = 0.32 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:25:41.126794: step 155230, loss = 0.31 (1528.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:25:41.981486: step 155240, loss = 0.33 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:25:42.834036: step 155250, loss = 0.41 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:25:43.707774: step 155260, loss = 0.39 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:25:44.586928: step 155270, loss = 0.36 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:25:45.463267: step 155280, loss = 0.31 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:25:46.334352: step 155290, loss = 0.34 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:25:47.297709: step 155300, loss = 0.28 (1328.7 examples/sec; 0.096 sec/batch)
2017-06-02 06:25:48.044833: step 155310, loss = 0.29 (1713.2 examples/sec; 0.075 sec/batch)
2017-06-02 06:25:48.919968: step 155320, loss = 0.26 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:25:49.808060: step 155330, loss = 0.26 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 06:25:50.667873: step 155340, loss = 0.25 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:25:51.530903: step 155350, loss = 0.28 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:25:52.398023: step 155360, loss = 0.31 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:25:53.251666: step 155370, loss = 0.37 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:25:54.107636: step 155380, loss = 0.34 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:25:54.977397: step 155390, loss = 0.33 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:25:55.933230: step 155400, loss = 0.34 (1339.1 examples/sec; 0.096 sec/batch)
2017-06-02 06:25:56.698054: step 155410, loss = 0.31 (1673.6 examples/sec; 0.076 sec/batch)
2017-06-02 06:25:57.544789: step 155420, loss = 0.41 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:25:58.457241: step 155430, loss = 0.36 (1402.8 examples/sec; 0.091 sec/batch)
2017-06-02 06:25:59.335261: step 155440, loss = 0.40 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:26:00.183884: step 155450, loss = 0.31 (1508.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:26:01.032074: step 155460, loss = 0.32 (1509.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:26:01.906882: step 155470, loss = 0.30 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:26:02.760416: step 155480, loss = 0.33 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:26:03.664994: step 155490, loss = 0.39 (1415.0 examples/sec; 0.090 sec/batch)
2017-06-02 06:26:04.605688: step 155500, loss = 0.32 (1360.7 examples/sec; 0.094 sec/batch)
2017-06-02 06:26:05.386462: step 155510, loss = 0.29 (1639.4 examples/sec; 0.078 sec/batch)
2017-06-02 06:26:06.245658: step 155520, loss = 0.29 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:26:07.105376: step 155530, loss = 0.27 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:26:07.959983: step 155540, loss = 0.37 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:26:08.812760: step 155550, loss = 0.33 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:26:09.694535: step 155560, loss = 0.39 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:26:10.563693: step 155570, loss = 0.28 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:26:11.420169: step 155580, loss = 0.33 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:26:12.276449: step 155590, loss = 0.29 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:26:13.280006: step 155600, loss = 0.36 (1275.5 examples/sec; 0.100 sec/batch)
2017-06-02 06:26:13.994748: step 155610, loss = 0.29 (1790.9 examples/sec; 0.071 sec/batch)
2017-06-02 06:26:14.874425: step 155620, loss = 0.31 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:26:15.755222: step 155630, loss = 0.35 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:26:16.612142: step 155640, loss = 0.25 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:26:17.496864: step 155650, loss = 0.27 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:26:18.341605: step 155660, loss = 0.33 (1515.3 examples/sec; 0.084 sec/batch)
2017-06-02 06:26:19.212651: step 155670, loss = 0.31 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:26:20.069302: step 155680, loss = 0.30 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:26:20.927771: step 155690, loss = 0.38 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:26:21.916696: step 155700, loss = 0.25 (1294.3 examples/sec; 0.099 sec/batch)
2017-06-02 06:26:22.685750: step 155710, loss = 0.31 (1664.4 examples/sec; 0.077 sec/batch)
2017-06-02 06:26:23.558768: step 155720, loss = 0.31 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:26:24.422436: step 155730, loss = 0.33 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:26:25.316017: step 155740, loss = 0.31 (1432.4 examples/sec; 0.089 sec/batch)
2017-06-02 06:26:26.175303: step 155750, loss = 0.29 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:26:27.053646: step 155760, loss = 0.29 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:26:27.916920: step 155770, loss = 0.33 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:26:28.772361: step 155780, loss = 0.38 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:26:29.627527: step 155790, loss = 0.19 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:26:30.609504: step 155800, loss = 0.39 (1303.5 examples/sec; 0.098 sec/batch)
2017-06-02 06:26:31.356170: step 155810, loss = 0.31 (1714.4 examples/sec; 0.075 sec/batch)
2017-06-02 06:26:32.238422: step 155820, loss = 0.27 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:26:33.120707: step 155830, loss = 0.31 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:26:33.977746: step 155840, loss = 0.28 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:26:34.823910: step 155850, loss = 0.43 (1512.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:26:35.695911: step 155860, loss = 0.25 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:26:36.566565: step 155870, loss = 0.36 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:26:37.414539: step 155880, loss = 0.29 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:26:38.255691: step 155890, loss = 0.29 (1521.7 examples/sec; 0.084 sec/batch)
2017-06-02 06:26:39.235777: step 155900, loss = 0.28 (1306.0 examples/sec; 0.098 sec/batch)
2017-06-02 06:26:39.972129: step 155910, loss = 0.44 (1738.3 examples/sec; 0.074 sec/batch)
2017-06-02 06:26:40.842951: step 155920, loss = 0.30 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:26:41.729894: step 155930, loss = 0.31 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 06:26:42.596060: step 155940, loss = 0.32 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:26:43.447218: step 155950, loss = 0.25 (1503.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:26:44.313079: step 155960, loss = 0.29 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:26:45.199689: step 155970, loss = 0.32 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 06:26:46.073728: step 155980, loss = 0.33 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:26:46.943563: step 155990, loss = 0.30 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:26:47.947500: step 156000, loss = 0.31 (1275.0 examples/sec; 0.100 sec/batch)
2017-06-02 06:26:48.712840: step 156010, loss = 0.29 (1672.5 examples/sec; 0.077 sec/batch)
2017-06-02 06:26:49.569623: step 156020, loss = 0.26 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:26:50.438346: step 156030, loss = 0.31 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:26:51.314357: step 156040, loss = 0.45 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:26:52.189849: step 156050, loss = 0.33 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:26:53.041049: step 156060, loss = 0.42 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:26:53.908709: step 156070, loss = 0.33 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:26:54.775142: step 156080, loss = 0.33 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:26:55.665434: step 156090, loss = 0.40 (1437.7 examples/sec; 0.089 sec/batch)
2017-06-02 06:26:56.634682: step 156100, loss = 0.33 (1320.6 examples/sec; 0.097 sec/batch)
2017-06-02 06:26:57.374780: step 156110, loss = 0.37 (1729.5 examples/sec; 0.074 sec/batch)
2017-06-02 06:26:58.263381: step 156120, loss = 0.32 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 06:26:59.109034: step 156130, loss = 0.38 (1513.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:26:59.948148: step 156140, loss = 0.37 (1525.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:27:00.821364: step 156150, loss = 0.34 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:27:01.683977: step 156160, loss = 0.39 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:27:02.526958: step 156170, loss = 0.25 (1518.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:27:03.385383: step 156180, loss = 0.33 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:27:04.236125: step 156190, loss = 0.34 (1504.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:27:05.182192: step 156200, loss = 0.36 (1353.0 examples/sec; 0.095 sec/batch)
2017-06-02 06:27:05.947411: step 156210, loss = 0.30 (1672.7 examples/sec; 0.077 sec/batch)
2017-06-02 06:27:06.825412: step 156220, loss = 0.40 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:27:07.722812: step 156230, loss = 0.29 (1426.3 examples/sec; 0.090 sec/batch)
2017-06-02 06:27:08.590392: step 156240, loss = 0.36 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:27:09.482272: step 156250, loss = 0.27 (1435.2 examples/sec; 0.089 sec/batch)
2017-06-02 06:27:10.343453: step 156260, loss = 0.30 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:27:11.205698: step 156270, loss = 0.37 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:27:12.080015: step 156280, loss = 0.31 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:27:12.958085: step 156290, loss = 0.31 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:27:13.928768: step 156300, loss = 0.23 (1318.7 examples/sec; 0.097 sec/batch)
2017-06-02 06:27:14.710564: step 156310, loss = 0.28 (1637.3 examples/sec; 0.078 sec/batch)
2017-06-02 06:27:15.589276: step 156320, loss = 0.46 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:27:16.469911: step 156330, loss = 0.33 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:27:17.330387: step 156340, loss = 0.26 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:27:18.177844: step 156350, loss = 0.23 (1510.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:27:19.046689: step 156360, loss = 0.30 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:27:19.921430: step 156370, loss = 0.30 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:27:20.801369: step 156380, loss = 0.37 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:27:21.662461: step 156390, loss = 0.28 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:27:22.644209: step 156400, loss = 0.38 (1303.8 examples/sec; 0.098 sec/batch)
2017-06-02 06:27:23.413307: step 156410, loss = 0.31 (1664.3 examples/sec; 0.077 sec/batch)
2017-06-02 06:27:24.278705: step 156420, loss = 0.31 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:27:25.136145: step 156430, loss = 0.26 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:27:26.006295: step 156440, loss = 0.29 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:27:26.867879: step 156450, loss = 0.40 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:27:27.749358: step 156460, loss = 0.26 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:27:28.635585: step 156470, loss = 0.34 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 06:27:29.502836: step 156480, loss = 0.28 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:27:30.380465: step 156490, loss = 0.36 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:27:31.362259: step 156500, loss = 0.30 (1303.8 examples/sec; 0.098 sec/batch)
2017-06-02 06:27:32.156873: step 156510, loss = 0.37 (1610.9 examples/sec; 0.079 sec/batch)
2017-06-02 06:27:33.044930: step 156520, loss = 0.31 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 06:27:33.916429: step 156530, loss = 0.26 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:27:34.771788: step 156540, loss = 0.32 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:27:35.619133: step 156550, loss = 0.29 (1510.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:27:36.485901: step 156560, loss = 0.28 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:27:37.339039: step 156570, loss = 0.22 (1500.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:27:38.190909: step 156580, loss = 0.26 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:27:39.069680: step 156590, loss = 0.42 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:27:40.039649: step 156600, loss = 0.38 (1319.6 examples/sec; 0.097 sec/batch)
2017-06-02 06:27:40.793341: step 156610, loss = 0.40 (1698.3 examples/sec; 0.075 sec/batch)
2017-06-02 06:27:41.680937: step 156620, loss = 0.23 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 06:27:42.553155: step 156630, loss = 0.23 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:27:43.420525: step 156640, loss = 0.30 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:27:44.291636: step 156650, loss = 0.27 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:27:45.149955: step 156660, loss = 0.28 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:27:46.024184: step 156670, loss = 0.28 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:27:46.896290: step 156680, loss = 0.27 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:27:47.754559: step 156690, loss = 0.29 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:27:48.729653: step 156700, loss = 0.29 (1312.7 examples/sec; 0.098 sec/batch)
2017-06-02 06:27:49.514762: step 156710, loss = 0.35 (1630.4 examples/sec; 0.079 sec/batch)
2017-06-02 06:27:50.375932: step 156720, loss = 0.33 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:27:51.247579: step 156730, loss = 0.26 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:27:52.127224: step 156740, loss = 0.34 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:27:52.978959: step 156750, loss = 0.31 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:27:53.844218: step 156760, loss = 0.33 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:27:54.700476: step 156770, loss = 0.38 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:27:55.573012: step 156780, loss = 0.30 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:27:56.419465: step 156790, loss = 0.24 (1512.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:27:57.392192: step 156800, loss = 0.30 (1315.9 examples/sec; 0.097 sec/batch)
2017-06-02 06:27:58.172264: step 156810, loss = 0.32 (1640.9 examples/sec; 0.078 sec/batch)
2017-06-02 06:27:59.046837: step 156820, loss = 0.35 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:27:59.932032: step 156830, loss = 0.37 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:28:00.805049: step 156840, loss = 0.34 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:28:01.681274: step 156850, loss = 0.29 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:28:02.539274: step 156860, loss = 0.38 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:28:03.381169: step 156870, loss = 0.31 (1520.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:28:04.253295: step 156880, loss = 0.49 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:28:05.096272: step 156890, loss = 0.31 (1518.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:28:06.070801: step 156900, loss = 0.32 (1313.4 examples/sec; 0.097 sec/batch)
2017-06-02 06:28:06.831154: step 156910, loss = 0.37 (1683.4 examples/sec; 0.076 sec/batch)
2017-06-02 06:28:07.721227: step 156920, loss = 0.31 (1438.1 examples/sec; 0.089 sec/batch)
2017-06-02 06:28:08.576300: step 156930, loss = 0.37 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:28:09.452121: step 156940, loss = 0.35 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:28:10.309344: step 156950, loss = 0.39 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:28:11.178531: step 156960, loss = 0.26 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:28:12.047096: step 156970, loss = 0.26 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:28:12.933089: step 156980, loss = 0.41 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 06:28:13.790448: step 156990, loss = 0.33 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:28:14.735574: step 157000, loss = 0.36 (1354.3 examples/sec; 0.095 sec/batch)
2017-06-02 06:28:15.508610: step 157010, loss = 0.30 (1655.8 examples/sec; 0.077 sec/batch)
2017-06-02 06:28:16.414753: step 157020, loss = 0.33 (1412.6 examples/sec; 0.091 sec/batch)
2017-06-02 06:28:17.295291: step 157030, loss = 0.29 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:28:18.164718: step 157040, loss = 0.35 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:28:19.016863: step 157050, loss = 0.27 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:28:19.896844: step 157060, loss = 0.26 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:28:20.750899: step 157070, loss = 0.40 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:28:21.599633: step 157080, loss = 0.26 (1508.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:28:22.451905: step 157090, loss = 0.25 (1501.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:28:23.416369: step 157100, loss = 0.30 (1327.1 examples/sec; 0.096 sec/batch)
2017-06-02 06:28:24.188637: step 157110, loss = 0.24 (1657.5 examples/sec; 0.077 sec/batch)
2017-06-02 06:28:25.069699: step 157120, loss = 0.33 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:28:25.933236: step 157130, loss = 0.29 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:28:26.827077: step 157140, loss = 0.38 (1432.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:28:27.711772: step 157150, loss = 0.37 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:28:28.564608: step 157160, loss = 0.34 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:28:29.437740: step 157170, loss = 0.28 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:28:30.286048: step 157180, loss = 0.27 (1508.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:28:31.158262: step 157190, loss = 0.27 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:28:32.124666: step 157200, loss = 0.32 (1324.5 examples/sec; 0.097 sec/batch)
2017-06-02 06:28:32.883997: step 157210, loss = 0.42 (1685.7 examples/sec; 0.076 sec/batch)
2017-06-02 06:28:33.730997: step 157220, loss = 0.31 (1511.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:28:34.586953: step 157230, loss = 0.28 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:28:35.422959: step 157240, loss = 0.31 (1531.1 examples/sec; 0.084 sec/batch)
2017-06-02 06:28:36.293120: step 157250, loss = 0.48 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:28:37.162821: step 157260, loss = 0.39 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:28:38.002088: step 157270, loss = 0.31 (1525.1 examples/sec; 0.084 sec/batch)
2017-06-02 06:28:38.900577: step 157280, loss = 0.26 (1424.6 examples/sec; 0.090 sec/batch)
2017-06-02 06:28:39.795829: step 157290, loss = 0.27 (1429.8 examples/sec; 0.090 sec/batch)
2017-06-02 06:28:40.761350: step 157300, loss = 0.31 (1325.7 examples/sec; 0.097 sec/batch)
2017-06-02 06:28:41.539614: step 157310, loss = 0.22 (1644.7 examples/sec; 0.078 sec/batch)
2017-06-02 06:28:42.401669: step 157320, loss = 0.30 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:28:43.294562: step 157330, loss = 0.36 (1433.6 examples/sec; 0.089 sec/batch)
2017-06-02 06:28:44.158469: step 157340, loss = 0.28 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:28:45.008312: step 157350, loss = 0.38 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:28:45.860249: step 157360, loss = 0.27 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:28:46.721344: step 157370, loss = 0.31 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:28:47.585296: step 157380, loss = 0.34 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:28:48.457418: step 157390, loss = 0.31 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:28:49.423538: step 157400, loss = 0.28 (1324.9 examples/sec; 0.097 sec/batch)
2017-06-02 06:28:50.177653: step 157410, loss = 0.26 (1697.3 examples/sec; 0.075 sec/batch)
2017-06-02 06:28:51.012639: step 157420, loss = 0.33 (1533.0 examples/sec; 0.083 sec/batch)
2017-06-02 06:28:51.908199: step 157430, loss = 0.32 (1429.3 examples/sec; 0.090 sec/batch)
2017-06-02 06:28:52.755772: step 157440, loss = 0.23 (1510.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:28:53.612954: step 157450, loss = 0.27 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:28:54.474819: step 157460, loss = 0.29 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:28:55.326855: step 157470, loss = 0.27 (1502.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:28:56.184543: step 157480, loss = 0.35 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:28:57.041201: step 157490, loss = 0.35 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:28:58.012915: step 157500, loss = 0.29 (1317.2 examples/sec; 0.097 sec/batch)
2017-06-02 06:28:58.792176: step 157510, loss = 0.30 (1642.6 examples/sec; 0.078 sec/batch)
2017-06-02 06:28:59.698819: step 157520, loss = 0.28 (1411.8 examples/sec; 0.091 sec/batch)
2017-06-02 06:29:00.593870: step 157530, loss = 0.34 (1430.1 examples/sec; 0.090 sec/batch)
2017-06-02 06:29:01.459825: step 157540, loss = 0.27 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:29:02.355295: step 157550, loss = 0.35 (1429.4 examples/sec; 0.090 sec/batch)
2017-06-02 06:29:03.229083: step 157560, loss = 0.28 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:29:04.109035: step 157570, loss = 0.29 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:29:04.960888: step 157580, loss = 0.33 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:29:05.810708: step 157590, loss = 0.30 (1506.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:29:06.766977: step 157600, loss = 0.42 (1338.5 examples/sec; 0.096 sec/batch)
2017-06-02 06:29:07.527815: step 157610, loss = 0.29 (1682.4 examples/sec; 0.076 sec/batch)
2017-06-02 06:29:08.387468: step 157620, loss = 0.38 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:29:09.258466: step 157630, loss = 0.32 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:29:10.131393: step 157640, loss = 0.34 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:29:10.988668: step 157650, loss = 0.31 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:29:11.832201: step 157660, loss = 0.34 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:29:12.713429: step 157670, loss = 0.31 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:29:13.566370: step 157680, loss = 0.29 (1500.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:29:14.426498: step 157690, loss = 0.43 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:29:15.419740: step 157700, loss = 0.30 (1288.7 examples/sec; 0.099 sec/batch)
2017-06-02 06:29:16.188867: step 157710, loss = 0.33 (1664.2 examples/sec; 0.077 sec/batch)
2017-06-02 06:29:17.047457: step 157720, loss = 0.28 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:29:17.930906: step 157730, loss = 0.28 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:29:18.804359: step 157740, loss = 0.31 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:29:19.638877: step 157750, loss = 0.33 (1533.8 examples/sec; 0.083 sec/batch)
2017-06-02 06:29:20.486424: step 157760, loss = 0.27 (1510.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:29:21.387305: step 157770, loss = 0.32 (1420.8 examples/sec; 0.090 sec/batch)
2017-06-02 06:29:22.233846: step 157780, loss = 0.28 (1512.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:29:23.102891: step 157790, loss = 0.28 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:29:24.067050: step 157800, loss = 0.40 (1327.6 examples/sec; 0.096 sec/batch)
2017-06-02 06:29:24.861781: step 157810, loss = 0.36 (1610.6 examples/sec; 0.079 sec/batch)
2017-06-02 06:29:25.743582: step 157820, loss = 0.33 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:29:26.632751: step 157830, loss = 0.25 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 06:29:27.513455: step 157840, loss = 0.31 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:29:28.382376: step 157850, loss = 0.34 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:29:29.259167: step 157860, loss = 0.34 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:29:30.107719: step 157870, loss = 0.30 (1508.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:29:30.979074: step 157880, loss = 0.37 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:29:31.850757: step 157890, loss = 0.36 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:29:32.819332: step 157900, loss = 0.30 (1321.5 examples/sec; 0.097 sec/batch)
2017-06-02 06:29:33.585782: step 157910, loss = 0.31 (1670.0 examples/sec; 0.077 sec/batch)
2017-06-02 06:29:34.460945: step 157920, loss = 0.37 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:29:35.334070: step 157930, loss = 0.25 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:29:36.206323: step 157940, loss = 0.19 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:29:37.093944: step 157950, loss = 0.41 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 06:29:37.984685: step 157960, loss = 0.38 (1437.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:29:38.874471: step 157970, loss = 0.30 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 06:29:39.718720: step 157980, loss = 0.38 (1516.1 examples/sec; 0.084 sec/batch)
2017-06-02 06:29:40.599937: step 157990, loss = 0.32 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:29:41.603498: step 158000, loss = 0.34 (1275.5 examples/sec; 0.100 sec/batch)
2017-06-02 06:29:42.332855: step 158010, loss = 0.35 (1755.0 examples/sec; 0.073 sec/batch)
2017-06-02 06:29:43.181928: step 158020, loss = 0.32 (1507.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:29:44.074489: step 158030, loss = 0.31 (1434.1 examples/sec; 0.089 sec/batch)
2017-06-02 06:29:44.947803: step 158040, loss = 0.36 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:29:45.825758: step 158050, loss = 0.30 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:29:46.648975: step 158060, loss = 0.30 (1554.9 examples/sec; 0.082 sec/batch)
2017-06-02 06:29:47.504218: step 158070, loss = 0.41 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:29:48.371291: step 158080, loss = 0.35 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:29:49.240132: step 158090, loss = 0.33 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:29:50.205825: step 158100, loss = 0.42 (1325.5 examples/sec; 0.097 sec/batch)
2017-06-02 06:29:50.967700: step 158110, loss = 0.34 (1680.1 examples/sec; 0.076 sec/batch)
2017-06-02 06:29:51.925376: step 158120, loss = 0.31 (1336.5 examples/sec; 0.096 sec/batch)
2017-06-02 06:29:52.786313: step 158130, loss = 0.27 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:29:53.656630: step 158140, loss = 0.35 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:29:54.525954: step 158150, loss = 0.21 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:29:55.371609: step 158160, loss = 0.24 (1513.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:29:56.241804: step 158170, loss = 0.33 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:29:57.154303: step 158180, loss = 0.31 (1402.7 examples/sec; 0.091 sec/batch)
2017-06-02 06:29:58.031531: step 158190, loss = 0.39 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:29:59.005514: step 158200, loss = 0.26 (1314.2 examples/sec; 0.097 sec/batch)
2017-06-02 06:29:59.773137: step 158210, loss = 0.32 (1667.5 examples/sec; 0.077 sec/batch)
2017-06-02 06:30:00.647310: step 158220, loss = 0.33 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:30:01.511369: step 158230, loss = 0.31 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:30:02.363282: step 158240, loss = 0.33 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:30:03.216043: step 158250, loss = 0.33 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:30:04.100344: step 158260, loss = 0.35 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:30:04.964324: step 158270, loss = 0.24 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:30:05.825699: step 158280, loss = 0.32 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:30:06.689896: step 158290, loss = 0.33 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:30:07.691169: step 158300, loss = 0.33 (1278.4 examples/sec; 0.100 sec/batch)
2017-06-02 06:30:08.453440: step 158310, loss = 0.39 (1679.2 examples/sec; 0.076 sec/batch)
2017-06-02 06:30:09.326218: step 158320, loss = 0.32 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:30:10.186531: step 158330, loss = 0.33 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:30:11.064535: step 158340, loss = 0.31 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:30:11.932916: step 158350, loss = 0.37 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:30:12.815163: step 158360, loss = 0.40 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:30:13.718331: step 158370, loss = 0.29 (1417.2 examples/sec; 0.090 sec/batch)
2017-06-02 06:30:14.596658: step 158380, loss = 0.36 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:30:15.484077: step 158390, loss = 0.42 (1442.4 examples/sec; 0.089 sec/batch)
2017-06-02 06:30:16.445776: step 158400, loss = 0.25 (1331.0 examples/sec; 0.096 sec/batch)
2017-06-02 06:30:17.209621: step 158410, loss = 0.36 (1675.7 examples/sec; 0.076 sec/batch)
2017-06-02 06:30:18.056853: step 158420, loss = 0.32 (1510.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:30:18.927861: step 158430, loss = 0.26 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:30:19.812468: step 158440, loss = 0.32 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:30:20.680396: step 158450, loss = 0.32 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:30:21.548364: step 158460, loss = 0.29 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:30:22.419227: step 158470, loss = 0.30 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:30:23.285346: step 158480, loss = 0.34 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:30:24.157585: step 158490, loss = 0.29 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:30:25.146685: step 158500, loss = 0.31 (1294.1 examples/sec; 0.099 sec/batch)
2017-06-02 06:30:25.929644: step 158510, loss = 0.22 (1634.8 examples/sec; 0.078 sec/batch)
2017-06-02 06:30:26.796461: step 158520, loss = 0.29 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:30:27.663738: step 158530, loss = 0.30 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:30:28.538441: step 158540, loss = 0.23 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:30:29.410719: step 158550, loss = 0.25 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:30:30.327705: step 158560, loss = 0.28 (1395.9 examples/sec; 0.092 sec/batch)
2017-06-02 06:30:31.195079: step 158570, loss = 0.39 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:30:32.051803: step 158580, loss = 0.38 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:30:32.931870: step 158590, loss = 0.35 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:30:33.913913: step 158600, loss = 0.27 (1303.4 examples/sec; 0.098 sec/batch)
2017-06-02 06:30:34.695696: step 158610, loss = 0.35 (1637.3 examples/sec; 0.078 sec/batch)
2017-06-02 06:30:35.561523: step 158620, loss = 0.29 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:30:36.438591: step 158630, loss = 0.33 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:30:37.293980: step 158640, loss = 0.28 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:30:38.147493: step 158650, loss = 0.36 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:30:39.033288: step 158660, loss = 0.41 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:30:39.906593: step 158670, loss = 0.33 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:30:40.788314: step 158680, loss = 0.30 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:30:41.647538: step 158690, loss = 0.30 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:30:42.586650: step 158700, loss = 0.36 (1363.0 examples/sec; 0.094 sec/batch)
2017-06-02 06:30:43.354798: step 158710, loss = 0.45 (1666.3 examples/sec; 0.077 sec/batch)
2017-06-02 06:30:44.201675: step 158720, loss = 0.33 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:30:45.060501: step 158730, loss = 0.40 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:30:45.924471: step 158740, loss = 0.36 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:30:46.781237: step 158750, loss = 0.37 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:30:47.647375: step 158760, loss = 0.33 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:30:48.505757: step 158770, loss = 0.30 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:30:49.361682: step 158780, loss = 0.35 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:30:50.238809: step 158790, loss = 0.24 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:30:51.214027: step 158800, loss = 0.28 (1312.5 examples/sec; 0.098 sec/batch)
2017-06-02 06:30:51.989708: step 158810, loss = 0.35 (1650.2 examples/sec; 0.078 sec/batch)
2017-06-02 06:30:52.873229: step 158820, loss = 0.36 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:30:53.731189: step 158830, loss = 0.34 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:30:54.586989: step 158840, loss = 0.28 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:30:55.443883: step 158850, loss = 0.28 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:30:56.329955: step 158860, loss = 0.41 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 06:30:57.205209: step 158870, loss = 0.37 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:30:58.085857: step 158880, loss = 0.36 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:30:58.960352: step 158890, loss = 0.29 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:30:59.983551: step 158900, loss = 0.37 (1251.0 examples/sec; 0.102 sec/batch)
2017-06-02 06:31:00.719526: step 158910, loss = 0.33 (1739.2 examples/sec; 0.074 sec/batch)
2017-06-02 06:31:01.584867: step 158920, loss = 0.28 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:31:02.456293: step 158930, loss = 0.45 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:31:03.309290: step 158940, loss = 0.33 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:31:04.181044: step 158950, loss = 0.38 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:31:05.057499: step 158960, loss = 0.38 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:31:05.917690: step 158970, loss = 0.33 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:31:06.788956: step 158980, loss = 0.22 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:31:07.653768: step 158990, loss = 0.26 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:31:08.611855: step 159000, loss = 0.37 (1336.0 examples/sec; 0.096 sec/batch)
2017-06-02 06:31:09.363125: step 159010, loss = 0.32 (1703.8 examples/sec; 0.075 sec/batch)
2017-06-02 06:31:10.230309: step 159020, loss = 0.28 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:31:11.091894: step 159030, loss = 0.28 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:31:11.961447: step 159040, loss = 0.24 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:31:12.851179: step 159050, loss = 0.30 (1438.6 examples/sec; 0.089 sec/batch)
2017-06-02 06:31:13.756457: step 159060, loss = 0.33 (1413.9 examples/sec; 0.091 sec/batch)
2017-06-02 06:31:14.621094: step 159070, loss = 0.38 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:31:15.509587: step 159080, loss = 0.38 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 06:31:16.371307: step 159090, loss = 0.29 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:31:17.428082: step 159100, loss = 0.29 (1211.2 examples/sec; 0.106 sec/batch)
2017-06-02 06:31:18.153446: step 159110, loss = 0.30 (1764.6 examples/sec; 0.073 sec/batch)
2017-06-02 06:31:19.029646: step 159120, loss = 0.29 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:31:19.892300: step 159130, loss = 0.34 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:31:20.782797: step 159140, loss = 0.38 (1437.4 examples/sec; 0.089 sec/batch)
2017-06-02 06:31:21.664699: step 159150, loss = 0.33 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:31:22.511938: step 159160, loss = 0.31 (1510.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:31:23.414673: step 159170, loss = 0.47 (1417.9 examples/sec; 0.090 sec/batch)
2017-06-02 06:31:24.298747: step 159180, loss = 0.31 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:31:25.158480: step 159190, loss = 0.28 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:31:26.105288: step 159200, loss = 0.32 (1351.9 examples/sec; 0.095 sec/batch)
2017-06-02 06:31:26.877802: step 159210, loss = 0.42 (1656.9 examples/sec; 0.077 sec/batch)
2017-06-02 06:31:27.754560: step 159220, loss = 0.42 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:31:28.614657: step 159230, loss = 0.29 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:31:29.501392: step 159240, loss = 0.28 (1443.5 examples/sec; 0.089 sec/batch)
2017-06-02 06:31:30.374879: step 159250, loss = 0.25 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:31:31.234435: step 159260, loss = 0.38 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:31:32.107933: step 159270, loss = 0.25 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:31:32.995308: step 159280, loss = 0.43 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 06:31:33.873862: step 159290, loss = 0.31 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:31:34.835811: step 159300, loss = 0.33 (1330.6 examples/sec; 0.096 sec/batch)
2017-06-02 06:31:35.582574: step 159310, loss = 0.44 (1714.1 examples/sec; 0.075 sec/batch)
2017-06-02 06:31:36.437789: step 159320, loss = 0.26 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:31:37.296150: step 159330, loss = 0.30 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:31:38.172918: step 159340, loss = 0.30 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:31:39.023069: step 159350, loss = 0.39 (1505.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:31:39.928318: step 159360, loss = 0.26 (1414.0 examples/sec; 0.091 sec/batch)
2017-06-02 06:31:40.796910: step 159370, loss = 0.31 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:31:41.674650: step 159380, loss = 0.26 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:31:42.548369: step 159390, loss = 0.29 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:31:43.546278: step 159400, loss = 0.37 (1282.7 examples/sec; 0.100 sec/batch)
2017-06-02 06:31:44.306229: step 159410, loss = 0.29 (1684.3 examples/sec; 0.076 sec/batch)
2017-06-02 06:31:45.174672: step 159420, loss = 0.40 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:31:46.035702: step 159430, loss = 0.35 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:31:46.918067: step 159440, loss = 0.35 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:31:47.794652: step 159450, loss = 0.28 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:31:48.660308: step 159460, loss = 0.27 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:31:49.526834: step 159470, loss = 0.28 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:31:50.398672: step 159480, loss = 0.39 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:31:51.286837: step 159490, loss = 0.29 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 06:31:52.253754: step 159500, loss = 0.29 (1323.8 examples/sec; 0.097 sec/batch)
2017-06-02 06:31:53.036203: step 159510, loss = 0.36 (1635.9 examples/sec; 0.078 sec/batch)
2017-06-02 06:31:53.884703: step 159520, loss = 0.30 (1508.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:31:54.758827: step 159530, loss = 0.36 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:31:55.621877: step 159540, loss = 0.30 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:31:56.502833: step 159550, loss = 0.28 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:31:57.385673: step 159560, loss = 0.26 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:31:58.270466: step 159570, loss = 0.30 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:31:59.139560: step 159580, loss = 0.27 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:32:00.027038: step 159590, loss = 0.42 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 06:32:01.010282: step 159600, loss = 0.29 (1301.8 examples/sec; 0.098 sec/batch)
2017-06-02 06:32:01.793566: step 159610, loss = 0.24 (1634.2 examples/sec; 0.078 sec/batch)
2017-06-02 06:32:02.672497: step 159620, loss = 0.27 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:32:03.567346: step 159630, loss = 0.26 (1430.4 examples/sec; 0.089 sec/batch)
2017-06-02 06:32:04.451308: step 159640, loss = 0.32 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:32:05.312817: step 159650, loss = 0.36 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:32:06.167654: step 159660, loss = 0.32 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:32:07.040916: step 159670, loss = 0.26 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:32:07.896324: step 159680, loss = 0.31 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:32:08.756385: step 159690, loss = 0.39 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:32:09.737272: step 159700, loss = 0.26 (1304.9 examples/sec; 0.098 sec/batch)
2017-06-02 06:32:10.517066: step 159710, loss = 0.33 (1641.5 examples/sec; 0.078 sec/batch)
2017-06-02 06:32:11.408575: step 159720, loss = 0.27 (1435.8 examples/sec; 0.089 sec/batch)
2017-06-02 06:32:12.292368: step 159730, loss = 0.27 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:32:13.165637: step 159740, loss = 0.28 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:32:14.008647: step 159750, loss = 0.32 (1518.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:32:14.865450: step 159760, loss = 0.28 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:32:15.719715: step 159770, loss = 0.27 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:32:16.577982: step 159780, loss = 0.35 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:32:17.431230: step 159790, loss = 0.35 (1500.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:32:18.404566: step 159800, loss = 0.43 (1315.1 examples/sec; 0.097 sec/batch)
2017-06-02 06:32:19.170140: step 159810, loss = 0.42 (1672.0 examples/sec; 0.077 sec/batch)
2017-06-02 06:32:20.030124: step 159820, loss = 0.35 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:32:20.876512: step 159830, loss = 0.26 (1512.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:32:21.738555: step 159840, loss = 0.38 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:32:22.608444: step 159850, loss = 0.29 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:32:23.491771: step 159860, loss = 0.29 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:32:24.385527: step 159870, loss = 0.32 (1432.2 examples/sec; 0.089 sec/batch)
2017-06-02 06:32:25.233130: step 159880, loss = 0.38 (1510.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:32:26.089687: step 159890, loss = 0.32 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:32:27.054208: step 159900, loss = 0.29 (1327.1 examples/sec; 0.096 sec/batch)
2017-06-02 06:32:27.833936: step 159910, loss = 0.24 (1641.6 examples/sec; 0.078 sec/batch)
2017-06-02 06:32:28.685897: step 159920, loss = 0.28 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:32:29.561206: step 159930, loss = 0.37 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:32:30.439930: step 159940, loss = 0.29 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:32:31.294829: step 159950, loss = 0.30 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:32:32.154832: step 159960, loss = 0.37 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:32:33.021888: step 159970, loss = 0.35 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:32:33.889473: step 159980, loss = 0.22 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:32:34.773957: step 159990, loss = 0.23 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:32:35.776509: step 160000, loss = 0.36 (1276.7 examples/sec; 0.100 sec/batch)
2017-06-02 06:32:36.492351: step 160010, loss = 0.22 (1788.1 examples/sec; 0.072 sec/batch)
2017-06-02 06:32:37.351747: step 160020, loss = 0.35 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:32:38.208719: step 160030, loss = 0.31 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:32:39.060569: step 160040, loss = 0.33 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:32:39.911635: step 160050, loss = 0.31 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:32:40.793609: step 160060, loss = 0.29 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:32:41.641882: step 160070, loss = 0.33 (1508.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:32:42.509912: step 160080, loss = 0.37 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:32:43.389696: step 160090, loss = 0.39 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:32:44.356029: step 160100, loss = 0.29 (1324.6 examples/sec; 0.097 sec/batch)
2017-06-02 06:32:45.124697: step 160110, loss = 0.36 (1665.2 examples/sec; 0.077 sec/batch)
2017-06-02 06:32:45.990839: step 160120, loss = 0.49 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:32:46.848065: step 160130, loss = 0.42 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:32:47.723067: step 160140, loss = 0.22 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:32:48.593347: step 160150, loss = 0.34 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:32:49.443906: step 160160, loss = 0.37 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:32:50.324338: step 160170, loss = 0.30 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:32:51.151907: step 160180, loss = 0.27 (1546.7 examples/sec; 0.083 sec/batch)
2017-06-02 06:32:52.005105: step 160190, loss = 0.37 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:32:52.972244: step 160200, loss = 0.30 (1323.5 examples/sec; 0.097 sec/batch)
2017-06-02 06:32:53.756007: step 160210, loss = 0.32 (1633.1 examples/sec; 0.078 sec/batch)
2017-06-02 06:32:54.615025: step 160220, loss = 0.32 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:32:55.489214: step 160230, loss = 0.27 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:32:56.337811: step 160240, loss = 0.30 (1508.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:32:57.195601: step 160250, loss = 0.30 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:32:58.083250: step 160260, loss = 0.27 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:32:58.936213: step 160270, loss = 0.28 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:32:59.806316: step 160280, loss = 0.29 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:33:00.683728: step 160290, loss = 0.33 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:33:01.683728: step 160300, loss = 0.33 (1280.0 examples/sec; 0.100 sec/batch)
2017-06-02 06:33:02.475275: step 160310, loss = 0.28 (1617.1 examples/sec; 0.079 sec/batch)
2017-06-02 06:33:03.341863: step 160320, loss = 0.39 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:33:04.196791: step 160330, loss = 0.39 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:33:05.100340: step 160340, loss = 0.26 (1416.6 examples/sec; 0.090 sec/batch)
2017-06-02 06:33:05.977280: step 160350, loss = 0.40 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:33:06.846452: step 160360, loss = 0.29 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:33:07.695000: step 160370, loss = 0.28 (1508.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:33:08.541281: step 160380, loss = 0.34 (1512.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:33:09.401551: step 160390, loss = 0.38 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:33:10.367656: step 160400, loss = 0.42 (1324.9 examples/sec; 0.097 sec/batch)
2017-06-02 06:33:11.129911: step 160410, loss = 0.31 (1679.2 examples/sec; 0.076 sec/batch)
2017-06-02 06:33:11.986209: step 160420, loss = 0.33 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:33:12.823042: step 160430, loss = 0.31 (1529.6 examples/sec; 0.084 sec/batch)
2017-06-02 06:33:13.710055: step 160440, loss = 0.31 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 06:33:14.570901: step 160450, loss = 0.39 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:33:15.448670: step 160460, loss = 0.30 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:33:16.331757: step 160470, loss = 0.33 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:33:17.189322: step 160480, loss = 0.27 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:33:18.075635: step 160490, loss = 0.36 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 06:33:19.020988: step 160500, loss = 0.38 (1354.0 examples/sec; 0.095 sec/batch)
2017-06-02 06:33:19.796486: step 160510, loss = 0.37 (1650.5 examples/sec; 0.078 sec/batch)
2017-06-02 06:33:20.655582: step 160520, loss = 0.31 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:33:21.524537: step 160530, loss = 0.29 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:33:22.380484: step 160540, loss = 0.32 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:33:23.247831: step 160550, loss = 0.41 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:33:24.113309: step 160560, loss = 0.26 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:33:24.963630: step 160570, loss = 0.27 (1505.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:33:25.849325: step 160580, loss = 0.31 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 06:33:26.716500: step 160590, loss = 0.32 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:33:27.695361: step 160600, loss = 0.31 (1307.6 examples/sec; 0.098 sec/batch)
2017-06-02 06:33:28.478870: step 160610, loss = 0.25 (1633.7 examples/sec; 0.078 sec/batch)
2017-06-02 06:33:29.370332: step 160620, loss = 0.36 (1435.8 examples/sec; 0.089 sec/batch)
2017-06-02 06:33:30.224257: step 160630, loss = 0.27 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:33:31.098034: step 160640, loss = 0.32 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:33:31.969638: step 160650, loss = 0.37 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:33:32.828553: step 160660, loss = 0.33 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:33:33.703501: step 160670, loss = 0.34 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:33:34.577716: step 160680, loss = 0.27 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:33:35.457021: step 160690, loss = 0.30 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:33:36.421277: step 160700, loss = 0.27 (1327.5 examples/sec; 0.096 sec/batch)
2017-06-02 06:33:37.164586: step 160710, loss = 0.25 (1722.0 examples/sec; 0.074 sec/batch)
2017-06-02 06:33:38.026921: step 160720, loss = 0.22 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:33:38.871552: step 160730, loss = 0.34 (1515.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:33:39.718418: step 160740, loss = 0.32 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:33:40.564024: step 160750, loss = 0.28 (1513.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:33:41.425852: step 160760, loss = 0.33 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:33:42.281974: step 160770, loss = 0.24 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:33:43.141986: step 160780, loss = 0.30 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:33:44.000618: step 160790, loss = 0.36 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:33:44.973831: step 160800, loss = 0.28 (1315.2 examples/sec; 0.097 sec/batch)
2017-06-02 06:33:45.736109: step 160810, loss = 0.40 (1679.2 examples/sec; 0.076 sec/batch)
2017-06-02 06:33:46.598814: step 160820, loss = 0.34 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:33:47.442259: step 160830, loss = 0.33 (1517.6 examples/sec; 0.084 sec/batch)
2017-06-02 06:33:48.301343: step 160840, loss = 0.39 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:33:49.185964: step 160850, loss = 0.42 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:33:50.047060: step 160860, loss = 0.31 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:33:50.900437: step 160870, loss = 0.38 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:33:51.786440: step 160880, loss = 0.31 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 06:33:52.647143: step 160890, loss = 0.28 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:33:53.629603: step 160900, loss = 0.35 (1302.9 examples/sec; 0.098 sec/batch)
2017-06-02 06:33:54.372076: step 160910, loss = 0.23 (1724.0 examples/sec; 0.074 sec/batch)
2017-06-02 06:33:55.249299: step 160920, loss = 0.29 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:33:56.114033: step 160930, loss = 0.32 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:33:56.985341: step 160940, loss = 0.27 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:33:57.874128: step 160950, loss = 0.28 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 06:33:58.764397: step 160960, loss = 0.31 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 06:33:59.623282: step 160970, loss = 0.31 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:34:00.479887: step 160980, loss = 0.36 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:34:01.355604: step 160990, loss = 0.30 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:34:02.318449: step 161000, loss = 0.35 (1329.4 examples/sec; 0.096 sec/batch)
2017-06-02 06:34:03.083674: step 161010, loss = 0.39 (1672.7 examples/sec; 0.077 sec/batch)
2017-06-02 06:34:03.946559: step 161020, loss = 0.33 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:34:04.804214: step 161030, loss = 0.28 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:34:05.662679: step 161040, loss = 0.28 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:34:06.536267: step 161050, loss = 0.34 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:34:07.400725: step 161060, loss = 0.28 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:34:08.260093: step 161070, loss = 0.33 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:34:09.129920: step 161080, loss = 0.31 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:34:09.981748: step 161090, loss = 0.27 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:34:10.929472: step 161100, loss = 0.31 (1350.6 examples/sec; 0.095 sec/batch)
2017-06-02 06:34:11.717933: step 161110, loss = 0.27 (1623.5 examples/sec; 0.079 sec/batch)
2017-06-02 06:34:12.582202: step 161120, loss = 0.42 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:34:13.450037: step 161130, loss = 0.33 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:34:14.312466: step 161140, loss = 0.29 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:34:15.173614: step 161150, loss = 0.24 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:34:16.027053: step 161160, loss = 0.37 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:34:16.905046: step 161170, loss = 0.35 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:34:17.776742: step 161180, loss = 0.25 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:34:18.640812: step 161190, loss = 0.34 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:34:19.605321: step 161200, loss = 0.23 (1327.1 examples/sec; 0.096 sec/batch)
2017-06-02 06:34:20.394710: step 161210, loss = 0.33 (1621.5 examples/sec; 0.079 sec/batch)
2017-06-02 06:34:21.238277: step 161220, loss = 0.34 (1517.3 examples/sec; 0.084 sec/batch)
2017-06-02 06:34:22.125232: step 161230, loss = 0.32 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 06:34:23.002045: step 161240, loss = 0.27 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:34:23.873117: step 161250, loss = 0.35 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:34:24.719096: step 161260, loss = 0.27 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:34:25.612175: step 161270, loss = 0.29 (1433.2 examples/sec; 0.089 sec/batch)
2017-06-02 06:34:26.488202: step 161280, loss = 0.28 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:34:27.354897: step 161290, loss = 0.28 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:34:28.307209: step 161300, loss = 0.28 (1344.1 examples/sec; 0.095 sec/batch)
2017-06-02 06:34:29.078054: step 161310, loss = 0.32 (1660.5 examples/sec; 0.077 sec/batch)
2017-06-02 06:34:29.948388: step 161320, loss = 0.33 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:34:30.816867: step 161330, loss = 0.36 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:34:31.713850: step 161340, loss = 0.27 (1427.0 examples/sec; 0.090 sec/batch)
2017-06-02 06:34:32.533800: step 161350, loss = 0.26 (1561.1 examples/sec; 0.082 sec/batch)
2017-06-02 06:34:33.428081: step 161360, loss = 0.32 (1431.3 examples/sec; 0.089 sec/batch)
2017-06-02 06:34:34.300126: step 161370, loss = 0.28 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:34:35.178180: step 161380, loss = 0.33 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:34:36.052715: step 161390, loss = 0.30 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:34:37.003768: step 161400, loss = 0.27 (1345.9 examples/sec; 0.095 sec/batch)
2017-06-02 06:34:37.795449: step 161410, loss = 0.32 (1616.8 examples/sec; 0.079 sec/batch)
2017-06-02 06:34:38.641293: step 161420, loss = 0.32 (1513.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:34:39.516159: step 161430, loss = 0.31 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:34:40.372568: step 161440, loss = 0.28 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:34:41.256177: step 161450, loss = 0.28 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:34:42.137624: step 161460, loss = 0.27 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:34:43.009321: step 161470, loss = 0.38 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:34:43.890007: step 161480, loss = 0.41 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:34:44.740194: step 161490, loss = 0.41 (1505.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:34:45.702663: step 161500, loss = 0.31 (1329.9 examples/sec; 0.096 sec/batch)
2017-06-02 06:34:46.462284: step 161510, loss = 0.31 (1685.1 examples/sec; 0.076 sec/batch)
2017-06-02 06:34:47.328655: step 161520, loss = 0.26 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:34:48.212759: step 161530, loss = 0.23 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:34:49.075118: step 161540, loss = 0.28 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:34:49.946349: step 161550, loss = 0.28 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:34:50.821215: step 161560, loss = 0.32 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:34:51.694145: step 161570, loss = 0.36 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:34:52.558537: step 161580, loss = 0.38 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:34:53.435273: step 161590, loss = 0.33 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:34:54.403095: step 161600, loss = 0.31 (1322.6 examples/sec; 0.097 sec/batch)
2017-06-02 06:34:55.174484: step 161610, loss = 0.31 (1659.4 examples/sec; 0.077 sec/batch)
2017-06-02 06:34:56.035693: step 161620, loss = 0.23 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:34:56.889292: step 161630, loss = 0.35 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:34:57.764312: step 161640, loss = 0.39 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:34:58.605727: step 161650, loss = 0.22 (1521.3 examples/sec; 0.084 sec/batch)
2017-06-02 06:34:59.467945: step 161660, loss = 0.29 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:35:00.337368: step 161670, loss = 0.34 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:35:01.192261: step 161680, loss = 0.23 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:35:02.048566: step 161690, loss = 0.27 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:35:03.050655: step 161700, loss = 0.25 (1277.3 examples/sec; 0.100 sec/batch)
2017-06-02 06:35:03.760110: step 161710, loss = 0.31 (1804.2 examples/sec; 0.071 sec/batch)
2017-06-02 06:35:04.629194: step 161720, loss = 0.38 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:35:05.503843: step 161730, loss = 0.27 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:35:06.325843: step 161740, loss = 0.30 (1557.2 examples/sec; 0.082 sec/batch)
2017-06-02 06:35:07.165581: step 161750, loss = 0.28 (1524.3 examples/sec; 0.084 sec/batch)
2017-06-02 06:35:08.024347: step 161760, loss = 0.28 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:35:08.870775: step 161770, loss = 0.34 (1512.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:35:09.753991: step 161780, loss = 0.35 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:35:10.590902: step 161790, loss = 0.27 (1529.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:35:11.587715: step 161800, loss = 0.35 (1284.1 examples/sec; 0.100 sec/batch)
2017-06-02 06:35:12.296868: step 161810, loss = 0.33 (1805.0 examples/sec; 0.071 sec/batch)
2017-06-02 06:35:13.161662: step 161820, loss = 0.34 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:35:14.043756: step 161830, loss = 0.35 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:35:14.885708: step 161840, loss = 0.43 (1520.3 examples/sec; 0.084 sec/batch)
2017-06-02 06:35:15.731861: step 161850, loss = 0.30 (1512.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:35:16.602641: step 161860, loss = 0.29 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:35:17.461195: step 161870, loss = 0.36 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:35:18.312231: step 161880, loss = 0.31 (1504.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:35:19.160480: step 161890, loss = 0.31 (1509.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:35:20.119611: step 161900, loss = 0.27 (1334.5 examples/sec; 0.096 sec/batch)
2017-06-02 06:35:20.864454: step 161910, loss = 0.34 (1718.5 examples/sec; 0.074 sec/batch)
2017-06-02 06:35:21.711968: step 161920, loss = 0.35 (1510.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:35:22.565642: step 161930, loss = 0.34 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:35:23.444181: step 161940, loss = 0.31 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:35:24.290432: step 161950, loss = 0.36 (1512.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:35:25.141320: step 161960, loss = 0.37 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:35:26.003003: step 161970, loss = 0.25 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:35:26.845944: step 161980, loss = 0.36 (1518.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:35:27.695147: step 161990, loss = 0.30 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:35:28.668103: step 162000, loss = 0.31 (1315.6 examples/sec; 0.097 sec/batch)
2017-06-02 06:35:29.442801: step 162010, loss = 0.34 (1652.2 examples/sec; 0.077 sec/batch)
2017-06-02 06:35:30.284645: step 162020, loss = 0.25 (1520.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:35:31.118533: step 162030, loss = 0.46 (1535.0 examples/sec; 0.083 sec/batch)
2017-06-02 06:35:31.966373: step 162040, loss = 0.27 (1509.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:35:32.807149: step 162050, loss = 0.24 (1522.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:35:33.682407: step 162060, loss = 0.25 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:35:34.533289: step 162070, loss = 0.32 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:35:35.386949: step 162080, loss = 0.29 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:35:36.212269: step 162090, loss = 0.30 (1550.9 examples/sec; 0.083 sec/batch)
2017-06-02 06:35:37.180193: step 162100, loss = 0.39 (1322.4 examples/sec; 0.097 sec/batch)
2017-06-02 06:35:37.943294: step 162110, loss = 0.25 (1677.4 examples/sec; 0.076 sec/batch)
2017-06-02 06:35:38.772200: step 162120, loss = 0.37 (1544.2 examples/sec; 0.083 sec/batch)
2017-06-02 06:35:39.615305: step 162130, loss = 0.30 (1518.2 examples/sec; 0.084 sec/batch)
2017-06-02 06:35:40.462133: step 162140, loss = 0.35 (1511.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:35:41.305681: step 162150, loss = 0.27 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:35:42.141679: step 162160, loss = 0.26 (1531.1 examples/sec; 0.084 sec/batch)
2017-06-02 06:35:43.017254: step 162170, loss = 0.32 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:35:43.877333: step 162180, loss = 0.30 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:35:44.721092: step 162190, loss = 0.39 (1517.0 examples/sec; 0.084 sec/batch)
2017-06-02 06:35:45.691483: step 162200, loss = 0.38 (1319.1 examples/sec; 0.097 sec/batch)
2017-06-02 06:35:46.471429: step 162210, loss = 0.34 (1641.1 examples/sec; 0.078 sec/batch)
2017-06-02 06:35:47.332326: step 162220, loss = 0.38 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:35:48.194319: step 162230, loss = 0.26 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:35:49.081136: step 162240, loss = 0.33 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 06:35:49.968301: step 162250, loss = 0.31 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 06:35:50.863792: step 162260, loss = 0.37 (1429.4 examples/sec; 0.090 sec/batch)
2017-06-02 06:35:51.726573: step 162270, loss = 0.27 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:35:52.606753: step 162280, loss = 0.33 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:35:53.484447: step 162290, loss = 0.33 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:35:54.455882: step 162300, loss = 0.35 (1317.6 examples/sec; 0.097 sec/batch)
2017-06-02 06:35:55.214255: step 162310, loss = 0.28 (1687.8 examples/sec; 0.076 sec/batch)
2017-06-02 06:35:56.060751: step 162320, loss = 0.33 (1512.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:35:56.916915: step 162330, loss = 0.35 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:35:57.809717: step 162340, loss = 0.37 (1433.7 examples/sec; 0.089 sec/batch)
2017-06-02 06:35:58.705250: step 162350, loss = 0.44 (1429.3 examples/sec; 0.090 sec/batch)
2017-06-02 06:35:59.593260: step 162360, loss = 0.23 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 06:36:00.450727: step 162370, loss = 0.35 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:36:01.304746: step 162380, loss = 0.26 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:36:02.148631: step 162390, loss = 0.32 (1516.8 examples/sec; 0.084 sec/batch)
2017-06-02 06:36:03.102729: step 162400, loss = 0.30 (1341.6 examples/sec; 0.095 sec/batch)
2017-06-02 06:36:03.883954: step 162410, loss = 0.39 (1638.4 examples/sec; 0.078 sec/batch)
2017-06-02 06:36:04.764883: step 162420, loss = 0.34 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:36:05.598902: step 162430, loss = 0.31 (1534.7 examples/sec; 0.083 sec/batch)
2017-06-02 06:36:06.439185: step 162440, loss = 0.31 (1523.3 examples/sec; 0.084 sec/batch)
2017-06-02 06:36:07.294582: step 162450, loss = 0.33 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:36:08.166996: step 162460, loss = 0.30 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:36:09.044601: step 162470, loss = 0.39 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:36:09.886837: step 162480, loss = 0.29 (1519.8 examples/sec; 0.084 sec/batch)
2017-06-02 06:36:10.770510: step 162490, loss = 0.35 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:36:11.745872: step 162500, loss = 0.23 (1312.3 examples/sec; 0.098 sec/batch)
2017-06-02 06:36:12.518358: step 162510, loss = 0.36 (1657.0 examples/sec; 0.077 sec/batch)
2017-06-02 06:36:13.376094: step 162520, loss = 0.28 (1492.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:36:14.220803: step 162530, loss = 0.30 (1515.1 examples/sec; 0.084 sec/batch)
2017-06-02 06:36:15.078930: step 162540, loss = 0.29 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:36:15.928082: step 162550, loss = 0.36 (1507.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:36:16.824101: step 162560, loss = 0.27 (1428.5 examples/sec; 0.090 sec/batch)
2017-06-02 06:36:17.681218: step 162570, loss = 0.32 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:36:18.570631: step 162580, loss = 0.32 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 06:36:19.402713: step 162590, loss = 0.31 (1538.3 examples/sec; 0.083 sec/batch)
2017-06-02 06:36:20.360280: step 162600, loss = 0.41 (1336.7 examples/sec; 0.096 sec/batch)
2017-06-02 06:36:21.115694: step 162610, loss = 0.28 (1694.4 examples/sec; 0.076 sec/batch)
2017-06-02 06:36:21.990005: step 162620, loss = 0.30 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:36:22.883460: step 162630, loss = 0.30 (1432.6 examples/sec; 0.089 sec/batch)
2017-06-02 06:36:23.757341: step 162640, loss = 0.35 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:36:24.613483: step 162650, loss = 0.32 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:36:25.473525: step 162660, loss = 0.35 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:36:26.345274: step 162670, loss = 0.24 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:36:27.226916: step 162680, loss = 0.34 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:36:28.084593: step 162690, loss = 0.43 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:36:29.039265: step 162700, loss = 0.34 (1340.8 examples/sec; 0.095 sec/batch)
2017-06-02 06:36:29.805055: step 162710, loss = 0.34 (1671.5 examples/sec; 0.077 sec/batch)
2017-06-02 06:36:30.691017: step 162720, loss = 0.33 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 06:36:31.544176: step 162730, loss = 0.38 (1500.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:36:32.428085: step 162740, loss = 0.29 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:36:33.287108: step 162750, loss = 0.37 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:36:34.141087: step 162760, loss = 0.35 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:36:34.998791: step 162770, loss = 0.32 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:36:35.876665: step 162780, loss = 0.33 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:36:36.738593: step 162790, loss = 0.36 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:36:37.725325: step 162800, loss = 0.33 (1297.2 examples/sec; 0.099 sec/batch)
2017-06-02 06:36:38.501970: step 162810, loss = 0.30 (1648.1 examples/sec; 0.078 sec/batch)
2017-06-02 06:36:39.356269: step 162820, loss = 0.25 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:36:40.214496: step 162830, loss = 0.35 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:36:41.071984: step 162840, loss = 0.24 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:36:41.942616: step 162850, loss = 0.31 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:36:42.823717: step 162860, loss = 0.29 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:36:43.684873: step 162870, loss = 0.29 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:36:44.544398: step 162880, loss = 0.29 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:36:45.415203: step 162890, loss = 0.27 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:36:46.392965: step 162900, loss = 0.37 (1309.1 examples/sec; 0.098 sec/batch)
2017-06-02 06:36:47.136177: step 162910, loss = 0.35 (1722.2 examples/sec; 0.074 sec/batch)
2017-06-02 06:36:48.011011: step 162920, loss = 0.29 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:36:48.851523: step 162930, loss = 0.31 (1522.9 examples/sec; 0.084 sec/batch)
2017-06-02 06:36:49.735200: step 162940, loss = 0.39 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:36:50.576146: step 162950, loss = 0.30 (1522.1 examples/sec; 0.084 sec/batch)
2017-06-02 06:36:51.433765: step 162960, loss = 0.43 (1492.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:36:52.305138: step 162970, loss = 0.33 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:36:53.194957: step 162980, loss = 0.28 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 06:36:54.075922: step 162990, loss = 0.26 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:36:55.058205: step 163000, loss = 0.26 (1303.1 examples/sec; 0.098 sec/batch)
2017-06-02 06:36:55.841976: step 163010, loss = 0.28 (1633.1 examples/sec; 0.078 sec/batch)
2017-06-02 06:36:56.715316: step 163020, loss = 0.29 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:36:57.594420: step 163030, loss = 0.42 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:36:58.471512: step 163040, loss = 0.27 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:36:59.337044: step 163050, loss = 0.30 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:37:00.187396: step 163060, loss = 0.36 (1505.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:37:01.048409: step 163070, loss = 0.42 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:37:01.897485: step 163080, loss = 0.25 (1507.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:37:02.758872: step 163090, loss = 0.33 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:37:03.703606: step 163100, loss = 0.37 (1354.9 examples/sec; 0.094 sec/batch)
2017-06-02 06:37:04.504662: step 163110, loss = 0.35 (1597.9 examples/sec; 0.080 sec/batch)
2017-06-02 06:37:05.382635: step 163120, loss = 0.35 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:37:06.258380: step 163130, loss = 0.34 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:37:07.140944: step 163140, loss = 0.33 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:37:08.008025: step 163150, loss = 0.29 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:37:08.874007: step 163160, loss = 0.28 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:37:09.727232: step 163170, loss = 0.27 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:37:10.594660: step 163180, loss = 0.26 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:37:11.449775: step 163190, loss = 0.32 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:37:12.399356: step 163200, loss = 0.30 (1348.0 examples/sec; 0.095 sec/batch)
2017-06-02 06:37:13.162628: step 163210, loss = 0.29 (1677.0 examples/sec; 0.076 sec/batch)
2017-06-02 06:37:14.019469: step 163220, loss = 0.34 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:37:14.899923: step 163230, loss = 0.36 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:37:15.763991: step 163240, loss = 0.36 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:37:16.613406: step 163250, loss = 0.28 (1506.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:37:17.482437: step 163260, loss = 0.27 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:37:18.368110: step 163270, loss = 0.23 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 06:37:19.219010: step 163280, loss = 0.41 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:37:20.073434: step 163290, loss = 0.37 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:37:21.084946: step 163300, loss = 0.36 (1265.4 examples/sec; 0.101 sec/batch)
2017-06-02 06:37:21.838228: step 163310, loss = 0.22 (1699.2 examples/sec; 0.075 sec/batch)
2017-06-02 06:37:22.695312: step 163320, loss = 0.35 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:37:23.580383: step 163330, loss = 0.24 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 06:37:24.442682: step 163340, loss = 0.30 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:37:25.276553: step 163350, loss = 0.35 (1535.0 examples/sec; 0.083 sec/batch)
2017-06-02 06:37:26.137288: step 163360, loss = 0.26 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:37:27.017949: step 163370, loss = 0.29 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:37:27.893833: step 163380, loss = 0.34 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:37:28.741341: step 163390, loss = 0.29 (1510.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:37:29.704572: step 163400, loss = 0.28 (1328.9 examples/sec; 0.096 sec/batch)
2017-06-02 06:37:30.468276: step 163410, loss = 0.30 (1676.0 examples/sec; 0.076 sec/batch)
2017-06-02 06:37:31.333895: step 163420, loss = 0.35 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:37:32.181533: step 163430, loss = 0.36 (1510.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:37:33.058777: step 163440, loss = 0.25 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:37:33.935494: step 163450, loss = 0.32 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:37:34.796903: step 163460, loss = 0.31 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:37:35.647622: step 163470, loss = 0.21 (1504.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:37:36.498975: step 163480, loss = 0.25 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:37:37.362366: step 163490, loss = 0.28 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:37:38.329049: step 163500, loss = 0.27 (1324.1 examples/sec; 0.097 sec/batch)
2017-06-02 06:37:39.090470: step 163510, loss = 0.34 (1681.1 examples/sec; 0.076 sec/batch)
2017-06-02 06:37:39.950555: step 163520, loss = 0.28 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:37:40.803929: step 163530, loss = 0.30 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:37:41.668549: step 163540, loss = 0.27 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:37:42.541846: step 163550, loss = 0.36 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:37:43.402233: step 163560, loss = 0.25 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:37:44.265992: step 163570, loss = 0.26 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:37:45.142190: step 163580, loss = 0.30 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:37:46.005537: step 163590, loss = 0.41 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:37:46.970779: step 163600, loss = 0.21 (1326.1 examples/sec; 0.097 sec/batch)
2017-06-02 06:37:47.739793: step 163610, loss = 0.36 (1664.5 examples/sec; 0.077 sec/batch)
2017-06-02 06:37:48.582108: step 163620, loss = 0.28 (1519.6 examples/sec; 0.084 sec/batch)
2017-06-02 06:37:49.450629: step 163630, loss = 0.37 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:37:50.307123: step 163640, loss = 0.29 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:37:51.199295: step 163650, loss = 0.27 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 06:37:52.068321: step 163660, loss = 0.34 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:37:52.946404: step 163670, loss = 0.30 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:37:53.802835: step 163680, loss = 0.29 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:37:54.655561: step 163690, loss = 0.33 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:37:55.606376: step 163700, loss = 0.32 (1346.2 examples/sec; 0.095 sec/batch)
2017-06-02 06:37:56.354788: step 163710, loss = 0.34 (1710.3 examples/sec; 0.075 sec/batch)
2017-06-02 06:37:57.225629: step 163720, loss = 0.30 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:37:58.084306: step 163730, loss = 0.30 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:37:58.933876: step 163740, loss = 0.29 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:37:59.820063: step 163750, loss = 0.30 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 06:38:00.700058: step 163760, loss = 0.40 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:38:01.528841: step 163770, loss = 0.32 (1544.4 examples/sec; 0.083 sec/batch)
2017-06-02 06:38:02.407084: step 163780, loss = 0.27 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:38:03.286432: step 163790, loss = 0.30 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:38:04.236457: step 163800, loss = 0.33 (1347.3 examples/sec; 0.095 sec/batch)
2017-06-02 06:38:04.991830: step 163810, loss = 0.35 (1694.5 examples/sec; 0.076 sec/batch)
2017-06-02 06:38:05.853415: step 163820, loss = 0.42 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:38:06.729298: step 163830, loss = 0.22 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:38:07.552511: step 163840, loss = 0.32 (1554.9 examples/sec; 0.082 sec/batch)
2017-06-02 06:38:08.428468: step 163850, loss = 0.32 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:38:09.315153: step 163860, loss = 0.35 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 06:38:10.203540: step 163870, loss = 0.32 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 06:38:11.059343: step 163880, loss = 0.28 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:38:11.935932: step 163890, loss = 0.20 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:38:12.900504: step 163900, loss = 0.29 (1327.0 examples/sec; 0.096 sec/batch)
2017-06-02 06:38:13.667852: step 163910, loss = 0.29 (1668.1 examples/sec; 0.077 sec/batch)
2017-06-02 06:38:14.514235: step 163920, loss = 0.28 (1512.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:38:15.388357: step 163930, loss = 0.32 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:38:16.260669: step 163940, loss = 0.26 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:38:17.137719: step 163950, loss = 0.30 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:38:17.997680: step 163960, loss = 0.25 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:38:18.867154: step 163970, loss = 0.24 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:38:19.740763: step 163980, loss = 0.30 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:38:20.613879: step 163990, loss = 0.24 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:38:21.578723: step 164000, loss = 0.30 (1326.7 examples/sec; 0.096 sec/batch)
2017-06-02 06:38:22.355136: step 164010, loss = 0.29 (1648.6 examples/sec; 0.078 sec/batch)
2017-06-02 06:38:23.227579: step 164020, loss = 0.33 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:38:24.105639: step 164030, loss = 0.28 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:38:24.981686: step 164040, loss = 0.28 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:38:25.852598: step 164050, loss = 0.29 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:38:26.726932: step 164060, loss = 0.31 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:38:27.604183: step 164070, loss = 0.33 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:38:28.486506: step 164080, loss = 0.22 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:38:29.355708: step 164090, loss = 0.30 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:38:30.284152: step 164100, loss = 0.24 (1378.7 examples/sec; 0.093 sec/batch)
2017-06-02 06:38:31.063845: step 164110, loss = 0.31 (1641.7 examples/sec; 0.078 sec/batch)
2017-06-02 06:38:31.919531: step 164120, loss = 0.37 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:38:32.755917: step 164130, loss = 0.29 (1530.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:38:33.632656: step 164140, loss = 0.24 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:38:34.501988: step 164150, loss = 0.30 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:38:35.365791: step 164160, loss = 0.25 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:38:36.226717: step 164170, loss = 0.26 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:38:37.110384: step 164180, loss = 0.37 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:38:37.999869: step 164190, loss = 0.36 (1439.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:38:38.937213: step 164200, loss = 0.26 (1365.6 examples/sec; 0.094 sec/batch)
2017-06-02 06:38:39.688079: step 164210, loss = 0.38 (1704.8 examples/sec; 0.075 sec/batch)
2017-06-02 06:38:40.562716: step 164220, loss = 0.25 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:38:41.446899: step 164230, loss = 0.34 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:38:42.320959: step 164240, loss = 0.39 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:38:43.210889: step 164250, loss = 0.24 (1438.3 examples/sec; 0.089 sec/batch)
2017-06-02 06:38:44.091376: step 164260, loss = 0.36 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:38:44.951195: step 164270, loss = 0.32 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:38:45.815267: step 164280, loss = 0.24 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:38:46.662282: step 164290, loss = 0.32 (1511.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:38:47.640039: step 164300, loss = 0.31 (1309.1 examples/sec; 0.098 sec/batch)
2017-06-02 06:38:48.415317: step 164310, loss = 0.34 (1651.0 examples/sec; 0.078 sec/batch)
2017-06-02 06:38:49.279039: step 164320, loss = 0.36 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:38:50.141952: step 164330, loss = 0.33 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:38:51.024295: step 164340, loss = 0.28 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:38:51.912341: step 164350, loss = 0.37 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 06:38:52.789586: step 164360, loss = 0.30 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:38:53.648103: step 164370, loss = 0.31 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:38:54.515570: step 164380, loss = 0.25 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:38:55.391834: step 164390, loss = 0.31 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:38:56.402360: step 164400, loss = 0.32 (1266.7 examples/sec; 0.101 sec/batch)
2017-06-02 06:38:57.108245: step 164410, loss = 0.43 (1813.3 examples/sec; 0.071 sec/batch)
2017-06-02 06:38:57.983378: step 164420, loss = 0.36 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:38:58.825463: step 164430, loss = 0.42 (1520.0 examples/sec; 0.084 sec/batch)
2017-06-02 06:38:59.718484: step 164440, loss = 0.29 (1433.3 examples/sec; 0.089 sec/batch)
2017-06-02 06:39:00.587563: step 164450, loss = 0.30 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:39:01.449076: step 164460, loss = 0.39 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:39:02.292621: step 164470, loss = 0.26 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:39:03.165396: step 164480, loss = 0.35 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:39:04.047323: step 164490, loss = 0.34 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:39:05.028994: step 164500, loss = 0.25 (1303.9 examples/sec; 0.098 sec/batch)
2017-06-02 06:39:05.804012: step 164510, loss = 0.26 (1651.6 examples/sec; 0.078 sec/batch)
2017-06-02 06:39:06.685196: step 164520, loss = 0.33 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:39:07.556464: step 164530, loss = 0.31 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:39:08.432622: step 164540, loss = 0.30 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:39:09.302613: step 164550, loss = 0.25 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:39:10.160658: step 164560, loss = 0.35 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:39:11.029266: step 164570, loss = 0.34 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:39:11.888609: step 164580, loss = 0.41 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:39:12.747070: step 164590, loss = 0.29 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:39:13.698316: step 164600, loss = 0.35 (1345.6 examples/sec; 0.095 sec/batch)
2017-06-02 06:39:14.488046: step 164610, loss = 0.23 (1620.8 examples/sec; 0.079 sec/batch)
2017-06-02 06:39:15.334322: step 164620, loss = 0.32 (1512.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:39:16.208342: step 164630, loss = 0.25 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:39:17.089687: step 164640, loss = 0.32 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:39:17.959500: step 164650, loss = 0.37 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:39:18.822361: step 164660, loss = 0.29 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:39:19.672414: step 164670, loss = 0.30 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:39:20.550145: step 164680, loss = 0.25 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:39:21.430936: step 164690, loss = 0.30 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:39:22.428094: step 164700, loss = 0.34 (1283.6 examples/sec; 0.100 sec/batch)
2017-06-02 06:39:23.165225: step 164710, loss = 0.42 (1736.5 examples/sec; 0.074 sec/batch)
2017-06-02 06:39:24.051601: step 164720, loss = 0.29 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 06:39:24.888518: step 164730, loss = 0.24 (1529.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:39:25.762863: step 164740, loss = 0.33 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:39:26.619611: step 164750, loss = 0.35 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:39:27.498138: step 164760, loss = 0.31 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:39:28.373450: step 164770, loss = 0.31 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:39:29.238614: step 164780, loss = 0.33 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:39:30.116876: step 164790, loss = 0.38 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:39:31.075060: step 164800, loss = 0.37 (1335.9 examples/sec; 0.096 sec/batch)
2017-06-02 06:39:31.888382: step 164810, loss = 0.28 (1573.8 examples/sec; 0.081 sec/batch)
2017-06-02 06:39:32.758871: step 164820, loss = 0.35 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:39:33.654522: step 164830, loss = 0.33 (1429.1 examples/sec; 0.090 sec/batch)
2017-06-02 06:39:34.541084: step 164840, loss = 0.28 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 06:39:35.423889: step 164850, loss = 0.31 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:39:36.302439: step 164860, loss = 0.49 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:39:37.162089: step 164870, loss = 0.34 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:39:38.014319: step 164880, loss = 0.32 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:39:38.891131: step 164890, loss = 0.28 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:39:39.877534: step 164900, loss = 0.31 (1297.6 examples/sec; 0.099 sec/batch)
2017-06-02 06:39:40.624957: step 164910, loss = 0.26 (1712.6 examples/sec; 0.075 sec/batch)
2017-06-02 06:39:41.474207: step 164920, loss = 0.30 (1507.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:39:42.336013: step 164930, loss = 0.35 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:39:43.206428: step 164940, loss = 0.34 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:39:44.071969: step 164950, loss = 0.34 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:39:44.940509: step 164960, loss = 0.24 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:39:45.816597: step 164970, loss = 0.39 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:39:46.657831: step 164980, loss = 0.27 (1521.6 examples/sec; 0.084 sec/batch)
2017-06-02 06:39:47.515387: step 164990, loss = 0.37 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:39:48.467149: step 165000, loss = 0.31 (1344.9 examples/sec; 0.095 sec/batch)
2017-06-02 06:39:49.240242: step 165010, loss = 0.27 (1655.7 examples/sec; 0.077 sec/batch)
2017-06-02 06:39:50.073022: step 165020, loss = 0.36 (1537.0 examples/sec; 0.083 sec/batch)
2017-06-02 06:39:50.927725: step 165030, loss = 0.32 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:39:51.845335: step 165040, loss = 0.31 (1394.9 examples/sec; 0.092 sec/batch)
2017-06-02 06:39:52.711898: step 165050, loss = 0.32 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:39:53.575296: step 165060, loss = 0.35 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:39:54.432715: step 165070, loss = 0.30 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:39:55.288511: step 165080, loss = 0.31 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:39:56.146131: step 165090, loss = 0.27 (1492.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:39:57.144149: step 165100, loss = 0.35 (1282.5 examples/sec; 0.100 sec/batch)
2017-06-02 06:39:57.870062: step 165110, loss = 0.34 (1763.3 examples/sec; 0.073 sec/batch)
2017-06-02 06:39:58.743594: step 165120, loss = 0.34 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:39:59.616382: step 165130, loss = 0.29 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:40:00.492399: step 165140, loss = 0.32 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:40:01.342438: step 165150, loss = 0.35 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:40:02.206336: step 165160, loss = 0.27 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:40:03.044076: step 165170, loss = 0.37 (1527.9 examples/sec; 0.084 sec/batch)
2017-06-02 06:40:03.916133: step 165180, loss = 0.37 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:40:04.808972: step 165190, loss = 0.31 (1433.6 examples/sec; 0.089 sec/batch)
2017-06-02 06:40:05.777506: step 165200, loss = 0.28 (1321.6 examples/sec; 0.097 sec/batch)
2017-06-02 06:40:06.525693: step 165210, loss = 0.29 (1710.8 examples/sec; 0.075 sec/batch)
2017-06-02 06:40:07.378504: step 165220, loss = 0.33 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:40:08.218904: step 165230, loss = 0.29 (1523.1 examples/sec; 0.084 sec/batch)
2017-06-02 06:40:09.074565: step 165240, loss = 0.32 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:40:09.941679: step 165250, loss = 0.35 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:40:10.826308: step 165260, loss = 0.24 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:40:11.691761: step 165270, loss = 0.27 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:40:12.557414: step 165280, loss = 0.29 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:40:13.454066: step 165290, loss = 0.30 (1427.5 examples/sec; 0.090 sec/batch)
2017-06-02 06:40:14.420940: step 165300, loss = 0.26 (1323.9 examples/sec; 0.097 sec/batch)
2017-06-02 06:40:15.194813: step 165310, loss = 0.33 (1654.0 examples/sec; 0.077 sec/batch)
2017-06-02 06:40:16.065180: step 165320, loss = 0.29 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:40:16.943481: step 165330, loss = 0.35 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:40:17.817953: step 165340, loss = 0.31 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:40:18.671884: step 165350, loss = 0.35 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:40:19.523475: step 165360, loss = 0.33 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:40:20.394589: step 165370, loss = 0.23 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:40:21.268243: step 165380, loss = 0.34 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:40:22.143235: step 165390, loss = 0.31 (1462.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:40:23.096051: step 165400, loss = 0.24 (1343.4 examples/sec; 0.095 sec/batch)
2017-06-02 06:40:23.890405: step 165410, loss = 0.28 (1611.4 examples/sec; 0.079 sec/batch)
2017-06-02 06:40:24.769328: step 165420, loss = 0.26 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:40:25.638031: step 165430, loss = 0.26 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:40:26.476570: step 165440, loss = 0.35 (1526.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:40:27.344402: step 165450, loss = 0.28 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:40:28.205810: step 165460, loss = 0.33 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:40:29.066551: step 165470, loss = 0.29 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:40:29.907645: step 165480, loss = 0.30 (1521.8 examples/sec; 0.084 sec/batch)
2017-06-02 06:40:30.767809: step 165490, loss = 0.28 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:40:31.707613: step 165500, loss = 0.31 (1362.0 examples/sec; 0.094 sec/batch)
2017-06-02 06:40:32.491258: step 165510, loss = 0.34 (1633.4 examples/sec; 0.078 sec/batch)
2017-06-02 06:40:33.371169: step 165520, loss = 0.34 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:40:34.217947: step 165530, loss = 0.29 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:40:35.078267: step 165540, loss = 0.32 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:40:35.923980: step 165550, loss = 0.35 (1513.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:40:36.792123: step 165560, loss = 0.28 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:40:37.668502: step 165570, loss = 0.32 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:40:38.538956: step 165580, loss = 0.26 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:40:39.418939: step 165590, loss = 0.41 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:40:40.439542: step 165600, loss = 0.35 (1254.2 examples/sec; 0.102 sec/batch)
2017-06-02 06:40:41.190381: step 165610, loss = 0.29 (1704.7 examples/sec; 0.075 sec/batch)
2017-06-02 06:40:42.086005: step 165620, loss = 0.35 (1429.2 examples/sec; 0.090 sec/batch)
2017-06-02 06:40:42.954687: step 165630, loss = 0.27 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:40:43.823276: step 165640, loss = 0.30 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:40:44.695055: step 165650, loss = 0.34 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:40:45.549276: step 165660, loss = 0.30 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:40:46.400573: step 165670, loss = 0.36 (1503.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:40:47.266530: step 165680, loss = 0.40 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:40:48.151804: step 165690, loss = 0.35 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 06:40:49.155972: step 165700, loss = 0.34 (1274.7 examples/sec; 0.100 sec/batch)
2017-06-02 06:40:49.888486: step 165710, loss = 0.29 (1747.4 examples/sec; 0.073 sec/batch)
2017-06-02 06:40:50.772642: step 165720, loss = 0.35 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:40:51.633653: step 165730, loss = 0.25 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:40:52.483526: step 165740, loss = 0.31 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:40:53.350850: step 165750, loss = 0.28 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:40:54.218887: step 165760, loss = 0.36 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:40:55.089310: step 165770, loss = 0.41 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:40:55.984836: step 165780, loss = 0.27 (1429.3 examples/sec; 0.090 sec/batch)
2017-06-02 06:40:56.870618: step 165790, loss = 0.25 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 06:40:57.877642: step 165800, loss = 0.24 (1271.1 examples/sec; 0.101 sec/batch)
2017-06-02 06:40:58.602411: step 165810, loss = 0.26 (1766.1 examples/sec; 0.072 sec/batch)
2017-06-02 06:40:59.489722: step 165820, loss = 0.30 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 06:41:00.345838: step 165830, loss = 0.28 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:41:01.188522: step 165840, loss = 0.30 (1519.0 examples/sec; 0.084 sec/batch)
2017-06-02 06:41:02.038581: step 165850, loss = 0.31 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:41:02.913806: step 165860, loss = 0.27 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:41:03.783367: step 165870, loss = 0.29 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:41:04.694876: step 165880, loss = 0.27 (1404.2 examples/sec; 0.091 sec/batch)
2017-06-02 06:41:05.583837: step 165890, loss = 0.31 (1439.9 examples/sec; 0.089 sec/batch)
2017-06-02 06:41:06.573029: step 165900, loss = 0.30 (1294.0 examples/sec; 0.099 sec/batch)
2017-06-02 06:41:07.353234: step 165910, loss = 0.43 (1640.6 examples/sec; 0.078 sec/batch)
2017-06-02 06:41:08.253098: step 165920, loss = 0.33 (1422.4 examples/sec; 0.090 sec/batch)
2017-06-02 06:41:09.123493: step 165930, loss = 0.33 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:41:09.985983: step 165940, loss = 0.40 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:41:10.841712: step 165950, loss = 0.25 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:41:11.726017: step 165960, loss = 0.34 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:41:12.636726: step 165970, loss = 0.26 (1405.5 examples/sec; 0.091 sec/batch)
2017-06-02 06:41:13.545983: step 165980, loss = 0.32 (1407.7 examples/sec; 0.091 sec/batch)
2017-06-02 06:41:14.430761: step 165990, loss = 0.28 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:41:15.412286: step 166000, loss = 0.33 (1304.1 examples/sec; 0.098 sec/batch)
2017-06-02 06:41:16.221080: step 166010, loss = 0.28 (1582.6 examples/sec; 0.081 sec/batch)
2017-06-02 06:41:17.100057: step 166020, loss = 0.33 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:41:17.981098: step 166030, loss = 0.27 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:41:18.843230: step 166040, loss = 0.26 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:41:19.722066: step 166050, loss = 0.24 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:41:20.572060: step 166060, loss = 0.37 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:41:21.460973: step 166070, loss = 0.36 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:41:22.332006: step 166080, loss = 0.25 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:41:23.228371: step 166090, loss = 0.38 (1428.0 examples/sec; 0.090 sec/batch)
2017-06-02 06:41:24.193114: step 166100, loss = 0.32 (1326.8 examples/sec; 0.096 sec/batch)
2017-06-02 06:41:24.963865: step 166110, loss = 0.33 (1660.7 examples/sec; 0.077 sec/batch)
2017-06-02 06:41:25.838174: step 166120, loss = 0.37 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:41:26.710542: step 166130, loss = 0.28 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:41:27.559423: step 166140, loss = 0.28 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:41:28.425392: step 166150, loss = 0.32 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:41:29.319476: step 166160, loss = 0.30 (1431.6 examples/sec; 0.089 sec/batch)
2017-06-02 06:41:30.181216: step 166170, loss = 0.33 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:41:31.050239: step 166180, loss = 0.23 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:41:31.901897: step 166190, loss = 0.31 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:41:32.920158: step 166200, loss = 0.28 (1257.0 examples/sec; 0.102 sec/batch)
2017-06-02 06:41:33.635320: step 166210, loss = 0.29 (1789.8 examples/sec; 0.072 sec/batch)
2017-06-02 06:41:34.487341: step 166220, loss = 0.34 (1502.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:41:35.363806: step 166230, loss = 0.25 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:41:36.200140: step 166240, loss = 0.36 (1530.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:41:37.074992: step 166250, loss = 0.30 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:41:37.923768: step 166260, loss = 0.32 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:41:38.777685: step 166270, loss = 0.39 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:41:39.634754: step 166280, loss = 0.21 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:41:40.518677: step 166290, loss = 0.40 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:41:41.478249: step 166300, loss = 0.38 (1333.9 examples/sec; 0.096 sec/batch)
2017-06-02 06:41:42.256776: step 166310, loss = 0.33 (1644.1 examples/sec; 0.078 sec/batch)
2017-06-02 06:41:43.125146: step 166320, loss = 0.38 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:41:43.986471: step 166330, loss = 0.26 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:41:44.846012: step 166340, loss = 0.32 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:41:45.710539: step 166350, loss = 0.30 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:41:46.569149: step 166360, loss = 0.20 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:41:47.433027: step 166370, loss = 0.33 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:41:48.304001: step 166380, loss = 0.22 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:41:49.162202: step 166390, loss = 0.27 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:41:50.130376: step 166400, loss = 0.32 (1322.1 examples/sec; 0.097 sec/batch)
2017-06-02 06:41:50.885094: step 166410, loss = 0.30 (1696.0 examples/sec; 0.075 sec/batch)
2017-06-02 06:41:51.760483: step 166420, loss = 0.27 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:41:52.608092: step 166430, loss = 0.41 (1510.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:41:53.485090: step 166440, loss = 0.33 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:41:54.356185: step 166450, loss = 0.29 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:41:55.212275: step 166460, loss = 0.34 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:41:56.085626: step 166470, loss = 0.27 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:41:56.937862: step 166480, loss = 0.34 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:41:57.798655: step 166490, loss = 0.33 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:41:58.742191: step 166500, loss = 0.34 (1356.6 examples/sec; 0.094 sec/batch)
2017-06-02 06:41:59.534433: step 166510, loss = 0.43 (1615.7 examples/sec; 0.079 sec/batch)
2017-06-02 06:42:00.386938: step 166520, loss = 0.33 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:42:01.265146: step 166530, loss = 0.30 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:42:02.148407: step 166540, loss = 0.37 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:42:03.010720: step 166550, loss = 0.29 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:42:03.850363: step 166560, loss = 0.24 (1524.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:42:04.697461: step 166570, loss = 0.35 (1511.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:42:05.586745: step 166580, loss = 0.33 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 06:42:06.477274: step 166590, loss = 0.30 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 06:42:07.486054: step 166600, loss = 0.24 (1268.9 examples/sec; 0.101 sec/batch)
2017-06-02 06:42:08.224803: step 166610, loss = 0.37 (1732.7 examples/sec; 0.074 sec/batch)
2017-06-02 06:42:09.092763: step 166620, loss = 0.32 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:42:09.945076: step 166630, loss = 0.28 (1501.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:42:10.779716: step 166640, loss = 0.27 (1533.6 examples/sec; 0.083 sec/batch)
2017-06-02 06:42:11.675853: step 166650, loss = 0.40 (1428.4 examples/sec; 0.090 sec/batch)
2017-06-02 06:42:12.507269: step 166660, loss = 0.23 (1539.5 examples/sec; 0.083 sec/batch)
2017-06-02 06:42:13.364142: step 166670, loss = 0.33 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:42:14.242420: step 166680, loss = 0.29 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:42:15.076716: step 166690, loss = 0.27 (1534.2 examples/sec; 0.083 sec/batch)
2017-06-02 06:42:16.037201: step 166700, loss = 0.37 (1332.7 examples/sec; 0.096 sec/batch)
2017-06-02 06:42:16.826447: step 166710, loss = 0.36 (1621.8 examples/sec; 0.079 sec/batch)
2017-06-02 06:42:17.700008: step 166720, loss = 0.24 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:42:18.574761: step 166730, loss = 0.36 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:42:19.457413: step 166740, loss = 0.28 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:42:20.319532: step 166750, loss = 0.22 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:42:21.170936: step 166760, loss = 0.30 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:42:22.031213: step 166770, loss = 0.40 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:42:22.864068: step 166780, loss = 0.32 (1536.8 examples/sec; 0.083 sec/batch)
2017-06-02 06:42:23.719791: step 166790, loss = 0.31 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:42:24.714125: step 166800, loss = 0.41 (1287.3 examples/sec; 0.099 sec/batch)
2017-06-02 06:42:25.466226: step 166810, loss = 0.31 (1701.9 examples/sec; 0.075 sec/batch)
2017-06-02 06:42:26.336596: step 166820, loss = 0.24 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:42:27.191308: step 166830, loss = 0.25 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:42:28.052912: step 166840, loss = 0.26 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:42:28.943239: step 166850, loss = 0.30 (1437.7 examples/sec; 0.089 sec/batch)
2017-06-02 06:42:29.832058: step 166860, loss = 0.39 (1440.1 examples/sec; 0.089 sec/batch)
2017-06-02 06:42:30.686579: step 166870, loss = 0.29 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:42:31.555271: step 166880, loss = 0.33 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:42:32.420098: step 166890, loss = 0.24 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:42:33.388800: step 166900, loss = 0.28 (1321.4 examples/sec; 0.097 sec/batch)
2017-06-02 06:42:34.154082: step 166910, loss = 0.27 (1672.6 examples/sec; 0.077 sec/batch)
2017-06-02 06:42:34.999754: step 166920, loss = 0.42 (1513.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:42:35.875545: step 166930, loss = 0.30 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:42:36.747590: step 166940, loss = 0.35 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:42:37.614410: step 166950, loss = 0.25 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:42:38.462080: step 166960, loss = 0.23 (1510.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:42:39.327348: step 166970, loss = 0.38 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:42:40.194111: step 166980, loss = 0.34 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:42:41.034640: step 166990, loss = 0.39 (1522.9 examples/sec; 0.084 sec/batch)
2017-06-02 06:42:41.985084: step 167000, loss = 0.30 (1346.7 examples/sec; 0.095 sec/batch)
2017-06-02 06:42:42.754555: step 167010, loss = 0.31 (1663.5 examples/sec; 0.077 sec/batch)
2017-06-02 06:42:43.610329: step 167020, loss = 0.31 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:42:44.482143: step 167030, loss = 0.36 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:42:45.346523: step 167040, loss = 0.33 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:42:46.189538: step 167050, loss = 0.34 (1518.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:42:47.043336: step 167060, loss = 0.27 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:42:47.902820: step 167070, loss = 0.44 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:42:48.775660: step 167080, loss = 0.46 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:42:49.613960: step 167090, loss = 0.32 (1526.9 examples/sec; 0.084 sec/batch)
2017-06-02 06:42:50.631899: step 167100, loss = 0.28 (1257.4 examples/sec; 0.102 sec/batch)
2017-06-02 06:42:51.354007: step 167110, loss = 0.32 (1772.6 examples/sec; 0.072 sec/batch)
2017-06-02 06:42:52.199574: step 167120, loss = 0.31 (1513.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:42:53.034545: step 167130, loss = 0.32 (1533.0 examples/sec; 0.083 sec/batch)
2017-06-02 06:42:53.912187: step 167140, loss = 0.31 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:42:54.774588: step 167150, loss = 0.33 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:42:55.639967: step 167160, loss = 0.27 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:42:56.490174: step 167170, loss = 0.38 (1505.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:42:57.361146: step 167180, loss = 0.34 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:42:58.222901: step 167190, loss = 0.24 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:42:59.174548: step 167200, loss = 0.28 (1345.0 examples/sec; 0.095 sec/batch)
2017-06-02 06:42:59.943763: step 167210, loss = 0.34 (1664.0 examples/sec; 0.077 sec/batch)
2017-06-02 06:43:00.808469: step 167220, loss = 0.29 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:43:01.660404: step 167230, loss = 0.40 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:43:02.514095: step 167240, loss = 0.36 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:43:03.382632: step 167250, loss = 0.30 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:43:04.256147: step 167260, loss = 0.32 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:43:05.104517: step 167270, loss = 0.38 (1508.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:43:05.947377: step 167280, loss = 0.37 (1518.6 examples/sec; 0.084 sec/batch)
2017-06-02 06:43:06.804017: step 167290, loss = 0.26 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:43:07.807437: step 167300, loss = 0.33 (1275.6 examples/sec; 0.100 sec/batch)
2017-06-02 06:43:08.567054: step 167310, loss = 0.37 (1685.1 examples/sec; 0.076 sec/batch)
2017-06-02 06:43:09.415871: step 167320, loss = 0.43 (1508.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:43:10.257169: step 167330, loss = 0.32 (1521.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:43:11.121626: step 167340, loss = 0.42 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:43:12.006071: step 167350, loss = 0.25 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:43:12.884793: step 167360, loss = 0.26 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:43:13.724248: step 167370, loss = 0.36 (1524.8 examples/sec; 0.084 sec/batch)
2017-06-02 06:43:14.589025: step 167380, loss = 0.30 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:43:15.457095: step 167390, loss = 0.31 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:43:16.422307: step 167400, loss = 0.32 (1326.1 examples/sec; 0.097 sec/batch)
2017-06-02 06:43:17.187453: step 167410, loss = 0.27 (1672.9 examples/sec; 0.077 sec/batch)
2017-06-02 06:43:18.033966: step 167420, loss = 0.31 (1512.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:43:18.898302: step 167430, loss = 0.40 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:43:19.758398: step 167440, loss = 0.34 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:43:20.640481: step 167450, loss = 0.35 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:43:21.497189: step 167460, loss = 0.33 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:43:22.351356: step 167470, loss = 0.37 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:43:23.221229: step 167480, loss = 0.36 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:43:24.089683: step 167490, loss = 0.26 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:43:25.095044: step 167500, loss = 0.34 (1273.2 examples/sec; 0.101 sec/batch)
2017-06-02 06:43:25.807887: step 167510, loss = 0.35 (1795.6 examples/sec; 0.071 sec/batch)
2017-06-02 06:43:26.648107: step 167520, loss = 0.36 (1523.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:43:27.478683: step 167530, loss = 0.30 (1541.1 examples/sec; 0.083 sec/batch)
2017-06-02 06:43:28.317698: step 167540, loss = 0.32 (1525.6 examples/sec; 0.084 sec/batch)
2017-06-02 06:43:29.142372: step 167550, loss = 0.28 (1552.1 examples/sec; 0.082 sec/batch)
2017-06-02 06:43:29.987165: step 167560, loss = 0.29 (1515.2 examples/sec; 0.084 sec/batch)
2017-06-02 06:43:30.857128: step 167570, loss = 0.30 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:43:31.720374: step 167580, loss = 0.26 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:43:32.578602: step 167590, loss = 0.28 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:43:33.529882: step 167600, loss = 0.30 (1345.5 examples/sec; 0.095 sec/batch)
2017-06-02 06:43:34.297604: step 167610, loss = 0.20 (1667.3 examples/sec; 0.077 sec/batch)
2017-06-02 06:43:35.156142: step 167620, loss = 0.27 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:43:36.021764: step 167630, loss = 0.31 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:43:36.891132: step 167640, loss = 0.31 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:43:37.766184: step 167650, loss = 0.37 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:43:38.622038: step 167660, loss = 0.32 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:43:39.465067: step 167670, loss = 0.31 (1518.3 examples/sec; 0.084 sec/batch)
2017-06-02 06:43:40.304905: step 167680, loss = 0.30 (1524.1 examples/sec; 0.084 sec/batch)
2017-06-02 06:43:41.175319: step 167690, loss = 0.25 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:43:42.200529: step 167700, loss = 0.33 (1248.5 examples/sec; 0.103 sec/batch)
2017-06-02 06:43:42.890479: step 167710, loss = 0.30 (1855.2 examples/sec; 0.069 sec/batch)
2017-06-02 06:43:43.727572: step 167720, loss = 0.34 (1529.1 examples/sec; 0.084 sec/batch)
2017-06-02 06:43:44.576928: step 167730, loss = 0.27 (1507.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:43:45.456481: step 167740, loss = 0.28 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:43:46.296706: step 167750, loss = 0.39 (1523.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:43:47.169333: step 167760, loss = 0.31 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:43:48.031402: step 167770, loss = 0.36 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:43:48.892231: step 167780, loss = 0.30 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:43:49.773173: step 167790, loss = 0.34 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:43:50.713280: step 167800, loss = 0.25 (1361.6 examples/sec; 0.094 sec/batch)
2017-06-02 06:43:51.487267: step 167810, loss = 0.37 (1653.8 examples/sec; 0.077 sec/batch)
2017-06-02 06:43:52.342674: step 167820, loss = 0.36 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:43:53.186221: step 167830, loss = 0.33 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:43:54.027878: step 167840, loss = 0.34 (1520.8 examples/sec; 0.084 sec/batch)
2017-06-02 06:43:54.918341: step 167850, loss = 0.31 (1437.4 examples/sec; 0.089 sec/batch)
2017-06-02 06:43:55.807689: step 167860, loss = 0.31 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 06:43:56.684485: step 167870, loss = 0.33 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:43:57.557650: step 167880, loss = 0.35 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:43:58.441725: step 167890, loss = 0.47 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:43:59.417160: step 167900, loss = 0.30 (1312.2 examples/sec; 0.098 sec/batch)
2017-06-02 06:44:00.208388: step 167910, loss = 0.27 (1617.7 examples/sec; 0.079 sec/batch)
2017-06-02 06:44:01.083424: step 167920, loss = 0.29 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:44:01.945785: step 167930, loss = 0.32 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:44:02.783848: step 167940, loss = 0.32 (1527.3 examples/sec; 0.084 sec/batch)
2017-06-02 06:44:03.643843: step 167950, loss = 0.34 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:44:04.502981: step 167960, loss = 0.28 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:44:05.368146: step 167970, loss = 0.28 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:44:06.241167: step 167980, loss = 0.32 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:44:07.094447: step 167990, loss = 0.23 (1500.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:44:08.081877: step 168000, loss = 0.36 (1296.3 examples/sec; 0.099 sec/batch)
2017-06-02 06:44:08.852960: step 168010, loss = 0.40 (1660.0 examples/sec; 0.077 sec/batch)
2017-06-02 06:44:09.713188: step 168020, loss = 0.28 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:44:10.593612: step 168030, loss = 0.30 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:44:11.437135: step 168040, loss = 0.38 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:44:12.309773: step 168050, loss = 0.37 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:44:13.153874: step 168060, loss = 0.27 (1516.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:44:14.032709: step 168070, loss = 0.29 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:44:14.901436: step 168080, loss = 0.39 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:44:15.762236: step 168090, loss = 0.37 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:44:16.732961: step 168100, loss = 0.42 (1318.6 examples/sec; 0.097 sec/batch)
2017-06-02 06:44:17.520324: step 168110, loss = 0.31 (1625.7 examples/sec; 0.079 sec/batch)
2017-06-02 06:44:18.347673: step 168120, loss = 0.30 (1547.1 examples/sec; 0.083 sec/batch)
2017-06-02 06:44:19.213271: step 168130, loss = 0.36 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:44:20.117706: step 168140, loss = 0.35 (1415.3 examples/sec; 0.090 sec/batch)
2017-06-02 06:44:21.019197: step 168150, loss = 0.35 (1419.8 examples/sec; 0.090 sec/batch)
2017-06-02 06:44:21.884465: step 168160, loss = 0.22 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:44:22.721935: step 168170, loss = 0.23 (1528.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:44:23.602428: step 168180, loss = 0.41 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:44:24.450646: step 168190, loss = 0.26 (1509.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:44:25.435144: step 168200, loss = 0.25 (1300.2 examples/sec; 0.098 sec/batch)
2017-06-02 06:44:26.194552: step 168210, loss = 0.24 (1685.5 examples/sec; 0.076 sec/batch)
2017-06-02 06:44:27.065527: step 168220, loss = 0.25 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:44:27.922260: step 168230, loss = 0.32 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:44:28.791987: step 168240, loss = 0.21 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:44:29.649044: step 168250, loss = 0.33 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:44:30.525914: step 168260, loss = 0.30 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:44:31.377113: step 168270, loss = 0.31 (1503.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:44:32.234931: step 168280, loss = 0.33 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:44:33.126877: step 168290, loss = 0.32 (1435.1 examples/sec; 0.089 sec/batch)
2017-06-02 06:44:34.111844: step 168300, loss = 0.26 (1299.5 examples/sec; 0.098 sec/batch)
2017-06-02 06:44:34.870164: step 168310, loss = 0.25 (1688.0 examples/sec; 0.076 sec/batch)
2017-06-02 06:44:35.746416: step 168320, loss = 0.30 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:44:36.603075: step 168330, loss = 0.29 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:44:37.448912: step 168340, loss = 0.26 (1513.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:44:38.309811: step 168350, loss = 0.38 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:44:39.190256: step 168360, loss = 0.29 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:44:40.033664: step 168370, loss = 0.26 (1517.7 examples/sec; 0.084 sec/batch)
2017-06-02 06:44:40.899669: step 168380, loss = 0.26 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:44:41.769223: step 168390, loss = 0.31 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:44:42.712391: step 168400, loss = 0.25 (1357.1 examples/sec; 0.094 sec/batch)
2017-06-02 06:44:43.482137: step 168410, loss = 0.28 (1662.9 examples/sec; 0.077 sec/batch)
2017-06-02 06:44:44.351126: step 168420, loss = 0.30 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:44:45.230474: step 168430, loss = 0.33 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:44:46.080154: step 168440, loss = 0.27 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:44:46.935471: step 168450, loss = 0.25 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:44:47.782910: step 168460, loss = 0.33 (1510.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:44:48.664554: step 168470, loss = 0.27 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:44:49.564117: step 168480, loss = 0.30 (1422.9 examples/sec; 0.090 sec/batch)
2017-06-02 06:44:50.439015: step 168490, loss = 0.25 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:44:51.423771: step 168500, loss = 0.27 (1299.8 examples/sec; 0.098 sec/batch)
2017-06-02 06:44:52.222975: step 168510, loss = 0.22 (1601.6 examples/sec; 0.080 sec/batch)
2017-06-02 06:44:53.100502: step 168520, loss = 0.28 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:44:53.986520: step 168530, loss = 0.32 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 06:44:54.864843: step 168540, loss = 0.35 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:44:55.742421: step 168550, loss = 0.32 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:44:56.631191: step 168560, loss = 0.29 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 06:44:57.501359: step 168570, loss = 0.36 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:44:58.355200: step 168580, loss = 0.31 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:44:59.250574: step 168590, loss = 0.24 (1429.6 examples/sec; 0.090 sec/batch)
2017-06-02 06:45:00.219376: step 168600, loss = 0.31 (1321.2 examples/sec; 0.097 sec/batch)
2017-06-02 06:45:00.998880: step 168610, loss = 0.30 (1642.1 examples/sec; 0.078 sec/batch)
2017-06-02 06:45:01.855082: step 168620, loss = 0.21 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:45:02.714088: step 168630, loss = 0.30 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:45:03.584637: step 168640, loss = 0.27 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:45:04.435922: step 168650, loss = 0.39 (1503.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:45:05.307917: step 168660, loss = 0.28 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:45:06.178823: step 168670, loss = 0.34 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:45:07.023684: step 168680, loss = 0.32 (1515.0 examples/sec; 0.084 sec/batch)
2017-06-02 06:45:07.912919: step 168690, loss = 0.37 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 06:45:08.884847: step 168700, loss = 0.24 (1317.0 examples/sec; 0.097 sec/batch)
2017-06-02 06:45:09.619356: step 168710, loss = 0.26 (1742.7 examples/sec; 0.073 sec/batch)
2017-06-02 06:45:10.501468: step 168720, loss = 0.34 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:45:11.353862: step 168730, loss = 0.28 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:45:12.240472: step 168740, loss = 0.38 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 06:45:13.104441: step 168750, loss = 0.25 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:45:13.979509: step 168760, loss = 0.34 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:45:14.849630: step 168770, loss = 0.35 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:45:15.713577: step 168780, loss = 0.37 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:45:16.578695: step 168790, loss = 0.33 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:45:17.601447: step 168800, loss = 0.24 (1251.5 examples/sec; 0.102 sec/batch)
2017-06-02 06:45:18.312519: step 168810, loss = 0.25 (1800.1 examples/sec; 0.071 sec/batch)
2017-06-02 06:45:19.187100: step 168820, loss = 0.30 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:45:20.039197: step 168830, loss = 0.40 (1502.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:45:20.901589: step 168840, loss = 0.37 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:45:21.762092: step 168850, loss = 0.28 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:45:22.631085: step 168860, loss = 0.21 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:45:23.483762: step 168870, loss = 0.28 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:45:24.361634: step 168880, loss = 0.27 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:45:25.250886: step 168890, loss = 0.23 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 06:45:26.210559: step 168900, loss = 0.33 (1333.8 examples/sec; 0.096 sec/batch)
2017-06-02 06:45:26.975616: step 168910, loss = 0.26 (1673.1 examples/sec; 0.077 sec/batch)
2017-06-02 06:45:27.818409: step 168920, loss = 0.26 (1518.8 examples/sec; 0.084 sec/batch)
2017-06-02 06:45:28.670956: step 168930, loss = 0.35 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:45:29.534706: step 168940, loss = 0.32 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:45:30.402090: step 168950, loss = 0.44 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:45:31.253646: step 168960, loss = 0.29 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:45:32.107706: step 168970, loss = 0.33 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:45:32.981976: step 168980, loss = 0.31 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:45:33.829365: step 168990, loss = 0.34 (1510.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:45:34.809794: step 169000, loss = 0.40 (1305.5 examples/sec; 0.098 sec/batch)
2017-06-02 06:45:35.546819: step 169010, loss = 0.24 (1736.7 examples/sec; 0.074 sec/batch)
2017-06-02 06:45:36.401558: step 169020, loss = 0.37 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:45:37.285939: step 169030, loss = 0.27 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:45:38.136786: step 169040, loss = 0.31 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:45:39.004837: step 169050, loss = 0.38 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:45:39.868473: step 169060, loss = 0.29 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:45:40.718489: step 169070, loss = 0.26 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:45:41.584620: step 169080, loss = 0.28 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:45:42.429986: step 169090, loss = 0.37 (1514.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:45:43.376521: step 169100, loss = 0.26 (1352.3 examples/sec; 0.095 sec/batch)
2017-06-02 06:45:44.125581: step 169110, loss = 0.36 (1708.8 examples/sec; 0.075 sec/batch)
2017-06-02 06:45:44.969646: step 169120, loss = 0.32 (1516.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:45:45.824065: step 169130, loss = 0.33 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:45:46.659166: step 169140, loss = 0.26 (1532.8 examples/sec; 0.084 sec/batch)
2017-06-02 06:45:47.524716: step 169150, loss = 0.24 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:45:48.382934: step 169160, loss = 0.24 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:45:49.246139: step 169170, loss = 0.29 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:45:50.115873: step 169180, loss = 0.41 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:45:50.947084: step 169190, loss = 0.35 (1539.9 examples/sec; 0.083 sec/batch)
2017-06-02 06:45:51.908522: step 169200, loss = 0.38 (1331.3 examples/sec; 0.096 sec/batch)
2017-06-02 06:45:52.667169: step 169210, loss = 0.29 (1687.2 examples/sec; 0.076 sec/batch)
2017-06-02 06:45:53.567522: step 169220, loss = 0.37 (1421.7 examples/sec; 0.090 sec/batch)
2017-06-02 06:45:54.404687: step 169230, loss = 0.25 (1529.0 examples/sec; 0.084 sec/batch)
2017-06-02 06:45:55.260915: step 169240, loss = 0.41 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:45:56.146666: step 169250, loss = 0.38 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 06:45:57.021324: step 169260, loss = 0.36 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:45:57.885030: step 169270, loss = 0.29 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:45:58.760516: step 169280, loss = 0.26 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:45:59.622808: step 169290, loss = 0.35 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:46:00.592110: step 169300, loss = 0.36 (1320.5 examples/sec; 0.097 sec/batch)
2017-06-02 06:46:01.356688: step 169310, loss = 0.33 (1674.1 examples/sec; 0.076 sec/batch)
2017-06-02 06:46:02.208944: step 169320, loss = 0.34 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:46:03.069375: step 169330, loss = 0.27 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:46:03.935958: step 169340, loss = 0.37 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:46:04.811509: step 169350, loss = 0.29 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:46:05.668218: step 169360, loss = 0.22 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:46:06.518426: step 169370, loss = 0.31 (1505.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:46:07.383579: step 169380, loss = 0.38 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:46:08.249578: step 169390, loss = 0.33 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:46:09.236353: step 169400, loss = 0.26 (1297.2 examples/sec; 0.099 sec/batch)
2017-06-02 06:46:10.008924: step 169410, loss = 0.30 (1656.8 examples/sec; 0.077 sec/batch)
2017-06-02 06:46:10.844423: step 169420, loss = 0.37 (1532.0 examples/sec; 0.084 sec/batch)
2017-06-02 06:46:11.694567: step 169430, loss = 0.32 (1505.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:46:12.549423: step 169440, loss = 0.29 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:46:13.407943: step 169450, loss = 0.31 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:46:14.276398: step 169460, loss = 0.43 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:46:15.124755: step 169470, loss = 0.27 (1508.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:46:16.001305: step 169480, loss = 0.30 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:46:16.861589: step 169490, loss = 0.39 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:46:17.818152: step 169500, loss = 0.32 (1338.1 examples/sec; 0.096 sec/batch)
2017-06-02 06:46:18.595568: step 169510, loss = 0.36 (1646.5 examples/sec; 0.078 sec/batch)
2017-06-02 06:46:19.444506: step 169520, loss = 0.33 (1507.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:46:20.317227: step 169530, loss = 0.25 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:46:21.147551: step 169540, loss = 0.26 (1541.6 examples/sec; 0.083 sec/batch)
2017-06-02 06:46:22.016371: step 169550, loss = 0.32 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:46:22.870186: step 169560, loss = 0.35 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:46:23.734259: step 169570, loss = 0.30 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:46:24.568704: step 169580, loss = 0.26 (1533.9 examples/sec; 0.083 sec/batch)
2017-06-02 06:46:25.437766: step 169590, loss = 0.30 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:46:26.448045: step 169600, loss = 0.26 (1266.9 examples/sec; 0.101 sec/batch)
2017-06-02 06:46:27.148320: step 169610, loss = 0.23 (1827.9 examples/sec; 0.070 sec/batch)
2017-06-02 06:46:28.005140: step 169620, loss = 0.24 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:46:28.868479: step 169630, loss = 0.29 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:46:29.729944: step 169640, loss = 0.31 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:46:30.586811: step 169650, loss = 0.32 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:46:31.461274: step 169660, loss = 0.30 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:46:32.312935: step 169670, loss = 0.27 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:46:33.194572: step 169680, loss = 0.27 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:46:34.062169: step 169690, loss = 0.37 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:46:35.027854: step 169700, loss = 0.28 (1325.5 examples/sec; 0.097 sec/batch)
2017-06-02 06:46:35.835100: step 169710, loss = 0.41 (1585.6 examples/sec; 0.081 sec/batch)
2017-06-02 06:46:36.716082: step 169720, loss = 0.31 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:46:37.586077: step 169730, loss = 0.35 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:46:38.453823: step 169740, loss = 0.30 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:46:39.308198: step 169750, loss = 0.27 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:46:40.188520: step 169760, loss = 0.29 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:46:41.038233: step 169770, loss = 0.24 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:46:41.904728: step 169780, loss = 0.32 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:46:42.768308: step 169790, loss = 0.28 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:46:43.751700: step 169800, loss = 0.28 (1301.6 examples/sec; 0.098 sec/batch)
2017-06-02 06:46:44.501502: step 169810, loss = 0.24 (1707.2 examples/sec; 0.075 sec/batch)
2017-06-02 06:46:45.369943: step 169820, loss = 0.33 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:46:46.227745: step 169830, loss = 0.24 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:46:47.091541: step 169840, loss = 0.38 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:46:47.975195: step 169850, loss = 0.25 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:46:48.836619: step 169860, loss = 0.31 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:46:49.712816: step 169870, loss = 0.23 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:46:50.594129: step 169880, loss = 0.26 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:46:51.441123: step 169890, loss = 0.31 (1511.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:46:52.436713: step 169900, loss = 0.28 (1285.7 examples/sec; 0.100 sec/batch)
2017-06-02 06:46:53.203706: step 169910, loss = 0.34 (1668.8 examples/sec; 0.077 sec/batch)
2017-06-02 06:46:54.069489: step 169920, loss = 0.32 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:46:54.930587: step 169930, loss = 0.33 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:46:55.794636: step 169940, loss = 0.28 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:46:56.652802: step 169950, loss = 0.32 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:46:57.517373: step 169960, loss = 0.41 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:46:58.376836: step 169970, loss = 0.33 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:46:59.260075: step 169980, loss = 0.36 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:47:00.125536: step 169990, loss = 0.29 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:47:01.073050: step 170000, loss = 0.29 (1350.9 examples/sec; 0.095 sec/batch)
2017-06-02 06:47:01.844866: step 170010, loss = 0.24 (1658.4 examples/sec; 0.077 sec/batch)
2017-06-02 06:47:02.696479: step 170020, loss = 0.27 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:47:03.577075: step 170030, loss = 0.27 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:47:04.454202: step 170040, loss = 0.31 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:47:05.305178: step 170050, loss = 0.29 (1504.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:47:06.177300: step 170060, loss = 0.34 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:47:07.041532: step 170070, loss = 0.31 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:47:07.917321: step 170080, loss = 0.32 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:47:08.791298: step 170090, loss = 0.31 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:47:09.774552: step 170100, loss = 0.37 (1301.8 examples/sec; 0.098 sec/batch)
2017-06-02 06:47:10.559093: step 170110, loss = 0.25 (1631.5 examples/sec; 0.078 sec/batch)
2017-06-02 06:47:11.402980: step 170120, loss = 0.32 (1516.8 examples/sec; 0.084 sec/batch)
2017-06-02 06:47:12.290066: step 170130, loss = 0.33 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 06:47:13.181364: step 170140, loss = 0.29 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 06:47:14.063661: step 170150, loss = 0.34 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:47:14.930634: step 170160, loss = 0.33 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:47:15.810502: step 170170, loss = 0.31 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:47:16.693224: step 170180, loss = 0.30 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:47:17.578801: step 170190, loss = 0.33 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 06:47:18.549676: step 170200, loss = 0.29 (1318.4 examples/sec; 0.097 sec/batch)
2017-06-02 06:47:19.341746: step 170210, loss = 0.34 (1616.0 examples/sec; 0.079 sec/batch)
2017-06-02 06:47:20.208831: step 170220, loss = 0.33 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:47:21.064856: step 170230, loss = 0.38 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:47:21.929845: step 170240, loss = 0.28 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:47:22.792869: step 170250, loss = 0.27 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:47:23.655161: step 170260, loss = 0.32 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:47:24.502584: step 170270, loss = 0.31 (1510.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:47:25.395037: step 170280, loss = 0.26 (1434.2 examples/sec; 0.089 sec/batch)
2017-06-02 06:47:26.271067: step 170290, loss = 0.39 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:47:27.240351: step 170300, loss = 0.35 (1320.6 examples/sec; 0.097 sec/batch)
2017-06-02 06:47:28.011565: step 170310, loss = 0.21 (1659.7 examples/sec; 0.077 sec/batch)
2017-06-02 06:47:28.900999: step 170320, loss = 0.29 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 06:47:29.771146: step 170330, loss = 0.23 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:47:30.625672: step 170340, loss = 0.37 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:47:31.501517: step 170350, loss = 0.35 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:47:32.378479: step 170360, loss = 0.25 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:47:33.249014: step 170370, loss = 0.27 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:47:34.116249: step 170380, loss = 0.25 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:47:34.991008: step 170390, loss = 0.27 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:47:35.933348: step 170400, loss = 0.25 (1358.3 examples/sec; 0.094 sec/batch)
2017-06-02 06:47:36.706638: step 170410, loss = 0.28 (1655.3 examples/sec; 0.077 sec/batch)
2017-06-02 06:47:37.617969: step 170420, loss = 0.26 (1404.5 examples/sec; 0.091 sec/batch)
2017-06-02 06:47:38.491199: step 170430, loss = 0.36 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:47:39.372367: step 170440, loss = 0.37 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:47:40.217671: step 170450, loss = 0.29 (1514.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:47:41.088959: step 170460, loss = 0.31 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:47:41.953174: step 170470, loss = 0.28 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:47:42.798798: step 170480, loss = 0.26 (1513.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:47:43.674441: step 170490, loss = 0.31 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:47:44.633748: step 170500, loss = 0.33 (1334.3 examples/sec; 0.096 sec/batch)
2017-06-02 06:47:45.410661: step 170510, loss = 0.28 (1647.5 examples/sec; 0.078 sec/batch)
2017-06-02 06:47:46.290747: step 170520, loss = 0.42 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:47:47.166911: step 170530, loss = 0.32 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:47:48.032043: step 170540, loss = 0.23 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:47:48.899984: step 170550, loss = 0.31 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:47:49.769419: step 170560, loss = 0.34 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:47:50.629611: step 170570, loss = 0.26 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:47:51.498474: step 170580, loss = 0.32 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:47:52.322834: step 170590, loss = 0.28 (1552.7 examples/sec; 0.082 sec/batch)
2017-06-02 06:47:53.322543: step 170600, loss = 0.23 (1280.4 examples/sec; 0.100 sec/batch)
2017-06-02 06:47:54.052941: step 170610, loss = 0.42 (1752.5 examples/sec; 0.073 sec/batch)
2017-06-02 06:47:54.886482: step 170620, loss = 0.29 (1535.6 examples/sec; 0.083 sec/batch)
2017-06-02 06:47:55.739100: step 170630, loss = 0.29 (1501.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:47:56.584267: step 170640, loss = 0.30 (1514.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:47:57.458198: step 170650, loss = 0.33 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:47:58.338906: step 170660, loss = 0.40 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:47:59.205990: step 170670, loss = 0.24 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:48:00.086239: step 170680, loss = 0.29 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:48:00.944494: step 170690, loss = 0.38 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:48:01.881519: step 170700, loss = 0.38 (1366.0 examples/sec; 0.094 sec/batch)
2017-06-02 06:48:02.631304: step 170710, loss = 0.37 (1707.2 examples/sec; 0.075 sec/batch)
2017-06-02 06:48:03.502654: step 170720, loss = 0.31 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:48:04.366270: step 170730, loss = 0.28 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:48:05.209861: step 170740, loss = 0.29 (1517.3 examples/sec; 0.084 sec/batch)
2017-06-02 06:48:06.068090: step 170750, loss = 0.37 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:48:06.934807: step 170760, loss = 0.31 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:48:07.783634: step 170770, loss = 0.43 (1508.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:48:08.647846: step 170780, loss = 0.30 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:48:09.493626: step 170790, loss = 0.27 (1513.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:48:10.454543: step 170800, loss = 0.26 (1332.1 examples/sec; 0.096 sec/batch)
2017-06-02 06:48:11.215684: step 170810, loss = 0.27 (1681.7 examples/sec; 0.076 sec/batch)
2017-06-02 06:48:12.103470: step 170820, loss = 0.30 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 06:48:12.956279: step 170830, loss = 0.34 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:48:13.832578: step 170840, loss = 0.20 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:48:14.702787: step 170850, loss = 0.31 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:48:15.579963: step 170860, loss = 0.37 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:48:16.426860: step 170870, loss = 0.42 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:48:17.301875: step 170880, loss = 0.43 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:48:18.157776: step 170890, loss = 0.26 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:48:19.117132: step 170900, loss = 0.32 (1334.2 examples/sec; 0.096 sec/batch)
2017-06-02 06:48:19.878645: step 170910, loss = 0.30 (1680.9 examples/sec; 0.076 sec/batch)
2017-06-02 06:48:20.761308: step 170920, loss = 0.39 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:48:21.630874: step 170930, loss = 0.31 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:48:22.485045: step 170940, loss = 0.30 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:48:23.333064: step 170950, loss = 0.33 (1509.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:48:24.187420: step 170960, loss = 0.31 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:48:25.068190: step 170970, loss = 0.38 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:48:25.938517: step 170980, loss = 0.29 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:48:26.796755: step 170990, loss = 0.35 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:48:27.819144: step 171000, loss = 0.30 (1251.9 examples/sec; 0.102 sec/batch)
2017-06-02 06:48:28.561329: step 171010, loss = 0.28 (1724.6 examples/sec; 0.074 sec/batch)
2017-06-02 06:48:29.433853: step 171020, loss = 0.36 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:48:30.316288: step 171030, loss = 0.29 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:48:31.184923: step 171040, loss = 0.28 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:48:32.073056: step 171050, loss = 0.31 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 06:48:32.968628: step 171060, loss = 0.31 (1429.3 examples/sec; 0.090 sec/batch)
2017-06-02 06:48:33.838899: step 171070, loss = 0.32 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:48:34.698315: step 171080, loss = 0.26 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:48:35.551198: step 171090, loss = 0.23 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:48:36.536921: step 171100, loss = 0.32 (1298.5 examples/sec; 0.099 sec/batch)
2017-06-02 06:48:37.326173: step 171110, loss = 0.33 (1621.8 examples/sec; 0.079 sec/batch)
2017-06-02 06:48:38.195489: step 171120, loss = 0.32 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:48:39.081162: step 171130, loss = 0.32 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 06:48:39.950747: step 171140, loss = 0.37 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:48:40.807484: step 171150, loss = 0.23 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:48:41.659812: step 171160, loss = 0.33 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:48:42.529757: step 171170, loss = 0.25 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:48:43.395326: step 171180, loss = 0.28 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:48:44.258233: step 171190, loss = 0.27 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:48:45.202213: step 171200, loss = 0.28 (1356.0 examples/sec; 0.094 sec/batch)
2017-06-02 06:48:45.970454: step 171210, loss = 0.36 (1666.1 examples/sec; 0.077 sec/batch)
2017-06-02 06:48:46.812105: step 171220, loss = 0.28 (1520.8 examples/sec; 0.084 sec/batch)
2017-06-02 06:48:47.660922: step 171230, loss = 0.27 (1508.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:48:48.537890: step 171240, loss = 0.41 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:48:49.392415: step 171250, loss = 0.23 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:48:50.225027: step 171260, loss = 0.35 (1537.3 examples/sec; 0.083 sec/batch)
2017-06-02 06:48:51.073642: step 171270, loss = 0.35 (1508.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:48:51.919737: step 171280, loss = 0.29 (1512.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:48:52.791474: step 171290, loss = 0.25 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:48:53.769702: step 171300, loss = 0.27 (1308.5 examples/sec; 0.098 sec/batch)
2017-06-02 06:48:54.521049: step 171310, loss = 0.30 (1703.6 examples/sec; 0.075 sec/batch)
2017-06-02 06:48:55.394844: step 171320, loss = 0.28 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:48:56.269809: step 171330, loss = 0.36 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:48:57.120480: step 171340, loss = 0.28 (1504.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:48:57.973914: step 171350, loss = 0.30 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:48:58.834304: step 171360, loss = 0.29 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:48:59.678137: step 171370, loss = 0.29 (1516.9 examples/sec; 0.084 sec/batch)
2017-06-02 06:49:00.521694: step 171380, loss = 0.53 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:49:01.381684: step 171390, loss = 0.31 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:49:02.341520: step 171400, loss = 0.34 (1333.5 examples/sec; 0.096 sec/batch)
2017-06-02 06:49:03.104427: step 171410, loss = 0.32 (1677.8 examples/sec; 0.076 sec/batch)
2017-06-02 06:49:03.974674: step 171420, loss = 0.28 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:49:04.840092: step 171430, loss = 0.36 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:49:05.691440: step 171440, loss = 0.28 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:49:06.566610: step 171450, loss = 0.35 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:49:07.406344: step 171460, loss = 0.25 (1524.3 examples/sec; 0.084 sec/batch)
2017-06-02 06:49:08.267494: step 171470, loss = 0.40 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:49:09.101641: step 171480, loss = 0.29 (1534.5 examples/sec; 0.083 sec/batch)
2017-06-02 06:49:09.950785: step 171490, loss = 0.28 (1507.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:49:10.905246: step 171500, loss = 0.39 (1341.1 examples/sec; 0.095 sec/batch)
2017-06-02 06:49:11.652087: step 171510, loss = 0.44 (1713.9 examples/sec; 0.075 sec/batch)
2017-06-02 06:49:12.518884: step 171520, loss = 0.30 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:49:13.334535: step 171530, loss = 0.29 (1569.3 examples/sec; 0.082 sec/batch)
2017-06-02 06:49:14.183372: step 171540, loss = 0.25 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:49:15.045788: step 171550, loss = 0.25 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:49:15.907603: step 171560, loss = 0.31 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:49:16.741165: step 171570, loss = 0.38 (1535.6 examples/sec; 0.083 sec/batch)
2017-06-02 06:49:17.597217: step 171580, loss = 0.24 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:49:18.439259: step 171590, loss = 0.25 (1520.1 examples/sec; 0.084 sec/batch)
2017-06-02 06:49:19.411240: step 171600, loss = 0.35 (1316.9 examples/sec; 0.097 sec/batch)
2017-06-02 06:49:20.174940: step 171610, loss = 0.31 (1676.0 examples/sec; 0.076 sec/batch)
2017-06-02 06:49:21.036015: step 171620, loss = 0.28 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:49:21.894917: step 171630, loss = 0.23 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:49:22.754759: step 171640, loss = 0.36 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:49:23.611327: step 171650, loss = 0.25 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:49:24.470971: step 171660, loss = 0.35 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:49:25.327698: step 171670, loss = 0.29 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:49:26.187316: step 171680, loss = 0.28 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:49:27.037424: step 171690, loss = 0.32 (1505.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:49:27.973467: step 171700, loss = 0.27 (1367.5 examples/sec; 0.094 sec/batch)
2017-06-02 06:49:28.750603: step 171710, loss = 0.30 (1647.1 examples/sec; 0.078 sec/batch)
2017-06-02 06:49:29.620889: step 171720, loss = 0.24 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:49:30.493607: step 171730, loss = 0.33 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:49:31.357542: step 171740, loss = 0.22 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:49:32.217039: step 171750, loss = 0.28 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:49:33.081525: step 171760, loss = 0.24 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:49:33.919128: step 171770, loss = 0.31 (1528.2 examples/sec; 0.084 sec/batch)
2017-06-02 06:49:34.767975: step 171780, loss = 0.26 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:49:35.629236: step 171790, loss = 0.30 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:49:36.576802: step 171800, loss = 0.32 (1350.8 examples/sec; 0.095 sec/batch)
2017-06-02 06:49:37.361607: step 171810, loss = 0.30 (1631.0 examples/sec; 0.078 sec/batch)
2017-06-02 06:49:38.221982: step 171820, loss = 0.34 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:49:39.094634: step 171830, loss = 0.28 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:49:39.979358: step 171840, loss = 0.38 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:49:40.861611: step 171850, loss = 0.26 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:49:41.740200: step 171860, loss = 0.36 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:49:42.602791: step 171870, loss = 0.32 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:49:43.460225: step 171880, loss = 0.20 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:49:44.306131: step 171890, loss = 0.28 (1513.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:49:45.292157: step 171900, loss = 0.30 (1298.1 examples/sec; 0.099 sec/batch)
2017-06-02 06:49:46.036245: step 171910, loss = 0.24 (1720.2 examples/sec; 0.074 sec/batch)
2017-06-02 06:49:46.906393: step 171920, loss = 0.37 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:49:47.772298: step 171930, loss = 0.35 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:49:48.626845: step 171940, loss = 0.30 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:49:49.477723: step 171950, loss = 0.27 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:49:50.339010: step 171960, loss = 0.41 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:49:51.210007: step 171970, loss = 0.26 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:49:52.155485: step 171980, loss = 0.22 (1353.8 examples/sec; 0.095 sec/batch)
2017-06-02 06:49:53.029333: step 171990, loss = 0.32 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:49:54.004353: step 172000, loss = 0.28 (1312.8 examples/sec; 0.098 sec/batch)
2017-06-02 06:49:54.769123: step 172010, loss = 0.43 (1673.7 examples/sec; 0.076 sec/batch)
2017-06-02 06:49:55.626910: step 172020, loss = 0.26 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:49:56.496424: step 172030, loss = 0.33 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:49:57.335860: step 172040, loss = 0.33 (1524.8 examples/sec; 0.084 sec/batch)
2017-06-02 06:49:58.202854: step 172050, loss = 0.21 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:49:59.077053: step 172060, loss = 0.24 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:49:59.932474: step 172070, loss = 0.32 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:50:00.804695: step 172080, loss = 0.35 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:50:01.680834: step 172090, loss = 0.28 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:50:02.626134: step 172100, loss = 0.30 (1354.1 examples/sec; 0.095 sec/batch)
2017-06-02 06:50:03.387537: step 172110, loss = 0.36 (1681.1 examples/sec; 0.076 sec/batch)
2017-06-02 06:50:04.258197: step 172120, loss = 0.36 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:50:05.112757: step 172130, loss = 0.27 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:50:05.973152: step 172140, loss = 0.23 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:50:06.834600: step 172150, loss = 0.21 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:50:07.685407: step 172160, loss = 0.26 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:50:08.532888: step 172170, loss = 0.22 (1510.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:50:09.394617: step 172180, loss = 0.30 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:50:10.250154: step 172190, loss = 0.32 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:50:11.228952: step 172200, loss = 0.33 (1307.7 examples/sec; 0.098 sec/batch)
2017-06-02 06:50:11.966033: step 172210, loss = 0.24 (1736.6 examples/sec; 0.074 sec/batch)
2017-06-02 06:50:12.812806: step 172220, loss = 0.29 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:50:13.646777: step 172230, loss = 0.28 (1534.8 examples/sec; 0.083 sec/batch)
2017-06-02 06:50:14.513117: step 172240, loss = 0.32 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:50:15.387684: step 172250, loss = 0.28 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:50:16.216214: step 172260, loss = 0.33 (1544.9 examples/sec; 0.083 sec/batch)
2017-06-02 06:50:17.071894: step 172270, loss = 0.28 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:50:17.934663: step 172280, loss = 0.31 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:50:18.807749: step 172290, loss = 0.38 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:50:19.776081: step 172300, loss = 0.38 (1321.9 examples/sec; 0.097 sec/batch)
2017-06-02 06:50:20.551760: step 172310, loss = 0.28 (1650.2 examples/sec; 0.078 sec/batch)
2017-06-02 06:50:21.420690: step 172320, loss = 0.28 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:50:22.284415: step 172330, loss = 0.30 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:50:23.152899: step 172340, loss = 0.26 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:50:24.019475: step 172350, loss = 0.31 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:50:24.882584: step 172360, loss = 0.28 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:50:25.748634: step 172370, loss = 0.34 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:50:26.609680: step 172380, loss = 0.30 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:50:27.475451: step 172390, loss = 0.26 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:50:28.496277: step 172400, loss = 0.27 (1253.9 examples/sec; 0.102 sec/batch)
2017-06-02 06:50:29.205883: step 172410, loss = 0.37 (1803.8 examples/sec; 0.071 sec/batch)
2017-06-02 06:50:30.075789: step 172420, loss = 0.27 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:50:30.909922: step 172430, loss = 0.29 (1534.5 examples/sec; 0.083 sec/batch)
2017-06-02 06:50:31.791091: step 172440, loss = 0.30 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:50:32.642052: step 172450, loss = 0.33 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:50:33.487295: step 172460, loss = 0.25 (1514.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:50:34.313273: step 172470, loss = 0.28 (1549.7 examples/sec; 0.083 sec/batch)
2017-06-02 06:50:35.176565: step 172480, loss = 0.25 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:50:36.059832: step 172490, loss = 0.28 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:50:37.030927: step 172500, loss = 0.40 (1318.1 examples/sec; 0.097 sec/batch)
2017-06-02 06:50:37.776375: step 172510, loss = 0.26 (1717.1 examples/sec; 0.075 sec/batch)
2017-06-02 06:50:38.621252: step 172520, loss = 0.35 (1515.0 examples/sec; 0.084 sec/batch)
2017-06-02 06:50:39.480952: step 172530, loss = 0.27 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:50:40.347166: step 172540, loss = 0.33 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:50:41.221756: step 172550, loss = 0.32 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:50:42.082650: step 172560, loss = 0.30 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:50:42.942015: step 172570, loss = 0.26 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:50:43.803654: step 172580, loss = 0.32 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:50:44.665665: step 172590, loss = 0.33 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:50:45.631562: step 172600, loss = 0.32 (1325.2 examples/sec; 0.097 sec/batch)
2017-06-02 06:50:46.406818: step 172610, loss = 0.20 (1651.1 examples/sec; 0.078 sec/batch)
2017-06-02 06:50:47.261819: step 172620, loss = 0.29 (1497.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:50:48.133086: step 172630, loss = 0.32 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:50:48.999684: step 172640, loss = 0.34 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:50:49.868274: step 172650, loss = 0.27 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:50:50.719190: step 172660, loss = 0.31 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:50:51.590401: step 172670, loss = 0.29 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:50:52.436194: step 172680, loss = 0.25 (1513.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:50:53.307356: step 172690, loss = 0.33 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:50:54.271887: step 172700, loss = 0.30 (1327.0 examples/sec; 0.096 sec/batch)
2017-06-02 06:50:55.018374: step 172710, loss = 0.35 (1714.7 examples/sec; 0.075 sec/batch)
2017-06-02 06:50:55.883960: step 172720, loss = 0.22 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:50:56.718525: step 172730, loss = 0.28 (1533.7 examples/sec; 0.083 sec/batch)
2017-06-02 06:50:57.591197: step 172740, loss = 0.23 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:50:58.448302: step 172750, loss = 0.30 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:50:59.319753: step 172760, loss = 0.35 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:51:00.198718: step 172770, loss = 0.35 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:51:01.056304: step 172780, loss = 0.30 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:51:01.916741: step 172790, loss = 0.29 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:51:02.925741: step 172800, loss = 0.23 (1268.6 examples/sec; 0.101 sec/batch)
2017-06-02 06:51:03.648967: step 172810, loss = 0.21 (1769.8 examples/sec; 0.072 sec/batch)
2017-06-02 06:51:04.472187: step 172820, loss = 0.32 (1554.9 examples/sec; 0.082 sec/batch)
2017-06-02 06:51:05.344629: step 172830, loss = 0.32 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:51:06.213229: step 172840, loss = 0.33 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:51:07.099824: step 172850, loss = 0.32 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 06:51:07.961909: step 172860, loss = 0.35 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:51:08.825033: step 172870, loss = 0.24 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:51:09.692200: step 172880, loss = 0.39 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:51:10.568783: step 172890, loss = 0.30 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:51:11.574044: step 172900, loss = 0.31 (1273.3 examples/sec; 0.101 sec/batch)
2017-06-02 06:51:12.346142: step 172910, loss = 0.30 (1657.8 examples/sec; 0.077 sec/batch)
2017-06-02 06:51:13.197507: step 172920, loss = 0.26 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:51:14.048990: step 172930, loss = 0.33 (1503.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:51:14.903794: step 172940, loss = 0.28 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:51:15.763468: step 172950, loss = 0.35 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:51:16.618668: step 172960, loss = 0.33 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:51:17.462215: step 172970, loss = 0.21 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:51:18.338775: step 172980, loss = 0.26 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:51:19.214815: step 172990, loss = 0.37 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:51:20.187235: step 173000, loss = 0.24 (1316.3 examples/sec; 0.097 sec/batch)
2017-06-02 06:51:20.956891: step 173010, loss = 0.33 (1663.1 examples/sec; 0.077 sec/batch)
2017-06-02 06:51:21.792282: step 173020, loss = 0.30 (1532.2 examples/sec; 0.084 sec/batch)
2017-06-02 06:51:22.679619: step 173030, loss = 0.35 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 06:51:23.542918: step 173040, loss = 0.30 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:51:24.420571: step 173050, loss = 0.30 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:51:25.290707: step 173060, loss = 0.37 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:51:26.145189: step 173070, loss = 0.27 (1498.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:51:27.006803: step 173080, loss = 0.32 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:51:27.880679: step 173090, loss = 0.33 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:51:28.893011: step 173100, loss = 0.27 (1264.4 examples/sec; 0.101 sec/batch)
2017-06-02 06:51:29.618655: step 173110, loss = 0.31 (1763.9 examples/sec; 0.073 sec/batch)
2017-06-02 06:51:30.508238: step 173120, loss = 0.32 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 06:51:31.380857: step 173130, loss = 0.30 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:51:32.211936: step 173140, loss = 0.32 (1540.2 examples/sec; 0.083 sec/batch)
2017-06-02 06:51:33.084020: step 173150, loss = 0.31 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:51:33.934997: step 173160, loss = 0.26 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:51:34.793214: step 173170, loss = 0.25 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:51:35.654874: step 173180, loss = 0.23 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:51:36.527424: step 173190, loss = 0.31 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:51:37.525547: step 173200, loss = 0.29 (1282.4 examples/sec; 0.100 sec/batch)
2017-06-02 06:51:38.281473: step 173210, loss = 0.38 (1693.3 examples/sec; 0.076 sec/batch)
2017-06-02 06:51:39.144327: step 173220, loss = 0.29 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:51:40.023428: step 173230, loss = 0.31 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:51:40.874218: step 173240, loss = 0.38 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:51:41.728089: step 173250, loss = 0.30 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:51:42.585609: step 173260, loss = 0.30 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:51:43.454937: step 173270, loss = 0.28 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:51:44.322756: step 173280, loss = 0.33 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:51:45.185481: step 173290, loss = 0.32 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:51:46.136373: step 173300, loss = 0.29 (1346.1 examples/sec; 0.095 sec/batch)
2017-06-02 06:51:46.887798: step 173310, loss = 0.31 (1703.4 examples/sec; 0.075 sec/batch)
2017-06-02 06:51:47.769098: step 173320, loss = 0.32 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:51:48.608632: step 173330, loss = 0.32 (1524.6 examples/sec; 0.084 sec/batch)
2017-06-02 06:51:49.474383: step 173340, loss = 0.34 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:51:50.349913: step 173350, loss = 0.35 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:51:51.213172: step 173360, loss = 0.25 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:51:52.084638: step 173370, loss = 0.27 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:51:52.911401: step 173380, loss = 0.27 (1548.2 examples/sec; 0.083 sec/batch)
2017-06-02 06:51:53.805229: step 173390, loss = 0.29 (1432.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:51:54.745763: step 173400, loss = 0.24 (1361.0 examples/sec; 0.094 sec/batch)
2017-06-02 06:51:55.510618: step 173410, loss = 0.30 (1673.5 examples/sec; 0.076 sec/batch)
2017-06-02 06:51:56.385130: step 173420, loss = 0.29 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:51:57.285389: step 173430, loss = 0.36 (1421.8 examples/sec; 0.090 sec/batch)
2017-06-02 06:51:58.166384: step 173440, loss = 0.27 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:51:59.033003: step 173450, loss = 0.31 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:51:59.897539: step 173460, loss = 0.24 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:52:00.758405: step 173470, loss = 0.30 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:52:01.624190: step 173480, loss = 0.37 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:52:02.483977: step 173490, loss = 0.29 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:52:03.485479: step 173500, loss = 0.25 (1278.1 examples/sec; 0.100 sec/batch)
2017-06-02 06:52:04.244214: step 173510, loss = 0.25 (1687.0 examples/sec; 0.076 sec/batch)
2017-06-02 06:52:05.113009: step 173520, loss = 0.33 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:52:05.994721: step 173530, loss = 0.38 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:52:06.866129: step 173540, loss = 0.29 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:52:07.758654: step 173550, loss = 0.24 (1434.1 examples/sec; 0.089 sec/batch)
2017-06-02 06:52:08.643748: step 173560, loss = 0.31 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 06:52:09.528643: step 173570, loss = 0.31 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:52:10.413295: step 173580, loss = 0.40 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:52:11.291149: step 173590, loss = 0.27 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:52:12.260173: step 173600, loss = 0.34 (1320.9 examples/sec; 0.097 sec/batch)
2017-06-02 06:52:13.033968: step 173610, loss = 0.24 (1654.2 examples/sec; 0.077 sec/batch)
2017-06-02 06:52:13.901719: step 173620, loss = 0.26 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:52:14.771371: step 173630, loss = 0.30 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:52:15.649592: step 173640, loss = 0.23 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:52:16.525835: step 173650, loss = 0.31 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:52:17.394711: step 173660, loss = 0.24 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:52:18.260450: step 173670, loss = 0.41 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:52:19.120993: step 173680, loss = 0.29 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:52:19.976094: step 173690, loss = 0.33 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:52:20.988674: step 173700, loss = 0.32 (1264.1 examples/sec; 0.101 sec/batch)
2017-06-02 06:52:21.718562: step 173710, loss = 0.28 (1753.7 examples/sec; 0.073 sec/batch)
2017-06-02 06:52:22.571097: step 173720, loss = 0.39 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:52:23.435109: step 173730, loss = 0.29 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:52:24.292542: step 173740, loss = 0.35 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:52:25.123388: step 173750, loss = 0.33 (1540.6 examples/sec; 0.083 sec/batch)
2017-06-02 06:52:26.012533: step 173760, loss = 0.30 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 06:52:26.895753: step 173770, loss = 0.29 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:52:27.802682: step 173780, loss = 0.26 (1411.3 examples/sec; 0.091 sec/batch)
2017-06-02 06:52:28.653214: step 173790, loss = 0.27 (1505.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:52:29.606316: step 173800, loss = 0.29 (1343.0 examples/sec; 0.095 sec/batch)
2017-06-02 06:52:30.381638: step 173810, loss = 0.28 (1650.9 examples/sec; 0.078 sec/batch)
2017-06-02 06:52:31.231645: step 173820, loss = 0.34 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:52:32.071333: step 173830, loss = 0.22 (1524.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:52:32.952037: step 173840, loss = 0.29 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:52:33.828718: step 173850, loss = 0.33 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:52:34.683553: step 173860, loss = 0.36 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:52:35.532049: step 173870, loss = 0.31 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:52:36.414678: step 173880, loss = 0.32 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:52:37.297328: step 173890, loss = 0.22 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:52:38.253271: step 173900, loss = 0.30 (1339.0 examples/sec; 0.096 sec/batch)
2017-06-02 06:52:39.043105: step 173910, loss = 0.22 (1620.6 examples/sec; 0.079 sec/batch)
2017-06-02 06:52:39.909269: step 173920, loss = 0.28 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:52:40.773802: step 173930, loss = 0.29 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:52:41.641290: step 173940, loss = 0.32 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:52:42.482327: step 173950, loss = 0.27 (1521.9 examples/sec; 0.084 sec/batch)
2017-06-02 06:52:43.333625: step 173960, loss = 0.24 (1503.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:52:44.203023: step 173970, loss = 0.33 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:52:45.070047: step 173980, loss = 0.30 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:52:45.929742: step 173990, loss = 0.28 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:52:46.877592: step 174000, loss = 0.25 (1350.4 examples/sec; 0.095 sec/batch)
2017-06-02 06:52:47.650493: step 174010, loss = 0.25 (1656.1 examples/sec; 0.077 sec/batch)
2017-06-02 06:52:48.512798: step 174020, loss = 0.43 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:52:49.366699: step 174030, loss = 0.23 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:52:50.238810: step 174040, loss = 0.26 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:52:51.116522: step 174050, loss = 0.29 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:52:51.983160: step 174060, loss = 0.39 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:52:52.834781: step 174070, loss = 0.45 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:52:53.704877: step 174080, loss = 0.32 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:52:54.562222: step 174090, loss = 0.23 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:52:55.535233: step 174100, loss = 0.31 (1315.5 examples/sec; 0.097 sec/batch)
2017-06-02 06:52:56.290687: step 174110, loss = 0.27 (1694.4 examples/sec; 0.076 sec/batch)
2017-06-02 06:52:57.156111: step 174120, loss = 0.30 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:52:58.012087: step 174130, loss = 0.29 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:52:58.900258: step 174140, loss = 0.26 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 06:52:59.783329: step 174150, loss = 0.31 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:53:00.638781: step 174160, loss = 0.34 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:53:01.517981: step 174170, loss = 0.31 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:53:02.356698: step 174180, loss = 0.28 (1526.2 examples/sec; 0.084 sec/batch)
2017-06-02 06:53:03.211780: step 174190, loss = 0.25 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:53:04.165421: step 174200, loss = 0.30 (1342.2 examples/sec; 0.095 sec/batch)
2017-06-02 06:53:04.934291: step 174210, loss = 0.30 (1664.8 examples/sec; 0.077 sec/batch)
2017-06-02 06:53:05.797674: step 174220, loss = 0.31 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:53:06.640647: step 174230, loss = 0.34 (1518.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:53:07.501836: step 174240, loss = 0.27 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:53:08.383357: step 174250, loss = 0.35 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:53:09.243975: step 174260, loss = 0.34 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:53:10.093995: step 174270, loss = 0.27 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:53:10.936667: step 174280, loss = 0.37 (1519.0 examples/sec; 0.084 sec/batch)
2017-06-02 06:53:11.782299: step 174290, loss = 0.30 (1513.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:53:12.721370: step 174300, loss = 0.29 (1363.0 examples/sec; 0.094 sec/batch)
2017-06-02 06:53:13.480515: step 174310, loss = 0.33 (1686.1 examples/sec; 0.076 sec/batch)
2017-06-02 06:53:14.334167: step 174320, loss = 0.27 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:53:15.215849: step 174330, loss = 0.22 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:53:16.074925: step 174340, loss = 0.28 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:53:16.935584: step 174350, loss = 0.31 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:53:17.812019: step 174360, loss = 0.32 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:53:18.708851: step 174370, loss = 0.29 (1427.2 examples/sec; 0.090 sec/batch)
2017-06-02 06:53:19.564117: step 174380, loss = 0.37 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:53:20.421946: step 174390, loss = 0.26 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:53:21.363829: step 174400, loss = 0.27 (1359.0 examples/sec; 0.094 sec/batch)
2017-06-02 06:53:22.128362: step 174410, loss = 0.26 (1674.2 examples/sec; 0.076 sec/batch)
2017-06-02 06:53:22.986486: step 174420, loss = 0.35 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:53:23.851145: step 174430, loss = 0.34 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:53:24.695374: step 174440, loss = 0.38 (1516.2 examples/sec; 0.084 sec/batch)
2017-06-02 06:53:25.572383: step 174450, loss = 0.22 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:53:26.428826: step 174460, loss = 0.30 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:53:27.298378: step 174470, loss = 0.25 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:53:28.137117: step 174480, loss = 0.30 (1526.1 examples/sec; 0.084 sec/batch)
2017-06-02 06:53:28.999816: step 174490, loss = 0.30 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:53:29.939567: step 174500, loss = 0.28 (1362.1 examples/sec; 0.094 sec/batch)
2017-06-02 06:53:30.687566: step 174510, loss = 0.32 (1711.2 examples/sec; 0.075 sec/batch)
2017-06-02 06:53:31.559515: step 174520, loss = 0.31 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:53:32.437867: step 174530, loss = 0.37 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:53:33.306437: step 174540, loss = 0.27 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:53:34.163036: step 174550, loss = 0.32 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:53:35.026027: step 174560, loss = 0.30 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:53:35.863255: step 174570, loss = 0.31 (1528.9 examples/sec; 0.084 sec/batch)
2017-06-02 06:53:36.696405: step 174580, loss = 0.35 (1536.3 examples/sec; 0.083 sec/batch)
2017-06-02 06:53:37.539432: step 174590, loss = 0.32 (1518.3 examples/sec; 0.084 sec/batch)
2017-06-02 06:53:38.496849: step 174600, loss = 0.27 (1336.9 examples/sec; 0.096 sec/batch)
2017-06-02 06:53:39.264890: step 174610, loss = 0.37 (1666.6 examples/sec; 0.077 sec/batch)
2017-06-02 06:53:40.123859: step 174620, loss = 0.36 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:53:40.957498: step 174630, loss = 0.36 (1535.4 examples/sec; 0.083 sec/batch)
2017-06-02 06:53:41.832579: step 174640, loss = 0.42 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:53:42.688526: step 174650, loss = 0.35 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:53:43.544497: step 174660, loss = 0.36 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:53:44.419800: step 174670, loss = 0.32 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:53:45.281874: step 174680, loss = 0.20 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:53:46.145930: step 174690, loss = 0.20 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:53:47.106148: step 174700, loss = 0.33 (1333.0 examples/sec; 0.096 sec/batch)
2017-06-02 06:53:47.875081: step 174710, loss = 0.30 (1664.7 examples/sec; 0.077 sec/batch)
2017-06-02 06:53:48.734083: step 174720, loss = 0.26 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:53:49.599359: step 174730, loss = 0.32 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:53:50.432885: step 174740, loss = 0.24 (1535.6 examples/sec; 0.083 sec/batch)
2017-06-02 06:53:51.291589: step 174750, loss = 0.26 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:53:52.154478: step 174760, loss = 0.31 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:53:53.006010: step 174770, loss = 0.23 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:53:53.873897: step 174780, loss = 0.31 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:53:54.717850: step 174790, loss = 0.26 (1516.6 examples/sec; 0.084 sec/batch)
2017-06-02 06:53:55.688287: step 174800, loss = 0.28 (1319.0 examples/sec; 0.097 sec/batch)
2017-06-02 06:53:56.436174: step 174810, loss = 0.28 (1711.5 examples/sec; 0.075 sec/batch)
2017-06-02 06:53:57.273709: step 174820, loss = 0.32 (1528.3 examples/sec; 0.084 sec/batch)
2017-06-02 06:53:58.114833: step 174830, loss = 0.31 (1521.8 examples/sec; 0.084 sec/batch)
2017-06-02 06:53:59.004648: step 174840, loss = 0.25 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 06:53:59.851387: step 174850, loss = 0.33 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:54:00.691096: step 174860, loss = 0.35 (1524.3 examples/sec; 0.084 sec/batch)
2017-06-02 06:54:01.542496: step 174870, loss = 0.37 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:54:02.394837: step 174880, loss = 0.24 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:54:03.238456: step 174890, loss = 0.39 (1517.3 examples/sec; 0.084 sec/batch)
2017-06-02 06:54:04.190607: step 174900, loss = 0.35 (1344.3 examples/sec; 0.095 sec/batch)
2017-06-02 06:54:04.937983: step 174910, loss = 0.24 (1712.7 examples/sec; 0.075 sec/batch)
2017-06-02 06:54:05.815364: step 174920, loss = 0.26 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:54:06.670007: step 174930, loss = 0.28 (1497.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:54:07.547459: step 174940, loss = 0.23 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:54:08.409758: step 174950, loss = 0.30 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:54:09.264786: step 174960, loss = 0.30 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:54:10.126510: step 174970, loss = 0.23 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:54:10.995561: step 174980, loss = 0.35 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:54:11.843983: step 174990, loss = 0.32 (1508.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:54:12.817487: step 175000, loss = 0.32 (1314.8 examples/sec; 0.097 sec/batch)
2017-06-02 06:54:13.549308: step 175010, loss = 0.34 (1749.1 examples/sec; 0.073 sec/batch)
2017-06-02 06:54:14.422454: step 175020, loss = 0.40 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:54:15.281570: step 175030, loss = 0.37 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:54:16.157800: step 175040, loss = 0.33 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:54:17.019697: step 175050, loss = 0.26 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:54:17.898760: step 175060, loss = 0.20 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:54:18.750001: step 175070, loss = 0.28 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:54:19.591187: step 175080, loss = 0.33 (1521.7 examples/sec; 0.084 sec/batch)
2017-06-02 06:54:20.445039: step 175090, loss = 0.30 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:54:21.403019: step 175100, loss = 0.26 (1336.1 examples/sec; 0.096 sec/batch)
2017-06-02 06:54:22.176435: step 175110, loss = 0.29 (1655.0 examples/sec; 0.077 sec/batch)
2017-06-02 06:54:23.038022: step 175120, loss = 0.35 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:54:23.905409: step 175130, loss = 0.35 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:54:24.765656: step 175140, loss = 0.42 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:54:25.630136: step 175150, loss = 0.31 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:54:26.503081: step 175160, loss = 0.40 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:54:27.371152: step 175170, loss = 0.21 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:54:28.220217: step 175180, loss = 0.33 (1507.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:54:29.072089: step 175190, loss = 0.33 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:54:30.042056: step 175200, loss = 0.30 (1319.6 examples/sec; 0.097 sec/batch)
2017-06-02 06:54:30.788392: step 175210, loss = 0.26 (1715.0 examples/sec; 0.075 sec/batch)
2017-06-02 06:54:31.672792: step 175220, loss = 0.28 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:54:32.523721: step 175230, loss = 0.23 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:54:33.424955: step 175240, loss = 0.30 (1420.3 examples/sec; 0.090 sec/batch)
2017-06-02 06:54:34.284748: step 175250, loss = 0.28 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:54:35.133317: step 175260, loss = 0.28 (1508.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:54:36.009510: step 175270, loss = 0.25 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:54:36.872201: step 175280, loss = 0.31 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:54:37.739482: step 175290, loss = 0.33 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:54:38.728903: step 175300, loss = 0.44 (1293.7 examples/sec; 0.099 sec/batch)
2017-06-02 06:54:39.465793: step 175310, loss = 0.24 (1737.1 examples/sec; 0.074 sec/batch)
2017-06-02 06:54:40.335656: step 175320, loss = 0.36 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:54:41.200676: step 175330, loss = 0.32 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:54:42.070166: step 175340, loss = 0.36 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:54:42.911424: step 175350, loss = 0.32 (1521.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:54:43.754930: step 175360, loss = 0.30 (1517.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:54:44.614838: step 175370, loss = 0.43 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:54:45.488920: step 175380, loss = 0.23 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:54:46.345120: step 175390, loss = 0.35 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:54:47.328024: step 175400, loss = 0.26 (1302.3 examples/sec; 0.098 sec/batch)
2017-06-02 06:54:48.088512: step 175410, loss = 0.24 (1683.1 examples/sec; 0.076 sec/batch)
2017-06-02 06:54:48.941815: step 175420, loss = 0.30 (1500.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:54:49.806439: step 175430, loss = 0.28 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:54:50.657211: step 175440, loss = 0.33 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:54:51.523470: step 175450, loss = 0.33 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:54:52.381329: step 175460, loss = 0.25 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:54:53.278048: step 175470, loss = 0.30 (1427.4 examples/sec; 0.090 sec/batch)
2017-06-02 06:54:54.165532: step 175480, loss = 0.30 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 06:54:55.033006: step 175490, loss = 0.27 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:54:56.005705: step 175500, loss = 0.28 (1315.9 examples/sec; 0.097 sec/batch)
2017-06-02 06:54:56.800549: step 175510, loss = 0.35 (1610.4 examples/sec; 0.079 sec/batch)
2017-06-02 06:54:57.682351: step 175520, loss = 0.32 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:54:58.553782: step 175530, loss = 0.29 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:54:59.435312: step 175540, loss = 0.25 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:55:00.287383: step 175550, loss = 0.26 (1502.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:55:01.159037: step 175560, loss = 0.39 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:55:02.021620: step 175570, loss = 0.29 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:55:02.881867: step 175580, loss = 0.32 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:55:03.752834: step 175590, loss = 0.34 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:55:04.725624: step 175600, loss = 0.34 (1315.8 examples/sec; 0.097 sec/batch)
2017-06-02 06:55:05.503860: step 175610, loss = 0.26 (1644.8 examples/sec; 0.078 sec/batch)
2017-06-02 06:55:06.374461: step 175620, loss = 0.24 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:55:07.248421: step 175630, loss = 0.44 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:55:08.107031: step 175640, loss = 0.28 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:55:08.961620: step 175650, loss = 0.30 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:55:09.832300: step 175660, loss = 0.24 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:55:10.710826: step 175670, loss = 0.29 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:55:11.581569: step 175680, loss = 0.33 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:55:12.444716: step 175690, loss = 0.26 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:55:13.405094: step 175700, loss = 0.30 (1332.8 examples/sec; 0.096 sec/batch)
2017-06-02 06:55:14.165207: step 175710, loss = 0.38 (1684.0 examples/sec; 0.076 sec/batch)
2017-06-02 06:55:15.007991: step 175720, loss = 0.27 (1518.8 examples/sec; 0.084 sec/batch)
2017-06-02 06:55:15.872188: step 175730, loss = 0.22 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:55:16.736438: step 175740, loss = 0.22 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:55:17.602503: step 175750, loss = 0.34 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:55:18.469750: step 175760, loss = 0.31 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:55:19.331040: step 175770, loss = 0.27 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:55:20.227871: step 175780, loss = 0.26 (1427.2 examples/sec; 0.090 sec/batch)
2017-06-02 06:55:21.126408: step 175790, loss = 0.29 (1424.6 examples/sec; 0.090 sec/batch)
2017-06-02 06:55:22.079387: step 175800, loss = 0.24 (1343.1 examples/sec; 0.095 sec/batch)
2017-06-02 06:55:22.844139: step 175810, loss = 0.32 (1673.8 examples/sec; 0.076 sec/batch)
2017-06-02 06:55:23.682051: step 175820, loss = 0.32 (1527.6 examples/sec; 0.084 sec/batch)
2017-06-02 06:55:24.541422: step 175830, loss = 0.29 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:55:25.401181: step 175840, loss = 0.37 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:55:26.260745: step 175850, loss = 0.29 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:55:27.114887: step 175860, loss = 0.31 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:55:27.976461: step 175870, loss = 0.27 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:55:28.856102: step 175880, loss = 0.21 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 06:55:29.727377: step 175890, loss = 0.39 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:55:30.701026: step 175900, loss = 0.18 (1314.6 examples/sec; 0.097 sec/batch)
2017-06-02 06:55:31.487371: step 175910, loss = 0.32 (1627.8 examples/sec; 0.079 sec/batch)
2017-06-02 06:55:32.330152: step 175920, loss = 0.37 (1518.8 examples/sec; 0.084 sec/batch)
2017-06-02 06:55:33.174958: step 175930, loss = 0.31 (1515.1 examples/sec; 0.084 sec/batch)
2017-06-02 06:55:34.046212: step 175940, loss = 0.28 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:55:34.879213: step 175950, loss = 0.38 (1536.6 examples/sec; 0.083 sec/batch)
2017-06-02 06:55:35.723446: step 175960, loss = 0.21 (1516.2 examples/sec; 0.084 sec/batch)
2017-06-02 06:55:36.570264: step 175970, loss = 0.28 (1511.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:55:37.416441: step 175980, loss = 0.26 (1512.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:55:38.289676: step 175990, loss = 0.32 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:55:39.271225: step 176000, loss = 0.32 (1304.1 examples/sec; 0.098 sec/batch)
2017-06-02 06:55:40.029313: step 176010, loss = 0.34 (1688.4 examples/sec; 0.076 sec/batch)
2017-06-02 06:55:40.879796: step 176020, loss = 0.30 (1505.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:55:41.722416: step 176030, loss = 0.29 (1519.1 examples/sec; 0.084 sec/batch)
2017-06-02 06:55:42.601237: step 176040, loss = 0.35 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:55:43.462454: step 176050, loss = 0.39 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:55:44.340863: step 176060, loss = 0.41 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:55:45.199260: step 176070, loss = 0.25 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:55:46.044670: step 176080, loss = 0.35 (1514.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:55:46.877748: step 176090, loss = 0.31 (1536.5 examples/sec; 0.083 sec/batch)
2017-06-02 06:55:47.818495: step 176100, loss = 0.27 (1360.6 examples/sec; 0.094 sec/batch)
2017-06-02 06:55:48.591285: step 176110, loss = 0.29 (1656.3 examples/sec; 0.077 sec/batch)
2017-06-02 06:55:49.472284: step 176120, loss = 0.34 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:55:50.337365: step 176130, loss = 0.22 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:55:51.183479: step 176140, loss = 0.29 (1512.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:55:52.032360: step 176150, loss = 0.27 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:55:52.891097: step 176160, loss = 0.40 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:55:53.766827: step 176170, loss = 0.28 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:55:54.652852: step 176180, loss = 0.29 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 06:55:55.545273: step 176190, loss = 0.24 (1434.3 examples/sec; 0.089 sec/batch)
2017-06-02 06:55:56.523479: step 176200, loss = 0.33 (1308.5 examples/sec; 0.098 sec/batch)
2017-06-02 06:55:57.326813: step 176210, loss = 0.32 (1593.4 examples/sec; 0.080 sec/batch)
2017-06-02 06:55:58.206975: step 176220, loss = 0.30 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:55:59.099581: step 176230, loss = 0.31 (1434.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:55:59.977892: step 176240, loss = 0.40 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:56:00.853096: step 176250, loss = 0.28 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:56:01.731675: step 176260, loss = 0.39 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:56:02.605379: step 176270, loss = 0.35 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:56:03.483512: step 176280, loss = 0.24 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:56:04.357439: step 176290, loss = 0.24 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:56:05.313916: step 176300, loss = 0.29 (1338.2 examples/sec; 0.096 sec/batch)
2017-06-02 06:56:06.079318: step 176310, loss = 0.26 (1672.3 examples/sec; 0.077 sec/batch)
2017-06-02 06:56:06.937237: step 176320, loss = 0.25 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:56:07.807603: step 176330, loss = 0.26 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:56:08.672609: step 176340, loss = 0.24 (1479.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:56:09.540803: step 176350, loss = 0.32 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:56:10.384921: step 176360, loss = 0.26 (1516.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:56:11.271051: step 176370, loss = 0.31 (1444.5 examples/sec; 0.089 sec/batch)
2017-06-02 06:56:12.130535: step 176380, loss = 0.27 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:56:13.012051: step 176390, loss = 0.34 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:56:14.000336: step 176400, loss = 0.37 (1295.2 examples/sec; 0.099 sec/batch)
2017-06-02 06:56:14.776436: step 176410, loss = 0.27 (1649.3 examples/sec; 0.078 sec/batch)
2017-06-02 06:56:15.633901: step 176420, loss = 0.46 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:56:16.508540: step 176430, loss = 0.31 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:56:17.394582: step 176440, loss = 0.28 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 06:56:18.291434: step 176450, loss = 0.30 (1427.2 examples/sec; 0.090 sec/batch)
2017-06-02 06:56:19.171254: step 176460, loss = 0.28 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:56:20.025217: step 176470, loss = 0.29 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:56:20.869865: step 176480, loss = 0.32 (1515.4 examples/sec; 0.084 sec/batch)
2017-06-02 06:56:21.734149: step 176490, loss = 0.34 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:56:22.681195: step 176500, loss = 0.29 (1351.6 examples/sec; 0.095 sec/batch)
2017-06-02 06:56:23.433586: step 176510, loss = 0.26 (1701.2 examples/sec; 0.075 sec/batch)
2017-06-02 06:56:24.290001: step 176520, loss = 0.40 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:56:25.157123: step 176530, loss = 0.31 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:56:26.028239: step 176540, loss = 0.29 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:56:26.896085: step 176550, loss = 0.25 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:56:27.728477: step 176560, loss = 0.26 (1537.7 examples/sec; 0.083 sec/batch)
2017-06-02 06:56:28.612928: step 176570, loss = 0.32 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:56:29.479902: step 176580, loss = 0.34 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:56:30.337512: step 176590, loss = 0.39 (1492.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:56:31.288465: step 176600, loss = 0.39 (1346.0 examples/sec; 0.095 sec/batch)
2017-06-02 06:56:32.072370: step 176610, loss = 0.29 (1632.9 examples/sec; 0.078 sec/batch)
2017-06-02 06:56:32.934682: step 176620, loss = 0.26 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:56:33.793582: step 176630, loss = 0.32 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:56:34.661712: step 176640, loss = 0.42 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:56:35.524301: step 176650, loss = 0.29 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:56:36.393276: step 176660, loss = 0.30 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:56:37.283119: step 176670, loss = 0.43 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 06:56:38.140481: step 176680, loss = 0.34 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:56:38.995447: step 176690, loss = 0.28 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:56:39.940328: step 176700, loss = 0.34 (1354.7 examples/sec; 0.094 sec/batch)
2017-06-02 06:56:40.723819: step 176710, loss = 0.26 (1633.7 examples/sec; 0.078 sec/batch)
2017-06-02 06:56:41.589377: step 176720, loss = 0.35 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:56:42.436948: step 176730, loss = 0.30 (1510.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:56:43.279628: step 176740, loss = 0.31 (1519.0 examples/sec; 0.084 sec/batch)
2017-06-02 06:56:44.121238: step 176750, loss = 0.31 (1520.9 examples/sec; 0.084 sec/batch)
2017-06-02 06:56:44.960321: step 176760, loss = 0.27 (1525.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:56:45.827199: step 176770, loss = 0.33 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:56:46.664593: step 176780, loss = 0.31 (1528.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:56:47.492154: step 176790, loss = 0.33 (1546.7 examples/sec; 0.083 sec/batch)
2017-06-02 06:56:48.437036: step 176800, loss = 0.34 (1354.7 examples/sec; 0.094 sec/batch)
2017-06-02 06:56:49.211486: step 176810, loss = 0.27 (1652.8 examples/sec; 0.077 sec/batch)
2017-06-02 06:56:50.079157: step 176820, loss = 0.32 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:56:50.924901: step 176830, loss = 0.32 (1513.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:56:51.758026: step 176840, loss = 0.30 (1536.4 examples/sec; 0.083 sec/batch)
2017-06-02 06:56:52.636782: step 176850, loss = 0.35 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:56:53.500576: step 176860, loss = 0.29 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:56:54.364393: step 176870, loss = 0.27 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:56:55.234179: step 176880, loss = 0.39 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:56:56.099454: step 176890, loss = 0.33 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:56:57.067756: step 176900, loss = 0.40 (1321.9 examples/sec; 0.097 sec/batch)
2017-06-02 06:56:57.834712: step 176910, loss = 0.28 (1668.9 examples/sec; 0.077 sec/batch)
2017-06-02 06:56:58.687827: step 176920, loss = 0.36 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 06:56:59.549936: step 176930, loss = 0.30 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:57:00.415185: step 176940, loss = 0.30 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:57:01.271874: step 176950, loss = 0.28 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:57:02.151193: step 176960, loss = 0.34 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:57:03.015707: step 176970, loss = 0.26 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:57:03.860200: step 176980, loss = 0.27 (1515.7 examples/sec; 0.084 sec/batch)
2017-06-02 06:57:04.710996: step 176990, loss = 0.38 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:57:05.687925: step 177000, loss = 0.29 (1310.2 examples/sec; 0.098 sec/batch)
2017-06-02 06:57:06.438642: step 177010, loss = 0.41 (1705.0 examples/sec; 0.075 sec/batch)
2017-06-02 06:57:07.311546: step 177020, loss = 0.29 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:57:08.172829: step 177030, loss = 0.29 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:57:09.041803: step 177040, loss = 0.32 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:57:09.919778: step 177050, loss = 0.31 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:57:10.765392: step 177060, loss = 0.32 (1513.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:57:11.603526: step 177070, loss = 0.42 (1527.2 examples/sec; 0.084 sec/batch)
2017-06-02 06:57:12.455780: step 177080, loss = 0.26 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:57:13.331006: step 177090, loss = 0.28 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:57:14.299902: step 177100, loss = 0.23 (1321.1 examples/sec; 0.097 sec/batch)
2017-06-02 06:57:15.070700: step 177110, loss = 0.24 (1660.6 examples/sec; 0.077 sec/batch)
2017-06-02 06:57:15.933147: step 177120, loss = 0.30 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:57:16.800214: step 177130, loss = 0.38 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:57:17.687195: step 177140, loss = 0.30 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 06:57:18.545837: step 177150, loss = 0.30 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:57:19.410172: step 177160, loss = 0.40 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:57:20.263637: step 177170, loss = 0.26 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:57:21.143226: step 177180, loss = 0.27 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:57:21.985699: step 177190, loss = 0.42 (1519.3 examples/sec; 0.084 sec/batch)
2017-06-02 06:57:22.951117: step 177200, loss = 0.33 (1325.9 examples/sec; 0.097 sec/batch)
2017-06-02 06:57:23.711835: step 177210, loss = 0.27 (1682.6 examples/sec; 0.076 sec/batch)
2017-06-02 06:57:24.589651: step 177220, loss = 0.39 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:57:25.481805: step 177230, loss = 0.35 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 06:57:26.335885: step 177240, loss = 0.27 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:57:27.222775: step 177250, loss = 0.23 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 06:57:28.100763: step 177260, loss = 0.33 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:57:28.979376: step 177270, loss = 0.33 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:57:29.826131: step 177280, loss = 0.33 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 06:57:30.674834: step 177290, loss = 0.36 (1508.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:57:31.613385: step 177300, loss = 0.37 (1363.8 examples/sec; 0.094 sec/batch)
2017-06-02 06:57:32.383766: step 177310, loss = 0.24 (1661.5 examples/sec; 0.077 sec/batch)
2017-06-02 06:57:33.244924: step 177320, loss = 0.33 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:57:34.119023: step 177330, loss = 0.29 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:57:35.020638: step 177340, loss = 0.31 (1419.7 examples/sec; 0.090 sec/batch)
2017-06-02 06:57:35.890665: step 177350, loss = 0.31 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:57:36.763727: step 177360, loss = 0.27 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:57:37.622760: step 177370, loss = 0.32 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:57:38.484342: step 177380, loss = 0.39 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:57:39.370794: step 177390, loss = 0.23 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:57:40.349795: step 177400, loss = 0.31 (1307.4 examples/sec; 0.098 sec/batch)
2017-06-02 06:57:41.124398: step 177410, loss = 0.27 (1652.5 examples/sec; 0.077 sec/batch)
2017-06-02 06:57:41.999666: step 177420, loss = 0.24 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:57:42.861345: step 177430, loss = 0.41 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:57:43.720260: step 177440, loss = 0.29 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:57:44.581651: step 177450, loss = 0.32 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:57:45.460661: step 177460, loss = 0.27 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:57:46.334584: step 177470, loss = 0.25 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:57:47.203193: step 177480, loss = 0.32 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:57:48.081445: step 177490, loss = 0.29 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:57:49.056346: step 177500, loss = 0.30 (1312.9 examples/sec; 0.097 sec/batch)
2017-06-02 06:57:49.810304: step 177510, loss = 0.32 (1697.7 examples/sec; 0.075 sec/batch)
2017-06-02 06:57:50.682290: step 177520, loss = 0.27 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:57:51.541577: step 177530, loss = 0.24 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:57:52.406639: step 177540, loss = 0.41 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:57:53.269570: step 177550, loss = 0.31 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:57:54.127894: step 177560, loss = 0.32 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:57:54.989172: step 177570, loss = 0.39 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:57:55.841501: step 177580, loss = 0.29 (1501.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:57:56.716552: step 177590, loss = 0.21 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 06:57:57.655874: step 177600, loss = 0.34 (1362.7 examples/sec; 0.094 sec/batch)
2017-06-02 06:57:58.444269: step 177610, loss = 0.30 (1623.6 examples/sec; 0.079 sec/batch)
2017-06-02 06:57:59.303015: step 177620, loss = 0.27 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:58:00.176500: step 177630, loss = 0.26 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:58:01.050872: step 177640, loss = 0.26 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:58:01.920818: step 177650, loss = 0.31 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:58:02.799694: step 177660, loss = 0.33 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 06:58:03.689027: step 177670, loss = 0.25 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 06:58:04.550282: step 177680, loss = 0.35 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:58:05.412682: step 177690, loss = 0.24 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:58:06.416522: step 177700, loss = 0.39 (1275.1 examples/sec; 0.100 sec/batch)
2017-06-02 06:58:07.107512: step 177710, loss = 0.23 (1852.5 examples/sec; 0.069 sec/batch)
2017-06-02 06:58:07.969555: step 177720, loss = 0.28 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 06:58:08.812511: step 177730, loss = 0.41 (1518.5 examples/sec; 0.084 sec/batch)
2017-06-02 06:58:09.681015: step 177740, loss = 0.30 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:58:10.555807: step 177750, loss = 0.31 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 06:58:11.417113: step 177760, loss = 0.23 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:58:12.285620: step 177770, loss = 0.28 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:58:13.157136: step 177780, loss = 0.28 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:58:14.014914: step 177790, loss = 0.33 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:58:14.958378: step 177800, loss = 0.29 (1356.7 examples/sec; 0.094 sec/batch)
2017-06-02 06:58:15.733241: step 177810, loss = 0.32 (1651.9 examples/sec; 0.077 sec/batch)
2017-06-02 06:58:16.605849: step 177820, loss = 0.34 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:58:17.467016: step 177830, loss = 0.30 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:58:18.337907: step 177840, loss = 0.27 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:58:19.194719: step 177850, loss = 0.34 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:58:20.042383: step 177860, loss = 0.32 (1510.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:58:20.884130: step 177870, loss = 0.25 (1520.7 examples/sec; 0.084 sec/batch)
2017-06-02 06:58:21.772515: step 177880, loss = 0.32 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 06:58:22.625555: step 177890, loss = 0.23 (1500.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:58:23.627383: step 177900, loss = 0.32 (1277.7 examples/sec; 0.100 sec/batch)
2017-06-02 06:58:24.334582: step 177910, loss = 0.32 (1810.0 examples/sec; 0.071 sec/batch)
2017-06-02 06:58:25.192752: step 177920, loss = 0.37 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:58:26.046669: step 177930, loss = 0.36 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:58:26.906630: step 177940, loss = 0.29 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 06:58:27.758107: step 177950, loss = 0.20 (1503.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:58:28.628986: step 177960, loss = 0.31 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:58:29.511107: step 177970, loss = 0.42 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 06:58:30.395164: step 177980, loss = 0.26 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 06:58:31.248597: step 177990, loss = 0.35 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:58:32.200168: step 178000, loss = 0.29 (1345.1 examples/sec; 0.095 sec/batch)
2017-06-02 06:58:32.966653: step 178010, loss = 0.42 (1670.0 examples/sec; 0.077 sec/batch)
2017-06-02 06:58:33.858706: step 178020, loss = 0.32 (1434.9 examples/sec; 0.089 sec/batch)
2017-06-02 06:58:34.730180: step 178030, loss = 0.34 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:58:35.589052: step 178040, loss = 0.40 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:58:36.472325: step 178050, loss = 0.30 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:58:37.353753: step 178060, loss = 0.31 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:58:38.212446: step 178070, loss = 0.34 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:58:39.109963: step 178080, loss = 0.24 (1426.2 examples/sec; 0.090 sec/batch)
2017-06-02 06:58:39.973888: step 178090, loss = 0.23 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:58:40.953761: step 178100, loss = 0.28 (1306.3 examples/sec; 0.098 sec/batch)
2017-06-02 06:58:41.742544: step 178110, loss = 0.34 (1622.7 examples/sec; 0.079 sec/batch)
2017-06-02 06:58:42.615031: step 178120, loss = 0.39 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:58:43.481877: step 178130, loss = 0.23 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:58:44.343746: step 178140, loss = 0.32 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:58:45.195335: step 178150, loss = 0.23 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 06:58:46.052262: step 178160, loss = 0.29 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:58:46.921022: step 178170, loss = 0.36 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:58:47.775306: step 178180, loss = 0.30 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 06:58:48.656099: step 178190, loss = 0.26 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 06:58:49.606813: step 178200, loss = 0.26 (1346.4 examples/sec; 0.095 sec/batch)
2017-06-02 06:58:50.380513: step 178210, loss = 0.47 (1654.4 examples/sec; 0.077 sec/batch)
2017-06-02 06:58:51.237665: step 178220, loss = 0.26 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 06:58:52.085874: step 178230, loss = 0.29 (1509.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:58:52.945011: step 178240, loss = 0.29 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:58:53.819607: step 178250, loss = 0.34 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:58:54.691608: step 178260, loss = 0.27 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:58:55.555202: step 178270, loss = 0.28 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:58:56.412483: step 178280, loss = 0.31 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:58:57.276148: step 178290, loss = 0.30 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:58:58.236591: step 178300, loss = 0.35 (1332.7 examples/sec; 0.096 sec/batch)
2017-06-02 06:58:59.003254: step 178310, loss = 0.23 (1669.6 examples/sec; 0.077 sec/batch)
2017-06-02 06:58:59.872645: step 178320, loss = 0.28 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:59:00.721886: step 178330, loss = 0.28 (1507.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:59:01.596228: step 178340, loss = 0.32 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:59:02.418358: step 178350, loss = 0.22 (1556.9 examples/sec; 0.082 sec/batch)
2017-06-02 06:59:03.284813: step 178360, loss = 0.29 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:59:04.153338: step 178370, loss = 0.35 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:59:05.028049: step 178380, loss = 0.30 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 06:59:05.897993: step 178390, loss = 0.24 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:59:06.842669: step 178400, loss = 0.28 (1355.0 examples/sec; 0.094 sec/batch)
2017-06-02 06:59:07.613399: step 178410, loss = 0.34 (1660.8 examples/sec; 0.077 sec/batch)
2017-06-02 06:59:08.475342: step 178420, loss = 0.31 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:59:09.345692: step 178430, loss = 0.35 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:59:10.173256: step 178440, loss = 0.40 (1546.7 examples/sec; 0.083 sec/batch)
2017-06-02 06:59:11.041787: step 178450, loss = 0.33 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:59:11.915239: step 178460, loss = 0.23 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:59:12.790463: step 178470, loss = 0.31 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 06:59:13.644064: step 178480, loss = 0.27 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:59:14.489473: step 178490, loss = 0.24 (1514.1 examples/sec; 0.085 sec/batch)
2017-06-02 06:59:15.467119: step 178500, loss = 0.27 (1309.3 examples/sec; 0.098 sec/batch)
2017-06-02 06:59:16.240113: step 178510, loss = 0.27 (1655.9 examples/sec; 0.077 sec/batch)
2017-06-02 06:59:17.129214: step 178520, loss = 0.35 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 06:59:17.982671: step 178530, loss = 0.23 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:59:18.843082: step 178540, loss = 0.27 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:59:19.710005: step 178550, loss = 0.29 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 06:59:20.595338: step 178560, loss = 0.28 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 06:59:21.487634: step 178570, loss = 0.30 (1434.5 examples/sec; 0.089 sec/batch)
2017-06-02 06:59:22.361738: step 178580, loss = 0.18 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:59:23.236272: step 178590, loss = 0.33 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 06:59:24.222027: step 178600, loss = 0.26 (1298.5 examples/sec; 0.099 sec/batch)
2017-06-02 06:59:25.017515: step 178610, loss = 0.28 (1609.1 examples/sec; 0.080 sec/batch)
2017-06-02 06:59:25.886245: step 178620, loss = 0.43 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 06:59:26.757065: step 178630, loss = 0.29 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 06:59:27.609545: step 178640, loss = 0.30 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:59:28.469504: step 178650, loss = 0.42 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:59:29.339194: step 178660, loss = 0.27 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:59:30.193357: step 178670, loss = 0.23 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:59:31.055599: step 178680, loss = 0.24 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:59:31.919779: step 178690, loss = 0.34 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:59:32.883131: step 178700, loss = 0.25 (1328.7 examples/sec; 0.096 sec/batch)
2017-06-02 06:59:33.635748: step 178710, loss = 0.23 (1700.8 examples/sec; 0.075 sec/batch)
2017-06-02 06:59:34.489931: step 178720, loss = 0.35 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:59:35.345283: step 178730, loss = 0.37 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 06:59:36.202196: step 178740, loss = 0.32 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:59:37.046089: step 178750, loss = 0.38 (1516.8 examples/sec; 0.084 sec/batch)
2017-06-02 06:59:37.904727: step 178760, loss = 0.36 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 06:59:38.756472: step 178770, loss = 0.32 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 06:59:39.631647: step 178780, loss = 0.29 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 06:59:40.499557: step 178790, loss = 0.37 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 06:59:41.479338: step 178800, loss = 0.31 (1306.4 examples/sec; 0.098 sec/batch)
2017-06-02 06:59:42.230070: step 178810, loss = 0.27 (1705.0 examples/sec; 0.075 sec/batch)
2017-06-02 06:59:43.090889: step 178820, loss = 0.36 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 06:59:43.959848: step 178830, loss = 0.34 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 06:59:44.821746: step 178840, loss = 0.22 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 06:59:45.674932: step 178850, loss = 0.27 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 06:59:46.521718: step 178860, loss = 0.23 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 06:59:47.402477: step 178870, loss = 0.23 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 06:59:48.273983: step 178880, loss = 0.24 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 06:59:49.141755: step 178890, loss = 0.38 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 06:59:50.131962: step 178900, loss = 0.30 (1292.6 examples/sec; 0.099 sec/batch)
2017-06-02 06:59:50.909515: step 178910, loss = 0.35 (1646.2 examples/sec; 0.078 sec/batch)
2017-06-02 06:59:51.854120: step 178920, loss = 0.34 (1355.1 examples/sec; 0.094 sec/batch)
2017-06-02 06:59:52.743030: step 178930, loss = 0.31 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 06:59:53.607793: step 178940, loss = 0.28 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 06:59:54.469961: step 178950, loss = 0.37 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 06:59:55.319365: step 178960, loss = 0.34 (1506.9 examples/sec; 0.085 sec/batch)
2017-06-02 06:59:56.194494: step 178970, loss = 0.23 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 06:59:57.056472: step 178980, loss = 0.31 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 06:59:57.910643: step 178990, loss = 0.32 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 06:59:58.857741: step 179000, loss = 0.38 (1351.5 examples/sec; 0.095 sec/batch)
2017-06-02 06:59:59.641571: step 179010, loss = 0.35 (1633.0 examples/sec; 0.078 sec/batch)
2017-06-02 07:00:00.491677: step 179020, loss = 0.24 (1505.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:00:01.343467: step 179030, loss = 0.35 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:00:02.205343: step 179040, loss = 0.27 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:00:03.051083: step 179050, loss = 0.35 (1513.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:00:03.922761: step 179060, loss = 0.29 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:00:04.765324: step 179070, loss = 0.38 (1519.2 examples/sec; 0.084 sec/batch)
2017-06-02 07:00:05.606813: step 179080, loss = 0.37 (1521.1 examples/sec; 0.084 sec/batch)
2017-06-02 07:00:06.468599: step 179090, loss = 0.30 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:00:07.459914: step 179100, loss = 0.29 (1291.2 examples/sec; 0.099 sec/batch)
2017-06-02 07:00:08.224118: step 179110, loss = 0.32 (1674.9 examples/sec; 0.076 sec/batch)
2017-06-02 07:00:09.050197: step 179120, loss = 0.42 (1549.5 examples/sec; 0.083 sec/batch)
2017-06-02 07:00:09.915898: step 179130, loss = 0.36 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:00:10.773532: step 179140, loss = 0.32 (1492.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:00:11.646130: step 179150, loss = 0.27 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:00:12.538398: step 179160, loss = 0.30 (1434.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:00:13.389145: step 179170, loss = 0.28 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:00:14.240943: step 179180, loss = 0.33 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:00:15.097666: step 179190, loss = 0.36 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:00:16.074777: step 179200, loss = 0.32 (1310.0 examples/sec; 0.098 sec/batch)
2017-06-02 07:00:16.830541: step 179210, loss = 0.32 (1693.7 examples/sec; 0.076 sec/batch)
2017-06-02 07:00:17.693329: step 179220, loss = 0.28 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:00:18.554507: step 179230, loss = 0.32 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:00:19.441092: step 179240, loss = 0.29 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:00:20.327015: step 179250, loss = 0.34 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:00:21.181760: step 179260, loss = 0.29 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:00:22.043166: step 179270, loss = 0.32 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:00:22.895301: step 179280, loss = 0.35 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:00:23.763833: step 179290, loss = 0.34 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:00:24.794678: step 179300, loss = 0.27 (1241.7 examples/sec; 0.103 sec/batch)
2017-06-02 07:00:25.521347: step 179310, loss = 0.40 (1761.5 examples/sec; 0.073 sec/batch)
2017-06-02 07:00:26.383898: step 179320, loss = 0.28 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:00:27.243826: step 179330, loss = 0.24 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:00:28.074146: step 179340, loss = 0.30 (1541.6 examples/sec; 0.083 sec/batch)
2017-06-02 07:00:28.929117: step 179350, loss = 0.32 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:00:29.793219: step 179360, loss = 0.31 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:00:30.644996: step 179370, loss = 0.26 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:00:31.513003: step 179380, loss = 0.31 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:00:32.369684: step 179390, loss = 0.27 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:00:33.359738: step 179400, loss = 0.33 (1292.9 examples/sec; 0.099 sec/batch)
2017-06-02 07:00:34.156802: step 179410, loss = 0.27 (1605.9 examples/sec; 0.080 sec/batch)
2017-06-02 07:00:35.013449: step 179420, loss = 0.39 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:00:35.875397: step 179430, loss = 0.30 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:00:36.712045: step 179440, loss = 0.33 (1529.9 examples/sec; 0.084 sec/batch)
2017-06-02 07:00:37.560872: step 179450, loss = 0.31 (1508.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:00:38.429406: step 179460, loss = 0.27 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:00:39.303079: step 179470, loss = 0.31 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:00:40.196473: step 179480, loss = 0.31 (1432.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:00:41.063068: step 179490, loss = 0.26 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:00:42.038647: step 179500, loss = 0.27 (1312.0 examples/sec; 0.098 sec/batch)
2017-06-02 07:00:42.815818: step 179510, loss = 0.28 (1647.0 examples/sec; 0.078 sec/batch)
2017-06-02 07:00:43.672342: step 179520, loss = 0.27 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:00:44.526587: step 179530, loss = 0.27 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:00:45.393396: step 179540, loss = 0.33 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:00:46.290189: step 179550, loss = 0.36 (1427.3 examples/sec; 0.090 sec/batch)
2017-06-02 07:00:47.134398: step 179560, loss = 0.28 (1516.2 examples/sec; 0.084 sec/batch)
2017-06-02 07:00:48.000301: step 179570, loss = 0.27 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:00:48.862798: step 179580, loss = 0.38 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:00:49.728903: step 179590, loss = 0.26 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:00:50.678342: step 179600, loss = 0.33 (1348.2 examples/sec; 0.095 sec/batch)
2017-06-02 07:00:51.431404: step 179610, loss = 0.27 (1699.7 examples/sec; 0.075 sec/batch)
2017-06-02 07:00:52.303790: step 179620, loss = 0.31 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:00:53.164325: step 179630, loss = 0.30 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:00:54.044540: step 179640, loss = 0.24 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:00:54.913044: step 179650, loss = 0.26 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:00:55.798387: step 179660, loss = 0.42 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:00:56.665216: step 179670, loss = 0.29 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:00:57.537977: step 179680, loss = 0.32 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:00:58.412524: step 179690, loss = 0.35 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:00:59.385903: step 179700, loss = 0.22 (1315.0 examples/sec; 0.097 sec/batch)
2017-06-02 07:01:00.188177: step 179710, loss = 0.33 (1595.5 examples/sec; 0.080 sec/batch)
2017-06-02 07:01:01.053465: step 179720, loss = 0.20 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:01:01.924954: step 179730, loss = 0.26 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:01:02.816388: step 179740, loss = 0.35 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:01:03.680104: step 179750, loss = 0.35 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:01:04.555450: step 179760, loss = 0.30 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:01:05.433579: step 179770, loss = 0.36 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:01:06.297966: step 179780, loss = 0.22 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:01:07.177213: step 179790, loss = 0.33 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:01:08.153851: step 179800, loss = 0.25 (1310.6 examples/sec; 0.098 sec/batch)
2017-06-02 07:01:08.948450: step 179810, loss = 0.26 (1610.9 examples/sec; 0.079 sec/batch)
2017-06-02 07:01:09.806320: step 179820, loss = 0.27 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:01:10.672686: step 179830, loss = 0.29 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:01:11.557204: step 179840, loss = 0.25 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:01:12.415710: step 179850, loss = 0.31 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:01:13.273579: step 179860, loss = 0.30 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:01:14.139325: step 179870, loss = 0.34 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:01:15.003614: step 179880, loss = 0.20 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:01:15.867359: step 179890, loss = 0.29 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:01:16.843657: step 179900, loss = 0.33 (1311.1 examples/sec; 0.098 sec/batch)
2017-06-02 07:01:17.593117: step 179910, loss = 0.23 (1707.9 examples/sec; 0.075 sec/batch)
2017-06-02 07:01:18.463355: step 179920, loss = 0.29 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:01:19.327180: step 179930, loss = 0.24 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:01:20.202467: step 179940, loss = 0.33 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:01:21.078447: step 179950, loss = 0.30 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:01:21.963591: step 179960, loss = 0.26 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:01:22.828364: step 179970, loss = 0.29 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:01:23.681288: step 179980, loss = 0.33 (1500.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:01:24.556468: step 179990, loss = 0.30 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:01:25.542632: step 180000, loss = 0.29 (1297.9 examples/sec; 0.099 sec/batch)
2017-06-02 07:01:26.298872: step 180010, loss = 0.39 (1692.6 examples/sec; 0.076 sec/batch)
2017-06-02 07:01:27.147939: step 180020, loss = 0.30 (1507.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:01:28.035889: step 180030, loss = 0.28 (1441.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:01:28.892778: step 180040, loss = 0.25 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:01:29.771338: step 180050, loss = 0.27 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:01:30.636091: step 180060, loss = 0.24 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:01:31.521589: step 180070, loss = 0.33 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:01:32.368396: step 180080, loss = 0.35 (1511.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:01:33.252029: step 180090, loss = 0.27 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:01:34.208566: step 180100, loss = 0.29 (1338.2 examples/sec; 0.096 sec/batch)
2017-06-02 07:01:34.981039: step 180110, loss = 0.23 (1657.0 examples/sec; 0.077 sec/batch)
2017-06-02 07:01:35.869402: step 180120, loss = 0.32 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:01:36.726703: step 180130, loss = 0.37 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:01:37.575966: step 180140, loss = 0.29 (1507.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:01:38.425566: step 180150, loss = 0.28 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:01:39.298212: step 180160, loss = 0.27 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:01:40.174225: step 180170, loss = 0.35 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:01:41.039482: step 180180, loss = 0.34 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:01:41.914602: step 180190, loss = 0.34 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:01:42.877607: step 180200, loss = 0.36 (1329.2 examples/sec; 0.096 sec/batch)
2017-06-02 07:01:43.643496: step 180210, loss = 0.27 (1671.3 examples/sec; 0.077 sec/batch)
2017-06-02 07:01:44.519941: step 180220, loss = 0.29 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:01:45.392890: step 180230, loss = 0.37 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:01:46.270488: step 180240, loss = 0.31 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:01:47.095550: step 180250, loss = 0.35 (1551.4 examples/sec; 0.083 sec/batch)
2017-06-02 07:01:47.961906: step 180260, loss = 0.34 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:01:48.848805: step 180270, loss = 0.32 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:01:49.721062: step 180280, loss = 0.36 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:01:50.593100: step 180290, loss = 0.30 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:01:51.566405: step 180300, loss = 0.31 (1315.1 examples/sec; 0.097 sec/batch)
2017-06-02 07:01:52.334430: step 180310, loss = 0.31 (1666.6 examples/sec; 0.077 sec/batch)
2017-06-02 07:01:53.190809: step 180320, loss = 0.25 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:01:54.049898: step 180330, loss = 0.31 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:01:54.925154: step 180340, loss = 0.36 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:01:55.782830: step 180350, loss = 0.27 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:01:56.658080: step 180360, loss = 0.23 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:01:57.544818: step 180370, loss = 0.30 (1443.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:01:58.410687: step 180380, loss = 0.39 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:01:59.282098: step 180390, loss = 0.35 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:02:00.261664: step 180400, loss = 0.24 (1306.7 examples/sec; 0.098 sec/batch)
2017-06-02 07:02:01.063633: step 180410, loss = 0.29 (1596.1 examples/sec; 0.080 sec/batch)
2017-06-02 07:02:01.919665: step 180420, loss = 0.37 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:02:02.791919: step 180430, loss = 0.34 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:02:03.679930: step 180440, loss = 0.31 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:02:04.573067: step 180450, loss = 0.23 (1433.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:02:05.451028: step 180460, loss = 0.35 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:02:06.330296: step 180470, loss = 0.34 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:02:07.218375: step 180480, loss = 0.32 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:02:08.111351: step 180490, loss = 0.28 (1433.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:02:09.093239: step 180500, loss = 0.30 (1303.6 examples/sec; 0.098 sec/batch)
2017-06-02 07:02:09.863853: step 180510, loss = 0.42 (1661.0 examples/sec; 0.077 sec/batch)
2017-06-02 07:02:10.735941: step 180520, loss = 0.30 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:02:11.615593: step 180530, loss = 0.33 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:02:12.480865: step 180540, loss = 0.27 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:02:13.382167: step 180550, loss = 0.32 (1420.2 examples/sec; 0.090 sec/batch)
2017-06-02 07:02:14.276334: step 180560, loss = 0.33 (1431.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:02:15.162688: step 180570, loss = 0.38 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:02:16.052494: step 180580, loss = 0.37 (1438.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:02:16.886245: step 180590, loss = 0.28 (1535.2 examples/sec; 0.083 sec/batch)
2017-06-02 07:02:17.885871: step 180600, loss = 0.27 (1280.5 examples/sec; 0.100 sec/batch)
2017-06-02 07:02:18.577289: step 180610, loss = 0.30 (1851.3 examples/sec; 0.069 sec/batch)
2017-06-02 07:02:19.436970: step 180620, loss = 0.23 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:02:20.302216: step 180630, loss = 0.24 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:02:21.177577: step 180640, loss = 0.37 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:02:22.042703: step 180650, loss = 0.27 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:02:22.902453: step 180660, loss = 0.24 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:02:23.768903: step 180670, loss = 0.31 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:02:24.640971: step 180680, loss = 0.30 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:02:25.517955: step 180690, loss = 0.29 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:02:26.490691: step 180700, loss = 0.42 (1315.9 examples/sec; 0.097 sec/batch)
2017-06-02 07:02:27.246526: step 180710, loss = 0.23 (1693.5 examples/sec; 0.076 sec/batch)
2017-06-02 07:02:28.101893: step 180720, loss = 0.28 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:02:28.939105: step 180730, loss = 0.35 (1528.8 examples/sec; 0.084 sec/batch)
2017-06-02 07:02:29.820349: step 180740, loss = 0.32 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:02:30.670292: step 180750, loss = 0.29 (1506.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:02:31.513720: step 180760, loss = 0.36 (1517.6 examples/sec; 0.084 sec/batch)
2017-06-02 07:02:32.380687: step 180770, loss = 0.24 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:02:33.268483: step 180780, loss = 0.22 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:02:34.113385: step 180790, loss = 0.37 (1515.0 examples/sec; 0.084 sec/batch)
2017-06-02 07:02:35.108416: step 180800, loss = 0.26 (1286.4 examples/sec; 0.100 sec/batch)
2017-06-02 07:02:35.872231: step 180810, loss = 0.35 (1675.8 examples/sec; 0.076 sec/batch)
2017-06-02 07:02:36.753673: step 180820, loss = 0.30 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:02:37.622074: step 180830, loss = 0.26 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:02:38.482870: step 180840, loss = 0.31 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:02:39.359931: step 180850, loss = 0.30 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:02:40.215190: step 180860, loss = 0.32 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:02:41.077764: step 180870, loss = 0.40 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:02:41.959286: step 180880, loss = 0.34 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:02:42.847807: step 180890, loss = 0.30 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:02:43.805779: step 180900, loss = 0.22 (1336.2 examples/sec; 0.096 sec/batch)
2017-06-02 07:02:44.583068: step 180910, loss = 0.29 (1646.8 examples/sec; 0.078 sec/batch)
2017-06-02 07:02:45.438707: step 180920, loss = 0.24 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:02:46.303057: step 180930, loss = 0.29 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:02:47.156213: step 180940, loss = 0.29 (1500.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:02:48.002271: step 180950, loss = 0.30 (1512.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:02:48.862226: step 180960, loss = 0.22 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:02:49.732245: step 180970, loss = 0.24 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:02:50.618533: step 180980, loss = 0.35 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:02:51.447190: step 180990, loss = 0.22 (1544.7 examples/sec; 0.083 sec/batch)
2017-06-02 07:02:52.405367: step 181000, loss = 0.27 (1335.9 examples/sec; 0.096 sec/batch)
2017-06-02 07:02:53.179339: step 181010, loss = 0.32 (1653.8 examples/sec; 0.077 sec/batch)
2017-06-02 07:02:54.050972: step 181020, loss = 0.39 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:02:54.919648: step 181030, loss = 0.31 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:02:55.797296: step 181040, loss = 0.28 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:02:56.652105: step 181050, loss = 0.41 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:02:57.499279: step 181060, loss = 0.25 (1510.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:02:58.377642: step 181070, loss = 0.28 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:02:59.244069: step 181080, loss = 0.26 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:03:00.109494: step 181090, loss = 0.21 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:03:01.133865: step 181100, loss = 0.27 (1249.6 examples/sec; 0.102 sec/batch)
2017-06-02 07:03:01.830609: step 181110, loss = 0.36 (1837.1 examples/sec; 0.070 sec/batch)
2017-06-02 07:03:02.681739: step 181120, loss = 0.34 (1503.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:03:03.521826: step 181130, loss = 0.36 (1523.7 examples/sec; 0.084 sec/batch)
2017-06-02 07:03:04.374003: step 181140, loss = 0.24 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:03:05.243822: step 181150, loss = 0.28 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:03:06.114267: step 181160, loss = 0.24 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:03:06.970406: step 181170, loss = 0.29 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:03:07.831182: step 181180, loss = 0.30 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:03:08.700786: step 181190, loss = 0.30 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:03:09.647079: step 181200, loss = 0.29 (1352.6 examples/sec; 0.095 sec/batch)
2017-06-02 07:03:10.396580: step 181210, loss = 0.26 (1707.8 examples/sec; 0.075 sec/batch)
2017-06-02 07:03:11.237938: step 181220, loss = 0.29 (1521.4 examples/sec; 0.084 sec/batch)
2017-06-02 07:03:12.102957: step 181230, loss = 0.26 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:03:12.963481: step 181240, loss = 0.33 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:03:13.813276: step 181250, loss = 0.36 (1506.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:03:14.650911: step 181260, loss = 0.45 (1528.1 examples/sec; 0.084 sec/batch)
2017-06-02 07:03:15.557116: step 181270, loss = 0.36 (1412.5 examples/sec; 0.091 sec/batch)
2017-06-02 07:03:16.413859: step 181280, loss = 0.25 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:03:17.277531: step 181290, loss = 0.26 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:03:18.249448: step 181300, loss = 0.27 (1317.0 examples/sec; 0.097 sec/batch)
2017-06-02 07:03:18.969173: step 181310, loss = 0.32 (1778.5 examples/sec; 0.072 sec/batch)
2017-06-02 07:03:19.821474: step 181320, loss = 0.27 (1501.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:03:20.700285: step 181330, loss = 0.31 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:03:21.584657: step 181340, loss = 0.41 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:03:22.450113: step 181350, loss = 0.35 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:03:23.314559: step 181360, loss = 0.32 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:03:24.194985: step 181370, loss = 0.27 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:03:25.036978: step 181380, loss = 0.34 (1520.2 examples/sec; 0.084 sec/batch)
2017-06-02 07:03:25.901695: step 181390, loss = 0.30 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:03:26.868120: step 181400, loss = 0.23 (1324.5 examples/sec; 0.097 sec/batch)
2017-06-02 07:03:27.633442: step 181410, loss = 0.36 (1672.5 examples/sec; 0.077 sec/batch)
2017-06-02 07:03:28.474660: step 181420, loss = 0.37 (1521.6 examples/sec; 0.084 sec/batch)
2017-06-02 07:03:29.321185: step 181430, loss = 0.28 (1512.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:03:30.150329: step 181440, loss = 0.26 (1543.8 examples/sec; 0.083 sec/batch)
2017-06-02 07:03:31.027657: step 181450, loss = 0.29 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:03:31.885554: step 181460, loss = 0.30 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:03:32.765111: step 181470, loss = 0.32 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:03:33.660540: step 181480, loss = 0.25 (1429.5 examples/sec; 0.090 sec/batch)
2017-06-02 07:03:34.550623: step 181490, loss = 0.29 (1438.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:03:35.506125: step 181500, loss = 0.30 (1339.6 examples/sec; 0.096 sec/batch)
2017-06-02 07:03:36.268260: step 181510, loss = 0.29 (1679.5 examples/sec; 0.076 sec/batch)
2017-06-02 07:03:37.153779: step 181520, loss = 0.30 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:03:38.032661: step 181530, loss = 0.28 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:03:38.894138: step 181540, loss = 0.29 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:03:39.776095: step 181550, loss = 0.32 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:03:40.647124: step 181560, loss = 0.32 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:03:41.505892: step 181570, loss = 0.34 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:03:42.388471: step 181580, loss = 0.33 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:03:43.273390: step 181590, loss = 0.30 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:03:44.273751: step 181600, loss = 0.32 (1279.5 examples/sec; 0.100 sec/batch)
2017-06-02 07:03:45.058908: step 181610, loss = 0.29 (1630.2 examples/sec; 0.079 sec/batch)
2017-06-02 07:03:45.947963: step 181620, loss = 0.41 (1439.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:03:46.829885: step 181630, loss = 0.34 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:03:47.717658: step 181640, loss = 0.31 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:03:48.607531: step 181650, loss = 0.31 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:03:49.469492: step 181660, loss = 0.30 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:03:50.329417: step 181670, loss = 0.24 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:03:51.222143: step 181680, loss = 0.30 (1433.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:03:52.116169: step 181690, loss = 0.29 (1431.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:03:53.080311: step 181700, loss = 0.32 (1327.6 examples/sec; 0.096 sec/batch)
2017-06-02 07:03:53.856459: step 181710, loss = 0.29 (1649.2 examples/sec; 0.078 sec/batch)
2017-06-02 07:03:54.733013: step 181720, loss = 0.36 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:03:55.602871: step 181730, loss = 0.36 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:03:56.492773: step 181740, loss = 0.31 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:03:57.354510: step 181750, loss = 0.32 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:03:58.213417: step 181760, loss = 0.26 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:03:59.090823: step 181770, loss = 0.23 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:03:59.976704: step 181780, loss = 0.28 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:04:00.840668: step 181790, loss = 0.35 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:04:01.812515: step 181800, loss = 0.26 (1317.1 examples/sec; 0.097 sec/batch)
2017-06-02 07:04:02.574554: step 181810, loss = 0.29 (1679.7 examples/sec; 0.076 sec/batch)
2017-06-02 07:04:03.439577: step 181820, loss = 0.30 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:04:04.337659: step 181830, loss = 0.40 (1425.3 examples/sec; 0.090 sec/batch)
2017-06-02 07:04:05.228959: step 181840, loss = 0.32 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:04:06.103522: step 181850, loss = 0.28 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:04:06.966953: step 181860, loss = 0.36 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:04:07.818948: step 181870, loss = 0.26 (1502.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:04:08.678967: step 181880, loss = 0.31 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:04:09.569202: step 181890, loss = 0.30 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:04:10.564622: step 181900, loss = 0.34 (1285.9 examples/sec; 0.100 sec/batch)
2017-06-02 07:04:11.325570: step 181910, loss = 0.25 (1682.1 examples/sec; 0.076 sec/batch)
2017-06-02 07:04:12.215496: step 181920, loss = 0.33 (1438.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:04:13.071931: step 181930, loss = 0.27 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:04:13.923050: step 181940, loss = 0.28 (1503.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:04:14.799288: step 181950, loss = 0.32 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:04:15.679849: step 181960, loss = 0.30 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:04:16.556345: step 181970, loss = 0.38 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:04:17.426779: step 181980, loss = 0.31 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:04:18.306299: step 181990, loss = 0.36 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:04:19.269738: step 182000, loss = 0.35 (1328.6 examples/sec; 0.096 sec/batch)
2017-06-02 07:04:20.043554: step 182010, loss = 0.24 (1654.1 examples/sec; 0.077 sec/batch)
2017-06-02 07:04:20.895379: step 182020, loss = 0.39 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:04:21.790554: step 182030, loss = 0.27 (1429.9 examples/sec; 0.090 sec/batch)
2017-06-02 07:04:22.676239: step 182040, loss = 0.26 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:04:23.568713: step 182050, loss = 0.25 (1434.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:04:24.424239: step 182060, loss = 0.28 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:04:25.295310: step 182070, loss = 0.29 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:04:26.172276: step 182080, loss = 0.27 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:04:27.049197: step 182090, loss = 0.36 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:04:27.998589: step 182100, loss = 0.32 (1348.2 examples/sec; 0.095 sec/batch)
2017-06-02 07:04:28.755859: step 182110, loss = 0.32 (1690.3 examples/sec; 0.076 sec/batch)
2017-06-02 07:04:29.638132: step 182120, loss = 0.38 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:04:30.527046: step 182130, loss = 0.34 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:04:31.398186: step 182140, loss = 0.33 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:04:32.275268: step 182150, loss = 0.22 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:04:33.135897: step 182160, loss = 0.24 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:04:33.998231: step 182170, loss = 0.23 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:04:34.875875: step 182180, loss = 0.23 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:04:35.721122: step 182190, loss = 0.27 (1514.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:04:36.688780: step 182200, loss = 0.30 (1322.8 examples/sec; 0.097 sec/batch)
2017-06-02 07:04:37.458192: step 182210, loss = 0.25 (1663.6 examples/sec; 0.077 sec/batch)
2017-06-02 07:04:38.336013: step 182220, loss = 0.24 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:04:39.220312: step 182230, loss = 0.26 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:04:40.083966: step 182240, loss = 0.34 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:04:40.971292: step 182250, loss = 0.33 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:04:41.840950: step 182260, loss = 0.32 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:04:42.727352: step 182270, loss = 0.30 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:04:43.611507: step 182280, loss = 0.27 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:04:44.482627: step 182290, loss = 0.30 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:04:45.455385: step 182300, loss = 0.29 (1315.8 examples/sec; 0.097 sec/batch)
2017-06-02 07:04:46.238098: step 182310, loss = 0.25 (1635.3 examples/sec; 0.078 sec/batch)
2017-06-02 07:04:47.108940: step 182320, loss = 0.35 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:04:47.970517: step 182330, loss = 0.35 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:04:48.854618: step 182340, loss = 0.32 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:04:49.735791: step 182350, loss = 0.31 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:04:50.600137: step 182360, loss = 0.32 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:04:51.477955: step 182370, loss = 0.41 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:04:52.323445: step 182380, loss = 0.32 (1513.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:04:53.185666: step 182390, loss = 0.30 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:04:54.155134: step 182400, loss = 0.25 (1320.3 examples/sec; 0.097 sec/batch)
2017-06-02 07:04:54.927489: step 182410, loss = 0.30 (1657.3 examples/sec; 0.077 sec/batch)
2017-06-02 07:04:55.804051: step 182420, loss = 0.37 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:04:56.663077: step 182430, loss = 0.29 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:04:57.549591: step 182440, loss = 0.30 (1443.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:04:58.420350: step 182450, loss = 0.47 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:04:59.299648: step 182460, loss = 0.29 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:05:00.172085: step 182470, loss = 0.28 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:05:01.037799: step 182480, loss = 0.30 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:05:01.923056: step 182490, loss = 0.31 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:05:02.875357: step 182500, loss = 0.32 (1344.1 examples/sec; 0.095 sec/batch)
2017-06-02 07:05:03.647363: step 182510, loss = 0.35 (1658.0 examples/sec; 0.077 sec/batch)
2017-06-02 07:05:04.492191: step 182520, loss = 0.32 (1515.1 examples/sec; 0.084 sec/batch)
2017-06-02 07:05:05.364460: step 182530, loss = 0.28 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:05:06.233134: step 182540, loss = 0.28 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:05:07.115186: step 182550, loss = 0.40 (1451.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:05:07.965348: step 182560, loss = 0.28 (1505.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:05:08.840097: step 182570, loss = 0.29 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:05:09.713959: step 182580, loss = 0.32 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:05:10.558809: step 182590, loss = 0.29 (1515.1 examples/sec; 0.084 sec/batch)
2017-06-02 07:05:11.548686: step 182600, loss = 0.32 (1293.1 examples/sec; 0.099 sec/batch)
2017-06-02 07:05:12.310611: step 182610, loss = 0.29 (1680.0 examples/sec; 0.076 sec/batch)
2017-06-02 07:05:13.158450: step 182620, loss = 0.32 (1509.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:05:14.006624: step 182630, loss = 0.29 (1509.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:05:14.886539: step 182640, loss = 0.25 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:05:15.779224: step 182650, loss = 0.33 (1433.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:05:16.673909: step 182660, loss = 0.40 (1430.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:05:17.562649: step 182670, loss = 0.45 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:05:18.418914: step 182680, loss = 0.31 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:05:19.306021: step 182690, loss = 0.25 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:05:20.268088: step 182700, loss = 0.34 (1330.4 examples/sec; 0.096 sec/batch)
2017-06-02 07:05:21.062392: step 182710, loss = 0.29 (1611.5 examples/sec; 0.079 sec/batch)
2017-06-02 07:05:21.934316: step 182720, loss = 0.36 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:05:22.821841: step 182730, loss = 0.32 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:05:23.676354: step 182740, loss = 0.32 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:05:24.557958: step 182750, loss = 0.34 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:05:25.423508: step 182760, loss = 0.27 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:05:26.286850: step 182770, loss = 0.36 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:05:27.183670: step 182780, loss = 0.27 (1427.3 examples/sec; 0.090 sec/batch)
2017-06-02 07:05:28.045133: step 182790, loss = 0.36 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:05:28.999780: step 182800, loss = 0.33 (1340.8 examples/sec; 0.095 sec/batch)
2017-06-02 07:05:29.763803: step 182810, loss = 0.28 (1675.3 examples/sec; 0.076 sec/batch)
2017-06-02 07:05:30.633472: step 182820, loss = 0.23 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:05:31.527058: step 182830, loss = 0.32 (1432.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:05:32.381966: step 182840, loss = 0.25 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:05:33.235908: step 182850, loss = 0.34 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:05:34.106690: step 182860, loss = 0.31 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:05:34.990406: step 182870, loss = 0.32 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:05:35.872451: step 182880, loss = 0.27 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:05:36.745674: step 182890, loss = 0.29 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:05:37.703067: step 182900, loss = 0.31 (1337.0 examples/sec; 0.096 sec/batch)
2017-06-02 07:05:38.468617: step 182910, loss = 0.23 (1672.0 examples/sec; 0.077 sec/batch)
2017-06-02 07:05:39.319601: step 182920, loss = 0.23 (1504.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:05:40.204676: step 182930, loss = 0.38 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:05:41.090752: step 182940, loss = 0.27 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:05:41.961237: step 182950, loss = 0.35 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:05:42.838770: step 182960, loss = 0.32 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:05:43.693149: step 182970, loss = 0.24 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:05:44.583496: step 182980, loss = 0.40 (1437.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:05:45.466290: step 182990, loss = 0.29 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:05:46.439522: step 183000, loss = 0.28 (1315.2 examples/sec; 0.097 sec/batch)
2017-06-02 07:05:47.216751: step 183010, loss = 0.32 (1646.9 examples/sec; 0.078 sec/batch)
2017-06-02 07:05:48.109816: step 183020, loss = 0.27 (1433.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:05:49.004474: step 183030, loss = 0.38 (1430.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:05:49.896089: step 183040, loss = 0.40 (1435.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:05:50.775538: step 183050, loss = 0.28 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:05:51.654741: step 183060, loss = 0.25 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:05:52.499933: step 183070, loss = 0.36 (1514.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:05:53.337452: step 183080, loss = 0.27 (1528.3 examples/sec; 0.084 sec/batch)
2017-06-02 07:05:54.211853: step 183090, loss = 0.38 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:05:55.175898: step 183100, loss = 0.26 (1327.7 examples/sec; 0.096 sec/batch)
2017-06-02 07:05:55.994649: step 183110, loss = 0.35 (1563.3 examples/sec; 0.082 sec/batch)
2017-06-02 07:05:56.884287: step 183120, loss = 0.27 (1438.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:05:57.748920: step 183130, loss = 0.34 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:05:58.615606: step 183140, loss = 0.32 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:05:59.479159: step 183150, loss = 0.29 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:06:00.354685: step 183160, loss = 0.26 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:06:01.234189: step 183170, loss = 0.26 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:06:02.111884: step 183180, loss = 0.41 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:06:02.983022: step 183190, loss = 0.30 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:06:03.937885: step 183200, loss = 0.35 (1340.5 examples/sec; 0.095 sec/batch)
2017-06-02 07:06:04.728455: step 183210, loss = 0.30 (1619.1 examples/sec; 0.079 sec/batch)
2017-06-02 07:06:05.608165: step 183220, loss = 0.30 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:06:06.485807: step 183230, loss = 0.28 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:06:07.394737: step 183240, loss = 0.26 (1408.3 examples/sec; 0.091 sec/batch)
2017-06-02 07:06:08.280559: step 183250, loss = 0.25 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:06:09.176574: step 183260, loss = 0.33 (1428.6 examples/sec; 0.090 sec/batch)
2017-06-02 07:06:10.065492: step 183270, loss = 0.35 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:06:10.952628: step 183280, loss = 0.26 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:06:11.857467: step 183290, loss = 0.20 (1414.6 examples/sec; 0.090 sec/batch)
2017-06-02 07:06:12.835452: step 183300, loss = 0.38 (1308.8 examples/sec; 0.098 sec/batch)
2017-06-02 07:06:13.645653: step 183310, loss = 0.32 (1579.9 examples/sec; 0.081 sec/batch)
2017-06-02 07:06:14.520489: step 183320, loss = 0.27 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:06:15.383461: step 183330, loss = 0.34 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:06:16.245836: step 183340, loss = 0.25 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:06:17.151194: step 183350, loss = 0.24 (1413.8 examples/sec; 0.091 sec/batch)
2017-06-02 07:06:18.026353: step 183360, loss = 0.33 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:06:18.909763: step 183370, loss = 0.29 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:06:19.776102: step 183380, loss = 0.29 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:06:20.676439: step 183390, loss = 0.22 (1421.7 examples/sec; 0.090 sec/batch)
2017-06-02 07:06:21.637673: step 183400, loss = 0.28 (1331.6 examples/sec; 0.096 sec/batch)
2017-06-02 07:06:22.433101: step 183410, loss = 0.23 (1609.2 examples/sec; 0.080 sec/batch)
2017-06-02 07:06:23.316463: step 183420, loss = 0.37 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:06:24.193673: step 183430, loss = 0.28 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:06:25.069152: step 183440, loss = 0.28 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:06:25.920826: step 183450, loss = 0.24 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:06:26.764958: step 183460, loss = 0.32 (1516.4 examples/sec; 0.084 sec/batch)
2017-06-02 07:06:27.614932: step 183470, loss = 0.29 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:06:28.475669: step 183480, loss = 0.29 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:06:29.363427: step 183490, loss = 0.35 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:06:30.305099: step 183500, loss = 0.32 (1359.3 examples/sec; 0.094 sec/batch)
2017-06-02 07:06:31.068087: step 183510, loss = 0.30 (1677.6 examples/sec; 0.076 sec/batch)
2017-06-02 07:06:31.918070: step 183520, loss = 0.28 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:06:32.790572: step 183530, loss = 0.33 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:06:33.666997: step 183540, loss = 0.28 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:06:34.521556: step 183550, loss = 0.29 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:06:35.398131: step 183560, loss = 0.30 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:06:36.266416: step 183570, loss = 0.22 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:06:37.130143: step 183580, loss = 0.29 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:06:37.978146: step 183590, loss = 0.29 (1509.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:06:38.917304: step 183600, loss = 0.37 (1362.9 examples/sec; 0.094 sec/batch)
2017-06-02 07:06:39.683284: step 183610, loss = 0.24 (1671.1 examples/sec; 0.077 sec/batch)
2017-06-02 07:06:40.576457: step 183620, loss = 0.31 (1433.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:06:41.453901: step 183630, loss = 0.22 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:06:42.299960: step 183640, loss = 0.26 (1512.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:06:43.195906: step 183650, loss = 0.36 (1428.6 examples/sec; 0.090 sec/batch)
2017-06-02 07:06:44.064508: step 183660, loss = 0.38 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:06:44.932106: step 183670, loss = 0.29 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:06:45.781873: step 183680, loss = 0.32 (1506.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:06:46.652353: step 183690, loss = 0.36 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:06:47.612040: step 183700, loss = 0.28 (1333.8 examples/sec; 0.096 sec/batch)
2017-06-02 07:06:48.375621: step 183710, loss = 0.38 (1676.3 examples/sec; 0.076 sec/batch)
2017-06-02 07:06:49.240064: step 183720, loss = 0.28 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:06:50.081744: step 183730, loss = 0.26 (1520.8 examples/sec; 0.084 sec/batch)
2017-06-02 07:06:50.942110: step 183740, loss = 0.29 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:06:51.813722: step 183750, loss = 0.23 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:06:52.701343: step 183760, loss = 0.30 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:06:53.590443: step 183770, loss = 0.31 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:06:54.479745: step 183780, loss = 0.24 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:06:55.336081: step 183790, loss = 0.33 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:06:56.330965: step 183800, loss = 0.30 (1286.6 examples/sec; 0.099 sec/batch)
2017-06-02 07:06:57.054117: step 183810, loss = 0.29 (1770.0 examples/sec; 0.072 sec/batch)
2017-06-02 07:06:57.920874: step 183820, loss = 0.31 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:06:58.765034: step 183830, loss = 0.30 (1516.3 examples/sec; 0.084 sec/batch)
2017-06-02 07:06:59.643258: step 183840, loss = 0.28 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:07:00.473837: step 183850, loss = 0.32 (1541.1 examples/sec; 0.083 sec/batch)
2017-06-02 07:07:01.334648: step 183860, loss = 0.33 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:07:02.204490: step 183870, loss = 0.31 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:07:03.076651: step 183880, loss = 0.36 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:07:03.945824: step 183890, loss = 0.29 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:07:04.950128: step 183900, loss = 0.25 (1274.5 examples/sec; 0.100 sec/batch)
2017-06-02 07:07:05.718195: step 183910, loss = 0.27 (1666.5 examples/sec; 0.077 sec/batch)
2017-06-02 07:07:06.583184: step 183920, loss = 0.42 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:07:07.416256: step 183930, loss = 0.26 (1536.5 examples/sec; 0.083 sec/batch)
2017-06-02 07:07:08.272939: step 183940, loss = 0.22 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:07:09.143357: step 183950, loss = 0.30 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:07:10.018974: step 183960, loss = 0.27 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:07:10.898406: step 183970, loss = 0.28 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:07:11.756185: step 183980, loss = 0.25 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:07:12.629400: step 183990, loss = 0.30 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:07:13.598130: step 184000, loss = 0.28 (1321.3 examples/sec; 0.097 sec/batch)
2017-06-02 07:07:14.380392: step 184010, loss = 0.31 (1636.3 examples/sec; 0.078 sec/batch)
2017-06-02 07:07:15.250052: step 184020, loss = 0.32 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:07:16.097472: step 184030, loss = 0.50 (1510.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:07:16.947155: step 184040, loss = 0.26 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:07:17.808069: step 184050, loss = 0.29 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:07:18.666361: step 184060, loss = 0.28 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:07:19.524928: step 184070, loss = 0.33 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:07:20.398024: step 184080, loss = 0.24 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:07:21.261118: step 184090, loss = 0.29 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:07:22.236621: step 184100, loss = 0.32 (1312.1 examples/sec; 0.098 sec/batch)
2017-06-02 07:07:23.022384: step 184110, loss = 0.28 (1629.0 examples/sec; 0.079 sec/batch)
2017-06-02 07:07:23.874576: step 184120, loss = 0.36 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:07:24.761852: step 184130, loss = 0.32 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:07:25.619388: step 184140, loss = 0.26 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:07:26.503006: step 184150, loss = 0.30 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:07:27.339431: step 184160, loss = 0.51 (1530.3 examples/sec; 0.084 sec/batch)
2017-06-02 07:07:28.224988: step 184170, loss = 0.34 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:07:29.050672: step 184180, loss = 0.27 (1550.2 examples/sec; 0.083 sec/batch)
2017-06-02 07:07:29.932998: step 184190, loss = 0.30 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:07:30.950500: step 184200, loss = 0.31 (1258.0 examples/sec; 0.102 sec/batch)
2017-06-02 07:07:31.658582: step 184210, loss = 0.29 (1807.7 examples/sec; 0.071 sec/batch)
2017-06-02 07:07:32.544717: step 184220, loss = 0.29 (1444.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:07:33.416614: step 184230, loss = 0.32 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:07:34.293368: step 184240, loss = 0.35 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:07:35.175629: step 184250, loss = 0.23 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:07:36.067839: step 184260, loss = 0.25 (1434.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:07:36.935257: step 184270, loss = 0.33 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:07:37.813136: step 184280, loss = 0.36 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:07:38.677348: step 184290, loss = 0.31 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:07:39.649564: step 184300, loss = 0.27 (1316.6 examples/sec; 0.097 sec/batch)
2017-06-02 07:07:40.438895: step 184310, loss = 0.31 (1621.6 examples/sec; 0.079 sec/batch)
2017-06-02 07:07:41.281431: step 184320, loss = 0.28 (1519.2 examples/sec; 0.084 sec/batch)
2017-06-02 07:07:42.165740: step 184330, loss = 0.36 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:07:43.016583: step 184340, loss = 0.35 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:07:43.888047: step 184350, loss = 0.28 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:07:44.761884: step 184360, loss = 0.41 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:07:45.630926: step 184370, loss = 0.37 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:07:46.497776: step 184380, loss = 0.34 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:07:47.361366: step 184390, loss = 0.26 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:07:48.346266: step 184400, loss = 0.35 (1299.6 examples/sec; 0.098 sec/batch)
2017-06-02 07:07:49.119554: step 184410, loss = 0.26 (1655.3 examples/sec; 0.077 sec/batch)
2017-06-02 07:07:50.012740: step 184420, loss = 0.32 (1433.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:07:50.901519: step 184430, loss = 0.31 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:07:51.773333: step 184440, loss = 0.34 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:07:52.649645: step 184450, loss = 0.29 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:07:53.527537: step 184460, loss = 0.29 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:07:54.392927: step 184470, loss = 0.28 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:07:55.274147: step 184480, loss = 0.28 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:07:56.153534: step 184490, loss = 0.30 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:07:57.136951: step 184500, loss = 0.30 (1301.6 examples/sec; 0.098 sec/batch)
2017-06-02 07:07:57.921314: step 184510, loss = 0.31 (1631.9 examples/sec; 0.078 sec/batch)
2017-06-02 07:07:58.793765: step 184520, loss = 0.33 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:07:59.695740: step 184530, loss = 0.31 (1419.1 examples/sec; 0.090 sec/batch)
2017-06-02 07:08:00.585968: step 184540, loss = 0.26 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:08:01.461537: step 184550, loss = 0.31 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:08:02.328613: step 184560, loss = 0.34 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:08:03.210715: step 184570, loss = 0.24 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:08:04.081663: step 184580, loss = 0.34 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:08:04.973355: step 184590, loss = 0.30 (1435.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:08:05.972428: step 184600, loss = 0.23 (1281.2 examples/sec; 0.100 sec/batch)
2017-06-02 07:08:06.746381: step 184610, loss = 0.30 (1653.9 examples/sec; 0.077 sec/batch)
2017-06-02 07:08:07.621739: step 184620, loss = 0.37 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:08:08.509091: step 184630, loss = 0.27 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:08:09.399760: step 184640, loss = 0.27 (1437.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:08:10.283860: step 184650, loss = 0.29 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:08:11.176116: step 184660, loss = 0.34 (1434.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:08:12.068110: step 184670, loss = 0.37 (1434.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:08:12.959016: step 184680, loss = 0.37 (1436.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:08:13.830498: step 184690, loss = 0.38 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:08:14.802668: step 184700, loss = 0.31 (1316.6 examples/sec; 0.097 sec/batch)
2017-06-02 07:08:15.591057: step 184710, loss = 0.31 (1623.6 examples/sec; 0.079 sec/batch)
2017-06-02 07:08:16.484287: step 184720, loss = 0.31 (1433.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:08:17.355020: step 184730, loss = 0.38 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:08:18.233476: step 184740, loss = 0.26 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:08:19.099602: step 184750, loss = 0.31 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:08:19.956634: step 184760, loss = 0.34 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:08:20.831387: step 184770, loss = 0.31 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:08:21.699488: step 184780, loss = 0.32 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:08:22.560426: step 184790, loss = 0.29 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:08:23.504494: step 184800, loss = 0.28 (1355.8 examples/sec; 0.094 sec/batch)
2017-06-02 07:08:24.301907: step 184810, loss = 0.28 (1605.2 examples/sec; 0.080 sec/batch)
2017-06-02 07:08:25.200986: step 184820, loss = 0.24 (1423.7 examples/sec; 0.090 sec/batch)
2017-06-02 07:08:26.049591: step 184830, loss = 0.33 (1508.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:08:26.927290: step 184840, loss = 0.35 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:08:27.810109: step 184850, loss = 0.29 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:08:28.693213: step 184860, loss = 0.36 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:08:29.556078: step 184870, loss = 0.28 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:08:30.428942: step 184880, loss = 0.34 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:08:31.331147: step 184890, loss = 0.34 (1418.8 examples/sec; 0.090 sec/batch)
2017-06-02 07:08:32.309343: step 184900, loss = 0.30 (1308.5 examples/sec; 0.098 sec/batch)
2017-06-02 07:08:33.087705: step 184910, loss = 0.30 (1644.5 examples/sec; 0.078 sec/batch)
2017-06-02 07:08:33.956933: step 184920, loss = 0.28 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:08:34.825870: step 184930, loss = 0.31 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:08:35.703647: step 184940, loss = 0.22 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:08:36.565562: step 184950, loss = 0.36 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:08:37.434644: step 184960, loss = 0.33 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:08:38.305689: step 184970, loss = 0.30 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:08:39.171206: step 184980, loss = 0.32 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:08:40.019680: step 184990, loss = 0.25 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:08:40.965528: step 185000, loss = 0.31 (1353.3 examples/sec; 0.095 sec/batch)
2017-06-02 07:08:41.724421: step 185010, loss = 0.24 (1686.7 examples/sec; 0.076 sec/batch)
2017-06-02 07:08:42.592541: step 185020, loss = 0.34 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:08:43.467546: step 185030, loss = 0.32 (1462.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:08:44.322531: step 185040, loss = 0.39 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:08:45.197295: step 185050, loss = 0.38 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:08:46.053988: step 185060, loss = 0.30 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:08:46.904208: step 185070, loss = 0.31 (1505.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:08:47.776946: step 185080, loss = 0.30 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:08:48.661004: step 185090, loss = 0.36 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:08:49.637273: step 185100, loss = 0.29 (1311.1 examples/sec; 0.098 sec/batch)
2017-06-02 07:08:50.381191: step 185110, loss = 0.37 (1720.7 examples/sec; 0.074 sec/batch)
2017-06-02 07:08:51.239863: step 185120, loss = 0.28 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:08:52.092996: step 185130, loss = 0.26 (1500.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:08:52.979376: step 185140, loss = 0.37 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:08:53.864032: step 185150, loss = 0.25 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:08:54.742171: step 185160, loss = 0.28 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:08:55.594435: step 185170, loss = 0.27 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:08:56.491574: step 185180, loss = 0.28 (1426.7 examples/sec; 0.090 sec/batch)
2017-06-02 07:08:57.359407: step 185190, loss = 0.44 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:08:58.308470: step 185200, loss = 0.26 (1348.7 examples/sec; 0.095 sec/batch)
2017-06-02 07:08:59.093701: step 185210, loss = 0.29 (1630.1 examples/sec; 0.079 sec/batch)
2017-06-02 07:08:59.994912: step 185220, loss = 0.38 (1420.3 examples/sec; 0.090 sec/batch)
2017-06-02 07:09:00.872616: step 185230, loss = 0.25 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:09:01.716548: step 185240, loss = 0.31 (1516.7 examples/sec; 0.084 sec/batch)
2017-06-02 07:09:02.591125: step 185250, loss = 0.31 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:09:03.462361: step 185260, loss = 0.25 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:09:04.326003: step 185270, loss = 0.27 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:09:05.234286: step 185280, loss = 0.27 (1409.2 examples/sec; 0.091 sec/batch)
2017-06-02 07:09:06.117998: step 185290, loss = 0.36 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:09:07.160855: step 185300, loss = 0.33 (1227.4 examples/sec; 0.104 sec/batch)
2017-06-02 07:09:07.874009: step 185310, loss = 0.33 (1794.9 examples/sec; 0.071 sec/batch)
2017-06-02 07:09:08.762811: step 185320, loss = 0.29 (1440.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:09:09.625322: step 185330, loss = 0.31 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:09:10.486428: step 185340, loss = 0.26 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:09:11.368202: step 185350, loss = 0.29 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:09:12.236248: step 185360, loss = 0.34 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:09:13.093195: step 185370, loss = 0.27 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:09:13.941812: step 185380, loss = 0.27 (1508.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:09:14.780364: step 185390, loss = 0.34 (1526.4 examples/sec; 0.084 sec/batch)
2017-06-02 07:09:15.745943: step 185400, loss = 0.32 (1325.6 examples/sec; 0.097 sec/batch)
2017-06-02 07:09:16.497069: step 185410, loss = 0.30 (1704.1 examples/sec; 0.075 sec/batch)
2017-06-02 07:09:17.331126: step 185420, loss = 0.31 (1534.7 examples/sec; 0.083 sec/batch)
2017-06-02 07:09:18.223861: step 185430, loss = 0.23 (1433.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:09:19.090284: step 185440, loss = 0.24 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:09:19.984064: step 185450, loss = 0.31 (1432.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:09:20.873630: step 185460, loss = 0.26 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:09:21.736727: step 185470, loss = 0.23 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:09:22.602687: step 185480, loss = 0.21 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:09:23.457593: step 185490, loss = 0.37 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:09:24.423177: step 185500, loss = 0.25 (1325.6 examples/sec; 0.097 sec/batch)
2017-06-02 07:09:25.200630: step 185510, loss = 0.31 (1646.4 examples/sec; 0.078 sec/batch)
2017-06-02 07:09:26.075822: step 185520, loss = 0.27 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:09:26.923939: step 185530, loss = 0.34 (1509.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:09:27.775479: step 185540, loss = 0.27 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:09:28.627922: step 185550, loss = 0.33 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:09:29.508243: step 185560, loss = 0.39 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:09:30.375111: step 185570, loss = 0.31 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:09:31.250014: step 185580, loss = 0.37 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:09:32.119840: step 185590, loss = 0.31 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:09:33.106387: step 185600, loss = 0.23 (1297.4 examples/sec; 0.099 sec/batch)
2017-06-02 07:09:33.851255: step 185610, loss = 0.31 (1718.4 examples/sec; 0.074 sec/batch)
2017-06-02 07:09:34.730397: step 185620, loss = 0.32 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:09:35.615524: step 185630, loss = 0.24 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:09:36.478927: step 185640, loss = 0.43 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:09:37.369246: step 185650, loss = 0.30 (1437.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:09:38.248875: step 185660, loss = 0.25 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:09:39.124778: step 185670, loss = 0.23 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:09:39.985972: step 185680, loss = 0.33 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:09:40.890074: step 185690, loss = 0.28 (1415.8 examples/sec; 0.090 sec/batch)
2017-06-02 07:09:41.849162: step 185700, loss = 0.30 (1334.6 examples/sec; 0.096 sec/batch)
2017-06-02 07:09:42.624441: step 185710, loss = 0.35 (1651.0 examples/sec; 0.078 sec/batch)
2017-06-02 07:09:43.500546: step 185720, loss = 0.23 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:09:44.360943: step 185730, loss = 0.23 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:09:45.241821: step 185740, loss = 0.43 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:09:46.098686: step 185750, loss = 0.31 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:09:46.973854: step 185760, loss = 0.45 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:09:47.835371: step 185770, loss = 0.33 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:09:48.701257: step 185780, loss = 0.30 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:09:49.580400: step 185790, loss = 0.35 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:09:50.555745: step 185800, loss = 0.26 (1312.4 examples/sec; 0.098 sec/batch)
2017-06-02 07:09:51.324712: step 185810, loss = 0.36 (1664.5 examples/sec; 0.077 sec/batch)
2017-06-02 07:09:52.301033: step 185820, loss = 0.30 (1311.0 examples/sec; 0.098 sec/batch)
2017-06-02 07:09:53.151910: step 185830, loss = 0.27 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:09:53.993444: step 185840, loss = 0.23 (1521.0 examples/sec; 0.084 sec/batch)
2017-06-02 07:09:54.865640: step 185850, loss = 0.33 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:09:55.753257: step 185860, loss = 0.29 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:09:56.637526: step 185870, loss = 0.20 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:09:57.525676: step 185880, loss = 0.27 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:09:58.401226: step 185890, loss = 0.25 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:09:59.362182: step 185900, loss = 0.30 (1332.0 examples/sec; 0.096 sec/batch)
2017-06-02 07:10:00.102363: step 185910, loss = 0.28 (1729.3 examples/sec; 0.074 sec/batch)
2017-06-02 07:10:00.965094: step 185920, loss = 0.23 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:10:01.834810: step 185930, loss = 0.31 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:10:02.702562: step 185940, loss = 0.30 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:10:03.570963: step 185950, loss = 0.32 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:10:04.452371: step 185960, loss = 0.32 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:10:05.294651: step 185970, loss = 0.24 (1519.7 examples/sec; 0.084 sec/batch)
2017-06-02 07:10:06.156852: step 185980, loss = 0.28 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:10:07.037786: step 185990, loss = 0.29 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:10:08.000809: step 186000, loss = 0.28 (1329.1 examples/sec; 0.096 sec/batch)
2017-06-02 07:10:08.767611: step 186010, loss = 0.32 (1669.3 examples/sec; 0.077 sec/batch)
2017-06-02 07:10:09.615485: step 186020, loss = 0.34 (1509.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:10:10.480893: step 186030, loss = 0.39 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:10:11.313832: step 186040, loss = 0.30 (1536.7 examples/sec; 0.083 sec/batch)
2017-06-02 07:10:12.191231: step 186050, loss = 0.21 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:10:13.066860: step 186060, loss = 0.34 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:10:13.910661: step 186070, loss = 0.28 (1517.0 examples/sec; 0.084 sec/batch)
2017-06-02 07:10:14.764890: step 186080, loss = 0.41 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:10:15.652812: step 186090, loss = 0.26 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:10:16.625298: step 186100, loss = 0.31 (1316.2 examples/sec; 0.097 sec/batch)
2017-06-02 07:10:17.403059: step 186110, loss = 0.27 (1645.7 examples/sec; 0.078 sec/batch)
2017-06-02 07:10:18.257320: step 186120, loss = 0.36 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:10:19.122888: step 186130, loss = 0.25 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:10:19.976802: step 186140, loss = 0.28 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:10:20.863567: step 186150, loss = 0.27 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:10:21.711717: step 186160, loss = 0.26 (1509.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:10:22.578525: step 186170, loss = 0.28 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:10:23.439324: step 186180, loss = 0.32 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:10:24.312378: step 186190, loss = 0.32 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:10:25.283065: step 186200, loss = 0.33 (1318.7 examples/sec; 0.097 sec/batch)
2017-06-02 07:10:26.050197: step 186210, loss = 0.34 (1668.6 examples/sec; 0.077 sec/batch)
2017-06-02 07:10:26.911124: step 186220, loss = 0.37 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:10:27.777711: step 186230, loss = 0.33 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:10:28.648240: step 186240, loss = 0.30 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:10:29.525125: step 186250, loss = 0.36 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:10:30.380453: step 186260, loss = 0.26 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:10:31.255406: step 186270, loss = 0.30 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:10:32.126559: step 186280, loss = 0.36 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:10:33.009012: step 186290, loss = 0.24 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:10:33.972520: step 186300, loss = 0.26 (1328.5 examples/sec; 0.096 sec/batch)
2017-06-02 07:10:34.760558: step 186310, loss = 0.29 (1624.3 examples/sec; 0.079 sec/batch)
2017-06-02 07:10:35.638623: step 186320, loss = 0.27 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:10:36.510874: step 186330, loss = 0.27 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:10:37.370318: step 186340, loss = 0.35 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:10:38.235282: step 186350, loss = 0.33 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:10:39.107789: step 186360, loss = 0.34 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:10:39.965841: step 186370, loss = 0.26 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:10:40.834194: step 186380, loss = 0.34 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:10:41.676872: step 186390, loss = 0.37 (1519.0 examples/sec; 0.084 sec/batch)
2017-06-02 07:10:42.711008: step 186400, loss = 0.35 (1237.7 examples/sec; 0.103 sec/batch)
2017-06-02 07:10:43.429258: step 186410, loss = 0.28 (1782.1 examples/sec; 0.072 sec/batch)
2017-06-02 07:10:44.265909: step 186420, loss = 0.31 (1529.9 examples/sec; 0.084 sec/batch)
2017-06-02 07:10:45.114066: step 186430, loss = 0.24 (1509.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:10:45.963977: step 186440, loss = 0.28 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:10:46.842850: step 186450, loss = 0.35 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:10:47.688888: step 186460, loss = 0.39 (1512.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:10:48.530252: step 186470, loss = 0.38 (1521.3 examples/sec; 0.084 sec/batch)
2017-06-02 07:10:49.400453: step 186480, loss = 0.29 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:10:50.277539: step 186490, loss = 0.23 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:10:51.249647: step 186500, loss = 0.31 (1316.7 examples/sec; 0.097 sec/batch)
2017-06-02 07:10:52.012281: step 186510, loss = 0.34 (1678.4 examples/sec; 0.076 sec/batch)
2017-06-02 07:10:52.892492: step 186520, loss = 0.25 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:10:53.787485: step 186530, loss = 0.31 (1430.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:10:54.625907: step 186540, loss = 0.36 (1526.7 examples/sec; 0.084 sec/batch)
2017-06-02 07:10:55.469644: step 186550, loss = 0.31 (1517.1 examples/sec; 0.084 sec/batch)
2017-06-02 07:10:56.351181: step 186560, loss = 0.31 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:10:57.224702: step 186570, loss = 0.37 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:10:58.087716: step 186580, loss = 0.27 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:10:58.956100: step 186590, loss = 0.28 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:10:59.922891: step 186600, loss = 0.31 (1324.0 examples/sec; 0.097 sec/batch)
2017-06-02 07:11:00.695904: step 186610, loss = 0.44 (1655.8 examples/sec; 0.077 sec/batch)
2017-06-02 07:11:01.576156: step 186620, loss = 0.25 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:11:02.461812: step 186630, loss = 0.32 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:11:03.348541: step 186640, loss = 0.40 (1443.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:11:04.248487: step 186650, loss = 0.25 (1422.3 examples/sec; 0.090 sec/batch)
2017-06-02 07:11:05.139967: step 186660, loss = 0.28 (1435.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:11:05.981849: step 186670, loss = 0.29 (1520.4 examples/sec; 0.084 sec/batch)
2017-06-02 07:11:06.863689: step 186680, loss = 0.25 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:11:07.749708: step 186690, loss = 0.30 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:11:08.752507: step 186700, loss = 0.26 (1276.4 examples/sec; 0.100 sec/batch)
2017-06-02 07:11:09.481654: step 186710, loss = 0.32 (1755.5 examples/sec; 0.073 sec/batch)
2017-06-02 07:11:10.372755: step 186720, loss = 0.29 (1436.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:11:11.254144: step 186730, loss = 0.26 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:11:12.114501: step 186740, loss = 0.31 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:11:12.978253: step 186750, loss = 0.30 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:11:13.841928: step 186760, loss = 0.41 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:11:14.700213: step 186770, loss = 0.31 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:11:15.551282: step 186780, loss = 0.25 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:11:16.408572: step 186790, loss = 0.36 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:11:17.400069: step 186800, loss = 0.32 (1291.0 examples/sec; 0.099 sec/batch)
2017-06-02 07:11:18.144767: step 186810, loss = 0.24 (1718.8 examples/sec; 0.074 sec/batch)
2017-06-02 07:11:19.025870: step 186820, loss = 0.29 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:11:19.855879: step 186830, loss = 0.25 (1542.1 examples/sec; 0.083 sec/batch)
2017-06-02 07:11:20.723322: step 186840, loss = 0.32 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:11:21.597871: step 186850, loss = 0.32 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:11:22.463666: step 186860, loss = 0.30 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:11:23.314434: step 186870, loss = 0.26 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:11:24.160376: step 186880, loss = 0.34 (1513.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:11:25.035372: step 186890, loss = 0.31 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:11:26.007904: step 186900, loss = 0.40 (1316.1 examples/sec; 0.097 sec/batch)
2017-06-02 07:11:26.790451: step 186910, loss = 0.31 (1635.7 examples/sec; 0.078 sec/batch)
2017-06-02 07:11:27.629269: step 186920, loss = 0.26 (1526.0 examples/sec; 0.084 sec/batch)
2017-06-02 07:11:28.492944: step 186930, loss = 0.39 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:11:29.360274: step 186940, loss = 0.35 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:11:30.219770: step 186950, loss = 0.39 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:11:31.093804: step 186960, loss = 0.30 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:11:31.944280: step 186970, loss = 0.31 (1505.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:11:32.804245: step 186980, loss = 0.34 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:11:33.669900: step 186990, loss = 0.26 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:11:34.638506: step 187000, loss = 0.34 (1321.5 examples/sec; 0.097 sec/batch)
2017-06-02 07:11:35.425320: step 187010, loss = 0.31 (1626.8 examples/sec; 0.079 sec/batch)
2017-06-02 07:11:36.275381: step 187020, loss = 0.33 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:11:37.152656: step 187030, loss = 0.42 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:11:38.013590: step 187040, loss = 0.26 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:11:38.872102: step 187050, loss = 0.24 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:11:39.719630: step 187060, loss = 0.34 (1510.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:11:40.623494: step 187070, loss = 0.31 (1416.1 examples/sec; 0.090 sec/batch)
2017-06-02 07:11:41.518590: step 187080, loss = 0.22 (1430.0 examples/sec; 0.090 sec/batch)
2017-06-02 07:11:42.411828: step 187090, loss = 0.29 (1433.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:11:43.408871: step 187100, loss = 0.28 (1283.8 examples/sec; 0.100 sec/batch)
2017-06-02 07:11:44.184546: step 187110, loss = 0.31 (1650.2 examples/sec; 0.078 sec/batch)
2017-06-02 07:11:45.085009: step 187120, loss = 0.30 (1421.5 examples/sec; 0.090 sec/batch)
2017-06-02 07:11:45.971188: step 187130, loss = 0.34 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:11:46.852984: step 187140, loss = 0.33 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:11:47.710772: step 187150, loss = 0.27 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:11:48.569429: step 187160, loss = 0.25 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:11:49.451903: step 187170, loss = 0.38 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:11:50.343423: step 187180, loss = 0.27 (1435.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:11:51.222724: step 187190, loss = 0.35 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:11:52.193697: step 187200, loss = 0.27 (1318.3 examples/sec; 0.097 sec/batch)
2017-06-02 07:11:52.974308: step 187210, loss = 0.31 (1639.7 examples/sec; 0.078 sec/batch)
2017-06-02 07:11:53.821841: step 187220, loss = 0.31 (1510.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:11:54.664957: step 187230, loss = 0.32 (1518.2 examples/sec; 0.084 sec/batch)
2017-06-02 07:11:55.519589: step 187240, loss = 0.23 (1497.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:11:56.401226: step 187250, loss = 0.30 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:11:57.272568: step 187260, loss = 0.29 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:11:58.145117: step 187270, loss = 0.41 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:11:59.007705: step 187280, loss = 0.36 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:11:59.878738: step 187290, loss = 0.24 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:12:00.819064: step 187300, loss = 0.22 (1361.2 examples/sec; 0.094 sec/batch)
2017-06-02 07:12:01.602954: step 187310, loss = 0.35 (1632.9 examples/sec; 0.078 sec/batch)
2017-06-02 07:12:02.461145: step 187320, loss = 0.32 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:12:03.321267: step 187330, loss = 0.34 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:12:04.182788: step 187340, loss = 0.31 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:12:05.069726: step 187350, loss = 0.26 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:12:05.969549: step 187360, loss = 0.34 (1422.5 examples/sec; 0.090 sec/batch)
2017-06-02 07:12:06.830178: step 187370, loss = 0.44 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:12:07.725824: step 187380, loss = 0.28 (1429.1 examples/sec; 0.090 sec/batch)
2017-06-02 07:12:08.577671: step 187390, loss = 0.28 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:12:09.538958: step 187400, loss = 0.38 (1331.5 examples/sec; 0.096 sec/batch)
2017-06-02 07:12:10.299068: step 187410, loss = 0.29 (1684.0 examples/sec; 0.076 sec/batch)
2017-06-02 07:12:11.146663: step 187420, loss = 0.27 (1510.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:12:11.997359: step 187430, loss = 0.38 (1504.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:12:12.857005: step 187440, loss = 0.28 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:12:13.739995: step 187450, loss = 0.26 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:12:14.584615: step 187460, loss = 0.27 (1515.5 examples/sec; 0.084 sec/batch)
2017-06-02 07:12:15.445429: step 187470, loss = 0.34 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:12:16.312420: step 187480, loss = 0.32 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:12:17.199448: step 187490, loss = 0.25 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:12:18.191983: step 187500, loss = 0.26 (1289.6 examples/sec; 0.099 sec/batch)
2017-06-02 07:12:18.961426: step 187510, loss = 0.29 (1663.5 examples/sec; 0.077 sec/batch)
2017-06-02 07:12:19.825034: step 187520, loss = 0.28 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:12:20.665442: step 187530, loss = 0.30 (1523.1 examples/sec; 0.084 sec/batch)
2017-06-02 07:12:21.517836: step 187540, loss = 0.36 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:12:22.377469: step 187550, loss = 0.24 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:12:23.222056: step 187560, loss = 0.31 (1515.5 examples/sec; 0.084 sec/batch)
2017-06-02 07:12:24.099499: step 187570, loss = 0.23 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:12:24.990508: step 187580, loss = 0.44 (1436.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:12:25.861392: step 187590, loss = 0.25 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:12:26.844247: step 187600, loss = 0.31 (1302.3 examples/sec; 0.098 sec/batch)
2017-06-02 07:12:27.617119: step 187610, loss = 0.35 (1656.2 examples/sec; 0.077 sec/batch)
2017-06-02 07:12:28.474888: step 187620, loss = 0.27 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:12:29.352180: step 187630, loss = 0.30 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:12:30.227787: step 187640, loss = 0.27 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:12:31.101663: step 187650, loss = 0.30 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:12:31.951884: step 187660, loss = 0.30 (1505.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:12:32.820257: step 187670, loss = 0.32 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:12:33.680304: step 187680, loss = 0.28 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:12:34.567432: step 187690, loss = 0.33 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:12:35.570961: step 187700, loss = 0.27 (1275.5 examples/sec; 0.100 sec/batch)
2017-06-02 07:12:36.316350: step 187710, loss = 0.29 (1717.2 examples/sec; 0.075 sec/batch)
2017-06-02 07:12:37.188101: step 187720, loss = 0.27 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:12:38.032026: step 187730, loss = 0.34 (1516.7 examples/sec; 0.084 sec/batch)
2017-06-02 07:12:38.907698: step 187740, loss = 0.23 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:12:39.802217: step 187750, loss = 0.33 (1430.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:12:40.650921: step 187760, loss = 0.28 (1508.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:12:41.518244: step 187770, loss = 0.29 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:12:42.392798: step 187780, loss = 0.32 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:12:43.248324: step 187790, loss = 0.30 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:12:44.246402: step 187800, loss = 0.33 (1282.5 examples/sec; 0.100 sec/batch)
2017-06-02 07:12:45.003501: step 187810, loss = 0.29 (1690.7 examples/sec; 0.076 sec/batch)
2017-06-02 07:12:45.894479: step 187820, loss = 0.27 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:12:46.778087: step 187830, loss = 0.28 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:12:47.659854: step 187840, loss = 0.34 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:12:48.541090: step 187850, loss = 0.31 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:12:49.417159: step 187860, loss = 0.28 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:12:50.276596: step 187870, loss = 0.32 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:12:51.160899: step 187880, loss = 0.38 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:12:52.021442: step 187890, loss = 0.23 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:12:53.011254: step 187900, loss = 0.35 (1293.2 examples/sec; 0.099 sec/batch)
2017-06-02 07:12:53.800361: step 187910, loss = 0.22 (1622.1 examples/sec; 0.079 sec/batch)
2017-06-02 07:12:54.693744: step 187920, loss = 0.37 (1432.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:12:55.575995: step 187930, loss = 0.31 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:12:56.452920: step 187940, loss = 0.27 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:12:57.332814: step 187950, loss = 0.27 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:12:58.199166: step 187960, loss = 0.29 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:12:59.084096: step 187970, loss = 0.28 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:12:59.920198: step 187980, loss = 0.28 (1530.9 examples/sec; 0.084 sec/batch)
2017-06-02 07:13:00.793390: step 187990, loss = 0.37 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:13:01.750375: step 188000, loss = 0.27 (1337.5 examples/sec; 0.096 sec/batch)
2017-06-02 07:13:02.509881: step 188010, loss = 0.42 (1685.3 examples/sec; 0.076 sec/batch)
2017-06-02 07:13:03.380109: step 188020, loss = 0.30 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:13:04.276356: step 188030, loss = 0.32 (1428.2 examples/sec; 0.090 sec/batch)
2017-06-02 07:13:05.175741: step 188040, loss = 0.30 (1423.2 examples/sec; 0.090 sec/batch)
2017-06-02 07:13:06.045807: step 188050, loss = 0.27 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:13:06.926273: step 188060, loss = 0.25 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:13:07.820769: step 188070, loss = 0.36 (1431.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:13:08.703209: step 188080, loss = 0.28 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:13:09.592333: step 188090, loss = 0.35 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:13:10.560634: step 188100, loss = 0.33 (1321.9 examples/sec; 0.097 sec/batch)
2017-06-02 07:13:11.330194: step 188110, loss = 0.34 (1663.3 examples/sec; 0.077 sec/batch)
2017-06-02 07:13:12.200744: step 188120, loss = 0.37 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:13:13.086052: step 188130, loss = 0.34 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:13:13.957667: step 188140, loss = 0.41 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:13:14.823599: step 188150, loss = 0.38 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:13:15.670419: step 188160, loss = 0.38 (1511.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:13:16.560123: step 188170, loss = 0.28 (1438.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:13:17.443844: step 188180, loss = 0.36 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:13:18.332017: step 188190, loss = 0.25 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:13:19.304076: step 188200, loss = 0.25 (1316.0 examples/sec; 0.097 sec/batch)
2017-06-02 07:13:20.092330: step 188210, loss = 0.29 (1623.8 examples/sec; 0.079 sec/batch)
2017-06-02 07:13:20.991572: step 188220, loss = 0.29 (1423.4 examples/sec; 0.090 sec/batch)
2017-06-02 07:13:21.871958: step 188230, loss = 0.33 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:13:22.740833: step 188240, loss = 0.25 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:13:23.629724: step 188250, loss = 0.30 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:13:24.504079: step 188260, loss = 0.27 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:13:25.389164: step 188270, loss = 0.22 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:13:26.264800: step 188280, loss = 0.40 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:13:27.148037: step 188290, loss = 0.34 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:13:28.097972: step 188300, loss = 0.26 (1347.5 examples/sec; 0.095 sec/batch)
2017-06-02 07:13:28.859239: step 188310, loss = 0.33 (1681.4 examples/sec; 0.076 sec/batch)
2017-06-02 07:13:29.710874: step 188320, loss = 0.39 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:13:30.556102: step 188330, loss = 0.37 (1514.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:13:31.388341: step 188340, loss = 0.32 (1538.0 examples/sec; 0.083 sec/batch)
2017-06-02 07:13:32.236074: step 188350, loss = 0.27 (1509.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:13:33.084706: step 188360, loss = 0.41 (1508.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:13:33.970208: step 188370, loss = 0.27 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:13:34.842193: step 188380, loss = 0.36 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:13:35.706229: step 188390, loss = 0.31 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:13:36.651876: step 188400, loss = 0.29 (1353.6 examples/sec; 0.095 sec/batch)
2017-06-02 07:13:37.421854: step 188410, loss = 0.27 (1662.4 examples/sec; 0.077 sec/batch)
2017-06-02 07:13:38.278795: step 188420, loss = 0.34 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:13:39.131636: step 188430, loss = 0.24 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:13:39.993955: step 188440, loss = 0.33 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:13:40.841166: step 188450, loss = 0.25 (1510.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:13:41.707202: step 188460, loss = 0.25 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:13:42.579642: step 188470, loss = 0.37 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:13:43.440904: step 188480, loss = 0.22 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:13:44.295320: step 188490, loss = 0.24 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:13:45.265818: step 188500, loss = 0.28 (1318.9 examples/sec; 0.097 sec/batch)
2017-06-02 07:13:46.029311: step 188510, loss = 0.32 (1676.5 examples/sec; 0.076 sec/batch)
2017-06-02 07:13:46.872037: step 188520, loss = 0.25 (1518.9 examples/sec; 0.084 sec/batch)
2017-06-02 07:13:47.740387: step 188530, loss = 0.27 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:13:48.622258: step 188540, loss = 0.39 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:13:49.480686: step 188550, loss = 0.27 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:13:50.334270: step 188560, loss = 0.31 (1499.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:13:51.209139: step 188570, loss = 0.30 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:13:52.055488: step 188580, loss = 0.29 (1512.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:13:52.946712: step 188590, loss = 0.41 (1436.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:13:53.910190: step 188600, loss = 0.33 (1328.5 examples/sec; 0.096 sec/batch)
2017-06-02 07:13:54.683099: step 188610, loss = 0.29 (1656.1 examples/sec; 0.077 sec/batch)
2017-06-02 07:13:55.527769: step 188620, loss = 0.22 (1515.4 examples/sec; 0.084 sec/batch)
2017-06-02 07:13:56.383372: step 188630, loss = 0.33 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:13:57.215079: step 188640, loss = 0.27 (1539.0 examples/sec; 0.083 sec/batch)
2017-06-02 07:13:58.064285: step 188650, loss = 0.33 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:13:58.941940: step 188660, loss = 0.25 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:13:59.831153: step 188670, loss = 0.24 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:14:00.692300: step 188680, loss = 0.27 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:14:01.534763: step 188690, loss = 0.31 (1519.4 examples/sec; 0.084 sec/batch)
2017-06-02 07:14:02.516035: step 188700, loss = 0.36 (1304.4 examples/sec; 0.098 sec/batch)
2017-06-02 07:14:03.248406: step 188710, loss = 0.36 (1747.8 examples/sec; 0.073 sec/batch)
2017-06-02 07:14:04.069713: step 188720, loss = 0.37 (1558.5 examples/sec; 0.082 sec/batch)
2017-06-02 07:14:04.906578: step 188730, loss = 0.29 (1529.5 examples/sec; 0.084 sec/batch)
2017-06-02 07:14:05.754071: step 188740, loss = 0.31 (1510.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:14:06.639195: step 188750, loss = 0.35 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:14:07.518248: step 188760, loss = 0.29 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:14:08.387648: step 188770, loss = 0.33 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:14:09.248958: step 188780, loss = 0.33 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:14:10.107156: step 188790, loss = 0.32 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:14:11.092153: step 188800, loss = 0.33 (1299.5 examples/sec; 0.098 sec/batch)
2017-06-02 07:14:11.882603: step 188810, loss = 0.32 (1619.4 examples/sec; 0.079 sec/batch)
2017-06-02 07:14:12.779151: step 188820, loss = 0.34 (1427.7 examples/sec; 0.090 sec/batch)
2017-06-02 07:14:13.626312: step 188830, loss = 0.24 (1510.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:14:14.502940: step 188840, loss = 0.26 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:14:15.383816: step 188850, loss = 0.23 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:14:16.278624: step 188860, loss = 0.27 (1430.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:14:17.140328: step 188870, loss = 0.31 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:14:18.024107: step 188880, loss = 0.29 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:14:18.888309: step 188890, loss = 0.25 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:14:19.869052: step 188900, loss = 0.33 (1305.1 examples/sec; 0.098 sec/batch)
2017-06-02 07:14:20.646983: step 188910, loss = 0.37 (1645.4 examples/sec; 0.078 sec/batch)
2017-06-02 07:14:21.530828: step 188920, loss = 0.31 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:14:22.398129: step 188930, loss = 0.37 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:14:23.284411: step 188940, loss = 0.34 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:14:24.199196: step 188950, loss = 0.23 (1399.2 examples/sec; 0.091 sec/batch)
2017-06-02 07:14:25.062054: step 188960, loss = 0.30 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:14:25.933408: step 188970, loss = 0.25 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:14:26.787497: step 188980, loss = 0.23 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:14:27.638978: step 188990, loss = 0.33 (1503.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:14:28.602615: step 189000, loss = 0.25 (1328.3 examples/sec; 0.096 sec/batch)
2017-06-02 07:14:29.377849: step 189010, loss = 0.27 (1651.1 examples/sec; 0.078 sec/batch)
2017-06-02 07:14:30.229282: step 189020, loss = 0.28 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:14:31.081976: step 189030, loss = 0.31 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:14:31.947898: step 189040, loss = 0.35 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:14:32.803232: step 189050, loss = 0.29 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:14:33.672384: step 189060, loss = 0.32 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:14:34.512302: step 189070, loss = 0.27 (1523.9 examples/sec; 0.084 sec/batch)
2017-06-02 07:14:35.392164: step 189080, loss = 0.30 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:14:36.258869: step 189090, loss = 0.32 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:14:37.218256: step 189100, loss = 0.26 (1334.2 examples/sec; 0.096 sec/batch)
2017-06-02 07:14:38.001478: step 189110, loss = 0.31 (1634.3 examples/sec; 0.078 sec/batch)
2017-06-02 07:14:38.856957: step 189120, loss = 0.24 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:14:39.741689: step 189130, loss = 0.35 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:14:40.614594: step 189140, loss = 0.40 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:14:41.503252: step 189150, loss = 0.36 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:14:42.355501: step 189160, loss = 0.30 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:14:43.213460: step 189170, loss = 0.34 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:14:44.083843: step 189180, loss = 0.32 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:14:44.930924: step 189190, loss = 0.28 (1511.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:14:45.870361: step 189200, loss = 0.24 (1362.5 examples/sec; 0.094 sec/batch)
2017-06-02 07:14:46.633108: step 189210, loss = 0.20 (1678.2 examples/sec; 0.076 sec/batch)
2017-06-02 07:14:47.508131: step 189220, loss = 0.30 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:14:48.368497: step 189230, loss = 0.33 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:14:49.221861: step 189240, loss = 0.29 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:14:50.102471: step 189250, loss = 0.35 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:14:50.968329: step 189260, loss = 0.30 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:14:51.837321: step 189270, loss = 0.26 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:14:52.689710: step 189280, loss = 0.35 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:14:53.552171: step 189290, loss = 0.27 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:14:54.530844: step 189300, loss = 0.30 (1307.9 examples/sec; 0.098 sec/batch)
2017-06-02 07:14:55.291362: step 189310, loss = 0.31 (1683.0 examples/sec; 0.076 sec/batch)
2017-06-02 07:14:56.181847: step 189320, loss = 0.37 (1437.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:14:57.029531: step 189330, loss = 0.33 (1510.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:14:57.860878: step 189340, loss = 0.39 (1539.7 examples/sec; 0.083 sec/batch)
2017-06-02 07:14:58.724334: step 189350, loss = 0.27 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:14:59.610993: step 189360, loss = 0.38 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:15:00.479161: step 189370, loss = 0.30 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:15:01.355576: step 189380, loss = 0.23 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:15:02.214663: step 189390, loss = 0.29 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:15:03.172044: step 189400, loss = 0.26 (1337.0 examples/sec; 0.096 sec/batch)
2017-06-02 07:15:03.939443: step 189410, loss = 0.22 (1668.0 examples/sec; 0.077 sec/batch)
2017-06-02 07:15:04.786746: step 189420, loss = 0.29 (1510.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:15:05.658959: step 189430, loss = 0.25 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:15:06.542568: step 189440, loss = 0.26 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:15:07.447523: step 189450, loss = 0.28 (1414.4 examples/sec; 0.090 sec/batch)
2017-06-02 07:15:08.314456: step 189460, loss = 0.24 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:15:09.159081: step 189470, loss = 0.28 (1515.5 examples/sec; 0.084 sec/batch)
2017-06-02 07:15:10.029013: step 189480, loss = 0.33 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:15:10.912805: step 189490, loss = 0.27 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:15:11.888534: step 189500, loss = 0.39 (1311.8 examples/sec; 0.098 sec/batch)
2017-06-02 07:15:12.658509: step 189510, loss = 0.33 (1662.4 examples/sec; 0.077 sec/batch)
2017-06-02 07:15:13.547934: step 189520, loss = 0.30 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:15:14.413167: step 189530, loss = 0.27 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:15:15.280232: step 189540, loss = 0.27 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:15:16.149312: step 189550, loss = 0.28 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:15:17.025617: step 189560, loss = 0.36 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:15:17.882769: step 189570, loss = 0.31 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:15:18.763178: step 189580, loss = 0.37 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:15:19.627721: step 189590, loss = 0.31 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:15:20.633117: step 189600, loss = 0.32 (1273.1 examples/sec; 0.101 sec/batch)
2017-06-02 07:15:21.349619: step 189610, loss = 0.31 (1786.5 examples/sec; 0.072 sec/batch)
2017-06-02 07:15:22.191103: step 189620, loss = 0.29 (1521.2 examples/sec; 0.084 sec/batch)
2017-06-02 07:15:23.073206: step 189630, loss = 0.35 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:15:23.934064: step 189640, loss = 0.25 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:15:24.770931: step 189650, loss = 0.28 (1529.5 examples/sec; 0.084 sec/batch)
2017-06-02 07:15:25.661669: step 189660, loss = 0.29 (1437.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:15:26.512210: step 189670, loss = 0.21 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:15:27.367670: step 189680, loss = 0.26 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:15:28.258390: step 189690, loss = 0.27 (1437.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:15:29.209299: step 189700, loss = 0.30 (1346.1 examples/sec; 0.095 sec/batch)
2017-06-02 07:15:29.977688: step 189710, loss = 0.31 (1665.8 examples/sec; 0.077 sec/batch)
2017-06-02 07:15:30.828353: step 189720, loss = 0.29 (1504.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:15:31.699475: step 189730, loss = 0.38 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:15:32.590479: step 189740, loss = 0.28 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:15:33.441768: step 189750, loss = 0.33 (1503.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:15:34.279476: step 189760, loss = 0.33 (1528.0 examples/sec; 0.084 sec/batch)
2017-06-02 07:15:35.157273: step 189770, loss = 0.29 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:15:36.007365: step 189780, loss = 0.31 (1505.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:15:36.899219: step 189790, loss = 0.28 (1435.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:15:37.870602: step 189800, loss = 0.30 (1317.7 examples/sec; 0.097 sec/batch)
2017-06-02 07:15:38.650458: step 189810, loss = 0.28 (1641.3 examples/sec; 0.078 sec/batch)
2017-06-02 07:15:39.491551: step 189820, loss = 0.24 (1521.8 examples/sec; 0.084 sec/batch)
2017-06-02 07:15:40.374431: step 189830, loss = 0.23 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:15:41.231291: step 189840, loss = 0.28 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:15:42.086064: step 189850, loss = 0.32 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:15:42.944628: step 189860, loss = 0.28 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:15:43.833471: step 189870, loss = 0.27 (1440.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:15:44.722182: step 189880, loss = 0.32 (1440.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:15:45.557400: step 189890, loss = 0.29 (1532.5 examples/sec; 0.084 sec/batch)
2017-06-02 07:15:46.583713: step 189900, loss = 0.27 (1247.2 examples/sec; 0.103 sec/batch)
2017-06-02 07:15:47.301115: step 189910, loss = 0.28 (1784.2 examples/sec; 0.072 sec/batch)
2017-06-02 07:15:48.137251: step 189920, loss = 0.36 (1530.9 examples/sec; 0.084 sec/batch)
2017-06-02 07:15:49.026182: step 189930, loss = 0.25 (1439.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:15:49.922816: step 189940, loss = 0.32 (1427.6 examples/sec; 0.090 sec/batch)
2017-06-02 07:15:50.809681: step 189950, loss = 0.35 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:15:51.676562: step 189960, loss = 0.30 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:15:52.551892: step 189970, loss = 0.28 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:15:53.433905: step 189980, loss = 0.25 (1451.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:15:54.302368: step 189990, loss = 0.26 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:15:55.312004: step 190000, loss = 0.33 (1267.8 examples/sec; 0.101 sec/batch)
2017-06-02 07:15:56.062884: step 190010, loss = 0.28 (1704.7 examples/sec; 0.075 sec/batch)
2017-06-02 07:15:56.951091: step 190020, loss = 0.28 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:15:57.808394: step 190030, loss = 0.25 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:15:58.649694: step 190040, loss = 0.33 (1521.5 examples/sec; 0.084 sec/batch)
2017-06-02 07:15:59.505096: step 190050, loss = 0.33 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:16:00.388913: step 190060, loss = 0.31 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:16:01.274077: step 190070, loss = 0.38 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:16:02.132715: step 190080, loss = 0.32 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:16:02.973884: step 190090, loss = 0.44 (1521.7 examples/sec; 0.084 sec/batch)
2017-06-02 07:16:03.928816: step 190100, loss = 0.23 (1340.4 examples/sec; 0.095 sec/batch)
2017-06-02 07:16:04.692698: step 190110, loss = 0.26 (1675.7 examples/sec; 0.076 sec/batch)
2017-06-02 07:16:05.566925: step 190120, loss = 0.31 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:16:06.421998: step 190130, loss = 0.31 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:16:07.284488: step 190140, loss = 0.20 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:16:08.123518: step 190150, loss = 0.24 (1525.6 examples/sec; 0.084 sec/batch)
2017-06-02 07:16:08.970154: step 190160, loss = 0.36 (1511.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:16:09.836518: step 190170, loss = 0.32 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:16:10.691602: step 190180, loss = 0.32 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:16:11.578740: step 190190, loss = 0.29 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:16:12.521632: step 190200, loss = 0.24 (1357.5 examples/sec; 0.094 sec/batch)
2017-06-02 07:16:13.294472: step 190210, loss = 0.27 (1656.2 examples/sec; 0.077 sec/batch)
2017-06-02 07:16:14.157859: step 190220, loss = 0.29 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:16:15.036628: step 190230, loss = 0.25 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:16:15.899995: step 190240, loss = 0.31 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:16:16.758146: step 190250, loss = 0.28 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:16:17.615280: step 190260, loss = 0.34 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:16:18.471397: step 190270, loss = 0.32 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:16:19.315878: step 190280, loss = 0.39 (1515.7 examples/sec; 0.084 sec/batch)
2017-06-02 07:16:20.181160: step 190290, loss = 0.31 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:16:21.162254: step 190300, loss = 0.36 (1304.7 examples/sec; 0.098 sec/batch)
2017-06-02 07:16:21.902120: step 190310, loss = 0.35 (1730.0 examples/sec; 0.074 sec/batch)
2017-06-02 07:16:22.741439: step 190320, loss = 0.34 (1525.1 examples/sec; 0.084 sec/batch)
2017-06-02 07:16:23.612865: step 190330, loss = 0.33 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:16:24.464387: step 190340, loss = 0.33 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:16:25.359197: step 190350, loss = 0.31 (1430.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:16:26.230021: step 190360, loss = 0.24 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:16:27.100208: step 190370, loss = 0.31 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:16:27.967657: step 190380, loss = 0.29 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:16:28.842358: step 190390, loss = 0.37 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:16:29.805435: step 190400, loss = 0.31 (1329.1 examples/sec; 0.096 sec/batch)
2017-06-02 07:16:30.550180: step 190410, loss = 0.30 (1718.7 examples/sec; 0.074 sec/batch)
2017-06-02 07:16:31.407581: step 190420, loss = 0.30 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:16:32.254711: step 190430, loss = 0.29 (1511.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:16:33.153772: step 190440, loss = 0.28 (1423.7 examples/sec; 0.090 sec/batch)
2017-06-02 07:16:34.058942: step 190450, loss = 0.21 (1414.1 examples/sec; 0.091 sec/batch)
2017-06-02 07:16:34.940251: step 190460, loss = 0.33 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:16:35.832039: step 190470, loss = 0.34 (1435.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:16:36.719691: step 190480, loss = 0.25 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:16:37.613276: step 190490, loss = 0.26 (1432.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:16:38.572183: step 190500, loss = 0.32 (1334.8 examples/sec; 0.096 sec/batch)
2017-06-02 07:16:39.343237: step 190510, loss = 0.29 (1660.1 examples/sec; 0.077 sec/batch)
2017-06-02 07:16:40.216195: step 190520, loss = 0.36 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:16:41.092633: step 190530, loss = 0.35 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:16:41.950560: step 190540, loss = 0.32 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:16:42.820315: step 190550, loss = 0.29 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:16:43.682749: step 190560, loss = 0.42 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:16:44.556303: step 190570, loss = 0.36 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:16:45.388295: step 190580, loss = 0.29 (1538.4 examples/sec; 0.083 sec/batch)
2017-06-02 07:16:46.234382: step 190590, loss = 0.37 (1512.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:16:47.205875: step 190600, loss = 0.26 (1317.6 examples/sec; 0.097 sec/batch)
2017-06-02 07:16:47.939906: step 190610, loss = 0.46 (1743.8 examples/sec; 0.073 sec/batch)
2017-06-02 07:16:48.808132: step 190620, loss = 0.24 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:16:49.677666: step 190630, loss = 0.25 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:16:50.545415: step 190640, loss = 0.31 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:16:51.394992: step 190650, loss = 0.25 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:16:52.268114: step 190660, loss = 0.28 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:16:53.123318: step 190670, loss = 0.28 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:16:53.972634: step 190680, loss = 0.23 (1507.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:16:54.853981: step 190690, loss = 0.28 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:16:55.829499: step 190700, loss = 0.26 (1312.1 examples/sec; 0.098 sec/batch)
2017-06-02 07:16:56.585971: step 190710, loss = 0.33 (1692.1 examples/sec; 0.076 sec/batch)
2017-06-02 07:16:57.438505: step 190720, loss = 0.34 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:16:58.294572: step 190730, loss = 0.26 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:16:59.147543: step 190740, loss = 0.37 (1500.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:17:00.021539: step 190750, loss = 0.26 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:17:00.907245: step 190760, loss = 0.28 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:17:01.762873: step 190770, loss = 0.33 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:17:02.614429: step 190780, loss = 0.24 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:17:03.480409: step 190790, loss = 0.37 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:17:04.433811: step 190800, loss = 0.32 (1342.6 examples/sec; 0.095 sec/batch)
2017-06-02 07:17:05.209237: step 190810, loss = 0.27 (1650.7 examples/sec; 0.078 sec/batch)
2017-06-02 07:17:06.084122: step 190820, loss = 0.30 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:17:06.932475: step 190830, loss = 0.31 (1508.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:17:07.823253: step 190840, loss = 0.29 (1436.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:17:08.682671: step 190850, loss = 0.21 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:17:09.543944: step 190860, loss = 0.28 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:17:10.376283: step 190870, loss = 0.35 (1537.8 examples/sec; 0.083 sec/batch)
2017-06-02 07:17:11.258618: step 190880, loss = 0.44 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:17:12.124056: step 190890, loss = 0.35 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:17:13.086898: step 190900, loss = 0.28 (1329.4 examples/sec; 0.096 sec/batch)
2017-06-02 07:17:13.858678: step 190910, loss = 0.34 (1658.5 examples/sec; 0.077 sec/batch)
2017-06-02 07:17:14.746282: step 190920, loss = 0.25 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:17:15.620961: step 190930, loss = 0.37 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:17:16.507352: step 190940, loss = 0.29 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:17:17.359083: step 190950, loss = 0.25 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:17:18.214653: step 190960, loss = 0.24 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:17:19.090143: step 190970, loss = 0.30 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:17:19.953895: step 190980, loss = 0.29 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:17:20.808641: step 190990, loss = 0.25 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:17:21.779919: step 191000, loss = 0.33 (1317.9 examples/sec; 0.097 sec/batch)
2017-06-02 07:17:22.537984: step 191010, loss = 0.27 (1688.5 examples/sec; 0.076 sec/batch)
2017-06-02 07:17:23.372215: step 191020, loss = 0.32 (1534.4 examples/sec; 0.083 sec/batch)
2017-06-02 07:17:24.263349: step 191030, loss = 0.24 (1436.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:17:25.137307: step 191040, loss = 0.42 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:17:26.012331: step 191050, loss = 0.25 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:17:26.879834: step 191060, loss = 0.27 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:17:27.735381: step 191070, loss = 0.27 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:17:28.596319: step 191080, loss = 0.26 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:17:29.460503: step 191090, loss = 0.27 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:17:30.492093: step 191100, loss = 0.36 (1240.8 examples/sec; 0.103 sec/batch)
2017-06-02 07:17:31.173067: step 191110, loss = 0.23 (1879.7 examples/sec; 0.068 sec/batch)
2017-06-02 07:17:32.061557: step 191120, loss = 0.25 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:17:32.953822: step 191130, loss = 0.28 (1434.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:17:33.844635: step 191140, loss = 0.22 (1436.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:17:34.707030: step 191150, loss = 0.32 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:17:35.551222: step 191160, loss = 0.30 (1516.2 examples/sec; 0.084 sec/batch)
2017-06-02 07:17:36.439399: step 191170, loss = 0.34 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:17:37.309679: step 191180, loss = 0.24 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:17:38.173656: step 191190, loss = 0.27 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:17:39.143280: step 191200, loss = 0.32 (1320.1 examples/sec; 0.097 sec/batch)
2017-06-02 07:17:39.923446: step 191210, loss = 0.29 (1640.7 examples/sec; 0.078 sec/batch)
2017-06-02 07:17:40.779034: step 191220, loss = 0.35 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:17:41.616636: step 191230, loss = 0.26 (1528.2 examples/sec; 0.084 sec/batch)
2017-06-02 07:17:42.492431: step 191240, loss = 0.22 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:17:43.339399: step 191250, loss = 0.38 (1511.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:17:44.192302: step 191260, loss = 0.23 (1500.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:17:45.058796: step 191270, loss = 0.30 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:17:45.939608: step 191280, loss = 0.36 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:17:46.798227: step 191290, loss = 0.35 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:17:47.763148: step 191300, loss = 0.30 (1326.5 examples/sec; 0.096 sec/batch)
2017-06-02 07:17:48.531074: step 191310, loss = 0.26 (1666.9 examples/sec; 0.077 sec/batch)
2017-06-02 07:17:49.408945: step 191320, loss = 0.26 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:17:50.288943: step 191330, loss = 0.31 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:17:51.156946: step 191340, loss = 0.27 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:17:52.001955: step 191350, loss = 0.30 (1514.8 examples/sec; 0.084 sec/batch)
2017-06-02 07:17:52.844061: step 191360, loss = 0.29 (1520.0 examples/sec; 0.084 sec/batch)
2017-06-02 07:17:53.727404: step 191370, loss = 0.27 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:17:54.612650: step 191380, loss = 0.31 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:17:55.496752: step 191390, loss = 0.29 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:17:56.525334: step 191400, loss = 0.30 (1244.4 examples/sec; 0.103 sec/batch)
2017-06-02 07:17:57.243526: step 191410, loss = 0.25 (1782.3 examples/sec; 0.072 sec/batch)
2017-06-02 07:17:58.126751: step 191420, loss = 0.27 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:17:58.997822: step 191430, loss = 0.27 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:17:59.877807: step 191440, loss = 0.31 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:18:00.770242: step 191450, loss = 0.25 (1434.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:18:01.661686: step 191460, loss = 0.37 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:18:02.527320: step 191470, loss = 0.26 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:18:03.396027: step 191480, loss = 0.31 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:18:04.255952: step 191490, loss = 0.25 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:18:05.234286: step 191500, loss = 0.30 (1308.3 examples/sec; 0.098 sec/batch)
2017-06-02 07:18:06.004691: step 191510, loss = 0.36 (1661.4 examples/sec; 0.077 sec/batch)
2017-06-02 07:18:06.880130: step 191520, loss = 0.23 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:18:07.757862: step 191530, loss = 0.30 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:18:08.648478: step 191540, loss = 0.27 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:18:09.548205: step 191550, loss = 0.29 (1422.7 examples/sec; 0.090 sec/batch)
2017-06-02 07:18:10.393825: step 191560, loss = 0.27 (1513.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:18:11.271624: step 191570, loss = 0.31 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:18:12.148630: step 191580, loss = 0.29 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:18:12.993803: step 191590, loss = 0.29 (1514.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:18:13.946927: step 191600, loss = 0.32 (1343.0 examples/sec; 0.095 sec/batch)
2017-06-02 07:18:14.702891: step 191610, loss = 0.26 (1693.2 examples/sec; 0.076 sec/batch)
2017-06-02 07:18:15.545457: step 191620, loss = 0.33 (1519.2 examples/sec; 0.084 sec/batch)
2017-06-02 07:18:16.414847: step 191630, loss = 0.28 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:18:17.309651: step 191640, loss = 0.34 (1430.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:18:18.193845: step 191650, loss = 0.27 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:18:19.042115: step 191660, loss = 0.23 (1509.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:18:19.925994: step 191670, loss = 0.38 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:18:20.804699: step 191680, loss = 0.31 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:18:21.660998: step 191690, loss = 0.37 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:18:22.623442: step 191700, loss = 0.27 (1329.9 examples/sec; 0.096 sec/batch)
2017-06-02 07:18:23.361181: step 191710, loss = 0.43 (1735.1 examples/sec; 0.074 sec/batch)
2017-06-02 07:18:24.201247: step 191720, loss = 0.24 (1523.7 examples/sec; 0.084 sec/batch)
2017-06-02 07:18:25.082120: step 191730, loss = 0.24 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:18:25.950015: step 191740, loss = 0.34 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:18:26.809414: step 191750, loss = 0.25 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:18:27.649769: step 191760, loss = 0.24 (1523.2 examples/sec; 0.084 sec/batch)
2017-06-02 07:18:28.496930: step 191770, loss = 0.27 (1510.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:18:29.360551: step 191780, loss = 0.32 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:18:30.232276: step 191790, loss = 0.36 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:18:31.173083: step 191800, loss = 0.35 (1360.5 examples/sec; 0.094 sec/batch)
2017-06-02 07:18:31.938627: step 191810, loss = 0.36 (1672.0 examples/sec; 0.077 sec/batch)
2017-06-02 07:18:32.803321: step 191820, loss = 0.32 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:18:33.670221: step 191830, loss = 0.28 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:18:34.525820: step 191840, loss = 0.25 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:18:35.402976: step 191850, loss = 0.28 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:18:36.268759: step 191860, loss = 0.26 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:18:37.147867: step 191870, loss = 0.26 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:18:38.002204: step 191880, loss = 0.37 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:18:38.836532: step 191890, loss = 0.29 (1534.2 examples/sec; 0.083 sec/batch)
2017-06-02 07:18:39.802503: step 191900, loss = 0.24 (1325.1 examples/sec; 0.097 sec/batch)
2017-06-02 07:18:40.537284: step 191910, loss = 0.31 (1742.0 examples/sec; 0.073 sec/batch)
2017-06-02 07:18:41.422467: step 191920, loss = 0.29 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:18:42.283795: step 191930, loss = 0.28 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:18:43.172341: step 191940, loss = 0.29 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:18:44.023883: step 191950, loss = 0.35 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:18:44.887938: step 191960, loss = 0.29 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:18:45.746775: step 191970, loss = 0.20 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:18:46.625110: step 191980, loss = 0.24 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:18:47.488244: step 191990, loss = 0.34 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:18:48.493568: step 192000, loss = 0.23 (1273.2 examples/sec; 0.101 sec/batch)
2017-06-02 07:18:49.255859: step 192010, loss = 0.24 (1679.2 examples/sec; 0.076 sec/batch)
2017-06-02 07:18:50.142016: step 192020, loss = 0.23 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:18:51.023101: step 192030, loss = 0.25 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:18:51.919864: step 192040, loss = 0.35 (1427.3 examples/sec; 0.090 sec/batch)
2017-06-02 07:18:52.787125: step 192050, loss = 0.35 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:18:53.665060: step 192060, loss = 0.27 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:18:54.520867: step 192070, loss = 0.32 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:18:55.371435: step 192080, loss = 0.27 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:18:56.239382: step 192090, loss = 0.34 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:18:57.229520: step 192100, loss = 0.27 (1292.8 examples/sec; 0.099 sec/batch)
2017-06-02 07:18:57.963833: step 192110, loss = 0.31 (1743.1 examples/sec; 0.073 sec/batch)
2017-06-02 07:18:58.819101: step 192120, loss = 0.33 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:18:59.669518: step 192130, loss = 0.30 (1505.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:19:00.533221: step 192140, loss = 0.34 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:19:01.416399: step 192150, loss = 0.29 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:19:02.279754: step 192160, loss = 0.29 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:19:03.163638: step 192170, loss = 0.26 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:19:04.049938: step 192180, loss = 0.25 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:19:04.907660: step 192190, loss = 0.28 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:19:05.878019: step 192200, loss = 0.31 (1319.1 examples/sec; 0.097 sec/batch)
2017-06-02 07:19:06.664567: step 192210, loss = 0.29 (1627.3 examples/sec; 0.079 sec/batch)
2017-06-02 07:19:07.541522: step 192220, loss = 0.31 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:19:08.430894: step 192230, loss = 0.33 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:19:09.319935: step 192240, loss = 0.34 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:19:10.183089: step 192250, loss = 0.32 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:19:11.065549: step 192260, loss = 0.25 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:19:11.903069: step 192270, loss = 0.31 (1528.3 examples/sec; 0.084 sec/batch)
2017-06-02 07:19:12.785272: step 192280, loss = 0.25 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:19:13.657806: step 192290, loss = 0.24 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:19:14.703382: step 192300, loss = 0.29 (1224.2 examples/sec; 0.105 sec/batch)
2017-06-02 07:19:15.392595: step 192310, loss = 0.30 (1857.2 examples/sec; 0.069 sec/batch)
2017-06-02 07:19:16.262630: step 192320, loss = 0.27 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:19:17.106217: step 192330, loss = 0.21 (1517.3 examples/sec; 0.084 sec/batch)
2017-06-02 07:19:17.959033: step 192340, loss = 0.20 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:19:18.830713: step 192350, loss = 0.28 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:19:19.680451: step 192360, loss = 0.24 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:19:20.543888: step 192370, loss = 0.28 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:19:21.377164: step 192380, loss = 0.23 (1536.1 examples/sec; 0.083 sec/batch)
2017-06-02 07:19:22.231958: step 192390, loss = 0.31 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:19:23.182937: step 192400, loss = 0.27 (1346.0 examples/sec; 0.095 sec/batch)
2017-06-02 07:19:23.947937: step 192410, loss = 0.31 (1673.2 examples/sec; 0.077 sec/batch)
2017-06-02 07:19:24.811311: step 192420, loss = 0.26 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:19:25.670816: step 192430, loss = 0.27 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:19:26.532568: step 192440, loss = 0.39 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:19:27.411721: step 192450, loss = 0.27 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:19:28.278701: step 192460, loss = 0.38 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:19:29.156932: step 192470, loss = 0.32 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:19:30.021358: step 192480, loss = 0.25 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:19:30.862919: step 192490, loss = 0.21 (1521.0 examples/sec; 0.084 sec/batch)
2017-06-02 07:19:31.806457: step 192500, loss = 0.37 (1356.6 examples/sec; 0.094 sec/batch)
2017-06-02 07:19:32.577360: step 192510, loss = 0.35 (1660.4 examples/sec; 0.077 sec/batch)
2017-06-02 07:19:33.428880: step 192520, loss = 0.32 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:19:34.280857: step 192530, loss = 0.32 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:19:35.165583: step 192540, loss = 0.27 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:19:36.035432: step 192550, loss = 0.32 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:19:36.863361: step 192560, loss = 0.22 (1546.0 examples/sec; 0.083 sec/batch)
2017-06-02 07:19:37.729508: step 192570, loss = 0.29 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:19:38.571575: step 192580, loss = 0.25 (1520.1 examples/sec; 0.084 sec/batch)
2017-06-02 07:19:39.425100: step 192590, loss = 0.29 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:19:40.403368: step 192600, loss = 0.35 (1308.4 examples/sec; 0.098 sec/batch)
2017-06-02 07:19:41.169820: step 192610, loss = 0.24 (1670.0 examples/sec; 0.077 sec/batch)
2017-06-02 07:19:42.017612: step 192620, loss = 0.31 (1509.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:19:42.870863: step 192630, loss = 0.31 (1500.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:19:43.752024: step 192640, loss = 0.26 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:19:44.619524: step 192650, loss = 0.31 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:19:45.480840: step 192660, loss = 0.38 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:19:46.322108: step 192670, loss = 0.28 (1521.5 examples/sec; 0.084 sec/batch)
2017-06-02 07:19:47.189953: step 192680, loss = 0.30 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:19:48.063907: step 192690, loss = 0.37 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:19:49.029293: step 192700, loss = 0.24 (1325.9 examples/sec; 0.097 sec/batch)
2017-06-02 07:19:49.803651: step 192710, loss = 0.28 (1653.0 examples/sec; 0.077 sec/batch)
2017-06-02 07:19:50.680577: step 192720, loss = 0.21 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:19:51.546500: step 192730, loss = 0.25 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:19:52.448860: step 192740, loss = 0.33 (1418.5 examples/sec; 0.090 sec/batch)
2017-06-02 07:19:53.326439: step 192750, loss = 0.37 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:19:54.206166: step 192760, loss = 0.34 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:19:55.087722: step 192770, loss = 0.32 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:19:55.945474: step 192780, loss = 0.37 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:19:56.813764: step 192790, loss = 0.35 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:19:57.753909: step 192800, loss = 0.27 (1361.5 examples/sec; 0.094 sec/batch)
2017-06-02 07:19:58.519917: step 192810, loss = 0.33 (1671.0 examples/sec; 0.077 sec/batch)
2017-06-02 07:19:59.418934: step 192820, loss = 0.28 (1423.8 examples/sec; 0.090 sec/batch)
2017-06-02 07:20:00.278003: step 192830, loss = 0.39 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:20:01.133827: step 192840, loss = 0.37 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:20:01.998483: step 192850, loss = 0.43 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:20:02.862361: step 192860, loss = 0.32 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:20:03.718956: step 192870, loss = 0.26 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:20:04.556376: step 192880, loss = 0.35 (1528.5 examples/sec; 0.084 sec/batch)
2017-06-02 07:20:05.401146: step 192890, loss = 0.27 (1515.2 examples/sec; 0.084 sec/batch)
2017-06-02 07:20:06.373725: step 192900, loss = 0.35 (1316.0 examples/sec; 0.097 sec/batch)
2017-06-02 07:20:07.142686: step 192910, loss = 0.29 (1664.6 examples/sec; 0.077 sec/batch)
2017-06-02 07:20:07.994325: step 192920, loss = 0.38 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:20:08.853218: step 192930, loss = 0.32 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:20:09.718311: step 192940, loss = 0.36 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:20:10.574597: step 192950, loss = 0.28 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:20:11.446927: step 192960, loss = 0.29 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:20:12.336217: step 192970, loss = 0.23 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:20:13.201469: step 192980, loss = 0.26 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:20:14.086419: step 192990, loss = 0.26 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:20:15.032934: step 193000, loss = 0.26 (1352.3 examples/sec; 0.095 sec/batch)
2017-06-02 07:20:15.796195: step 193010, loss = 0.30 (1677.0 examples/sec; 0.076 sec/batch)
2017-06-02 07:20:16.681151: step 193020, loss = 0.33 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:20:17.591921: step 193030, loss = 0.31 (1405.4 examples/sec; 0.091 sec/batch)
2017-06-02 07:20:18.483278: step 193040, loss = 0.26 (1436.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:20:19.353229: step 193050, loss = 0.27 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:20:20.230227: step 193060, loss = 0.28 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:20:21.104976: step 193070, loss = 0.23 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:20:21.973330: step 193080, loss = 0.35 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:20:22.854700: step 193090, loss = 0.31 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:20:23.843450: step 193100, loss = 0.40 (1294.6 examples/sec; 0.099 sec/batch)
2017-06-02 07:20:24.597011: step 193110, loss = 0.22 (1698.6 examples/sec; 0.075 sec/batch)
2017-06-02 07:20:25.463607: step 193120, loss = 0.28 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:20:26.320623: step 193130, loss = 0.28 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:20:27.201890: step 193140, loss = 0.28 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:20:28.079462: step 193150, loss = 0.31 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:20:28.965907: step 193160, loss = 0.36 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:20:29.797308: step 193170, loss = 0.36 (1539.6 examples/sec; 0.083 sec/batch)
2017-06-02 07:20:30.684093: step 193180, loss = 0.37 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:20:31.546304: step 193190, loss = 0.33 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:20:32.591773: step 193200, loss = 0.27 (1224.3 examples/sec; 0.105 sec/batch)
2017-06-02 07:20:33.307041: step 193210, loss = 0.20 (1789.5 examples/sec; 0.072 sec/batch)
2017-06-02 07:20:34.180073: step 193220, loss = 0.28 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:20:35.030687: step 193230, loss = 0.43 (1504.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:20:35.886312: step 193240, loss = 0.33 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:20:36.750471: step 193250, loss = 0.34 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:20:37.635432: step 193260, loss = 0.24 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:20:38.523315: step 193270, loss = 0.36 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:20:39.404293: step 193280, loss = 0.25 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:20:40.280633: step 193290, loss = 0.38 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:20:41.266574: step 193300, loss = 0.24 (1298.3 examples/sec; 0.099 sec/batch)
2017-06-02 07:20:42.065744: step 193310, loss = 0.30 (1601.7 examples/sec; 0.080 sec/batch)
2017-06-02 07:20:42.937342: step 193320, loss = 0.27 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:20:43.804910: step 193330, loss = 0.30 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:20:44.690315: step 193340, loss = 0.30 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:20:45.555586: step 193350, loss = 0.26 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:20:46.418928: step 193360, loss = 0.36 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:20:47.271301: step 193370, loss = 0.28 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:20:48.147565: step 193380, loss = 0.27 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:20:49.015628: step 193390, loss = 0.36 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:20:49.989843: step 193400, loss = 0.42 (1313.9 examples/sec; 0.097 sec/batch)
2017-06-02 07:20:50.752599: step 193410, loss = 0.30 (1678.1 examples/sec; 0.076 sec/batch)
2017-06-02 07:20:51.613101: step 193420, loss = 0.25 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:20:52.485307: step 193430, loss = 0.29 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:20:53.352681: step 193440, loss = 0.28 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:20:54.207920: step 193450, loss = 0.26 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:20:55.070855: step 193460, loss = 0.28 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:20:55.932063: step 193470, loss = 0.28 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:20:56.768911: step 193480, loss = 0.28 (1529.6 examples/sec; 0.084 sec/batch)
2017-06-02 07:20:57.616971: step 193490, loss = 0.36 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:20:58.588912: step 193500, loss = 0.29 (1317.0 examples/sec; 0.097 sec/batch)
2017-06-02 07:20:59.349328: step 193510, loss = 0.43 (1683.3 examples/sec; 0.076 sec/batch)
2017-06-02 07:21:00.219686: step 193520, loss = 0.38 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:21:01.084787: step 193530, loss = 0.21 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:21:01.944792: step 193540, loss = 0.32 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:21:02.803782: step 193550, loss = 0.29 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:21:03.679252: step 193560, loss = 0.37 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:21:04.557789: step 193570, loss = 0.30 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:21:05.429374: step 193580, loss = 0.29 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:21:06.277883: step 193590, loss = 0.30 (1508.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:21:07.225008: step 193600, loss = 0.28 (1351.5 examples/sec; 0.095 sec/batch)
2017-06-02 07:21:08.013597: step 193610, loss = 0.28 (1623.2 examples/sec; 0.079 sec/batch)
2017-06-02 07:21:08.859644: step 193620, loss = 0.30 (1512.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:21:09.716300: step 193630, loss = 0.25 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:21:10.581281: step 193640, loss = 0.27 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:21:11.437493: step 193650, loss = 0.31 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:21:12.287409: step 193660, loss = 0.39 (1506.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:21:13.160261: step 193670, loss = 0.34 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:21:14.019265: step 193680, loss = 0.27 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:21:14.874911: step 193690, loss = 0.29 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:21:15.839362: step 193700, loss = 0.22 (1327.2 examples/sec; 0.096 sec/batch)
2017-06-02 07:21:16.610440: step 193710, loss = 0.38 (1660.0 examples/sec; 0.077 sec/batch)
2017-06-02 07:21:17.476153: step 193720, loss = 0.30 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:21:18.333434: step 193730, loss = 0.33 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:21:19.187319: step 193740, loss = 0.43 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:21:20.043402: step 193750, loss = 0.34 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:21:20.888950: step 193760, loss = 0.28 (1513.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:21:21.742759: step 193770, loss = 0.29 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:21:22.620537: step 193780, loss = 0.25 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:21:23.521878: step 193790, loss = 0.33 (1420.1 examples/sec; 0.090 sec/batch)
2017-06-02 07:21:24.486922: step 193800, loss = 0.25 (1326.4 examples/sec; 0.097 sec/batch)
2017-06-02 07:21:25.253232: step 193810, loss = 0.32 (1670.4 examples/sec; 0.077 sec/batch)
2017-06-02 07:21:26.138064: step 193820, loss = 0.32 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:21:26.986191: step 193830, loss = 0.33 (1509.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:21:27.851031: step 193840, loss = 0.32 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:21:28.722838: step 193850, loss = 0.27 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:21:29.568021: step 193860, loss = 0.32 (1514.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:21:30.406049: step 193870, loss = 0.30 (1527.4 examples/sec; 0.084 sec/batch)
2017-06-02 07:21:31.255207: step 193880, loss = 0.27 (1507.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:21:32.118520: step 193890, loss = 0.23 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:21:33.104170: step 193900, loss = 0.34 (1298.6 examples/sec; 0.099 sec/batch)
2017-06-02 07:21:33.855286: step 193910, loss = 0.32 (1704.1 examples/sec; 0.075 sec/batch)
2017-06-02 07:21:34.733995: step 193920, loss = 0.28 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:21:35.578010: step 193930, loss = 0.33 (1516.6 examples/sec; 0.084 sec/batch)
2017-06-02 07:21:36.449998: step 193940, loss = 0.32 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:21:37.284779: step 193950, loss = 0.26 (1533.3 examples/sec; 0.083 sec/batch)
2017-06-02 07:21:38.154695: step 193960, loss = 0.32 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:21:39.026018: step 193970, loss = 0.28 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:21:39.915873: step 193980, loss = 0.31 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:21:40.792964: step 193990, loss = 0.27 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:21:41.779793: step 194000, loss = 0.43 (1297.1 examples/sec; 0.099 sec/batch)
2017-06-02 07:21:42.549473: step 194010, loss = 0.30 (1663.0 examples/sec; 0.077 sec/batch)
2017-06-02 07:21:43.427485: step 194020, loss = 0.29 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:21:44.316989: step 194030, loss = 0.26 (1439.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:21:45.189264: step 194040, loss = 0.36 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:21:46.067313: step 194050, loss = 0.34 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:21:46.970389: step 194060, loss = 0.29 (1417.4 examples/sec; 0.090 sec/batch)
2017-06-02 07:21:47.852858: step 194070, loss = 0.23 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:21:48.732933: step 194080, loss = 0.26 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:21:49.611868: step 194090, loss = 0.33 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:21:50.596452: step 194100, loss = 0.36 (1300.0 examples/sec; 0.098 sec/batch)
2017-06-02 07:21:51.384496: step 194110, loss = 0.27 (1624.3 examples/sec; 0.079 sec/batch)
2017-06-02 07:21:52.270144: step 194120, loss = 0.34 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:21:53.144518: step 194130, loss = 0.33 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:21:54.024079: step 194140, loss = 0.26 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:21:54.860704: step 194150, loss = 0.33 (1530.0 examples/sec; 0.084 sec/batch)
2017-06-02 07:21:55.716526: step 194160, loss = 0.30 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:21:56.584648: step 194170, loss = 0.26 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:21:57.439910: step 194180, loss = 0.32 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:21:58.298211: step 194190, loss = 0.28 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:21:59.311669: step 194200, loss = 0.23 (1263.0 examples/sec; 0.101 sec/batch)
2017-06-02 07:22:00.047611: step 194210, loss = 0.31 (1739.3 examples/sec; 0.074 sec/batch)
2017-06-02 07:22:00.885950: step 194220, loss = 0.25 (1526.8 examples/sec; 0.084 sec/batch)
2017-06-02 07:22:01.746667: step 194230, loss = 0.22 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:22:02.595392: step 194240, loss = 0.30 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:22:03.452187: step 194250, loss = 0.30 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:22:04.315191: step 194260, loss = 0.28 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:22:05.185137: step 194270, loss = 0.32 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:22:06.047088: step 194280, loss = 0.31 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:22:06.915957: step 194290, loss = 0.21 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:22:07.878387: step 194300, loss = 0.38 (1330.0 examples/sec; 0.096 sec/batch)
2017-06-02 07:22:08.643173: step 194310, loss = 0.34 (1673.7 examples/sec; 0.076 sec/batch)
2017-06-02 07:22:09.507313: step 194320, loss = 0.24 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:22:10.339609: step 194330, loss = 0.21 (1537.9 examples/sec; 0.083 sec/batch)
2017-06-02 07:22:11.217425: step 194340, loss = 0.32 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:22:12.081479: step 194350, loss = 0.38 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:22:12.964681: step 194360, loss = 0.36 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:22:13.829994: step 194370, loss = 0.26 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:22:14.704770: step 194380, loss = 0.32 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:22:15.560598: step 194390, loss = 0.35 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:22:16.524106: step 194400, loss = 0.28 (1328.5 examples/sec; 0.096 sec/batch)
2017-06-02 07:22:17.273670: step 194410, loss = 0.28 (1707.7 examples/sec; 0.075 sec/batch)
2017-06-02 07:22:18.150470: step 194420, loss = 0.32 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:22:19.007341: step 194430, loss = 0.36 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:22:19.878688: step 194440, loss = 0.35 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:22:20.736142: step 194450, loss = 0.27 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:22:21.601354: step 194460, loss = 0.36 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:22:22.470537: step 194470, loss = 0.33 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:22:23.364110: step 194480, loss = 0.34 (1432.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:22:24.239795: step 194490, loss = 0.24 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:22:25.206761: step 194500, loss = 0.29 (1323.7 examples/sec; 0.097 sec/batch)
2017-06-02 07:22:25.962995: step 194510, loss = 0.25 (1692.6 examples/sec; 0.076 sec/batch)
2017-06-02 07:22:26.839508: step 194520, loss = 0.27 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:22:27.682183: step 194530, loss = 0.29 (1519.0 examples/sec; 0.084 sec/batch)
2017-06-02 07:22:28.553798: step 194540, loss = 0.26 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:22:29.416969: step 194550, loss = 0.28 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:22:30.298860: step 194560, loss = 0.25 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:22:31.172630: step 194570, loss = 0.31 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:22:32.042169: step 194580, loss = 0.28 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:22:32.899153: step 194590, loss = 0.46 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:22:33.885151: step 194600, loss = 0.40 (1298.2 examples/sec; 0.099 sec/batch)
2017-06-02 07:22:34.662567: step 194610, loss = 0.30 (1646.5 examples/sec; 0.078 sec/batch)
2017-06-02 07:22:35.540044: step 194620, loss = 0.27 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:22:36.436033: step 194630, loss = 0.23 (1428.6 examples/sec; 0.090 sec/batch)
2017-06-02 07:22:37.310479: step 194640, loss = 0.37 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:22:38.194852: step 194650, loss = 0.26 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:22:39.069953: step 194660, loss = 0.31 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:22:39.938829: step 194670, loss = 0.28 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:22:40.801889: step 194680, loss = 0.31 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:22:41.674463: step 194690, loss = 0.30 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:22:42.638884: step 194700, loss = 0.38 (1327.2 examples/sec; 0.096 sec/batch)
2017-06-02 07:22:43.387042: step 194710, loss = 0.24 (1710.9 examples/sec; 0.075 sec/batch)
2017-06-02 07:22:44.239437: step 194720, loss = 0.31 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:22:45.108111: step 194730, loss = 0.31 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:22:45.985778: step 194740, loss = 0.33 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:22:46.860018: step 194750, loss = 0.39 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:22:47.734448: step 194760, loss = 0.36 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:22:48.612367: step 194770, loss = 0.31 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:22:49.504875: step 194780, loss = 0.23 (1434.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:22:50.382571: step 194790, loss = 0.38 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:22:51.358775: step 194800, loss = 0.24 (1311.2 examples/sec; 0.098 sec/batch)
2017-06-02 07:22:52.115411: step 194810, loss = 0.31 (1691.7 examples/sec; 0.076 sec/batch)
2017-06-02 07:22:52.975852: step 194820, loss = 0.23 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:22:53.863926: step 194830, loss = 0.31 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:22:54.730042: step 194840, loss = 0.34 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:22:55.586377: step 194850, loss = 0.24 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:22:56.451067: step 194860, loss = 0.29 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:22:57.310935: step 194870, loss = 0.34 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:22:58.181294: step 194880, loss = 0.25 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:22:59.040183: step 194890, loss = 0.30 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:23:00.020994: step 194900, loss = 0.38 (1305.0 examples/sec; 0.098 sec/batch)
2017-06-02 07:23:00.799908: step 194910, loss = 0.24 (1643.3 examples/sec; 0.078 sec/batch)
2017-06-02 07:23:01.683168: step 194920, loss = 0.33 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:23:02.548119: step 194930, loss = 0.37 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:23:03.401170: step 194940, loss = 0.39 (1500.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:23:04.268519: step 194950, loss = 0.31 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:23:05.143814: step 194960, loss = 0.32 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:23:06.014024: step 194970, loss = 0.23 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:23:06.854070: step 194980, loss = 0.29 (1523.7 examples/sec; 0.084 sec/batch)
2017-06-02 07:23:07.735601: step 194990, loss = 0.23 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:23:08.684417: step 195000, loss = 0.27 (1349.0 examples/sec; 0.095 sec/batch)
2017-06-02 07:23:09.476376: step 195010, loss = 0.26 (1616.2 examples/sec; 0.079 sec/batch)
2017-06-02 07:23:10.345532: step 195020, loss = 0.23 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:23:11.201298: step 195030, loss = 0.40 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:23:12.048858: step 195040, loss = 0.33 (1510.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:23:12.915278: step 195050, loss = 0.33 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:23:13.794741: step 195060, loss = 0.32 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:23:14.666061: step 195070, loss = 0.32 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:23:15.521599: step 195080, loss = 0.26 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:23:16.384761: step 195090, loss = 0.37 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:23:17.343408: step 195100, loss = 0.25 (1335.2 examples/sec; 0.096 sec/batch)
2017-06-02 07:23:18.118300: step 195110, loss = 0.21 (1651.9 examples/sec; 0.077 sec/batch)
2017-06-02 07:23:18.951010: step 195120, loss = 0.27 (1537.2 examples/sec; 0.083 sec/batch)
2017-06-02 07:23:19.797263: step 195130, loss = 0.39 (1512.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:23:20.667424: step 195140, loss = 0.30 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:23:21.508839: step 195150, loss = 0.31 (1521.3 examples/sec; 0.084 sec/batch)
2017-06-02 07:23:22.361023: step 195160, loss = 0.33 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:23:23.231383: step 195170, loss = 0.48 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:23:24.079614: step 195180, loss = 0.32 (1509.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:23:24.958309: step 195190, loss = 0.25 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:23:25.913511: step 195200, loss = 0.34 (1340.0 examples/sec; 0.096 sec/batch)
2017-06-02 07:23:26.653218: step 195210, loss = 0.24 (1730.4 examples/sec; 0.074 sec/batch)
2017-06-02 07:23:27.511184: step 195220, loss = 0.24 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:23:28.374002: step 195230, loss = 0.24 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:23:29.212691: step 195240, loss = 0.37 (1526.2 examples/sec; 0.084 sec/batch)
2017-06-02 07:23:30.043747: step 195250, loss = 0.34 (1540.2 examples/sec; 0.083 sec/batch)
2017-06-02 07:23:30.902446: step 195260, loss = 0.24 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:23:31.751475: step 195270, loss = 0.24 (1507.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:23:32.593397: step 195280, loss = 0.27 (1520.3 examples/sec; 0.084 sec/batch)
2017-06-02 07:23:33.433557: step 195290, loss = 0.24 (1523.5 examples/sec; 0.084 sec/batch)
2017-06-02 07:23:34.416851: step 195300, loss = 0.33 (1301.7 examples/sec; 0.098 sec/batch)
2017-06-02 07:23:35.147985: step 195310, loss = 0.30 (1750.7 examples/sec; 0.073 sec/batch)
2017-06-02 07:23:36.005393: step 195320, loss = 0.31 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:23:36.888907: step 195330, loss = 0.25 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:23:37.756715: step 195340, loss = 0.25 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:23:38.620490: step 195350, loss = 0.33 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:23:39.474451: step 195360, loss = 0.22 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:23:40.329306: step 195370, loss = 0.28 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:23:41.179928: step 195380, loss = 0.31 (1504.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:23:42.056833: step 195390, loss = 0.24 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:23:43.018813: step 195400, loss = 0.28 (1330.6 examples/sec; 0.096 sec/batch)
2017-06-02 07:23:43.809592: step 195410, loss = 0.36 (1618.7 examples/sec; 0.079 sec/batch)
2017-06-02 07:23:44.683631: step 195420, loss = 0.32 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:23:45.555633: step 195430, loss = 0.37 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:23:46.395075: step 195440, loss = 0.31 (1524.8 examples/sec; 0.084 sec/batch)
2017-06-02 07:23:47.234089: step 195450, loss = 0.31 (1525.6 examples/sec; 0.084 sec/batch)
2017-06-02 07:23:48.094455: step 195460, loss = 0.39 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:23:48.963054: step 195470, loss = 0.20 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:23:49.812762: step 195480, loss = 0.32 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:23:50.678917: step 195490, loss = 0.35 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:23:51.635980: step 195500, loss = 0.27 (1337.4 examples/sec; 0.096 sec/batch)
2017-06-02 07:23:52.405421: step 195510, loss = 0.34 (1663.5 examples/sec; 0.077 sec/batch)
2017-06-02 07:23:53.289852: step 195520, loss = 0.29 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:23:54.144662: step 195530, loss = 0.31 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:23:55.000571: step 195540, loss = 0.35 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:23:55.878636: step 195550, loss = 0.29 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:23:56.755136: step 195560, loss = 0.29 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:23:57.617574: step 195570, loss = 0.26 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:23:58.499040: step 195580, loss = 0.27 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:23:59.361993: step 195590, loss = 0.24 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:24:00.319008: step 195600, loss = 0.27 (1337.5 examples/sec; 0.096 sec/batch)
2017-06-02 07:24:01.089200: step 195610, loss = 0.27 (1661.9 examples/sec; 0.077 sec/batch)
2017-06-02 07:24:01.947329: step 195620, loss = 0.27 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:24:02.823387: step 195630, loss = 0.19 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:24:03.698809: step 195640, loss = 0.27 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:24:04.591377: step 195650, loss = 0.28 (1434.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:24:05.431932: step 195660, loss = 0.28 (1522.8 examples/sec; 0.084 sec/batch)
2017-06-02 07:24:06.296584: step 195670, loss = 0.27 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:24:07.180194: step 195680, loss = 0.30 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:24:08.037041: step 195690, loss = 0.32 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:24:09.003110: step 195700, loss = 0.26 (1325.0 examples/sec; 0.097 sec/batch)
2017-06-02 07:24:09.795787: step 195710, loss = 0.31 (1614.8 examples/sec; 0.079 sec/batch)
2017-06-02 07:24:10.652540: step 195720, loss = 0.31 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:24:11.522923: step 195730, loss = 0.31 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:24:12.394193: step 195740, loss = 0.33 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:24:13.259102: step 195750, loss = 0.27 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:24:14.122970: step 195760, loss = 0.32 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:24:14.984725: step 195770, loss = 0.29 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:24:15.852403: step 195780, loss = 0.30 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:24:16.692320: step 195790, loss = 0.30 (1524.0 examples/sec; 0.084 sec/batch)
2017-06-02 07:24:17.664778: step 195800, loss = 0.22 (1316.2 examples/sec; 0.097 sec/batch)
2017-06-02 07:24:18.427743: step 195810, loss = 0.31 (1677.7 examples/sec; 0.076 sec/batch)
2017-06-02 07:24:19.304358: step 195820, loss = 0.27 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:24:20.162363: step 195830, loss = 0.28 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:24:21.042111: step 195840, loss = 0.23 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:24:21.906419: step 195850, loss = 0.39 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:24:22.770307: step 195860, loss = 0.27 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:24:23.633490: step 195870, loss = 0.38 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:24:24.482049: step 195880, loss = 0.28 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:24:25.341477: step 195890, loss = 0.26 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:24:26.343913: step 195900, loss = 0.27 (1276.9 examples/sec; 0.100 sec/batch)
2017-06-02 07:24:27.097282: step 195910, loss = 0.33 (1699.0 examples/sec; 0.075 sec/batch)
2017-06-02 07:24:27.981086: step 195920, loss = 0.33 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:24:28.867418: step 195930, loss = 0.39 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:24:29.748572: step 195940, loss = 0.34 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:24:30.629659: step 195950, loss = 0.23 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:24:31.529878: step 195960, loss = 0.29 (1421.9 examples/sec; 0.090 sec/batch)
2017-06-02 07:24:32.438487: step 195970, loss = 0.32 (1408.7 examples/sec; 0.091 sec/batch)
2017-06-02 07:24:33.329678: step 195980, loss = 0.37 (1436.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:24:34.208124: step 195990, loss = 0.40 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:24:35.245922: step 196000, loss = 0.35 (1233.4 examples/sec; 0.104 sec/batch)
2017-06-02 07:24:35.968451: step 196010, loss = 0.29 (1771.5 examples/sec; 0.072 sec/batch)
2017-06-02 07:24:36.844475: step 196020, loss = 0.26 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:24:37.710312: step 196030, loss = 0.26 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:24:38.549578: step 196040, loss = 0.27 (1525.1 examples/sec; 0.084 sec/batch)
2017-06-02 07:24:39.420930: step 196050, loss = 0.33 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:24:40.293902: step 196060, loss = 0.33 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:24:41.151897: step 196070, loss = 0.24 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:24:42.027292: step 196080, loss = 0.26 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:24:42.882292: step 196090, loss = 0.28 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:24:43.824201: step 196100, loss = 0.24 (1358.9 examples/sec; 0.094 sec/batch)
2017-06-02 07:24:44.599268: step 196110, loss = 0.28 (1651.5 examples/sec; 0.078 sec/batch)
2017-06-02 07:24:45.455401: step 196120, loss = 0.44 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:24:46.312339: step 196130, loss = 0.27 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:24:47.156561: step 196140, loss = 0.29 (1516.2 examples/sec; 0.084 sec/batch)
2017-06-02 07:24:48.029981: step 196150, loss = 0.32 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:24:48.885729: step 196160, loss = 0.23 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:24:49.753306: step 196170, loss = 0.30 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:24:50.633402: step 196180, loss = 0.34 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:24:51.485622: step 196190, loss = 0.35 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:24:52.470106: step 196200, loss = 0.25 (1300.2 examples/sec; 0.098 sec/batch)
2017-06-02 07:24:53.230388: step 196210, loss = 0.26 (1683.6 examples/sec; 0.076 sec/batch)
2017-06-02 07:24:54.099702: step 196220, loss = 0.36 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:24:54.931963: step 196230, loss = 0.25 (1538.0 examples/sec; 0.083 sec/batch)
2017-06-02 07:24:55.784892: step 196240, loss = 0.35 (1500.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:24:56.641460: step 196250, loss = 0.32 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:24:57.506106: step 196260, loss = 0.27 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:24:58.343501: step 196270, loss = 0.32 (1528.5 examples/sec; 0.084 sec/batch)
2017-06-02 07:24:59.206064: step 196280, loss = 0.38 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:25:00.087565: step 196290, loss = 0.28 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:25:01.049415: step 196300, loss = 0.28 (1330.8 examples/sec; 0.096 sec/batch)
2017-06-02 07:25:01.838105: step 196310, loss = 0.27 (1623.0 examples/sec; 0.079 sec/batch)
2017-06-02 07:25:02.719217: step 196320, loss = 0.33 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:25:03.568918: step 196330, loss = 0.35 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:25:04.427282: step 196340, loss = 0.36 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:25:05.287442: step 196350, loss = 0.33 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:25:06.132519: step 196360, loss = 0.26 (1514.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:25:07.020016: step 196370, loss = 0.30 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:25:07.883988: step 196380, loss = 0.37 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:25:08.742218: step 196390, loss = 0.26 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:25:09.737633: step 196400, loss = 0.29 (1285.9 examples/sec; 0.100 sec/batch)
2017-06-02 07:25:10.481316: step 196410, loss = 0.38 (1721.2 examples/sec; 0.074 sec/batch)
2017-06-02 07:25:11.343341: step 196420, loss = 0.35 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:25:12.196047: step 196430, loss = 0.34 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:25:13.070511: step 196440, loss = 0.28 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:25:13.947382: step 196450, loss = 0.30 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:25:14.823227: step 196460, loss = 0.24 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:25:15.673987: step 196470, loss = 0.32 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:25:16.525248: step 196480, loss = 0.22 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:25:17.388007: step 196490, loss = 0.36 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:25:18.332080: step 196500, loss = 0.24 (1355.8 examples/sec; 0.094 sec/batch)
2017-06-02 07:25:19.093190: step 196510, loss = 0.28 (1681.8 examples/sec; 0.076 sec/batch)
2017-06-02 07:25:19.972454: step 196520, loss = 0.29 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:25:20.807225: step 196530, loss = 0.27 (1533.4 examples/sec; 0.083 sec/batch)
2017-06-02 07:25:21.663805: step 196540, loss = 0.34 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:25:22.514765: step 196550, loss = 0.31 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:25:23.398494: step 196560, loss = 0.30 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:25:24.272818: step 196570, loss = 0.45 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:25:25.140742: step 196580, loss = 0.39 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:25:26.013870: step 196590, loss = 0.27 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:25:26.968940: step 196600, loss = 0.29 (1340.2 examples/sec; 0.096 sec/batch)
2017-06-02 07:25:27.741501: step 196610, loss = 0.42 (1656.8 examples/sec; 0.077 sec/batch)
2017-06-02 07:25:28.606045: step 196620, loss = 0.31 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:25:29.486071: step 196630, loss = 0.29 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:25:30.370355: step 196640, loss = 0.32 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:25:31.263111: step 196650, loss = 0.26 (1433.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:25:32.115635: step 196660, loss = 0.26 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:25:32.990263: step 196670, loss = 0.27 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:25:33.842469: step 196680, loss = 0.30 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:25:34.716689: step 196690, loss = 0.36 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:25:35.677905: step 196700, loss = 0.25 (1331.7 examples/sec; 0.096 sec/batch)
2017-06-02 07:25:36.442303: step 196710, loss = 0.31 (1674.5 examples/sec; 0.076 sec/batch)
2017-06-02 07:25:37.322654: step 196720, loss = 0.28 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:25:38.169483: step 196730, loss = 0.30 (1511.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:25:39.038345: step 196740, loss = 0.36 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:25:39.920650: step 196750, loss = 0.29 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:25:40.817160: step 196760, loss = 0.26 (1427.8 examples/sec; 0.090 sec/batch)
2017-06-02 07:25:41.669156: step 196770, loss = 0.31 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:25:42.530943: step 196780, loss = 0.26 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:25:43.380145: step 196790, loss = 0.26 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:25:44.350706: step 196800, loss = 0.24 (1318.8 examples/sec; 0.097 sec/batch)
2017-06-02 07:25:45.121104: step 196810, loss = 0.29 (1661.5 examples/sec; 0.077 sec/batch)
2017-06-02 07:25:46.012217: step 196820, loss = 0.32 (1436.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:25:46.874643: step 196830, loss = 0.28 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:25:47.747612: step 196840, loss = 0.30 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:25:48.603079: step 196850, loss = 0.29 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:25:49.464065: step 196860, loss = 0.32 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:25:50.311447: step 196870, loss = 0.26 (1510.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:25:51.166634: step 196880, loss = 0.33 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:25:52.031227: step 196890, loss = 0.30 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:25:52.987795: step 196900, loss = 0.31 (1338.1 examples/sec; 0.096 sec/batch)
2017-06-02 07:25:53.758954: step 196910, loss = 0.21 (1659.8 examples/sec; 0.077 sec/batch)
2017-06-02 07:25:54.622131: step 196920, loss = 0.25 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:25:55.468432: step 196930, loss = 0.33 (1512.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:25:56.301310: step 196940, loss = 0.29 (1536.8 examples/sec; 0.083 sec/batch)
2017-06-02 07:25:57.138716: step 196950, loss = 0.33 (1528.5 examples/sec; 0.084 sec/batch)
2017-06-02 07:25:57.985146: step 196960, loss = 0.26 (1512.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:25:58.858310: step 196970, loss = 0.23 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:25:59.695081: step 196980, loss = 0.40 (1529.7 examples/sec; 0.084 sec/batch)
2017-06-02 07:26:00.514724: step 196990, loss = 0.37 (1561.7 examples/sec; 0.082 sec/batch)
2017-06-02 07:26:01.470349: step 197000, loss = 0.36 (1339.5 examples/sec; 0.096 sec/batch)
2017-06-02 07:26:02.245234: step 197010, loss = 0.27 (1651.8 examples/sec; 0.077 sec/batch)
2017-06-02 07:26:03.111450: step 197020, loss = 0.29 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:26:03.999116: step 197030, loss = 0.39 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:26:04.871497: step 197040, loss = 0.22 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:26:05.735869: step 197050, loss = 0.27 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:26:06.593303: step 197060, loss = 0.29 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:26:07.459781: step 197070, loss = 0.33 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:26:08.329271: step 197080, loss = 0.31 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:26:09.173186: step 197090, loss = 0.31 (1516.8 examples/sec; 0.084 sec/batch)
2017-06-02 07:26:10.149538: step 197100, loss = 0.24 (1311.0 examples/sec; 0.098 sec/batch)
2017-06-02 07:26:10.904381: step 197110, loss = 0.27 (1695.7 examples/sec; 0.075 sec/batch)
2017-06-02 07:26:11.778965: step 197120, loss = 0.30 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:26:12.631065: step 197130, loss = 0.24 (1502.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:26:13.494005: step 197140, loss = 0.33 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:26:14.366254: step 197150, loss = 0.24 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:26:15.227971: step 197160, loss = 0.33 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:26:16.083012: step 197170, loss = 0.34 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:26:16.972544: step 197180, loss = 0.27 (1439.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:26:17.828464: step 197190, loss = 0.32 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:26:18.798200: step 197200, loss = 0.30 (1319.9 examples/sec; 0.097 sec/batch)
2017-06-02 07:26:19.588859: step 197210, loss = 0.35 (1618.9 examples/sec; 0.079 sec/batch)
2017-06-02 07:26:20.450271: step 197220, loss = 0.24 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:26:21.326537: step 197230, loss = 0.27 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:26:22.178765: step 197240, loss = 0.31 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:26:23.058100: step 197250, loss = 0.30 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:26:23.921484: step 197260, loss = 0.29 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:26:24.787500: step 197270, loss = 0.27 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:26:25.656162: step 197280, loss = 0.34 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:26:26.518103: step 197290, loss = 0.31 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:26:27.483948: step 197300, loss = 0.33 (1325.3 examples/sec; 0.097 sec/batch)
2017-06-02 07:26:28.251158: step 197310, loss = 0.39 (1668.4 examples/sec; 0.077 sec/batch)
2017-06-02 07:26:29.128951: step 197320, loss = 0.32 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:26:29.983294: step 197330, loss = 0.26 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:26:30.864760: step 197340, loss = 0.32 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:26:31.748645: step 197350, loss = 0.33 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:26:32.625343: step 197360, loss = 0.29 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:26:33.480471: step 197370, loss = 0.28 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:26:34.338287: step 197380, loss = 0.37 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:26:35.214845: step 197390, loss = 0.34 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:26:36.238019: step 197400, loss = 0.29 (1251.0 examples/sec; 0.102 sec/batch)
2017-06-02 07:26:36.945969: step 197410, loss = 0.30 (1808.0 examples/sec; 0.071 sec/batch)
2017-06-02 07:26:37.806240: step 197420, loss = 0.33 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:26:38.656845: step 197430, loss = 0.30 (1504.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:26:39.533004: step 197440, loss = 0.34 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:26:40.401472: step 197450, loss = 0.26 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:26:41.275144: step 197460, loss = 0.33 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:26:42.120864: step 197470, loss = 0.25 (1513.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:26:42.991934: step 197480, loss = 0.29 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:26:43.873926: step 197490, loss = 0.30 (1451.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:26:44.838636: step 197500, loss = 0.31 (1326.8 examples/sec; 0.096 sec/batch)
2017-06-02 07:26:45.631023: step 197510, loss = 0.42 (1615.4 examples/sec; 0.079 sec/batch)
2017-06-02 07:26:46.482784: step 197520, loss = 0.24 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:26:47.343742: step 197530, loss = 0.28 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:26:48.238078: step 197540, loss = 0.29 (1431.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:26:49.134327: step 197550, loss = 0.30 (1428.2 examples/sec; 0.090 sec/batch)
2017-06-02 07:26:50.013222: step 197560, loss = 0.37 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:26:50.920288: step 197570, loss = 0.30 (1411.1 examples/sec; 0.091 sec/batch)
2017-06-02 07:26:51.779262: step 197580, loss = 0.26 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:26:52.634492: step 197590, loss = 0.31 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:26:53.596375: step 197600, loss = 0.27 (1330.7 examples/sec; 0.096 sec/batch)
2017-06-02 07:26:54.337620: step 197610, loss = 0.37 (1726.8 examples/sec; 0.074 sec/batch)
2017-06-02 07:26:55.205881: step 197620, loss = 0.29 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:26:56.063571: step 197630, loss = 0.39 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:26:56.893237: step 197640, loss = 0.30 (1542.8 examples/sec; 0.083 sec/batch)
2017-06-02 07:26:57.753198: step 197650, loss = 0.35 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:26:58.607299: step 197660, loss = 0.27 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:26:59.469071: step 197670, loss = 0.33 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:27:00.321067: step 197680, loss = 0.23 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:27:01.182662: step 197690, loss = 0.24 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:27:02.200626: step 197700, loss = 0.28 (1257.4 examples/sec; 0.102 sec/batch)
2017-06-02 07:27:02.915371: step 197710, loss = 0.25 (1790.8 examples/sec; 0.071 sec/batch)
2017-06-02 07:27:03.809790: step 197720, loss = 0.32 (1431.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:27:04.673966: step 197730, loss = 0.31 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:27:05.539660: step 197740, loss = 0.24 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:27:06.411133: step 197750, loss = 0.36 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:27:07.286970: step 197760, loss = 0.30 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:27:08.138080: step 197770, loss = 0.25 (1503.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:27:09.005399: step 197780, loss = 0.42 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:27:09.865582: step 197790, loss = 0.29 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:27:10.817965: step 197800, loss = 0.29 (1344.0 examples/sec; 0.095 sec/batch)
2017-06-02 07:27:11.577544: step 197810, loss = 0.33 (1685.2 examples/sec; 0.076 sec/batch)
2017-06-02 07:27:12.467971: step 197820, loss = 0.37 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:27:13.339456: step 197830, loss = 0.35 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:27:14.193275: step 197840, loss = 0.25 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:27:15.021869: step 197850, loss = 0.33 (1544.8 examples/sec; 0.083 sec/batch)
2017-06-02 07:27:15.912608: step 197860, loss = 0.43 (1437.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:27:16.776867: step 197870, loss = 0.30 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:27:17.633037: step 197880, loss = 0.28 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:27:18.476172: step 197890, loss = 0.29 (1518.1 examples/sec; 0.084 sec/batch)
2017-06-02 07:27:19.434541: step 197900, loss = 0.29 (1335.6 examples/sec; 0.096 sec/batch)
2017-06-02 07:27:20.184532: step 197910, loss = 0.32 (1706.7 examples/sec; 0.075 sec/batch)
2017-06-02 07:27:21.032271: step 197920, loss = 0.39 (1509.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:27:21.880222: step 197930, loss = 0.31 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:27:22.745998: step 197940, loss = 0.21 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:27:23.611162: step 197950, loss = 0.25 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:27:24.477939: step 197960, loss = 0.26 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:27:25.351803: step 197970, loss = 0.29 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:27:26.216811: step 197980, loss = 0.33 (1479.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:27:27.081534: step 197990, loss = 0.32 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:27:28.065411: step 198000, loss = 0.27 (1301.0 examples/sec; 0.098 sec/batch)
2017-06-02 07:27:28.827255: step 198010, loss = 0.22 (1680.1 examples/sec; 0.076 sec/batch)
2017-06-02 07:27:29.687090: step 198020, loss = 0.29 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:27:30.536652: step 198030, loss = 0.49 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:27:31.404004: step 198040, loss = 0.32 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:27:32.262949: step 198050, loss = 0.25 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:27:33.117955: step 198060, loss = 0.27 (1497.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:27:33.979853: step 198070, loss = 0.27 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:27:34.840981: step 198080, loss = 0.27 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:27:35.725345: step 198090, loss = 0.21 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:27:36.673128: step 198100, loss = 0.28 (1350.5 examples/sec; 0.095 sec/batch)
2017-06-02 07:27:37.464685: step 198110, loss = 0.29 (1617.1 examples/sec; 0.079 sec/batch)
2017-06-02 07:27:38.336824: step 198120, loss = 0.23 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:27:39.211187: step 198130, loss = 0.30 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:27:40.091796: step 198140, loss = 0.29 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:27:40.952248: step 198150, loss = 0.28 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:27:41.823915: step 198160, loss = 0.36 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:27:42.711400: step 198170, loss = 0.27 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:27:43.576152: step 198180, loss = 0.33 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:27:44.408884: step 198190, loss = 0.35 (1537.1 examples/sec; 0.083 sec/batch)
2017-06-02 07:27:45.364659: step 198200, loss = 0.30 (1339.2 examples/sec; 0.096 sec/batch)
2017-06-02 07:27:46.123034: step 198210, loss = 0.26 (1687.8 examples/sec; 0.076 sec/batch)
2017-06-02 07:27:46.992276: step 198220, loss = 0.31 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:27:47.874253: step 198230, loss = 0.32 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:27:48.737113: step 198240, loss = 0.35 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:27:49.584674: step 198250, loss = 0.37 (1510.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:27:50.442632: step 198260, loss = 0.34 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:27:51.329553: step 198270, loss = 0.29 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:27:52.188928: step 198280, loss = 0.24 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:27:53.032895: step 198290, loss = 0.29 (1516.6 examples/sec; 0.084 sec/batch)
2017-06-02 07:27:54.005618: step 198300, loss = 0.24 (1315.9 examples/sec; 0.097 sec/batch)
2017-06-02 07:27:54.787403: step 198310, loss = 0.41 (1637.3 examples/sec; 0.078 sec/batch)
2017-06-02 07:27:55.656007: step 198320, loss = 0.26 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:27:56.531389: step 198330, loss = 0.31 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:27:57.401706: step 198340, loss = 0.21 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:27:58.259032: step 198350, loss = 0.24 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:27:59.121665: step 198360, loss = 0.24 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:27:59.977928: step 198370, loss = 0.35 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:28:00.862696: step 198380, loss = 0.21 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:28:01.715280: step 198390, loss = 0.47 (1501.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:28:02.702401: step 198400, loss = 0.28 (1296.7 examples/sec; 0.099 sec/batch)
2017-06-02 07:28:03.491634: step 198410, loss = 0.27 (1621.8 examples/sec; 0.079 sec/batch)
2017-06-02 07:28:04.338474: step 198420, loss = 0.25 (1511.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:28:05.222252: step 198430, loss = 0.25 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:28:06.083159: step 198440, loss = 0.35 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:28:06.963376: step 198450, loss = 0.24 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:28:07.834835: step 198460, loss = 0.37 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:28:08.707656: step 198470, loss = 0.32 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:28:09.596042: step 198480, loss = 0.34 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:28:10.474141: step 198490, loss = 0.34 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:28:11.455299: step 198500, loss = 0.35 (1304.6 examples/sec; 0.098 sec/batch)
2017-06-02 07:28:12.230285: step 198510, loss = 0.31 (1651.6 examples/sec; 0.077 sec/batch)
2017-06-02 07:28:13.115907: step 198520, loss = 0.35 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:28:13.982531: step 198530, loss = 0.31 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:28:14.849048: step 198540, loss = 0.43 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:28:15.708278: step 198550, loss = 0.25 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:28:16.559645: step 198560, loss = 0.24 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:28:17.420438: step 198570, loss = 0.35 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:28:18.266136: step 198580, loss = 0.31 (1513.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:28:19.133634: step 198590, loss = 0.30 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:28:20.083022: step 198600, loss = 0.32 (1348.2 examples/sec; 0.095 sec/batch)
2017-06-02 07:28:20.843845: step 198610, loss = 0.29 (1682.4 examples/sec; 0.076 sec/batch)
2017-06-02 07:28:21.709712: step 198620, loss = 0.26 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:28:22.551309: step 198630, loss = 0.30 (1520.9 examples/sec; 0.084 sec/batch)
2017-06-02 07:28:23.443077: step 198640, loss = 0.38 (1435.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:28:24.285672: step 198650, loss = 0.32 (1519.1 examples/sec; 0.084 sec/batch)
2017-06-02 07:28:25.129949: step 198660, loss = 0.37 (1516.1 examples/sec; 0.084 sec/batch)
2017-06-02 07:28:25.992214: step 198670, loss = 0.24 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:28:26.867652: step 198680, loss = 0.27 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:28:27.735583: step 198690, loss = 0.23 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:28:28.692346: step 198700, loss = 0.36 (1337.8 examples/sec; 0.096 sec/batch)
2017-06-02 07:28:29.472779: step 198710, loss = 0.27 (1640.1 examples/sec; 0.078 sec/batch)
2017-06-02 07:28:30.330640: step 198720, loss = 0.25 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:28:31.196477: step 198730, loss = 0.41 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:28:32.078176: step 198740, loss = 0.29 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:28:32.973463: step 198750, loss = 0.41 (1429.7 examples/sec; 0.090 sec/batch)
2017-06-02 07:28:33.843911: step 198760, loss = 0.26 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:28:34.714439: step 198770, loss = 0.37 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:28:35.613860: step 198780, loss = 0.32 (1423.1 examples/sec; 0.090 sec/batch)
2017-06-02 07:28:36.473213: step 198790, loss = 0.27 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:28:37.438428: step 198800, loss = 0.32 (1326.1 examples/sec; 0.097 sec/batch)
2017-06-02 07:28:38.209609: step 198810, loss = 0.32 (1659.8 examples/sec; 0.077 sec/batch)
2017-06-02 07:28:39.084648: step 198820, loss = 0.32 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:28:39.972837: step 198830, loss = 0.31 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:28:40.860727: step 198840, loss = 0.34 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:28:41.728215: step 198850, loss = 0.32 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:28:42.603239: step 198860, loss = 0.33 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:28:43.452707: step 198870, loss = 0.29 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:28:44.312145: step 198880, loss = 0.25 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:28:45.188841: step 198890, loss = 0.28 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:28:46.159257: step 198900, loss = 0.20 (1319.0 examples/sec; 0.097 sec/batch)
2017-06-02 07:28:46.943828: step 198910, loss = 0.25 (1631.5 examples/sec; 0.078 sec/batch)
2017-06-02 07:28:47.782127: step 198920, loss = 0.32 (1526.9 examples/sec; 0.084 sec/batch)
2017-06-02 07:28:48.654438: step 198930, loss = 0.33 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:28:49.516840: step 198940, loss = 0.27 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:28:50.386345: step 198950, loss = 0.25 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:28:51.260715: step 198960, loss = 0.38 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:28:52.146331: step 198970, loss = 0.27 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:28:52.982981: step 198980, loss = 0.30 (1529.9 examples/sec; 0.084 sec/batch)
2017-06-02 07:28:53.832962: step 198990, loss = 0.29 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:28:54.814133: step 199000, loss = 0.27 (1304.6 examples/sec; 0.098 sec/batch)
2017-06-02 07:28:55.596669: step 199010, loss = 0.32 (1635.7 examples/sec; 0.078 sec/batch)
2017-06-02 07:28:56.440543: step 199020, loss = 0.40 (1516.8 examples/sec; 0.084 sec/batch)
2017-06-02 07:28:57.290451: step 199030, loss = 0.42 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:28:58.185742: step 199040, loss = 0.39 (1429.7 examples/sec; 0.090 sec/batch)
2017-06-02 07:28:59.055764: step 199050, loss = 0.26 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:28:59.921316: step 199060, loss = 0.33 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:29:00.780413: step 199070, loss = 0.28 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:29:01.647982: step 199080, loss = 0.23 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:29:02.530286: step 199090, loss = 0.33 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:29:03.483262: step 199100, loss = 0.39 (1343.2 examples/sec; 0.095 sec/batch)
2017-06-02 07:29:04.232638: step 199110, loss = 0.31 (1708.1 examples/sec; 0.075 sec/batch)
2017-06-02 07:29:05.106606: step 199120, loss = 0.37 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:29:05.984086: step 199130, loss = 0.25 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:29:06.846080: step 199140, loss = 0.28 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:29:07.735596: step 199150, loss = 0.35 (1439.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:29:08.616312: step 199160, loss = 0.29 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:29:09.497601: step 199170, loss = 0.26 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:29:10.371159: step 199180, loss = 0.29 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:29:11.260132: step 199190, loss = 0.31 (1439.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:29:12.229552: step 199200, loss = 0.28 (1320.4 examples/sec; 0.097 sec/batch)
2017-06-02 07:29:12.991586: step 199210, loss = 0.30 (1679.7 examples/sec; 0.076 sec/batch)
2017-06-02 07:29:13.828014: step 199220, loss = 0.26 (1530.3 examples/sec; 0.084 sec/batch)
2017-06-02 07:29:14.670299: step 199230, loss = 0.25 (1519.7 examples/sec; 0.084 sec/batch)
2017-06-02 07:29:15.528294: step 199240, loss = 0.28 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:29:16.393882: step 199250, loss = 0.30 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:29:17.262259: step 199260, loss = 0.39 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:29:18.136113: step 199270, loss = 0.32 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:29:18.995962: step 199280, loss = 0.23 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:29:19.867933: step 199290, loss = 0.34 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:29:20.834776: step 199300, loss = 0.33 (1323.9 examples/sec; 0.097 sec/batch)
2017-06-02 07:29:21.641163: step 199310, loss = 0.31 (1587.3 examples/sec; 0.081 sec/batch)
2017-06-02 07:29:22.528692: step 199320, loss = 0.31 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:29:23.367708: step 199330, loss = 0.24 (1525.6 examples/sec; 0.084 sec/batch)
2017-06-02 07:29:24.222903: step 199340, loss = 0.39 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:29:25.087460: step 199350, loss = 0.29 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:29:25.940232: step 199360, loss = 0.35 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:29:26.792154: step 199370, loss = 0.34 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:29:27.650100: step 199380, loss = 0.27 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:29:28.515678: step 199390, loss = 0.24 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:29:29.527397: step 199400, loss = 0.30 (1265.2 examples/sec; 0.101 sec/batch)
2017-06-02 07:29:30.229280: step 199410, loss = 0.30 (1823.7 examples/sec; 0.070 sec/batch)
2017-06-02 07:29:31.098261: step 199420, loss = 0.32 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:29:31.980183: step 199430, loss = 0.32 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:29:32.839399: step 199440, loss = 0.35 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:29:33.689741: step 199450, loss = 0.33 (1505.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:29:34.567622: step 199460, loss = 0.37 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:29:35.425275: step 199470, loss = 0.26 (1492.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:29:36.261354: step 199480, loss = 0.32 (1531.0 examples/sec; 0.084 sec/batch)
2017-06-02 07:29:37.118565: step 199490, loss = 0.28 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:29:38.096173: step 199500, loss = 0.35 (1309.3 examples/sec; 0.098 sec/batch)
2017-06-02 07:29:38.848013: step 199510, loss = 0.30 (1702.5 examples/sec; 0.075 sec/batch)
2017-06-02 07:29:39.733027: step 199520, loss = 0.33 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:29:40.588442: step 199530, loss = 0.34 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:29:41.447213: step 199540, loss = 0.38 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:29:42.311886: step 199550, loss = 0.19 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:29:43.193295: step 199560, loss = 0.31 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:29:44.078535: step 199570, loss = 0.26 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:29:44.942319: step 199580, loss = 0.28 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:29:45.791796: step 199590, loss = 0.27 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:29:46.751468: step 199600, loss = 0.24 (1333.8 examples/sec; 0.096 sec/batch)
2017-06-02 07:29:47.521283: step 199610, loss = 0.23 (1662.8 examples/sec; 0.077 sec/batch)
2017-06-02 07:29:48.373784: step 199620, loss = 0.26 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:29:49.208998: step 199630, loss = 0.32 (1532.5 examples/sec; 0.084 sec/batch)
2017-06-02 07:29:50.037502: step 199640, loss = 0.29 (1545.0 examples/sec; 0.083 sec/batch)
2017-06-02 07:29:50.907043: step 199650, loss = 0.22 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:29:51.768284: step 199660, loss = 0.28 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:29:52.676776: step 199670, loss = 0.30 (1408.9 examples/sec; 0.091 sec/batch)
2017-06-02 07:29:53.545340: step 199680, loss = 0.27 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:29:54.391907: step 199690, loss = 0.33 (1512.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:29:55.361466: step 199700, loss = 0.25 (1320.2 examples/sec; 0.097 sec/batch)
2017-06-02 07:29:56.125714: step 199710, loss = 0.39 (1674.9 examples/sec; 0.076 sec/batch)
2017-06-02 07:29:56.996720: step 199720, loss = 0.32 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:29:57.878704: step 199730, loss = 0.29 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:29:58.748966: step 199740, loss = 0.34 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:29:59.610399: step 199750, loss = 0.29 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:30:00.473024: step 199760, loss = 0.29 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:30:01.348077: step 199770, loss = 0.22 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:30:02.197858: step 199780, loss = 0.28 (1506.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:30:03.089383: step 199790, loss = 0.29 (1435.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:30:04.088076: step 199800, loss = 0.27 (1281.7 examples/sec; 0.100 sec/batch)
2017-06-02 07:30:04.790724: step 199810, loss = 0.22 (1821.7 examples/sec; 0.070 sec/batch)
2017-06-02 07:30:05.659260: step 199820, loss = 0.25 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:30:06.519254: step 199830, loss = 0.29 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:30:07.373268: step 199840, loss = 0.25 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:30:08.244898: step 199850, loss = 0.26 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:30:09.090648: step 199860, loss = 0.34 (1513.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:30:09.952851: step 199870, loss = 0.36 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:30:10.836395: step 199880, loss = 0.27 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:30:11.685797: step 199890, loss = 0.32 (1506.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:30:12.640852: step 199900, loss = 0.32 (1340.2 examples/sec; 0.096 sec/batch)
2017-06-02 07:30:13.419756: step 199910, loss = 0.32 (1643.3 examples/sec; 0.078 sec/batch)
2017-06-02 07:30:14.274032: step 199920, loss = 0.30 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:30:15.121025: step 199930, loss = 0.31 (1511.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:30:15.975725: step 199940, loss = 0.26 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:30:16.848596: step 199950, loss = 0.30 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:30:17.731969: step 199960, loss = 0.35 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:30:18.589008: step 199970, loss = 0.32 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:30:19.459767: step 199980, loss = 0.33 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:30:20.321686: step 199990, loss = 0.34 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:30:21.298444: step 200000, loss = 0.35 (1310.5 examples/sec; 0.098 sec/batch)
2017-06-02 07:30:22.072319: step 200010, loss = 0.25 (1654.0 examples/sec; 0.077 sec/batch)
2017-06-02 07:30:22.923057: step 200020, loss = 0.30 (1504.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:30:23.763529: step 200030, loss = 0.27 (1523.0 examples/sec; 0.084 sec/batch)
2017-06-02 07:30:24.619023: step 200040, loss = 0.26 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:30:25.478702: step 200050, loss = 0.34 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:30:26.354120: step 200060, loss = 0.29 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:30:27.244394: step 200070, loss = 0.27 (1437.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:30:28.102830: step 200080, loss = 0.31 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:30:29.003834: step 200090, loss = 0.32 (1420.6 examples/sec; 0.090 sec/batch)
2017-06-02 07:30:29.972846: step 200100, loss = 0.23 (1320.9 examples/sec; 0.097 sec/batch)
2017-06-02 07:30:30.730623: step 200110, loss = 0.32 (1689.2 examples/sec; 0.076 sec/batch)
2017-06-02 07:30:31.584364: step 200120, loss = 0.31 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:30:32.472844: step 200130, loss = 0.33 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:30:33.350947: step 200140, loss = 0.23 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:30:34.217236: step 200150, loss = 0.30 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:30:35.064932: step 200160, loss = 0.31 (1510.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:30:35.925084: step 200170, loss = 0.42 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:30:36.799301: step 200180, loss = 0.24 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:30:37.676683: step 200190, loss = 0.24 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:30:38.654006: step 200200, loss = 0.36 (1309.7 examples/sec; 0.098 sec/batch)
2017-06-02 07:30:39.429468: step 200210, loss = 0.29 (1650.6 examples/sec; 0.078 sec/batch)
2017-06-02 07:30:40.260092: step 200220, loss = 0.31 (1541.0 examples/sec; 0.083 sec/batch)
2017-06-02 07:30:41.109021: step 200230, loss = 0.23 (1507.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:30:41.947419: step 200240, loss = 0.27 (1526.7 examples/sec; 0.084 sec/batch)
2017-06-02 07:30:42.803818: step 200250, loss = 0.37 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:30:43.676893: step 200260, loss = 0.28 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:30:44.552346: step 200270, loss = 0.23 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:30:45.422249: step 200280, loss = 0.22 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:30:46.286840: step 200290, loss = 0.32 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:30:47.262950: step 200300, loss = 0.23 (1311.3 examples/sec; 0.098 sec/batch)
2017-06-02 07:30:48.027066: step 200310, loss = 0.31 (1675.1 examples/sec; 0.076 sec/batch)
2017-06-02 07:30:48.892745: step 200320, loss = 0.30 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:30:49.754470: step 200330, loss = 0.36 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:30:50.621035: step 200340, loss = 0.29 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:30:51.472662: step 200350, loss = 0.23 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:30:52.359261: step 200360, loss = 0.29 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:30:53.215538: step 200370, loss = 0.34 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:30:54.082187: step 200380, loss = 0.34 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:30:54.935630: step 200390, loss = 0.27 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:30:55.878078: step 200400, loss = 0.27 (1358.2 examples/sec; 0.094 sec/batch)
2017-06-02 07:30:56.620302: step 200410, loss = 0.29 (1724.6 examples/sec; 0.074 sec/batch)
2017-06-02 07:30:57.480929: step 200420, loss = 0.31 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:30:58.348080: step 200430, loss = 0.27 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:30:59.194873: step 200440, loss = 0.21 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:31:00.057021: step 200450, loss = 0.37 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:31:00.907788: step 200460, loss = 0.29 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:31:01.757358: step 200470, loss = 0.21 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:31:02.622186: step 200480, loss = 0.26 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:31:03.476829: step 200490, loss = 0.24 (1497.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:31:04.455155: step 200500, loss = 0.23 (1308.4 examples/sec; 0.098 sec/batch)
2017-06-02 07:31:05.232185: step 200510, loss = 0.32 (1647.3 examples/sec; 0.078 sec/batch)
2017-06-02 07:31:06.090768: step 200520, loss = 0.28 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:31:06.950664: step 200530, loss = 0.29 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:31:07.811553: step 200540, loss = 0.42 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:31:08.644705: step 200550, loss = 0.33 (1536.3 examples/sec; 0.083 sec/batch)
2017-06-02 07:31:09.508271: step 200560, loss = 0.31 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:31:10.353584: step 200570, loss = 0.24 (1514.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:31:11.248507: step 200580, loss = 0.30 (1430.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:31:12.126371: step 200590, loss = 0.25 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:31:13.104454: step 200600, loss = 0.29 (1308.7 examples/sec; 0.098 sec/batch)
2017-06-02 07:31:13.848423: step 200610, loss = 0.30 (1720.5 examples/sec; 0.074 sec/batch)
2017-06-02 07:31:14.703714: step 200620, loss = 0.42 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:31:15.569414: step 200630, loss = 0.31 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:31:16.445676: step 200640, loss = 0.34 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:31:17.295903: step 200650, loss = 0.24 (1505.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:31:18.150016: step 200660, loss = 0.33 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:31:19.024435: step 200670, loss = 0.28 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:31:19.892203: step 200680, loss = 0.34 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:31:20.767810: step 200690, loss = 0.27 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:31:21.724780: step 200700, loss = 0.25 (1337.6 examples/sec; 0.096 sec/batch)
2017-06-02 07:31:22.480616: step 200710, loss = 0.29 (1693.5 examples/sec; 0.076 sec/batch)
2017-06-02 07:31:23.365569: step 200720, loss = 0.24 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:31:24.218765: step 200730, loss = 0.31 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:31:25.091912: step 200740, loss = 0.33 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:31:25.951677: step 200750, loss = 0.33 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:31:26.809713: step 200760, loss = 0.34 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:31:27.690794: step 200770, loss = 0.26 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:31:28.506360: step 200780, loss = 0.32 (1569.5 examples/sec; 0.082 sec/batch)
2017-06-02 07:31:29.360339: step 200790, loss = 0.33 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:31:30.331911: step 200800, loss = 0.35 (1317.4 examples/sec; 0.097 sec/batch)
2017-06-02 07:31:31.109956: step 200810, loss = 0.30 (1645.2 examples/sec; 0.078 sec/batch)
2017-06-02 07:31:31.962795: step 200820, loss = 0.25 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:31:32.820276: step 200830, loss = 0.29 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:31:33.669212: step 200840, loss = 0.30 (1507.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:31:34.524278: step 200850, loss = 0.30 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:31:35.386182: step 200860, loss = 0.42 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:31:36.233091: step 200870, loss = 0.37 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:31:37.086628: step 200880, loss = 0.35 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:31:37.951436: step 200890, loss = 0.31 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:31:38.932117: step 200900, loss = 0.27 (1305.2 examples/sec; 0.098 sec/batch)
2017-06-02 07:31:39.704468: step 200910, loss = 0.32 (1657.3 examples/sec; 0.077 sec/batch)
2017-06-02 07:31:40.561615: step 200920, loss = 0.28 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:31:41.400878: step 200930, loss = 0.34 (1525.1 examples/sec; 0.084 sec/batch)
2017-06-02 07:31:42.281271: step 200940, loss = 0.29 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:31:43.147631: step 200950, loss = 0.33 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:31:44.010463: step 200960, loss = 0.38 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:31:44.877354: step 200970, loss = 0.36 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:31:45.725993: step 200980, loss = 0.37 (1508.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:31:46.580412: step 200990, loss = 0.30 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:31:47.552955: step 201000, loss = 0.31 (1316.1 examples/sec; 0.097 sec/batch)
2017-06-02 07:31:48.312244: step 201010, loss = 0.21 (1685.8 examples/sec; 0.076 sec/batch)
2017-06-02 07:31:49.187272: step 201020, loss = 0.37 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:31:50.046755: step 201030, loss = 0.39 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:31:50.908649: step 201040, loss = 0.31 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:31:51.807558: step 201050, loss = 0.31 (1423.9 examples/sec; 0.090 sec/batch)
2017-06-02 07:31:52.673196: step 201060, loss = 0.28 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:31:53.535037: step 201070, loss = 0.27 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:31:54.391163: step 201080, loss = 0.27 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:31:55.266053: step 201090, loss = 0.34 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:31:56.305068: step 201100, loss = 0.34 (1231.9 examples/sec; 0.104 sec/batch)
2017-06-02 07:31:57.031150: step 201110, loss = 0.41 (1762.9 examples/sec; 0.073 sec/batch)
2017-06-02 07:31:57.895930: step 201120, loss = 0.26 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:31:58.737378: step 201130, loss = 0.38 (1521.2 examples/sec; 0.084 sec/batch)
2017-06-02 07:31:59.595573: step 201140, loss = 0.35 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:32:00.453785: step 201150, loss = 0.29 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:32:01.314714: step 201160, loss = 0.33 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:32:02.170297: step 201170, loss = 0.23 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:32:03.045563: step 201180, loss = 0.31 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:32:03.913013: step 201190, loss = 0.35 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:32:04.878082: step 201200, loss = 0.26 (1326.3 examples/sec; 0.097 sec/batch)
2017-06-02 07:32:05.645032: step 201210, loss = 0.28 (1669.0 examples/sec; 0.077 sec/batch)
2017-06-02 07:32:06.499580: step 201220, loss = 0.29 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:32:07.344833: step 201230, loss = 0.32 (1514.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:32:08.200077: step 201240, loss = 0.22 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:32:09.030878: step 201250, loss = 0.33 (1540.6 examples/sec; 0.083 sec/batch)
2017-06-02 07:32:09.880647: step 201260, loss = 0.31 (1506.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:32:10.763280: step 201270, loss = 0.31 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:32:11.625154: step 201280, loss = 0.25 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:32:12.467535: step 201290, loss = 0.29 (1519.5 examples/sec; 0.084 sec/batch)
2017-06-02 07:32:13.433693: step 201300, loss = 0.35 (1324.8 examples/sec; 0.097 sec/batch)
2017-06-02 07:32:14.198099: step 201310, loss = 0.31 (1674.5 examples/sec; 0.076 sec/batch)
2017-06-02 07:32:15.050067: step 201320, loss = 0.39 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:32:15.914939: step 201330, loss = 0.34 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:32:16.793472: step 201340, loss = 0.34 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:32:17.682558: step 201350, loss = 0.32 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:32:18.550625: step 201360, loss = 0.33 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:32:19.436239: step 201370, loss = 0.27 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:32:20.326123: step 201380, loss = 0.23 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:32:21.218811: step 201390, loss = 0.33 (1433.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:32:22.211473: step 201400, loss = 0.26 (1289.4 examples/sec; 0.099 sec/batch)
2017-06-02 07:32:22.987380: step 201410, loss = 0.24 (1649.7 examples/sec; 0.078 sec/batch)
2017-06-02 07:32:23.873448: step 201420, loss = 0.28 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:32:24.763011: step 201430, loss = 0.31 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:32:25.644378: step 201440, loss = 0.24 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:32:26.523286: step 201450, loss = 0.26 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:32:27.413140: step 201460, loss = 0.28 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:32:28.298737: step 201470, loss = 0.29 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:32:29.174843: step 201480, loss = 0.24 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:32:30.042143: step 201490, loss = 0.27 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:32:31.039333: step 201500, loss = 0.23 (1283.6 examples/sec; 0.100 sec/batch)
2017-06-02 07:32:31.818797: step 201510, loss = 0.29 (1642.2 examples/sec; 0.078 sec/batch)
2017-06-02 07:32:32.698252: step 201520, loss = 0.37 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:32:33.576837: step 201530, loss = 0.27 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:32:34.440230: step 201540, loss = 0.26 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:32:35.312908: step 201550, loss = 0.38 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:32:36.182026: step 201560, loss = 0.28 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:32:37.028119: step 201570, loss = 0.25 (1512.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:32:37.893143: step 201580, loss = 0.30 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:32:38.737818: step 201590, loss = 0.30 (1515.4 examples/sec; 0.084 sec/batch)
2017-06-02 07:32:39.714063: step 201600, loss = 0.32 (1311.1 examples/sec; 0.098 sec/batch)
2017-06-02 07:32:40.454835: step 201610, loss = 0.32 (1727.9 examples/sec; 0.074 sec/batch)
2017-06-02 07:32:41.318493: step 201620, loss = 0.35 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:32:42.149903: step 201630, loss = 0.27 (1539.6 examples/sec; 0.083 sec/batch)
2017-06-02 07:32:43.022551: step 201640, loss = 0.37 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:32:43.871017: step 201650, loss = 0.33 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:32:44.686627: step 201660, loss = 0.23 (1569.4 examples/sec; 0.082 sec/batch)
2017-06-02 07:32:45.555642: step 201670, loss = 0.26 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:32:46.425359: step 201680, loss = 0.32 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:32:47.308832: step 201690, loss = 0.27 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:32:48.350270: step 201700, loss = 0.27 (1229.1 examples/sec; 0.104 sec/batch)
2017-06-02 07:32:49.056712: step 201710, loss = 0.32 (1811.9 examples/sec; 0.071 sec/batch)
2017-06-02 07:32:49.920347: step 201720, loss = 0.31 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:32:50.788320: step 201730, loss = 0.25 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:32:51.670095: step 201740, loss = 0.37 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:32:52.525935: step 201750, loss = 0.28 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:32:53.397085: step 201760, loss = 0.26 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:32:54.237162: step 201770, loss = 0.40 (1523.7 examples/sec; 0.084 sec/batch)
2017-06-02 07:32:55.110217: step 201780, loss = 0.29 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:32:55.963364: step 201790, loss = 0.30 (1500.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:32:56.953414: step 201800, loss = 0.40 (1292.9 examples/sec; 0.099 sec/batch)
2017-06-02 07:32:57.717654: step 201810, loss = 0.40 (1674.9 examples/sec; 0.076 sec/batch)
2017-06-02 07:32:58.564452: step 201820, loss = 0.36 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:32:59.432894: step 201830, loss = 0.29 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:33:00.308909: step 201840, loss = 0.34 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:33:01.175985: step 201850, loss = 0.34 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:33:02.025166: step 201860, loss = 0.29 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:33:02.908335: step 201870, loss = 0.36 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:33:03.791263: step 201880, loss = 0.25 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:33:04.642288: step 201890, loss = 0.30 (1504.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:33:05.561591: step 201900, loss = 0.31 (1392.3 examples/sec; 0.092 sec/batch)
2017-06-02 07:33:06.346397: step 201910, loss = 0.28 (1631.0 examples/sec; 0.078 sec/batch)
2017-06-02 07:33:07.222659: step 201920, loss = 0.36 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:33:08.101633: step 201930, loss = 0.27 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:33:08.954309: step 201940, loss = 0.29 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:33:09.826975: step 201950, loss = 0.23 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:33:10.698785: step 201960, loss = 0.24 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:33:11.572850: step 201970, loss = 0.25 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:33:12.435162: step 201980, loss = 0.31 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:33:13.282481: step 201990, loss = 0.22 (1510.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:33:14.254112: step 202000, loss = 0.28 (1317.4 examples/sec; 0.097 sec/batch)
2017-06-02 07:33:15.000284: step 202010, loss = 0.32 (1715.4 examples/sec; 0.075 sec/batch)
2017-06-02 07:33:15.877916: step 202020, loss = 0.32 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:33:16.751832: step 202030, loss = 0.27 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:33:17.615537: step 202040, loss = 0.22 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:33:18.460170: step 202050, loss = 0.29 (1515.4 examples/sec; 0.084 sec/batch)
2017-06-02 07:33:19.318712: step 202060, loss = 0.38 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:33:20.179425: step 202070, loss = 0.37 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:33:21.061177: step 202080, loss = 0.33 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:33:21.920313: step 202090, loss = 0.34 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:33:22.862097: step 202100, loss = 0.30 (1359.1 examples/sec; 0.094 sec/batch)
2017-06-02 07:33:23.642477: step 202110, loss = 0.25 (1640.2 examples/sec; 0.078 sec/batch)
2017-06-02 07:33:24.521815: step 202120, loss = 0.24 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:33:25.368621: step 202130, loss = 0.27 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:33:26.249503: step 202140, loss = 0.31 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:33:27.111105: step 202150, loss = 0.30 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:33:28.004063: step 202160, loss = 0.25 (1433.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:33:28.860691: step 202170, loss = 0.37 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:33:29.723986: step 202180, loss = 0.45 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:33:30.569164: step 202190, loss = 0.26 (1514.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:33:31.529904: step 202200, loss = 0.36 (1332.3 examples/sec; 0.096 sec/batch)
2017-06-02 07:33:32.316981: step 202210, loss = 0.29 (1626.3 examples/sec; 0.079 sec/batch)
2017-06-02 07:33:33.179340: step 202220, loss = 0.27 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:33:34.054765: step 202230, loss = 0.31 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:33:34.912892: step 202240, loss = 0.36 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:33:35.801929: step 202250, loss = 0.28 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:33:36.667417: step 202260, loss = 0.36 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:33:37.519284: step 202270, loss = 0.31 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:33:38.382336: step 202280, loss = 0.33 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:33:39.256634: step 202290, loss = 0.31 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:33:40.248640: step 202300, loss = 0.32 (1290.3 examples/sec; 0.099 sec/batch)
2017-06-02 07:33:41.023041: step 202310, loss = 0.37 (1652.9 examples/sec; 0.077 sec/batch)
2017-06-02 07:33:41.882801: step 202320, loss = 0.29 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:33:42.729324: step 202330, loss = 0.32 (1512.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:33:43.609513: step 202340, loss = 0.30 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:33:44.501679: step 202350, loss = 0.34 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:33:45.380612: step 202360, loss = 0.27 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:33:46.233115: step 202370, loss = 0.31 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:33:47.104214: step 202380, loss = 0.33 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:33:47.983228: step 202390, loss = 0.22 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:33:48.949873: step 202400, loss = 0.22 (1324.2 examples/sec; 0.097 sec/batch)
2017-06-02 07:33:49.733239: step 202410, loss = 0.32 (1634.0 examples/sec; 0.078 sec/batch)
2017-06-02 07:33:50.604833: step 202420, loss = 0.24 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:33:51.480759: step 202430, loss = 0.30 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:33:52.340550: step 202440, loss = 0.34 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:33:53.211930: step 202450, loss = 0.24 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:33:54.087362: step 202460, loss = 0.39 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:33:54.932870: step 202470, loss = 0.38 (1513.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:33:55.805476: step 202480, loss = 0.26 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:33:56.656256: step 202490, loss = 0.26 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:33:57.629844: step 202500, loss = 0.22 (1314.7 examples/sec; 0.097 sec/batch)
2017-06-02 07:33:58.387628: step 202510, loss = 0.30 (1689.1 examples/sec; 0.076 sec/batch)
2017-06-02 07:33:59.240090: step 202520, loss = 0.27 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:34:00.116580: step 202530, loss = 0.25 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:34:00.954607: step 202540, loss = 0.24 (1527.4 examples/sec; 0.084 sec/batch)
2017-06-02 07:34:01.798543: step 202550, loss = 0.27 (1516.7 examples/sec; 0.084 sec/batch)
2017-06-02 07:34:02.650458: step 202560, loss = 0.35 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:34:03.508589: step 202570, loss = 0.30 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:34:04.368236: step 202580, loss = 0.31 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:34:05.223550: step 202590, loss = 0.30 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:34:06.199687: step 202600, loss = 0.29 (1311.3 examples/sec; 0.098 sec/batch)
2017-06-02 07:34:06.962086: step 202610, loss = 0.25 (1679.0 examples/sec; 0.076 sec/batch)
2017-06-02 07:34:07.816537: step 202620, loss = 0.40 (1498.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:34:08.667573: step 202630, loss = 0.37 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:34:09.527503: step 202640, loss = 0.24 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:34:10.372804: step 202650, loss = 0.27 (1514.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:34:11.248761: step 202660, loss = 0.37 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:34:12.122266: step 202670, loss = 0.33 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:34:12.956088: step 202680, loss = 0.31 (1535.1 examples/sec; 0.083 sec/batch)
2017-06-02 07:34:13.813786: step 202690, loss = 0.23 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:34:14.773997: step 202700, loss = 0.29 (1333.1 examples/sec; 0.096 sec/batch)
2017-06-02 07:34:15.553481: step 202710, loss = 0.32 (1642.1 examples/sec; 0.078 sec/batch)
2017-06-02 07:34:16.444239: step 202720, loss = 0.26 (1437.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:34:17.334029: step 202730, loss = 0.28 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:34:18.221622: step 202740, loss = 0.27 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:34:19.116972: step 202750, loss = 0.34 (1429.6 examples/sec; 0.090 sec/batch)
2017-06-02 07:34:20.003874: step 202760, loss = 0.25 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:34:20.879517: step 202770, loss = 0.26 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:34:21.746629: step 202780, loss = 0.31 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:34:22.605424: step 202790, loss = 0.31 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:34:23.580891: step 202800, loss = 0.31 (1312.2 examples/sec; 0.098 sec/batch)
2017-06-02 07:34:24.330376: step 202810, loss = 0.31 (1707.8 examples/sec; 0.075 sec/batch)
2017-06-02 07:34:25.182836: step 202820, loss = 0.28 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:34:26.041659: step 202830, loss = 0.35 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:34:26.872770: step 202840, loss = 0.28 (1540.2 examples/sec; 0.083 sec/batch)
2017-06-02 07:34:27.756581: step 202850, loss = 0.32 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:34:28.644170: step 202860, loss = 0.30 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:34:29.519017: step 202870, loss = 0.35 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:34:30.386755: step 202880, loss = 0.31 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:34:31.243695: step 202890, loss = 0.29 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:34:32.219068: step 202900, loss = 0.30 (1312.3 examples/sec; 0.098 sec/batch)
2017-06-02 07:34:32.982133: step 202910, loss = 0.38 (1677.5 examples/sec; 0.076 sec/batch)
2017-06-02 07:34:33.841705: step 202920, loss = 0.31 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:34:34.672463: step 202930, loss = 0.26 (1540.7 examples/sec; 0.083 sec/batch)
2017-06-02 07:34:35.530522: step 202940, loss = 0.34 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:34:36.383141: step 202950, loss = 0.25 (1501.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:34:37.266908: step 202960, loss = 0.26 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:34:38.130042: step 202970, loss = 0.33 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:34:39.006015: step 202980, loss = 0.36 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:34:39.883492: step 202990, loss = 0.28 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:34:40.869447: step 203000, loss = 0.33 (1298.2 examples/sec; 0.099 sec/batch)
2017-06-02 07:34:41.648157: step 203010, loss = 0.38 (1643.8 examples/sec; 0.078 sec/batch)
2017-06-02 07:34:42.509072: step 203020, loss = 0.27 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:34:43.388872: step 203030, loss = 0.28 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:34:44.265438: step 203040, loss = 0.33 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:34:45.139075: step 203050, loss = 0.28 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:34:46.009461: step 203060, loss = 0.28 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:34:46.895126: step 203070, loss = 0.23 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:34:47.776605: step 203080, loss = 0.30 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:34:48.661554: step 203090, loss = 0.26 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:34:49.651798: step 203100, loss = 0.32 (1292.6 examples/sec; 0.099 sec/batch)
2017-06-02 07:34:50.409403: step 203110, loss = 0.33 (1689.6 examples/sec; 0.076 sec/batch)
2017-06-02 07:34:51.293078: step 203120, loss = 0.27 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:34:52.148585: step 203130, loss = 0.34 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:34:53.031522: step 203140, loss = 0.29 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:34:53.893634: step 203150, loss = 0.35 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:34:54.746491: step 203160, loss = 0.24 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:34:55.593924: step 203170, loss = 0.28 (1510.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:34:56.433257: step 203180, loss = 0.29 (1525.1 examples/sec; 0.084 sec/batch)
2017-06-02 07:34:57.289232: step 203190, loss = 0.25 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:34:58.253146: step 203200, loss = 0.36 (1327.9 examples/sec; 0.096 sec/batch)
2017-06-02 07:34:59.028715: step 203210, loss = 0.33 (1650.4 examples/sec; 0.078 sec/batch)
2017-06-02 07:34:59.878157: step 203220, loss = 0.31 (1506.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:35:00.749084: step 203230, loss = 0.33 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:35:01.633010: step 203240, loss = 0.31 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:35:02.503988: step 203250, loss = 0.25 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:35:03.395935: step 203260, loss = 0.31 (1435.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:35:04.284448: step 203270, loss = 0.25 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:35:05.130443: step 203280, loss = 0.28 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:35:05.988621: step 203290, loss = 0.42 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:35:06.995862: step 203300, loss = 0.25 (1270.8 examples/sec; 0.101 sec/batch)
2017-06-02 07:35:07.708242: step 203310, loss = 0.26 (1796.8 examples/sec; 0.071 sec/batch)
2017-06-02 07:35:08.563791: step 203320, loss = 0.22 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:35:09.400323: step 203330, loss = 0.36 (1530.1 examples/sec; 0.084 sec/batch)
2017-06-02 07:35:10.244376: step 203340, loss = 0.37 (1516.5 examples/sec; 0.084 sec/batch)
2017-06-02 07:35:11.115280: step 203350, loss = 0.19 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:35:11.985789: step 203360, loss = 0.24 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:35:12.855179: step 203370, loss = 0.29 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:35:13.709428: step 203380, loss = 0.28 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:35:14.585684: step 203390, loss = 0.21 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:35:15.551988: step 203400, loss = 0.29 (1324.6 examples/sec; 0.097 sec/batch)
2017-06-02 07:35:16.323302: step 203410, loss = 0.31 (1659.5 examples/sec; 0.077 sec/batch)
2017-06-02 07:35:17.172299: step 203420, loss = 0.31 (1507.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:35:18.042985: step 203430, loss = 0.27 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:35:18.914473: step 203440, loss = 0.25 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:35:19.767246: step 203450, loss = 0.24 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:35:20.601913: step 203460, loss = 0.29 (1533.6 examples/sec; 0.083 sec/batch)
2017-06-02 07:35:21.451874: step 203470, loss = 0.32 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:35:22.296758: step 203480, loss = 0.26 (1515.0 examples/sec; 0.084 sec/batch)
2017-06-02 07:35:23.155378: step 203490, loss = 0.34 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:35:24.123108: step 203500, loss = 0.30 (1322.7 examples/sec; 0.097 sec/batch)
2017-06-02 07:35:24.878667: step 203510, loss = 0.29 (1694.1 examples/sec; 0.076 sec/batch)
2017-06-02 07:35:25.713261: step 203520, loss = 0.30 (1533.7 examples/sec; 0.083 sec/batch)
2017-06-02 07:35:26.538187: step 203530, loss = 0.26 (1551.6 examples/sec; 0.082 sec/batch)
2017-06-02 07:35:27.379135: step 203540, loss = 0.30 (1522.1 examples/sec; 0.084 sec/batch)
2017-06-02 07:35:28.238417: step 203550, loss = 0.33 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:35:29.087622: step 203560, loss = 0.33 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:35:29.968812: step 203570, loss = 0.22 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:35:30.845881: step 203580, loss = 0.25 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:35:31.702683: step 203590, loss = 0.45 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:35:32.709332: step 203600, loss = 0.34 (1271.5 examples/sec; 0.101 sec/batch)
2017-06-02 07:35:33.423848: step 203610, loss = 0.30 (1791.4 examples/sec; 0.071 sec/batch)
2017-06-02 07:35:34.287930: step 203620, loss = 0.35 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:35:35.169698: step 203630, loss = 0.36 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:35:36.022440: step 203640, loss = 0.29 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:35:36.874069: step 203650, loss = 0.24 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:35:37.726215: step 203660, loss = 0.29 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:35:38.603814: step 203670, loss = 0.33 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:35:39.471473: step 203680, loss = 0.35 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:35:40.346660: step 203690, loss = 0.31 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:35:41.358946: step 203700, loss = 0.32 (1264.5 examples/sec; 0.101 sec/batch)
2017-06-02 07:35:42.103682: step 203710, loss = 0.30 (1718.7 examples/sec; 0.074 sec/batch)
2017-06-02 07:35:42.962307: step 203720, loss = 0.33 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:35:43.811843: step 203730, loss = 0.22 (1506.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:35:44.674777: step 203740, loss = 0.28 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:35:45.531012: step 203750, loss = 0.27 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:35:46.414515: step 203760, loss = 0.34 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:35:47.261102: step 203770, loss = 0.34 (1512.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:35:48.104335: step 203780, loss = 0.26 (1518.0 examples/sec; 0.084 sec/batch)
2017-06-02 07:35:48.974072: step 203790, loss = 0.28 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:35:49.931810: step 203800, loss = 0.20 (1336.5 examples/sec; 0.096 sec/batch)
2017-06-02 07:35:50.704996: step 203810, loss = 0.31 (1655.5 examples/sec; 0.077 sec/batch)
2017-06-02 07:35:51.564496: step 203820, loss = 0.22 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:35:52.421207: step 203830, loss = 0.27 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:35:53.292370: step 203840, loss = 0.28 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:35:54.123587: step 203850, loss = 0.31 (1539.9 examples/sec; 0.083 sec/batch)
2017-06-02 07:35:54.998575: step 203860, loss = 0.27 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:35:55.854831: step 203870, loss = 0.24 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:35:56.719434: step 203880, loss = 0.22 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:35:57.599633: step 203890, loss = 0.29 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:35:58.584964: step 203900, loss = 0.35 (1299.0 examples/sec; 0.099 sec/batch)
2017-06-02 07:35:59.350531: step 203910, loss = 0.24 (1672.0 examples/sec; 0.077 sec/batch)
2017-06-02 07:36:00.213884: step 203920, loss = 0.25 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:36:01.092184: step 203930, loss = 0.29 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:36:01.960586: step 203940, loss = 0.30 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:36:02.818100: step 203950, loss = 0.32 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:36:03.714592: step 203960, loss = 0.29 (1427.8 examples/sec; 0.090 sec/batch)
2017-06-02 07:36:04.577925: step 203970, loss = 0.32 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:36:05.455420: step 203980, loss = 0.29 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:36:06.314394: step 203990, loss = 0.32 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:36:07.275287: step 204000, loss = 0.25 (1332.1 examples/sec; 0.096 sec/batch)
2017-06-02 07:36:08.042434: step 204010, loss = 0.28 (1668.5 examples/sec; 0.077 sec/batch)
2017-06-02 07:36:08.880650: step 204020, loss = 0.29 (1527.1 examples/sec; 0.084 sec/batch)
2017-06-02 07:36:09.749545: step 204030, loss = 0.29 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:36:10.620769: step 204040, loss = 0.27 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:36:11.451250: step 204050, loss = 0.30 (1541.3 examples/sec; 0.083 sec/batch)
2017-06-02 07:36:12.323296: step 204060, loss = 0.38 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:36:13.215511: step 204070, loss = 0.35 (1434.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:36:14.091526: step 204080, loss = 0.51 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:36:14.961354: step 204090, loss = 0.33 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:36:15.948764: step 204100, loss = 0.29 (1296.3 examples/sec; 0.099 sec/batch)
2017-06-02 07:36:16.669352: step 204110, loss = 0.26 (1776.3 examples/sec; 0.072 sec/batch)
2017-06-02 07:36:17.529483: step 204120, loss = 0.33 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:36:18.384280: step 204130, loss = 0.23 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:36:19.248301: step 204140, loss = 0.34 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:36:20.110234: step 204150, loss = 0.33 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:36:20.977818: step 204160, loss = 0.35 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:36:21.839593: step 204170, loss = 0.30 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:36:22.692639: step 204180, loss = 0.25 (1500.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:36:23.550963: step 204190, loss = 0.36 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:36:24.534867: step 204200, loss = 0.24 (1300.9 examples/sec; 0.098 sec/batch)
2017-06-02 07:36:25.299103: step 204210, loss = 0.30 (1674.9 examples/sec; 0.076 sec/batch)
2017-06-02 07:36:26.142455: step 204220, loss = 0.21 (1517.7 examples/sec; 0.084 sec/batch)
2017-06-02 07:36:26.999763: step 204230, loss = 0.31 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:36:27.854079: step 204240, loss = 0.27 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:36:28.713240: step 204250, loss = 0.25 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:36:29.592656: step 204260, loss = 0.36 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:36:30.481414: step 204270, loss = 0.33 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:36:31.354310: step 204280, loss = 0.34 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:36:32.217517: step 204290, loss = 0.23 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:36:33.194847: step 204300, loss = 0.30 (1309.7 examples/sec; 0.098 sec/batch)
2017-06-02 07:36:33.985094: step 204310, loss = 0.28 (1619.7 examples/sec; 0.079 sec/batch)
2017-06-02 07:36:34.869374: step 204320, loss = 0.23 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:36:35.757564: step 204330, loss = 0.32 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:36:36.631075: step 204340, loss = 0.23 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:36:37.495686: step 204350, loss = 0.35 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:36:38.365696: step 204360, loss = 0.23 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:36:39.223921: step 204370, loss = 0.33 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:36:40.070158: step 204380, loss = 0.25 (1512.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:36:40.938939: step 204390, loss = 0.28 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:36:41.927961: step 204400, loss = 0.36 (1294.2 examples/sec; 0.099 sec/batch)
2017-06-02 07:36:42.674534: step 204410, loss = 0.38 (1714.5 examples/sec; 0.075 sec/batch)
2017-06-02 07:36:43.537011: step 204420, loss = 0.30 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:36:44.428925: step 204430, loss = 0.31 (1435.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:36:45.277244: step 204440, loss = 0.33 (1508.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:36:46.127749: step 204450, loss = 0.33 (1505.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:36:46.982164: step 204460, loss = 0.27 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:36:47.852976: step 204470, loss = 0.28 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:36:48.728231: step 204480, loss = 0.39 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:36:49.584634: step 204490, loss = 0.31 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:36:50.532605: step 204500, loss = 0.44 (1350.2 examples/sec; 0.095 sec/batch)
2017-06-02 07:36:51.307077: step 204510, loss = 0.29 (1652.7 examples/sec; 0.077 sec/batch)
2017-06-02 07:36:52.171330: step 204520, loss = 0.26 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:36:53.010404: step 204530, loss = 0.29 (1525.5 examples/sec; 0.084 sec/batch)
2017-06-02 07:36:53.889635: step 204540, loss = 0.26 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:36:54.754433: step 204550, loss = 0.32 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:36:55.645812: step 204560, loss = 0.35 (1436.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:36:56.506744: step 204570, loss = 0.35 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:36:57.371491: step 204580, loss = 0.37 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:36:58.219820: step 204590, loss = 0.33 (1508.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:36:59.176685: step 204600, loss = 0.34 (1337.7 examples/sec; 0.096 sec/batch)
2017-06-02 07:36:59.954463: step 204610, loss = 0.24 (1645.7 examples/sec; 0.078 sec/batch)
2017-06-02 07:37:00.813541: step 204620, loss = 0.26 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:37:01.704485: step 204630, loss = 0.28 (1436.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:37:02.570898: step 204640, loss = 0.35 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:37:03.418171: step 204650, loss = 0.40 (1510.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:37:04.296522: step 204660, loss = 0.24 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:37:05.176187: step 204670, loss = 0.26 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:37:06.043791: step 204680, loss = 0.26 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:37:06.920446: step 204690, loss = 0.33 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:37:07.900394: step 204700, loss = 0.25 (1306.2 examples/sec; 0.098 sec/batch)
2017-06-02 07:37:08.651645: step 204710, loss = 0.35 (1703.8 examples/sec; 0.075 sec/batch)
2017-06-02 07:37:09.506025: step 204720, loss = 0.30 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:37:10.361060: step 204730, loss = 0.29 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:37:11.233259: step 204740, loss = 0.27 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:37:12.124445: step 204750, loss = 0.25 (1436.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:37:13.001764: step 204760, loss = 0.23 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:37:13.854146: step 204770, loss = 0.26 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:37:14.720595: step 204780, loss = 0.33 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:37:15.578196: step 204790, loss = 0.27 (1492.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:37:16.566245: step 204800, loss = 0.26 (1295.5 examples/sec; 0.099 sec/batch)
2017-06-02 07:37:17.327397: step 204810, loss = 0.29 (1681.7 examples/sec; 0.076 sec/batch)
2017-06-02 07:37:18.204197: step 204820, loss = 0.26 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:37:19.065466: step 204830, loss = 0.33 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:37:19.905798: step 204840, loss = 0.25 (1523.2 examples/sec; 0.084 sec/batch)
2017-06-02 07:37:20.762024: step 204850, loss = 0.31 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:37:21.635803: step 204860, loss = 0.32 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:37:22.510993: step 204870, loss = 0.28 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:37:23.383487: step 204880, loss = 0.38 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:37:24.237780: step 204890, loss = 0.26 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:37:25.206125: step 204900, loss = 0.28 (1321.8 examples/sec; 0.097 sec/batch)
2017-06-02 07:37:25.987215: step 204910, loss = 0.21 (1638.8 examples/sec; 0.078 sec/batch)
2017-06-02 07:37:26.843866: step 204920, loss = 0.33 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:37:27.700510: step 204930, loss = 0.39 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:37:28.550529: step 204940, loss = 0.24 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:37:29.432042: step 204950, loss = 0.38 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:37:30.296363: step 204960, loss = 0.28 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:37:31.145941: step 204970, loss = 0.36 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:37:31.997613: step 204980, loss = 0.29 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:37:32.883457: step 204990, loss = 0.28 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:37:33.842457: step 205000, loss = 0.33 (1334.7 examples/sec; 0.096 sec/batch)
2017-06-02 07:37:34.626102: step 205010, loss = 0.35 (1633.4 examples/sec; 0.078 sec/batch)
2017-06-02 07:37:35.494631: step 205020, loss = 0.41 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:37:36.362314: step 205030, loss = 0.29 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:37:37.245283: step 205040, loss = 0.38 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:37:38.116610: step 205050, loss = 0.29 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:37:39.001335: step 205060, loss = 0.34 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:37:39.860695: step 205070, loss = 0.33 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:37:40.727728: step 205080, loss = 0.22 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:37:41.603279: step 205090, loss = 0.27 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:37:42.575736: step 205100, loss = 0.24 (1316.3 examples/sec; 0.097 sec/batch)
2017-06-02 07:37:43.366750: step 205110, loss = 0.33 (1618.2 examples/sec; 0.079 sec/batch)
2017-06-02 07:37:44.226838: step 205120, loss = 0.24 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:37:45.110129: step 205130, loss = 0.28 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:37:45.991014: step 205140, loss = 0.29 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:37:46.831373: step 205150, loss = 0.24 (1523.2 examples/sec; 0.084 sec/batch)
2017-06-02 07:37:47.710333: step 205160, loss = 0.37 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:37:48.578001: step 205170, loss = 0.30 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:37:49.466677: step 205180, loss = 0.26 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:37:50.334896: step 205190, loss = 0.32 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:37:51.333811: step 205200, loss = 0.28 (1281.4 examples/sec; 0.100 sec/batch)
2017-06-02 07:37:52.125927: step 205210, loss = 0.32 (1616.0 examples/sec; 0.079 sec/batch)
2017-06-02 07:37:53.002521: step 205220, loss = 0.23 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:37:53.871598: step 205230, loss = 0.25 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:37:54.712108: step 205240, loss = 0.27 (1522.9 examples/sec; 0.084 sec/batch)
2017-06-02 07:37:55.571247: step 205250, loss = 0.33 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:37:56.440127: step 205260, loss = 0.32 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:37:57.305431: step 205270, loss = 0.25 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:37:58.176748: step 205280, loss = 0.30 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:37:59.029796: step 205290, loss = 0.27 (1500.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:38:00.009112: step 205300, loss = 0.30 (1307.0 examples/sec; 0.098 sec/batch)
2017-06-02 07:38:00.748865: step 205310, loss = 0.40 (1730.3 examples/sec; 0.074 sec/batch)
2017-06-02 07:38:01.579179: step 205320, loss = 0.25 (1541.6 examples/sec; 0.083 sec/batch)
2017-06-02 07:38:02.460803: step 205330, loss = 0.29 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:38:03.353462: step 205340, loss = 0.24 (1433.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:38:04.196595: step 205350, loss = 0.29 (1518.1 examples/sec; 0.084 sec/batch)
2017-06-02 07:38:05.089496: step 205360, loss = 0.24 (1433.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:38:05.924137: step 205370, loss = 0.31 (1533.6 examples/sec; 0.083 sec/batch)
2017-06-02 07:38:06.817961: step 205380, loss = 0.27 (1432.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:38:07.705614: step 205390, loss = 0.36 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:38:08.710906: step 205400, loss = 0.29 (1273.3 examples/sec; 0.101 sec/batch)
2017-06-02 07:38:09.487128: step 205410, loss = 0.33 (1649.0 examples/sec; 0.078 sec/batch)
2017-06-02 07:38:10.366409: step 205420, loss = 0.36 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:38:11.275621: step 205430, loss = 0.27 (1407.8 examples/sec; 0.091 sec/batch)
2017-06-02 07:38:12.140301: step 205440, loss = 0.33 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:38:13.024512: step 205450, loss = 0.23 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:38:13.913893: step 205460, loss = 0.33 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:38:14.789739: step 205470, loss = 0.40 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:38:15.658223: step 205480, loss = 0.29 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:38:16.537756: step 205490, loss = 0.27 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:38:17.494407: step 205500, loss = 0.32 (1338.0 examples/sec; 0.096 sec/batch)
2017-06-02 07:38:18.242074: step 205510, loss = 0.25 (1712.0 examples/sec; 0.075 sec/batch)
2017-06-02 07:38:19.083699: step 205520, loss = 0.27 (1520.8 examples/sec; 0.084 sec/batch)
2017-06-02 07:38:19.975693: step 205530, loss = 0.31 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:38:20.822973: step 205540, loss = 0.34 (1510.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:38:21.705955: step 205550, loss = 0.33 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:38:22.572913: step 205560, loss = 0.30 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:38:23.457929: step 205570, loss = 0.27 (1446.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:38:24.317582: step 205580, loss = 0.28 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:38:25.185263: step 205590, loss = 0.26 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:38:26.140264: step 205600, loss = 0.39 (1340.3 examples/sec; 0.096 sec/batch)
2017-06-02 07:38:26.903396: step 205610, loss = 0.24 (1677.3 examples/sec; 0.076 sec/batch)
2017-06-02 07:38:27.770096: step 205620, loss = 0.36 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:38:28.638760: step 205630, loss = 0.31 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:38:29.498125: step 205640, loss = 0.29 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:38:30.372515: step 205650, loss = 0.24 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:38:31.211681: step 205660, loss = 0.23 (1525.3 examples/sec; 0.084 sec/batch)
2017-06-02 07:38:32.087415: step 205670, loss = 0.31 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:38:32.945424: step 205680, loss = 0.29 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:38:33.814359: step 205690, loss = 0.28 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:38:34.779219: step 205700, loss = 0.26 (1326.6 examples/sec; 0.096 sec/batch)
2017-06-02 07:38:35.544036: step 205710, loss = 0.32 (1673.6 examples/sec; 0.076 sec/batch)
2017-06-02 07:38:36.405555: step 205720, loss = 0.34 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:38:37.268818: step 205730, loss = 0.37 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:38:38.127261: step 205740, loss = 0.24 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:38:39.000122: step 205750, loss = 0.30 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:38:39.851223: step 205760, loss = 0.34 (1503.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:38:40.709869: step 205770, loss = 0.39 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:38:41.570988: step 205780, loss = 0.36 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:38:42.435336: step 205790, loss = 0.28 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:38:43.423307: step 205800, loss = 0.21 (1295.6 examples/sec; 0.099 sec/batch)
2017-06-02 07:38:44.206636: step 205810, loss = 0.23 (1634.0 examples/sec; 0.078 sec/batch)
2017-06-02 07:38:45.082697: step 205820, loss = 0.33 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:38:45.932496: step 205830, loss = 0.28 (1506.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:38:46.801358: step 205840, loss = 0.32 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:38:47.679658: step 205850, loss = 0.32 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:38:48.562520: step 205860, loss = 0.27 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:38:49.453421: step 205870, loss = 0.35 (1436.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:38:50.335964: step 205880, loss = 0.27 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:38:51.187884: step 205890, loss = 0.22 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:38:52.189472: step 205900, loss = 0.32 (1278.0 examples/sec; 0.100 sec/batch)
2017-06-02 07:38:52.967119: step 205910, loss = 0.25 (1646.0 examples/sec; 0.078 sec/batch)
2017-06-02 07:38:53.832975: step 205920, loss = 0.25 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:38:54.712331: step 205930, loss = 0.33 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:38:55.583024: step 205940, loss = 0.33 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:38:56.464096: step 205950, loss = 0.30 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:38:57.331196: step 205960, loss = 0.29 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:38:58.174539: step 205970, loss = 0.38 (1517.8 examples/sec; 0.084 sec/batch)
2017-06-02 07:38:59.010471: step 205980, loss = 0.36 (1531.2 examples/sec; 0.084 sec/batch)
2017-06-02 07:38:59.848653: step 205990, loss = 0.27 (1527.1 examples/sec; 0.084 sec/batch)
2017-06-02 07:39:00.841643: step 206000, loss = 0.30 (1289.0 examples/sec; 0.099 sec/batch)
2017-06-02 07:39:01.536020: step 206010, loss = 0.27 (1843.4 examples/sec; 0.069 sec/batch)
2017-06-02 07:39:02.372237: step 206020, loss = 0.32 (1530.7 examples/sec; 0.084 sec/batch)
2017-06-02 07:39:03.207023: step 206030, loss = 0.29 (1533.3 examples/sec; 0.083 sec/batch)
2017-06-02 07:39:04.053053: step 206040, loss = 0.28 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:39:04.920369: step 206050, loss = 0.28 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:39:05.811982: step 206060, loss = 0.41 (1435.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:39:06.688983: step 206070, loss = 0.26 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:39:07.560077: step 206080, loss = 0.28 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:39:08.429374: step 206090, loss = 0.27 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:39:09.386812: step 206100, loss = 0.35 (1336.9 examples/sec; 0.096 sec/batch)
2017-06-02 07:39:10.140758: step 206110, loss = 0.36 (1697.8 examples/sec; 0.075 sec/batch)
2017-06-02 07:39:11.010044: step 206120, loss = 0.42 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:39:11.896360: step 206130, loss = 0.29 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:39:12.753933: step 206140, loss = 0.31 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:39:13.594990: step 206150, loss = 0.31 (1521.9 examples/sec; 0.084 sec/batch)
2017-06-02 07:39:14.433629: step 206160, loss = 0.24 (1526.3 examples/sec; 0.084 sec/batch)
2017-06-02 07:39:15.305939: step 206170, loss = 0.28 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:39:16.174208: step 206180, loss = 0.38 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:39:17.016902: step 206190, loss = 0.30 (1518.9 examples/sec; 0.084 sec/batch)
2017-06-02 07:39:17.959290: step 206200, loss = 0.27 (1358.3 examples/sec; 0.094 sec/batch)
2017-06-02 07:39:18.734297: step 206210, loss = 0.28 (1651.6 examples/sec; 0.078 sec/batch)
2017-06-02 07:39:19.573103: step 206220, loss = 0.29 (1526.0 examples/sec; 0.084 sec/batch)
2017-06-02 07:39:20.462317: step 206230, loss = 0.27 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:39:21.332148: step 206240, loss = 0.29 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:39:22.196294: step 206250, loss = 0.28 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:39:23.061140: step 206260, loss = 0.25 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:39:23.917431: step 206270, loss = 0.24 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:39:24.788005: step 206280, loss = 0.25 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:39:25.623080: step 206290, loss = 0.30 (1532.8 examples/sec; 0.084 sec/batch)
2017-06-02 07:39:26.556269: step 206300, loss = 0.33 (1371.6 examples/sec; 0.093 sec/batch)
2017-06-02 07:39:27.340817: step 206310, loss = 0.25 (1631.5 examples/sec; 0.078 sec/batch)
2017-06-02 07:39:28.202130: step 206320, loss = 0.26 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:39:29.097092: step 206330, loss = 0.28 (1430.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:39:29.957637: step 206340, loss = 0.28 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:39:30.826669: step 206350, loss = 0.33 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:39:31.689616: step 206360, loss = 0.30 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:39:32.569936: step 206370, loss = 0.25 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:39:33.423833: step 206380, loss = 0.32 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:39:34.300102: step 206390, loss = 0.31 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:39:35.296974: step 206400, loss = 0.25 (1284.0 examples/sec; 0.100 sec/batch)
2017-06-02 07:39:36.025874: step 206410, loss = 0.23 (1756.0 examples/sec; 0.073 sec/batch)
2017-06-02 07:39:36.878357: step 206420, loss = 0.27 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:39:37.747315: step 206430, loss = 0.27 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:39:38.604074: step 206440, loss = 0.24 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:39:39.473516: step 206450, loss = 0.26 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:39:40.350025: step 206460, loss = 0.29 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:39:41.207116: step 206470, loss = 0.26 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:39:42.072839: step 206480, loss = 0.27 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:39:42.960206: step 206490, loss = 0.30 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:39:43.946034: step 206500, loss = 0.30 (1298.4 examples/sec; 0.099 sec/batch)
2017-06-02 07:39:44.665965: step 206510, loss = 0.26 (1778.0 examples/sec; 0.072 sec/batch)
2017-06-02 07:39:45.551521: step 206520, loss = 0.27 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:39:46.415291: step 206530, loss = 0.30 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:39:47.286612: step 206540, loss = 0.29 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:39:48.162207: step 206550, loss = 0.30 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:39:49.022856: step 206560, loss = 0.29 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:39:49.901530: step 206570, loss = 0.33 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:39:50.749587: step 206580, loss = 0.32 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:39:51.599179: step 206590, loss = 0.29 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:39:52.684272: step 206600, loss = 0.37 (1179.6 examples/sec; 0.109 sec/batch)
2017-06-02 07:39:53.457478: step 206610, loss = 0.30 (1655.4 examples/sec; 0.077 sec/batch)
2017-06-02 07:39:54.324061: step 206620, loss = 0.39 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:39:55.215033: step 206630, loss = 0.28 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:39:56.046699: step 206640, loss = 0.26 (1539.1 examples/sec; 0.083 sec/batch)
2017-06-02 07:39:56.911628: step 206650, loss = 0.30 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:39:57.789044: step 206660, loss = 0.33 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:39:58.611360: step 206670, loss = 0.33 (1556.6 examples/sec; 0.082 sec/batch)
2017-06-02 07:39:59.471605: step 206680, loss = 0.24 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:40:00.338502: step 206690, loss = 0.32 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:40:01.278862: step 206700, loss = 0.23 (1361.2 examples/sec; 0.094 sec/batch)
2017-06-02 07:40:02.053280: step 206710, loss = 0.26 (1652.9 examples/sec; 0.077 sec/batch)
2017-06-02 07:40:02.910808: step 206720, loss = 0.34 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:40:03.779016: step 206730, loss = 0.30 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:40:04.639120: step 206740, loss = 0.27 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:40:05.508934: step 206750, loss = 0.33 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:40:06.369333: step 206760, loss = 0.29 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:40:07.248879: step 206770, loss = 0.34 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:40:08.120872: step 206780, loss = 0.30 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:40:08.990658: step 206790, loss = 0.28 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:40:09.951039: step 206800, loss = 0.28 (1332.8 examples/sec; 0.096 sec/batch)
2017-06-02 07:40:10.679356: step 206810, loss = 0.38 (1757.5 examples/sec; 0.073 sec/batch)
2017-06-02 07:40:11.530712: step 206820, loss = 0.24 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:40:12.389355: step 206830, loss = 0.24 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:40:13.256298: step 206840, loss = 0.21 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:40:14.122949: step 206850, loss = 0.25 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:40:14.975951: step 206860, loss = 0.26 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:40:15.832897: step 206870, loss = 0.32 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:40:16.721704: step 206880, loss = 0.31 (1440.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:40:17.587285: step 206890, loss = 0.25 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:40:18.559170: step 206900, loss = 0.29 (1317.0 examples/sec; 0.097 sec/batch)
2017-06-02 07:40:19.336147: step 206910, loss = 0.30 (1647.4 examples/sec; 0.078 sec/batch)
2017-06-02 07:40:20.200894: step 206920, loss = 0.32 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:40:21.067595: step 206930, loss = 0.26 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:40:21.938565: step 206940, loss = 0.34 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:40:22.789594: step 206950, loss = 0.24 (1504.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:40:23.649421: step 206960, loss = 0.27 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:40:24.513429: step 206970, loss = 0.28 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:40:25.370313: step 206980, loss = 0.23 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:40:26.220314: step 206990, loss = 0.31 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:40:27.205934: step 207000, loss = 0.29 (1298.7 examples/sec; 0.099 sec/batch)
2017-06-02 07:40:27.949144: step 207010, loss = 0.28 (1722.3 examples/sec; 0.074 sec/batch)
2017-06-02 07:40:28.827618: step 207020, loss = 0.27 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:40:29.691249: step 207030, loss = 0.30 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:40:30.555159: step 207040, loss = 0.24 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:40:31.436343: step 207050, loss = 0.34 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:40:32.291551: step 207060, loss = 0.28 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:40:33.156086: step 207070, loss = 0.29 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:40:34.013624: step 207080, loss = 0.31 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:40:34.878850: step 207090, loss = 0.26 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:40:35.872576: step 207100, loss = 0.23 (1288.1 examples/sec; 0.099 sec/batch)
2017-06-02 07:40:36.633532: step 207110, loss = 0.26 (1682.1 examples/sec; 0.076 sec/batch)
2017-06-02 07:40:37.510044: step 207120, loss = 0.27 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:40:38.372273: step 207130, loss = 0.35 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:40:39.234344: step 207140, loss = 0.28 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:40:40.115773: step 207150, loss = 0.34 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:40:40.986692: step 207160, loss = 0.37 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:40:41.826832: step 207170, loss = 0.30 (1523.5 examples/sec; 0.084 sec/batch)
2017-06-02 07:40:42.671756: step 207180, loss = 0.27 (1514.9 examples/sec; 0.084 sec/batch)
2017-06-02 07:40:43.553560: step 207190, loss = 0.37 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:40:44.498360: step 207200, loss = 0.26 (1354.9 examples/sec; 0.094 sec/batch)
2017-06-02 07:40:45.257789: step 207210, loss = 0.24 (1685.4 examples/sec; 0.076 sec/batch)
2017-06-02 07:40:46.126702: step 207220, loss = 0.37 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:40:46.985212: step 207230, loss = 0.26 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:40:47.857973: step 207240, loss = 0.27 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:40:48.705370: step 207250, loss = 0.22 (1510.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:40:49.572094: step 207260, loss = 0.26 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:40:50.446023: step 207270, loss = 0.22 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:40:51.321156: step 207280, loss = 0.25 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:40:52.173075: step 207290, loss = 0.26 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:40:53.149528: step 207300, loss = 0.21 (1310.9 examples/sec; 0.098 sec/batch)
2017-06-02 07:40:53.913987: step 207310, loss = 0.29 (1674.4 examples/sec; 0.076 sec/batch)
2017-06-02 07:40:54.770177: step 207320, loss = 0.35 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:40:55.645072: step 207330, loss = 0.26 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:40:56.499500: step 207340, loss = 0.33 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:40:57.349487: step 207350, loss = 0.34 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:40:58.188293: step 207360, loss = 0.25 (1525.9 examples/sec; 0.084 sec/batch)
2017-06-02 07:40:59.026255: step 207370, loss = 0.30 (1527.5 examples/sec; 0.084 sec/batch)
2017-06-02 07:40:59.906923: step 207380, loss = 0.26 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:41:00.761621: step 207390, loss = 0.36 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:41:01.718483: step 207400, loss = 0.35 (1337.7 examples/sec; 0.096 sec/batch)
2017-06-02 07:41:02.454835: step 207410, loss = 0.32 (1738.3 examples/sec; 0.074 sec/batch)
2017-06-02 07:41:03.316492: step 207420, loss = 0.32 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:41:04.205354: step 207430, loss = 0.31 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:41:05.090026: step 207440, loss = 0.25 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:41:05.958137: step 207450, loss = 0.26 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:41:06.793701: step 207460, loss = 0.24 (1531.9 examples/sec; 0.084 sec/batch)
2017-06-02 07:41:07.651007: step 207470, loss = 0.30 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:41:08.515720: step 207480, loss = 0.31 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:41:09.370773: step 207490, loss = 0.28 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:41:10.336856: step 207500, loss = 0.26 (1324.9 examples/sec; 0.097 sec/batch)
2017-06-02 07:41:11.101995: step 207510, loss = 0.24 (1672.9 examples/sec; 0.077 sec/batch)
2017-06-02 07:41:11.972429: step 207520, loss = 0.35 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:41:12.824267: step 207530, loss = 0.41 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:41:13.672761: step 207540, loss = 0.31 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:41:14.533333: step 207550, loss = 0.31 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:41:15.417963: step 207560, loss = 0.27 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:41:16.270226: step 207570, loss = 0.36 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:41:17.136972: step 207580, loss = 0.27 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:41:18.035404: step 207590, loss = 0.32 (1424.7 examples/sec; 0.090 sec/batch)
2017-06-02 07:41:19.071490: step 207600, loss = 0.26 (1235.4 examples/sec; 0.104 sec/batch)
2017-06-02 07:41:19.801134: step 207610, loss = 0.34 (1754.3 examples/sec; 0.073 sec/batch)
2017-06-02 07:41:20.652921: step 207620, loss = 0.28 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:41:21.517549: step 207630, loss = 0.27 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:41:22.402029: step 207640, loss = 0.27 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:41:23.259047: step 207650, loss = 0.38 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:41:24.119169: step 207660, loss = 0.32 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:41:24.991043: step 207670, loss = 0.35 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:41:25.853764: step 207680, loss = 0.42 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:41:26.701036: step 207690, loss = 0.36 (1510.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:41:27.686161: step 207700, loss = 0.26 (1299.3 examples/sec; 0.099 sec/batch)
2017-06-02 07:41:28.465839: step 207710, loss = 0.31 (1641.7 examples/sec; 0.078 sec/batch)
2017-06-02 07:41:29.343852: step 207720, loss = 0.28 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:41:30.217982: step 207730, loss = 0.29 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:41:31.070982: step 207740, loss = 0.34 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:41:31.937809: step 207750, loss = 0.20 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:41:32.786329: step 207760, loss = 0.32 (1508.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:41:33.664635: step 207770, loss = 0.23 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:41:34.531366: step 207780, loss = 0.43 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:41:35.410376: step 207790, loss = 0.27 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:41:36.398526: step 207800, loss = 0.21 (1295.3 examples/sec; 0.099 sec/batch)
2017-06-02 07:41:37.179106: step 207810, loss = 0.29 (1639.8 examples/sec; 0.078 sec/batch)
2017-06-02 07:41:38.037998: step 207820, loss = 0.21 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:41:38.903725: step 207830, loss = 0.26 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:41:39.798102: step 207840, loss = 0.32 (1431.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:41:40.644174: step 207850, loss = 0.30 (1512.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:41:41.492717: step 207860, loss = 0.41 (1508.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:41:42.379156: step 207870, loss = 0.27 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:41:43.247462: step 207880, loss = 0.25 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:41:44.122938: step 207890, loss = 0.32 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:41:45.114055: step 207900, loss = 0.26 (1291.5 examples/sec; 0.099 sec/batch)
2017-06-02 07:41:45.884249: step 207910, loss = 0.33 (1661.9 examples/sec; 0.077 sec/batch)
2017-06-02 07:41:46.748503: step 207920, loss = 0.28 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:41:47.609807: step 207930, loss = 0.22 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:41:48.442339: step 207940, loss = 0.27 (1537.5 examples/sec; 0.083 sec/batch)
2017-06-02 07:41:49.308064: step 207950, loss = 0.28 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:41:50.173576: step 207960, loss = 0.33 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:41:51.047656: step 207970, loss = 0.35 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:41:51.930397: step 207980, loss = 0.28 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:41:52.803297: step 207990, loss = 0.25 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:41:53.772974: step 208000, loss = 0.27 (1320.0 examples/sec; 0.097 sec/batch)
2017-06-02 07:41:54.547073: step 208010, loss = 0.35 (1653.5 examples/sec; 0.077 sec/batch)
2017-06-02 07:41:55.417106: step 208020, loss = 0.40 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:41:56.270229: step 208030, loss = 0.28 (1500.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:41:57.143428: step 208040, loss = 0.25 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:41:57.975080: step 208050, loss = 0.31 (1539.1 examples/sec; 0.083 sec/batch)
2017-06-02 07:41:58.815828: step 208060, loss = 0.24 (1522.5 examples/sec; 0.084 sec/batch)
2017-06-02 07:41:59.678167: step 208070, loss = 0.27 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:42:00.565074: step 208080, loss = 0.36 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:42:01.432696: step 208090, loss = 0.26 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:42:02.399815: step 208100, loss = 0.27 (1323.5 examples/sec; 0.097 sec/batch)
2017-06-02 07:42:03.172040: step 208110, loss = 0.40 (1657.6 examples/sec; 0.077 sec/batch)
2017-06-02 07:42:04.027618: step 208120, loss = 0.22 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:42:04.880580: step 208130, loss = 0.27 (1500.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:42:05.753016: step 208140, loss = 0.32 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:42:06.613712: step 208150, loss = 0.24 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:42:07.476923: step 208160, loss = 0.32 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:42:08.353836: step 208170, loss = 0.25 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:42:09.249563: step 208180, loss = 0.26 (1429.0 examples/sec; 0.090 sec/batch)
2017-06-02 07:42:10.098127: step 208190, loss = 0.23 (1508.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:42:11.092352: step 208200, loss = 0.27 (1287.4 examples/sec; 0.099 sec/batch)
2017-06-02 07:42:11.861790: step 208210, loss = 0.34 (1663.5 examples/sec; 0.077 sec/batch)
2017-06-02 07:42:12.732287: step 208220, loss = 0.30 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:42:13.570720: step 208230, loss = 0.27 (1526.7 examples/sec; 0.084 sec/batch)
2017-06-02 07:42:14.441194: step 208240, loss = 0.31 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:42:15.311550: step 208250, loss = 0.29 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:42:16.188244: step 208260, loss = 0.29 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:42:17.053948: step 208270, loss = 0.36 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:42:17.938784: step 208280, loss = 0.31 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:42:18.799656: step 208290, loss = 0.24 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:42:19.742781: step 208300, loss = 0.31 (1357.2 examples/sec; 0.094 sec/batch)
2017-06-02 07:42:20.527539: step 208310, loss = 0.33 (1631.1 examples/sec; 0.078 sec/batch)
2017-06-02 07:42:21.401044: step 208320, loss = 0.33 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:42:22.284583: step 208330, loss = 0.31 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:42:23.149888: step 208340, loss = 0.28 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:42:24.025634: step 208350, loss = 0.33 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:42:24.891778: step 208360, loss = 0.27 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:42:25.739686: step 208370, loss = 0.36 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:42:26.600180: step 208380, loss = 0.28 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:42:27.437377: step 208390, loss = 0.35 (1528.9 examples/sec; 0.084 sec/batch)
2017-06-02 07:42:28.418268: step 208400, loss = 0.31 (1304.9 examples/sec; 0.098 sec/batch)
2017-06-02 07:42:29.190102: step 208410, loss = 0.27 (1658.4 examples/sec; 0.077 sec/batch)
2017-06-02 07:42:30.042563: step 208420, loss = 0.26 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:42:30.904262: step 208430, loss = 0.30 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:42:31.768555: step 208440, loss = 0.29 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:42:32.604504: step 208450, loss = 0.38 (1531.2 examples/sec; 0.084 sec/batch)
2017-06-02 07:42:33.472519: step 208460, loss = 0.32 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:42:34.329221: step 208470, loss = 0.24 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:42:35.168447: step 208480, loss = 0.31 (1525.2 examples/sec; 0.084 sec/batch)
2017-06-02 07:42:36.035454: step 208490, loss = 0.28 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:42:37.021174: step 208500, loss = 0.25 (1298.5 examples/sec; 0.099 sec/batch)
2017-06-02 07:42:37.789534: step 208510, loss = 0.27 (1665.9 examples/sec; 0.077 sec/batch)
2017-06-02 07:42:38.647472: step 208520, loss = 0.26 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:42:39.506285: step 208530, loss = 0.28 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:42:40.364890: step 208540, loss = 0.31 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:42:41.242301: step 208550, loss = 0.32 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:42:42.094005: step 208560, loss = 0.27 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:42:42.928902: step 208570, loss = 0.27 (1533.1 examples/sec; 0.083 sec/batch)
2017-06-02 07:42:43.778539: step 208580, loss = 0.23 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:42:44.665388: step 208590, loss = 0.31 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:42:45.693522: step 208600, loss = 0.27 (1245.0 examples/sec; 0.103 sec/batch)
2017-06-02 07:42:46.392643: step 208610, loss = 0.31 (1830.9 examples/sec; 0.070 sec/batch)
2017-06-02 07:42:47.243196: step 208620, loss = 0.23 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:42:48.116942: step 208630, loss = 0.31 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:42:48.968776: step 208640, loss = 0.31 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:42:49.864579: step 208650, loss = 0.28 (1428.9 examples/sec; 0.090 sec/batch)
2017-06-02 07:42:50.712165: step 208660, loss = 0.34 (1510.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:42:51.578017: step 208670, loss = 0.26 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:42:52.474535: step 208680, loss = 0.30 (1427.7 examples/sec; 0.090 sec/batch)
2017-06-02 07:42:53.348570: step 208690, loss = 0.31 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:42:54.347947: step 208700, loss = 0.36 (1280.8 examples/sec; 0.100 sec/batch)
2017-06-02 07:42:55.101822: step 208710, loss = 0.30 (1697.9 examples/sec; 0.075 sec/batch)
2017-06-02 07:42:55.974835: step 208720, loss = 0.29 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:42:56.851113: step 208730, loss = 0.29 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:42:57.710443: step 208740, loss = 0.29 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:42:58.569768: step 208750, loss = 0.25 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:42:59.437722: step 208760, loss = 0.28 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:43:00.290280: step 208770, loss = 0.31 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:43:01.137343: step 208780, loss = 0.32 (1511.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:43:02.009054: step 208790, loss = 0.21 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:43:02.946054: step 208800, loss = 0.33 (1366.1 examples/sec; 0.094 sec/batch)
2017-06-02 07:43:03.722771: step 208810, loss = 0.29 (1647.9 examples/sec; 0.078 sec/batch)
2017-06-02 07:43:04.598180: step 208820, loss = 0.24 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:43:05.484363: step 208830, loss = 0.28 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:43:06.338292: step 208840, loss = 0.39 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:43:07.191727: step 208850, loss = 0.34 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:43:08.063920: step 208860, loss = 0.31 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:43:08.918816: step 208870, loss = 0.21 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:43:09.753712: step 208880, loss = 0.27 (1533.1 examples/sec; 0.083 sec/batch)
2017-06-02 07:43:10.635625: step 208890, loss = 0.32 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:43:11.604999: step 208900, loss = 0.35 (1320.4 examples/sec; 0.097 sec/batch)
2017-06-02 07:43:12.384126: step 208910, loss = 0.31 (1642.9 examples/sec; 0.078 sec/batch)
2017-06-02 07:43:13.228898: step 208920, loss = 0.31 (1515.2 examples/sec; 0.084 sec/batch)
2017-06-02 07:43:14.090829: step 208930, loss = 0.29 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:43:14.956844: step 208940, loss = 0.48 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:43:15.801925: step 208950, loss = 0.27 (1514.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:43:16.683304: step 208960, loss = 0.36 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:43:17.550530: step 208970, loss = 0.29 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:43:18.401536: step 208980, loss = 0.35 (1504.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:43:19.259674: step 208990, loss = 0.26 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:43:20.256312: step 209000, loss = 0.28 (1284.3 examples/sec; 0.100 sec/batch)
2017-06-02 07:43:20.995172: step 209010, loss = 0.34 (1732.4 examples/sec; 0.074 sec/batch)
2017-06-02 07:43:21.852138: step 209020, loss = 0.30 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:43:22.746812: step 209030, loss = 0.34 (1430.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:43:23.621575: step 209040, loss = 0.32 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:43:24.490515: step 209050, loss = 0.33 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:43:25.361985: step 209060, loss = 0.36 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:43:26.240614: step 209070, loss = 0.34 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:43:27.098319: step 209080, loss = 0.25 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:43:27.957971: step 209090, loss = 0.27 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:43:28.914545: step 209100, loss = 0.32 (1338.1 examples/sec; 0.096 sec/batch)
2017-06-02 07:43:29.699047: step 209110, loss = 0.29 (1631.6 examples/sec; 0.078 sec/batch)
2017-06-02 07:43:30.564431: step 209120, loss = 0.27 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:43:31.469144: step 209130, loss = 0.33 (1414.8 examples/sec; 0.090 sec/batch)
2017-06-02 07:43:32.357381: step 209140, loss = 0.26 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:43:33.228829: step 209150, loss = 0.25 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:43:34.114096: step 209160, loss = 0.25 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:43:35.004502: step 209170, loss = 0.27 (1437.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:43:35.897381: step 209180, loss = 0.34 (1433.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:43:36.768015: step 209190, loss = 0.31 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:43:37.759564: step 209200, loss = 0.29 (1290.9 examples/sec; 0.099 sec/batch)
2017-06-02 07:43:38.528324: step 209210, loss = 0.30 (1665.0 examples/sec; 0.077 sec/batch)
2017-06-02 07:43:39.418761: step 209220, loss = 0.29 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:43:40.297175: step 209230, loss = 0.22 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:43:41.154201: step 209240, loss = 0.27 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:43:42.016253: step 209250, loss = 0.29 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:43:42.879544: step 209260, loss = 0.36 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:43:43.761983: step 209270, loss = 0.25 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:43:44.645700: step 209280, loss = 0.36 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:43:45.529185: step 209290, loss = 0.37 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:43:46.507948: step 209300, loss = 0.26 (1307.8 examples/sec; 0.098 sec/batch)
2017-06-02 07:43:47.243333: step 209310, loss = 0.29 (1740.6 examples/sec; 0.074 sec/batch)
2017-06-02 07:43:48.086970: step 209320, loss = 0.26 (1517.2 examples/sec; 0.084 sec/batch)
2017-06-02 07:43:48.961211: step 209330, loss = 0.29 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:43:49.811982: step 209340, loss = 0.44 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:43:50.655109: step 209350, loss = 0.35 (1518.2 examples/sec; 0.084 sec/batch)
2017-06-02 07:43:51.503999: step 209360, loss = 0.28 (1507.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:43:52.363041: step 209370, loss = 0.28 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:43:53.224626: step 209380, loss = 0.30 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:43:54.087496: step 209390, loss = 0.26 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:43:55.091165: step 209400, loss = 0.33 (1275.3 examples/sec; 0.100 sec/batch)
2017-06-02 07:43:55.839681: step 209410, loss = 0.28 (1710.1 examples/sec; 0.075 sec/batch)
2017-06-02 07:43:56.728681: step 209420, loss = 0.33 (1439.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:43:57.603677: step 209430, loss = 0.25 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:43:58.495602: step 209440, loss = 0.31 (1435.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:43:59.359297: step 209450, loss = 0.23 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:44:00.245579: step 209460, loss = 0.27 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:44:01.152233: step 209470, loss = 0.33 (1411.8 examples/sec; 0.091 sec/batch)
2017-06-02 07:44:02.036097: step 209480, loss = 0.28 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:44:02.907713: step 209490, loss = 0.21 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:44:03.882415: step 209500, loss = 0.31 (1313.2 examples/sec; 0.097 sec/batch)
2017-06-02 07:44:04.664311: step 209510, loss = 0.29 (1637.1 examples/sec; 0.078 sec/batch)
2017-06-02 07:44:05.555853: step 209520, loss = 0.28 (1435.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:44:06.439379: step 209530, loss = 0.29 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:44:07.325015: step 209540, loss = 0.23 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:44:08.210174: step 209550, loss = 0.26 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:44:09.091367: step 209560, loss = 0.27 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:44:09.967312: step 209570, loss = 0.23 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:44:10.847335: step 209580, loss = 0.28 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:44:11.732334: step 209590, loss = 0.32 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:44:12.702431: step 209600, loss = 0.29 (1319.5 examples/sec; 0.097 sec/batch)
2017-06-02 07:44:13.466738: step 209610, loss = 0.31 (1674.7 examples/sec; 0.076 sec/batch)
2017-06-02 07:44:14.317262: step 209620, loss = 0.33 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:44:15.216843: step 209630, loss = 0.43 (1422.9 examples/sec; 0.090 sec/batch)
2017-06-02 07:44:16.077986: step 209640, loss = 0.40 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:44:16.936161: step 209650, loss = 0.42 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:44:17.799627: step 209660, loss = 0.27 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:44:18.643797: step 209670, loss = 0.27 (1516.3 examples/sec; 0.084 sec/batch)
2017-06-02 07:44:19.522359: step 209680, loss = 0.32 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:44:20.391821: step 209690, loss = 0.33 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:44:21.370788: step 209700, loss = 0.19 (1307.5 examples/sec; 0.098 sec/batch)
2017-06-02 07:44:22.148312: step 209710, loss = 0.26 (1646.3 examples/sec; 0.078 sec/batch)
2017-06-02 07:44:23.022828: step 209720, loss = 0.27 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:44:23.892249: step 209730, loss = 0.34 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:44:24.773334: step 209740, loss = 0.34 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:44:25.636904: step 209750, loss = 0.37 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:44:26.506360: step 209760, loss = 0.28 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:44:27.347111: step 209770, loss = 0.32 (1522.5 examples/sec; 0.084 sec/batch)
2017-06-02 07:44:28.198698: step 209780, loss = 0.38 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:44:29.071552: step 209790, loss = 0.30 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:44:30.039058: step 209800, loss = 0.27 (1323.0 examples/sec; 0.097 sec/batch)
2017-06-02 07:44:30.770840: step 209810, loss = 0.28 (1749.2 examples/sec; 0.073 sec/batch)
2017-06-02 07:44:31.628569: step 209820, loss = 0.30 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:44:32.473895: step 209830, loss = 0.24 (1514.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:44:33.316788: step 209840, loss = 0.30 (1518.6 examples/sec; 0.084 sec/batch)
2017-06-02 07:44:34.199035: step 209850, loss = 0.27 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:44:35.069117: step 209860, loss = 0.35 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:44:35.941463: step 209870, loss = 0.25 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:44:36.814048: step 209880, loss = 0.35 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:44:37.661946: step 209890, loss = 0.32 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:44:38.602668: step 209900, loss = 0.29 (1360.7 examples/sec; 0.094 sec/batch)
2017-06-02 07:44:39.352647: step 209910, loss = 0.28 (1706.7 examples/sec; 0.075 sec/batch)
2017-06-02 07:44:40.237328: step 209920, loss = 0.23 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:44:41.100078: step 209930, loss = 0.35 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:44:41.982439: step 209940, loss = 0.28 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:44:42.858436: step 209950, loss = 0.29 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:44:43.715750: step 209960, loss = 0.23 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:44:44.572004: step 209970, loss = 0.26 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:44:45.422804: step 209980, loss = 0.22 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:44:46.260585: step 209990, loss = 0.27 (1527.8 examples/sec; 0.084 sec/batch)
2017-06-02 07:44:47.267020: step 210000, loss = 0.32 (1271.8 examples/sec; 0.101 sec/batch)
2017-06-02 07:44:47.965076: step 210010, loss = 0.31 (1833.7 examples/sec; 0.070 sec/batch)
2017-06-02 07:44:48.848703: step 210020, loss = 0.29 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:44:49.738549: step 210030, loss = 0.31 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:44:50.580237: step 210040, loss = 0.36 (1520.8 examples/sec; 0.084 sec/batch)
2017-06-02 07:44:51.442006: step 210050, loss = 0.26 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:44:52.283442: step 210060, loss = 0.28 (1521.2 examples/sec; 0.084 sec/batch)
2017-06-02 07:44:53.138271: step 210070, loss = 0.31 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:44:54.007401: step 210080, loss = 0.26 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:44:54.854756: step 210090, loss = 0.38 (1510.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:44:55.812631: step 210100, loss = 0.32 (1336.3 examples/sec; 0.096 sec/batch)
2017-06-02 07:44:56.580169: step 210110, loss = 0.31 (1667.6 examples/sec; 0.077 sec/batch)
2017-06-02 07:44:57.426653: step 210120, loss = 0.29 (1512.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:44:58.299752: step 210130, loss = 0.28 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:44:59.189862: step 210140, loss = 0.29 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:45:00.043689: step 210150, loss = 0.29 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:45:00.898684: step 210160, loss = 0.27 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:45:01.767015: step 210170, loss = 0.35 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:45:02.622576: step 210180, loss = 0.46 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:45:03.494313: step 210190, loss = 0.35 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:45:04.469364: step 210200, loss = 0.24 (1312.8 examples/sec; 0.098 sec/batch)
2017-06-02 07:45:05.240420: step 210210, loss = 0.28 (1660.1 examples/sec; 0.077 sec/batch)
2017-06-02 07:45:06.123437: step 210220, loss = 0.32 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:45:06.981830: step 210230, loss = 0.29 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:45:07.831526: step 210240, loss = 0.30 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:45:08.678313: step 210250, loss = 0.30 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:45:09.526254: step 210260, loss = 0.34 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:45:10.381652: step 210270, loss = 0.21 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:45:11.290587: step 210280, loss = 0.40 (1408.3 examples/sec; 0.091 sec/batch)
2017-06-02 07:45:12.147936: step 210290, loss = 0.38 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:45:13.086676: step 210300, loss = 0.24 (1363.5 examples/sec; 0.094 sec/batch)
2017-06-02 07:45:13.847745: step 210310, loss = 0.24 (1681.9 examples/sec; 0.076 sec/batch)
2017-06-02 07:45:14.703580: step 210320, loss = 0.29 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:45:15.571532: step 210330, loss = 0.33 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:45:16.423496: step 210340, loss = 0.31 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:45:17.291002: step 210350, loss = 0.30 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:45:18.142350: step 210360, loss = 0.25 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:45:19.004880: step 210370, loss = 0.41 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:45:19.843809: step 210380, loss = 0.21 (1525.8 examples/sec; 0.084 sec/batch)
2017-06-02 07:45:20.704461: step 210390, loss = 0.28 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:45:21.675133: step 210400, loss = 0.34 (1318.7 examples/sec; 0.097 sec/batch)
2017-06-02 07:45:22.438178: step 210410, loss = 0.36 (1677.5 examples/sec; 0.076 sec/batch)
2017-06-02 07:45:23.316071: step 210420, loss = 0.30 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:45:24.193445: step 210430, loss = 0.31 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:45:25.050034: step 210440, loss = 0.31 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:45:25.929757: step 210450, loss = 0.34 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:45:26.811553: step 210460, loss = 0.27 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:45:27.692675: step 210470, loss = 0.31 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:45:28.547431: step 210480, loss = 0.24 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:45:29.418305: step 210490, loss = 0.31 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:45:30.408038: step 210500, loss = 0.34 (1293.3 examples/sec; 0.099 sec/batch)
2017-06-02 07:45:31.181023: step 210510, loss = 0.21 (1655.9 examples/sec; 0.077 sec/batch)
2017-06-02 07:45:32.036810: step 210520, loss = 0.29 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:45:32.908870: step 210530, loss = 0.25 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:45:33.783675: step 210540, loss = 0.22 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:45:34.627869: step 210550, loss = 0.26 (1516.2 examples/sec; 0.084 sec/batch)
2017-06-02 07:45:35.477160: step 210560, loss = 0.34 (1507.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:45:36.352363: step 210570, loss = 0.31 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:45:37.253880: step 210580, loss = 0.23 (1419.8 examples/sec; 0.090 sec/batch)
2017-06-02 07:45:38.098424: step 210590, loss = 0.27 (1515.6 examples/sec; 0.084 sec/batch)
2017-06-02 07:45:39.043378: step 210600, loss = 0.37 (1354.6 examples/sec; 0.094 sec/batch)
2017-06-02 07:45:39.828838: step 210610, loss = 0.33 (1629.6 examples/sec; 0.079 sec/batch)
2017-06-02 07:45:40.691971: step 210620, loss = 0.32 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:45:41.561409: step 210630, loss = 0.21 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:45:42.414796: step 210640, loss = 0.30 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:45:43.318823: step 210650, loss = 0.29 (1415.9 examples/sec; 0.090 sec/batch)
2017-06-02 07:45:44.208195: step 210660, loss = 0.30 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:45:45.109297: step 210670, loss = 0.27 (1420.5 examples/sec; 0.090 sec/batch)
2017-06-02 07:45:45.987799: step 210680, loss = 0.30 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:45:46.868772: step 210690, loss = 0.31 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:45:47.869436: step 210700, loss = 0.30 (1279.1 examples/sec; 0.100 sec/batch)
2017-06-02 07:45:48.679448: step 210710, loss = 0.27 (1580.2 examples/sec; 0.081 sec/batch)
2017-06-02 07:45:49.581000: step 210720, loss = 0.41 (1419.8 examples/sec; 0.090 sec/batch)
2017-06-02 07:45:50.464226: step 210730, loss = 0.29 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:45:51.373328: step 210740, loss = 0.32 (1408.0 examples/sec; 0.091 sec/batch)
2017-06-02 07:45:52.270706: step 210750, loss = 0.34 (1426.4 examples/sec; 0.090 sec/batch)
2017-06-02 07:45:53.146040: step 210760, loss = 0.32 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:45:54.013992: step 210770, loss = 0.25 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:45:54.855315: step 210780, loss = 0.32 (1521.4 examples/sec; 0.084 sec/batch)
2017-06-02 07:45:55.693875: step 210790, loss = 0.28 (1526.4 examples/sec; 0.084 sec/batch)
2017-06-02 07:45:56.644518: step 210800, loss = 0.44 (1346.4 examples/sec; 0.095 sec/batch)
2017-06-02 07:45:57.395965: step 210810, loss = 0.25 (1703.4 examples/sec; 0.075 sec/batch)
2017-06-02 07:45:58.232192: step 210820, loss = 0.33 (1530.7 examples/sec; 0.084 sec/batch)
2017-06-02 07:45:59.078024: step 210830, loss = 0.22 (1513.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:45:59.928183: step 210840, loss = 0.35 (1505.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:46:00.801361: step 210850, loss = 0.26 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:46:01.654970: step 210860, loss = 0.23 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:46:02.507417: step 210870, loss = 0.25 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:46:03.335805: step 210880, loss = 0.26 (1545.2 examples/sec; 0.083 sec/batch)
2017-06-02 07:46:04.200538: step 210890, loss = 0.33 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:46:05.199884: step 210900, loss = 0.25 (1280.8 examples/sec; 0.100 sec/batch)
2017-06-02 07:46:05.949765: step 210910, loss = 0.25 (1706.9 examples/sec; 0.075 sec/batch)
2017-06-02 07:46:06.821117: step 210920, loss = 0.33 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:46:07.668331: step 210930, loss = 0.34 (1510.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:46:08.516608: step 210940, loss = 0.29 (1508.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:46:09.373529: step 210950, loss = 0.29 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:46:10.220183: step 210960, loss = 0.31 (1511.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:46:11.126760: step 210970, loss = 0.21 (1411.9 examples/sec; 0.091 sec/batch)
2017-06-02 07:46:12.018012: step 210980, loss = 0.32 (1436.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:46:12.885821: step 210990, loss = 0.29 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:46:13.842626: step 211000, loss = 0.22 (1337.8 examples/sec; 0.096 sec/batch)
2017-06-02 07:46:14.636344: step 211010, loss = 0.29 (1612.7 examples/sec; 0.079 sec/batch)
2017-06-02 07:46:15.514263: step 211020, loss = 0.26 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:46:16.394411: step 211030, loss = 0.31 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:46:17.285121: step 211040, loss = 0.25 (1437.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:46:18.183024: step 211050, loss = 0.27 (1425.5 examples/sec; 0.090 sec/batch)
2017-06-02 07:46:19.067553: step 211060, loss = 0.32 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:46:19.939876: step 211070, loss = 0.29 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:46:20.797027: step 211080, loss = 0.30 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:46:21.685159: step 211090, loss = 0.29 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:46:22.707098: step 211100, loss = 0.35 (1252.5 examples/sec; 0.102 sec/batch)
2017-06-02 07:46:23.409346: step 211110, loss = 0.33 (1822.7 examples/sec; 0.070 sec/batch)
2017-06-02 07:46:24.289674: step 211120, loss = 0.28 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:46:25.183959: step 211130, loss = 0.28 (1431.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:46:26.070533: step 211140, loss = 0.31 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:46:26.936937: step 211150, loss = 0.29 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:46:27.831217: step 211160, loss = 0.32 (1431.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:46:28.724862: step 211170, loss = 0.31 (1432.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:46:29.630076: step 211180, loss = 0.24 (1414.0 examples/sec; 0.091 sec/batch)
2017-06-02 07:46:30.515642: step 211190, loss = 0.30 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:46:31.492011: step 211200, loss = 0.31 (1310.9 examples/sec; 0.098 sec/batch)
2017-06-02 07:46:32.256752: step 211210, loss = 0.38 (1673.8 examples/sec; 0.076 sec/batch)
2017-06-02 07:46:33.131366: step 211220, loss = 0.32 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:46:34.020958: step 211230, loss = 0.27 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:46:34.918882: step 211240, loss = 0.38 (1425.5 examples/sec; 0.090 sec/batch)
2017-06-02 07:46:35.795296: step 211250, loss = 0.34 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:46:36.664143: step 211260, loss = 0.27 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:46:37.543110: step 211270, loss = 0.30 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:46:38.410027: step 211280, loss = 0.24 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:46:39.282264: step 211290, loss = 0.31 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:46:40.240541: step 211300, loss = 0.38 (1335.7 examples/sec; 0.096 sec/batch)
2017-06-02 07:46:41.029129: step 211310, loss = 0.27 (1623.2 examples/sec; 0.079 sec/batch)
2017-06-02 07:46:41.898573: step 211320, loss = 0.28 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:46:42.768706: step 211330, loss = 0.31 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:46:43.615802: step 211340, loss = 0.30 (1511.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:46:44.479949: step 211350, loss = 0.33 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:46:45.361988: step 211360, loss = 0.37 (1451.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:46:46.208602: step 211370, loss = 0.29 (1511.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:46:47.069331: step 211380, loss = 0.25 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:46:47.923297: step 211390, loss = 0.34 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:46:48.912002: step 211400, loss = 0.29 (1294.6 examples/sec; 0.099 sec/batch)
2017-06-02 07:46:49.687552: step 211410, loss = 0.26 (1650.4 examples/sec; 0.078 sec/batch)
2017-06-02 07:46:50.558030: step 211420, loss = 0.28 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:46:51.407155: step 211430, loss = 0.39 (1507.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:46:52.289728: step 211440, loss = 0.29 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:46:53.167260: step 211450, loss = 0.33 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:46:54.032152: step 211460, loss = 0.33 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:46:54.904817: step 211470, loss = 0.23 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:46:55.802875: step 211480, loss = 0.35 (1425.3 examples/sec; 0.090 sec/batch)
2017-06-02 07:46:56.689676: step 211490, loss = 0.29 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:46:57.650855: step 211500, loss = 0.31 (1331.7 examples/sec; 0.096 sec/batch)
2017-06-02 07:46:58.451424: step 211510, loss = 0.30 (1598.9 examples/sec; 0.080 sec/batch)
2017-06-02 07:46:59.342348: step 211520, loss = 0.25 (1436.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:47:00.210555: step 211530, loss = 0.28 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:47:01.066615: step 211540, loss = 0.31 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:47:01.962228: step 211550, loss = 0.27 (1429.1 examples/sec; 0.090 sec/batch)
2017-06-02 07:47:02.840886: step 211560, loss = 0.25 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:47:03.682047: step 211570, loss = 0.32 (1521.7 examples/sec; 0.084 sec/batch)
2017-06-02 07:47:04.568785: step 211580, loss = 0.30 (1443.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:47:05.458788: step 211590, loss = 0.27 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:47:06.430500: step 211600, loss = 0.44 (1317.3 examples/sec; 0.097 sec/batch)
2017-06-02 07:47:07.205758: step 211610, loss = 0.28 (1651.1 examples/sec; 0.078 sec/batch)
2017-06-02 07:47:08.071870: step 211620, loss = 0.23 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:47:08.931761: step 211630, loss = 0.29 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:47:09.796053: step 211640, loss = 0.35 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:47:10.699516: step 211650, loss = 0.29 (1416.8 examples/sec; 0.090 sec/batch)
2017-06-02 07:47:11.580551: step 211660, loss = 0.29 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:47:12.458576: step 211670, loss = 0.27 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:47:13.324231: step 211680, loss = 0.34 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:47:14.195376: step 211690, loss = 0.37 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:47:15.173815: step 211700, loss = 0.27 (1308.2 examples/sec; 0.098 sec/batch)
2017-06-02 07:47:15.930748: step 211710, loss = 0.25 (1691.1 examples/sec; 0.076 sec/batch)
2017-06-02 07:47:16.810100: step 211720, loss = 0.27 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:47:17.656954: step 211730, loss = 0.23 (1511.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:47:18.530340: step 211740, loss = 0.30 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:47:19.406123: step 211750, loss = 0.27 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:47:20.258752: step 211760, loss = 0.27 (1501.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:47:21.129163: step 211770, loss = 0.37 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:47:21.991883: step 211780, loss = 0.26 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:47:22.860055: step 211790, loss = 0.43 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:47:23.816321: step 211800, loss = 0.25 (1338.5 examples/sec; 0.096 sec/batch)
2017-06-02 07:47:24.603477: step 211810, loss = 0.33 (1626.1 examples/sec; 0.079 sec/batch)
2017-06-02 07:47:25.452216: step 211820, loss = 0.35 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:47:26.334969: step 211830, loss = 0.34 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:47:27.210675: step 211840, loss = 0.28 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:47:28.090821: step 211850, loss = 0.25 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:47:28.968832: step 211860, loss = 0.28 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:47:29.801162: step 211870, loss = 0.27 (1537.9 examples/sec; 0.083 sec/batch)
2017-06-02 07:47:30.670032: step 211880, loss = 0.34 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:47:31.530492: step 211890, loss = 0.35 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:47:32.497666: step 211900, loss = 0.37 (1323.4 examples/sec; 0.097 sec/batch)
2017-06-02 07:47:33.281086: step 211910, loss = 0.30 (1633.9 examples/sec; 0.078 sec/batch)
2017-06-02 07:47:34.147473: step 211920, loss = 0.25 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:47:35.021850: step 211930, loss = 0.23 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:47:35.875114: step 211940, loss = 0.36 (1500.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:47:36.707152: step 211950, loss = 0.27 (1538.4 examples/sec; 0.083 sec/batch)
2017-06-02 07:47:37.591214: step 211960, loss = 0.25 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:47:38.457483: step 211970, loss = 0.26 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:47:39.325479: step 211980, loss = 0.29 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:47:40.185628: step 211990, loss = 0.33 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:47:41.153255: step 212000, loss = 0.32 (1322.8 examples/sec; 0.097 sec/batch)
2017-06-02 07:47:41.891315: step 212010, loss = 0.29 (1734.3 examples/sec; 0.074 sec/batch)
2017-06-02 07:47:42.755846: step 212020, loss = 0.29 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:47:43.615710: step 212030, loss = 0.30 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:47:44.465774: step 212040, loss = 0.24 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:47:45.292770: step 212050, loss = 0.24 (1547.8 examples/sec; 0.083 sec/batch)
2017-06-02 07:47:46.172362: step 212060, loss = 0.29 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:47:47.046242: step 212070, loss = 0.34 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:47:47.898160: step 212080, loss = 0.32 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:47:48.782308: step 212090, loss = 0.32 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:47:49.748609: step 212100, loss = 0.29 (1324.6 examples/sec; 0.097 sec/batch)
2017-06-02 07:47:50.537156: step 212110, loss = 0.21 (1623.2 examples/sec; 0.079 sec/batch)
2017-06-02 07:47:51.384053: step 212120, loss = 0.27 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:47:52.237676: step 212130, loss = 0.38 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:47:53.106122: step 212140, loss = 0.30 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:47:53.974502: step 212150, loss = 0.29 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:47:54.843358: step 212160, loss = 0.27 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:47:55.680422: step 212170, loss = 0.27 (1529.2 examples/sec; 0.084 sec/batch)
2017-06-02 07:47:56.566211: step 212180, loss = 0.25 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:47:57.431928: step 212190, loss = 0.28 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:47:58.394903: step 212200, loss = 0.33 (1329.2 examples/sec; 0.096 sec/batch)
2017-06-02 07:47:59.194953: step 212210, loss = 0.33 (1599.9 examples/sec; 0.080 sec/batch)
2017-06-02 07:48:00.101048: step 212220, loss = 0.28 (1412.7 examples/sec; 0.091 sec/batch)
2017-06-02 07:48:00.987210: step 212230, loss = 0.33 (1444.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:48:01.858563: step 212240, loss = 0.28 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:48:02.736827: step 212250, loss = 0.29 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:48:03.619269: step 212260, loss = 0.31 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:48:04.482178: step 212270, loss = 0.36 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:48:05.344705: step 212280, loss = 0.33 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:48:06.202510: step 212290, loss = 0.37 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:48:07.191580: step 212300, loss = 0.25 (1294.1 examples/sec; 0.099 sec/batch)
2017-06-02 07:48:07.959161: step 212310, loss = 0.30 (1667.6 examples/sec; 0.077 sec/batch)
2017-06-02 07:48:08.833539: step 212320, loss = 0.35 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:48:09.716674: step 212330, loss = 0.33 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:48:10.577666: step 212340, loss = 0.28 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:48:11.429047: step 212350, loss = 0.27 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:48:12.320242: step 212360, loss = 0.25 (1436.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:48:13.195493: step 212370, loss = 0.32 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:48:14.068252: step 212380, loss = 0.32 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:48:14.925119: step 212390, loss = 0.29 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:48:15.873046: step 212400, loss = 0.24 (1350.3 examples/sec; 0.095 sec/batch)
2017-06-02 07:48:16.645271: step 212410, loss = 0.28 (1657.5 examples/sec; 0.077 sec/batch)
2017-06-02 07:48:17.511333: step 212420, loss = 0.37 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:48:18.365302: step 212430, loss = 0.29 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:48:19.254014: step 212440, loss = 0.25 (1440.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:48:20.104876: step 212450, loss = 0.32 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:48:20.946437: step 212460, loss = 0.31 (1521.0 examples/sec; 0.084 sec/batch)
2017-06-02 07:48:21.801965: step 212470, loss = 0.31 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:48:22.648752: step 212480, loss = 0.42 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:48:23.515465: step 212490, loss = 0.25 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:48:24.539148: step 212500, loss = 0.27 (1250.4 examples/sec; 0.102 sec/batch)
2017-06-02 07:48:25.261474: step 212510, loss = 0.30 (1772.0 examples/sec; 0.072 sec/batch)
2017-06-02 07:48:26.113072: step 212520, loss = 0.34 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:48:26.973126: step 212530, loss = 0.35 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:48:27.861194: step 212540, loss = 0.25 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:48:28.716319: step 212550, loss = 0.38 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:48:29.584639: step 212560, loss = 0.30 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:48:30.465120: step 212570, loss = 0.27 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:48:31.325557: step 212580, loss = 0.34 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:48:32.191260: step 212590, loss = 0.35 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:48:33.153893: step 212600, loss = 0.29 (1329.7 examples/sec; 0.096 sec/batch)
2017-06-02 07:48:33.923686: step 212610, loss = 0.22 (1662.8 examples/sec; 0.077 sec/batch)
2017-06-02 07:48:34.768216: step 212620, loss = 0.40 (1515.6 examples/sec; 0.084 sec/batch)
2017-06-02 07:48:35.638123: step 212630, loss = 0.21 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:48:36.523180: step 212640, loss = 0.22 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:48:37.377394: step 212650, loss = 0.26 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:48:38.253471: step 212660, loss = 0.21 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:48:39.125685: step 212670, loss = 0.34 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:48:39.986179: step 212680, loss = 0.35 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:48:40.881300: step 212690, loss = 0.28 (1430.0 examples/sec; 0.090 sec/batch)
2017-06-02 07:48:41.856919: step 212700, loss = 0.26 (1312.0 examples/sec; 0.098 sec/batch)
2017-06-02 07:48:42.619378: step 212710, loss = 0.31 (1678.8 examples/sec; 0.076 sec/batch)
2017-06-02 07:48:43.473840: step 212720, loss = 0.27 (1498.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:48:44.331688: step 212730, loss = 0.31 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:48:45.199207: step 212740, loss = 0.30 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:48:46.076204: step 212750, loss = 0.28 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:48:46.943428: step 212760, loss = 0.28 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:48:47.807948: step 212770, loss = 0.29 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:48:48.671927: step 212780, loss = 0.32 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:48:49.580333: step 212790, loss = 0.23 (1409.0 examples/sec; 0.091 sec/batch)
2017-06-02 07:48:50.551669: step 212800, loss = 0.28 (1317.8 examples/sec; 0.097 sec/batch)
2017-06-02 07:48:51.314267: step 212810, loss = 0.41 (1678.5 examples/sec; 0.076 sec/batch)
2017-06-02 07:48:52.171677: step 212820, loss = 0.26 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:48:53.071034: step 212830, loss = 0.25 (1423.2 examples/sec; 0.090 sec/batch)
2017-06-02 07:48:53.953774: step 212840, loss = 0.25 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:48:54.815136: step 212850, loss = 0.31 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:48:55.698178: step 212860, loss = 0.34 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:48:56.558648: step 212870, loss = 0.32 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:48:57.430363: step 212880, loss = 0.29 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:48:58.284137: step 212890, loss = 0.38 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:48:59.248775: step 212900, loss = 0.40 (1326.9 examples/sec; 0.096 sec/batch)
2017-06-02 07:49:00.011544: step 212910, loss = 0.24 (1678.1 examples/sec; 0.076 sec/batch)
2017-06-02 07:49:00.882504: step 212920, loss = 0.29 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:49:01.767760: step 212930, loss = 0.34 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:49:02.637331: step 212940, loss = 0.33 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:49:03.511793: step 212950, loss = 0.29 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:49:04.397166: step 212960, loss = 0.26 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:49:05.258204: step 212970, loss = 0.28 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:49:06.098082: step 212980, loss = 0.38 (1524.0 examples/sec; 0.084 sec/batch)
2017-06-02 07:49:06.938379: step 212990, loss = 0.24 (1523.3 examples/sec; 0.084 sec/batch)
2017-06-02 07:49:07.920076: step 213000, loss = 0.31 (1303.9 examples/sec; 0.098 sec/batch)
2017-06-02 07:49:08.693556: step 213010, loss = 0.29 (1654.9 examples/sec; 0.077 sec/batch)
2017-06-02 07:49:09.543323: step 213020, loss = 0.27 (1506.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:49:10.388674: step 213030, loss = 0.38 (1514.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:49:11.238867: step 213040, loss = 0.33 (1505.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:49:12.085240: step 213050, loss = 0.28 (1512.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:49:12.931875: step 213060, loss = 0.27 (1511.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:49:13.789590: step 213070, loss = 0.27 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:49:14.650120: step 213080, loss = 0.34 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:49:15.519449: step 213090, loss = 0.24 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:49:16.502380: step 213100, loss = 0.36 (1302.2 examples/sec; 0.098 sec/batch)
2017-06-02 07:49:17.252510: step 213110, loss = 0.35 (1706.4 examples/sec; 0.075 sec/batch)
2017-06-02 07:49:18.116116: step 213120, loss = 0.29 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:49:18.976630: step 213130, loss = 0.22 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:49:19.849524: step 213140, loss = 0.31 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:49:20.734255: step 213150, loss = 0.33 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:49:21.622074: step 213160, loss = 0.26 (1441.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:49:22.515999: step 213170, loss = 0.31 (1431.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:49:23.413755: step 213180, loss = 0.23 (1425.8 examples/sec; 0.090 sec/batch)
2017-06-02 07:49:24.286971: step 213190, loss = 0.27 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:49:25.275283: step 213200, loss = 0.41 (1295.1 examples/sec; 0.099 sec/batch)
2017-06-02 07:49:26.071407: step 213210, loss = 0.35 (1607.8 examples/sec; 0.080 sec/batch)
2017-06-02 07:49:26.940124: step 213220, loss = 0.31 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:49:27.831079: step 213230, loss = 0.37 (1436.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:49:28.719372: step 213240, loss = 0.32 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:49:29.598887: step 213250, loss = 0.29 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:49:30.496965: step 213260, loss = 0.36 (1425.3 examples/sec; 0.090 sec/batch)
2017-06-02 07:49:31.370157: step 213270, loss = 0.34 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:49:32.245822: step 213280, loss = 0.31 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:49:33.134304: step 213290, loss = 0.27 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:49:34.099818: step 213300, loss = 0.33 (1325.7 examples/sec; 0.097 sec/batch)
2017-06-02 07:49:34.892417: step 213310, loss = 0.31 (1615.0 examples/sec; 0.079 sec/batch)
2017-06-02 07:49:35.805219: step 213320, loss = 0.30 (1402.3 examples/sec; 0.091 sec/batch)
2017-06-02 07:49:36.688483: step 213330, loss = 0.23 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:49:37.576050: step 213340, loss = 0.37 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:49:38.447592: step 213350, loss = 0.25 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:49:39.355090: step 213360, loss = 0.36 (1410.5 examples/sec; 0.091 sec/batch)
2017-06-02 07:49:40.239949: step 213370, loss = 0.27 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:49:41.107398: step 213380, loss = 0.26 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:49:41.958785: step 213390, loss = 0.34 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:49:42.945045: step 213400, loss = 0.32 (1297.8 examples/sec; 0.099 sec/batch)
2017-06-02 07:49:43.729797: step 213410, loss = 0.22 (1631.1 examples/sec; 0.078 sec/batch)
2017-06-02 07:49:44.639790: step 213420, loss = 0.28 (1406.6 examples/sec; 0.091 sec/batch)
2017-06-02 07:49:45.509522: step 213430, loss = 0.32 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:49:46.380291: step 213440, loss = 0.33 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:49:47.248167: step 213450, loss = 0.26 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:49:48.133207: step 213460, loss = 0.23 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:49:49.019176: step 213470, loss = 0.27 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:49:49.916116: step 213480, loss = 0.35 (1427.1 examples/sec; 0.090 sec/batch)
2017-06-02 07:49:50.774532: step 213490, loss = 0.35 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:49:51.765713: step 213500, loss = 0.35 (1291.4 examples/sec; 0.099 sec/batch)
2017-06-02 07:49:52.599572: step 213510, loss = 0.22 (1535.0 examples/sec; 0.083 sec/batch)
2017-06-02 07:49:53.466221: step 213520, loss = 0.32 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:49:54.325899: step 213530, loss = 0.20 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:49:55.190866: step 213540, loss = 0.30 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:49:56.068440: step 213550, loss = 0.26 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:49:56.948125: step 213560, loss = 0.23 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:49:57.789049: step 213570, loss = 0.32 (1522.1 examples/sec; 0.084 sec/batch)
2017-06-02 07:49:58.658690: step 213580, loss = 0.26 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:49:59.548568: step 213590, loss = 0.30 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:50:00.553759: step 213600, loss = 0.31 (1273.4 examples/sec; 0.101 sec/batch)
2017-06-02 07:50:01.287728: step 213610, loss = 0.26 (1743.9 examples/sec; 0.073 sec/batch)
2017-06-02 07:50:02.133959: step 213620, loss = 0.25 (1512.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:50:02.954848: step 213630, loss = 0.35 (1559.3 examples/sec; 0.082 sec/batch)
2017-06-02 07:50:03.808117: step 213640, loss = 0.21 (1500.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:50:04.681076: step 213650, loss = 0.28 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:50:05.533972: step 213660, loss = 0.19 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:50:06.402670: step 213670, loss = 0.28 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:50:07.259467: step 213680, loss = 0.29 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:50:08.109677: step 213690, loss = 0.34 (1505.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:50:09.092353: step 213700, loss = 0.32 (1302.6 examples/sec; 0.098 sec/batch)
2017-06-02 07:50:09.825237: step 213710, loss = 0.26 (1746.5 examples/sec; 0.073 sec/batch)
2017-06-02 07:50:10.664804: step 213720, loss = 0.32 (1524.6 examples/sec; 0.084 sec/batch)
2017-06-02 07:50:11.530900: step 213730, loss = 0.26 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:50:12.361082: step 213740, loss = 0.24 (1541.8 examples/sec; 0.083 sec/batch)
2017-06-02 07:50:13.198338: step 213750, loss = 0.32 (1528.8 examples/sec; 0.084 sec/batch)
2017-06-02 07:50:14.076551: step 213760, loss = 0.31 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:50:14.943210: step 213770, loss = 0.28 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:50:15.807961: step 213780, loss = 0.26 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:50:16.692144: step 213790, loss = 0.26 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:50:17.688678: step 213800, loss = 0.25 (1284.4 examples/sec; 0.100 sec/batch)
2017-06-02 07:50:18.464524: step 213810, loss = 0.29 (1649.8 examples/sec; 0.078 sec/batch)
2017-06-02 07:50:19.361615: step 213820, loss = 0.28 (1426.8 examples/sec; 0.090 sec/batch)
2017-06-02 07:50:20.246430: step 213830, loss = 0.31 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:50:21.145951: step 213840, loss = 0.32 (1423.0 examples/sec; 0.090 sec/batch)
2017-06-02 07:50:22.037319: step 213850, loss = 0.31 (1436.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:50:22.912681: step 213860, loss = 0.30 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:50:23.811205: step 213870, loss = 0.25 (1424.6 examples/sec; 0.090 sec/batch)
2017-06-02 07:50:24.704929: step 213880, loss = 0.30 (1432.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:50:25.580887: step 213890, loss = 0.29 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:50:26.561619: step 213900, loss = 0.29 (1305.1 examples/sec; 0.098 sec/batch)
2017-06-02 07:50:27.329666: step 213910, loss = 0.29 (1666.6 examples/sec; 0.077 sec/batch)
2017-06-02 07:50:28.222527: step 213920, loss = 0.31 (1433.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:50:29.104244: step 213930, loss = 0.28 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:50:29.979972: step 213940, loss = 0.31 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:50:30.859029: step 213950, loss = 0.36 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:50:31.751410: step 213960, loss = 0.32 (1434.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:50:32.625406: step 213970, loss = 0.29 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:50:33.492565: step 213980, loss = 0.24 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:50:34.379096: step 213990, loss = 0.30 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:50:35.406885: step 214000, loss = 0.29 (1245.4 examples/sec; 0.103 sec/batch)
2017-06-02 07:50:36.110246: step 214010, loss = 0.29 (1819.8 examples/sec; 0.070 sec/batch)
2017-06-02 07:50:36.987714: step 214020, loss = 0.23 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:50:37.863411: step 214030, loss = 0.22 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:50:38.755593: step 214040, loss = 0.37 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:50:39.585131: step 214050, loss = 0.27 (1543.0 examples/sec; 0.083 sec/batch)
2017-06-02 07:50:40.476106: step 214060, loss = 0.30 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:50:41.329889: step 214070, loss = 0.29 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:50:42.222922: step 214080, loss = 0.24 (1433.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:50:43.086699: step 214090, loss = 0.23 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:50:44.032004: step 214100, loss = 0.41 (1354.0 examples/sec; 0.095 sec/batch)
2017-06-02 07:50:44.805173: step 214110, loss = 0.32 (1655.5 examples/sec; 0.077 sec/batch)
2017-06-02 07:50:45.680598: step 214120, loss = 0.30 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:50:46.568090: step 214130, loss = 0.33 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:50:47.453419: step 214140, loss = 0.25 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:50:48.324204: step 214150, loss = 0.31 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:50:49.184008: step 214160, loss = 0.23 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:50:50.051722: step 214170, loss = 0.33 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:50:50.902613: step 214180, loss = 0.24 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:50:51.774511: step 214190, loss = 0.33 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:50:52.757264: step 214200, loss = 0.25 (1302.5 examples/sec; 0.098 sec/batch)
2017-06-02 07:50:53.533149: step 214210, loss = 0.28 (1649.8 examples/sec; 0.078 sec/batch)
2017-06-02 07:50:54.384176: step 214220, loss = 0.28 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:50:55.267621: step 214230, loss = 0.27 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:50:56.134169: step 214240, loss = 0.36 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:50:57.018697: step 214250, loss = 0.30 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:50:57.890381: step 214260, loss = 0.28 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:50:58.760140: step 214270, loss = 0.28 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:50:59.630467: step 214280, loss = 0.42 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:51:00.499098: step 214290, loss = 0.39 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:51:01.484344: step 214300, loss = 0.27 (1299.2 examples/sec; 0.099 sec/batch)
2017-06-02 07:51:02.243111: step 214310, loss = 0.30 (1686.9 examples/sec; 0.076 sec/batch)
2017-06-02 07:51:03.112309: step 214320, loss = 0.23 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:51:03.991769: step 214330, loss = 0.30 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:51:04.852524: step 214340, loss = 0.26 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:51:05.722402: step 214350, loss = 0.26 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:51:06.600212: step 214360, loss = 0.33 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:51:07.460148: step 214370, loss = 0.30 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:51:08.331258: step 214380, loss = 0.28 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:51:09.200079: step 214390, loss = 0.31 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:51:10.166999: step 214400, loss = 0.31 (1323.8 examples/sec; 0.097 sec/batch)
2017-06-02 07:51:10.965913: step 214410, loss = 0.28 (1602.2 examples/sec; 0.080 sec/batch)
2017-06-02 07:51:11.822797: step 214420, loss = 0.30 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:51:12.678420: step 214430, loss = 0.31 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:51:13.537617: step 214440, loss = 0.22 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:51:14.403409: step 214450, loss = 0.22 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:51:15.277171: step 214460, loss = 0.28 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:51:16.157978: step 214470, loss = 0.29 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:51:17.060105: step 214480, loss = 0.38 (1418.9 examples/sec; 0.090 sec/batch)
2017-06-02 07:51:17.923464: step 214490, loss = 0.26 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:51:18.922591: step 214500, loss = 0.31 (1281.1 examples/sec; 0.100 sec/batch)
2017-06-02 07:51:19.716084: step 214510, loss = 0.25 (1613.1 examples/sec; 0.079 sec/batch)
2017-06-02 07:51:20.603039: step 214520, loss = 0.28 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:51:21.473022: step 214530, loss = 0.28 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:51:22.343943: step 214540, loss = 0.36 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:51:23.217654: step 214550, loss = 0.32 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:51:24.100849: step 214560, loss = 0.30 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:51:24.978622: step 214570, loss = 0.33 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:51:25.870706: step 214580, loss = 0.20 (1434.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:51:26.752528: step 214590, loss = 0.27 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:51:27.709556: step 214600, loss = 0.31 (1337.5 examples/sec; 0.096 sec/batch)
2017-06-02 07:51:28.482327: step 214610, loss = 0.29 (1656.3 examples/sec; 0.077 sec/batch)
2017-06-02 07:51:29.364569: step 214620, loss = 0.33 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:51:30.229183: step 214630, loss = 0.31 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:51:31.085515: step 214640, loss = 0.29 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:51:31.970206: step 214650, loss = 0.31 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:51:32.859108: step 214660, loss = 0.29 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:51:33.734173: step 214670, loss = 0.35 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:51:34.621003: step 214680, loss = 0.25 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:51:35.491685: step 214690, loss = 0.36 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:51:36.489814: step 214700, loss = 0.29 (1282.4 examples/sec; 0.100 sec/batch)
2017-06-02 07:51:37.259071: step 214710, loss = 0.27 (1664.0 examples/sec; 0.077 sec/batch)
2017-06-02 07:51:38.138720: step 214720, loss = 0.37 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:51:39.023324: step 214730, loss = 0.40 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:51:39.928415: step 214740, loss = 0.39 (1414.2 examples/sec; 0.091 sec/batch)
2017-06-02 07:51:40.800012: step 214750, loss = 0.35 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:51:41.661989: step 214760, loss = 0.31 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:51:42.527339: step 214770, loss = 0.31 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:51:43.418171: step 214780, loss = 0.22 (1436.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:51:44.290832: step 214790, loss = 0.36 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:51:45.260315: step 214800, loss = 0.30 (1320.3 examples/sec; 0.097 sec/batch)
2017-06-02 07:51:46.017410: step 214810, loss = 0.35 (1690.7 examples/sec; 0.076 sec/batch)
2017-06-02 07:51:46.894058: step 214820, loss = 0.29 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:51:47.760649: step 214830, loss = 0.27 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:51:48.645056: step 214840, loss = 0.24 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:51:49.507742: step 214850, loss = 0.34 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:51:50.377624: step 214860, loss = 0.31 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:51:51.273365: step 214870, loss = 0.19 (1429.0 examples/sec; 0.090 sec/batch)
2017-06-02 07:51:52.144278: step 214880, loss = 0.27 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:51:53.010870: step 214890, loss = 0.28 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:51:54.058349: step 214900, loss = 0.28 (1222.0 examples/sec; 0.105 sec/batch)
2017-06-02 07:51:54.784678: step 214910, loss = 0.29 (1762.3 examples/sec; 0.073 sec/batch)
2017-06-02 07:51:55.640975: step 214920, loss = 0.34 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:51:56.491351: step 214930, loss = 0.23 (1505.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:51:57.377847: step 214940, loss = 0.24 (1443.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:51:58.208340: step 214950, loss = 0.23 (1541.2 examples/sec; 0.083 sec/batch)
2017-06-02 07:51:59.055033: step 214960, loss = 0.31 (1511.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:51:59.917212: step 214970, loss = 0.27 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:52:00.791518: step 214980, loss = 0.38 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:52:01.675814: step 214990, loss = 0.28 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:52:02.618342: step 215000, loss = 0.33 (1358.0 examples/sec; 0.094 sec/batch)
2017-06-02 07:52:03.393708: step 215010, loss = 0.32 (1650.8 examples/sec; 0.078 sec/batch)
2017-06-02 07:52:04.256392: step 215020, loss = 0.30 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:52:05.096038: step 215030, loss = 0.30 (1524.5 examples/sec; 0.084 sec/batch)
2017-06-02 07:52:05.943415: step 215040, loss = 0.25 (1510.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:52:06.801852: step 215050, loss = 0.24 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:52:07.684319: step 215060, loss = 0.33 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:52:08.558334: step 215070, loss = 0.41 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:52:09.406155: step 215080, loss = 0.28 (1509.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:52:10.247785: step 215090, loss = 0.29 (1520.9 examples/sec; 0.084 sec/batch)
2017-06-02 07:52:11.210894: step 215100, loss = 0.37 (1329.0 examples/sec; 0.096 sec/batch)
2017-06-02 07:52:11.979348: step 215110, loss = 0.26 (1665.7 examples/sec; 0.077 sec/batch)
2017-06-02 07:52:12.841279: step 215120, loss = 0.32 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:52:13.693062: step 215130, loss = 0.32 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:52:14.529440: step 215140, loss = 0.30 (1530.4 examples/sec; 0.084 sec/batch)
2017-06-02 07:52:15.388216: step 215150, loss = 0.44 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:52:16.279080: step 215160, loss = 0.32 (1436.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:52:17.143681: step 215170, loss = 0.37 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:52:18.019420: step 215180, loss = 0.32 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:52:18.889327: step 215190, loss = 0.28 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:52:19.866452: step 215200, loss = 0.33 (1310.0 examples/sec; 0.098 sec/batch)
2017-06-02 07:52:20.602551: step 215210, loss = 0.26 (1738.9 examples/sec; 0.074 sec/batch)
2017-06-02 07:52:21.491921: step 215220, loss = 0.30 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:52:22.366628: step 215230, loss = 0.38 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:52:23.248014: step 215240, loss = 0.30 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:52:24.120854: step 215250, loss = 0.18 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:52:24.995377: step 215260, loss = 0.28 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:52:25.877616: step 215270, loss = 0.25 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:52:26.754653: step 215280, loss = 0.28 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:52:27.648626: step 215290, loss = 0.39 (1431.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:52:28.646998: step 215300, loss = 0.29 (1282.1 examples/sec; 0.100 sec/batch)
2017-06-02 07:52:29.431168: step 215310, loss = 0.26 (1632.3 examples/sec; 0.078 sec/batch)
2017-06-02 07:52:30.307210: step 215320, loss = 0.37 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:52:31.168166: step 215330, loss = 0.25 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:52:32.006529: step 215340, loss = 0.31 (1526.8 examples/sec; 0.084 sec/batch)
2017-06-02 07:52:32.855461: step 215350, loss = 0.23 (1507.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:52:33.712091: step 215360, loss = 0.25 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:52:34.571603: step 215370, loss = 0.32 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:52:35.449696: step 215380, loss = 0.24 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:52:36.306409: step 215390, loss = 0.31 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:52:37.279559: step 215400, loss = 0.29 (1315.3 examples/sec; 0.097 sec/batch)
2017-06-02 07:52:38.053591: step 215410, loss = 0.28 (1653.7 examples/sec; 0.077 sec/batch)
2017-06-02 07:52:38.923606: step 215420, loss = 0.30 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:52:39.790493: step 215430, loss = 0.36 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:52:40.647402: step 215440, loss = 0.28 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:52:41.537802: step 215450, loss = 0.31 (1437.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:52:42.398508: step 215460, loss = 0.27 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:52:43.257172: step 215470, loss = 0.26 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:52:44.104069: step 215480, loss = 0.25 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:52:44.966106: step 215490, loss = 0.27 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:52:45.926224: step 215500, loss = 0.27 (1333.1 examples/sec; 0.096 sec/batch)
2017-06-02 07:52:46.687447: step 215510, loss = 0.22 (1681.5 examples/sec; 0.076 sec/batch)
2017-06-02 07:52:47.551135: step 215520, loss = 0.25 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:52:48.409310: step 215530, loss = 0.33 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:52:49.261967: step 215540, loss = 0.30 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:52:50.133566: step 215550, loss = 0.28 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:52:51.031562: step 215560, loss = 0.24 (1425.4 examples/sec; 0.090 sec/batch)
2017-06-02 07:52:51.872321: step 215570, loss = 0.22 (1522.4 examples/sec; 0.084 sec/batch)
2017-06-02 07:52:52.720048: step 215580, loss = 0.35 (1509.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:52:53.560389: step 215590, loss = 0.21 (1523.2 examples/sec; 0.084 sec/batch)
2017-06-02 07:52:54.553097: step 215600, loss = 0.22 (1289.4 examples/sec; 0.099 sec/batch)
2017-06-02 07:52:55.286696: step 215610, loss = 0.35 (1744.8 examples/sec; 0.073 sec/batch)
2017-06-02 07:52:56.145506: step 215620, loss = 0.30 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:52:57.016181: step 215630, loss = 0.31 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:52:57.887511: step 215640, loss = 0.23 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:52:58.762799: step 215650, loss = 0.26 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:52:59.626523: step 215660, loss = 0.37 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:53:00.466702: step 215670, loss = 0.23 (1523.5 examples/sec; 0.084 sec/batch)
2017-06-02 07:53:01.308920: step 215680, loss = 0.25 (1519.8 examples/sec; 0.084 sec/batch)
2017-06-02 07:53:02.165366: step 215690, loss = 0.28 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:53:03.144858: step 215700, loss = 0.24 (1306.8 examples/sec; 0.098 sec/batch)
2017-06-02 07:53:03.908190: step 215710, loss = 0.23 (1676.9 examples/sec; 0.076 sec/batch)
2017-06-02 07:53:04.785256: step 215720, loss = 0.52 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:53:05.666892: step 215730, loss = 0.26 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:53:06.551013: step 215740, loss = 0.34 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:53:07.422471: step 215750, loss = 0.32 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:53:08.290935: step 215760, loss = 0.25 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:53:09.171546: step 215770, loss = 0.30 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:53:10.036100: step 215780, loss = 0.24 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:53:10.911034: step 215790, loss = 0.28 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:53:11.908710: step 215800, loss = 0.30 (1283.0 examples/sec; 0.100 sec/batch)
2017-06-02 07:53:12.677860: step 215810, loss = 0.24 (1664.2 examples/sec; 0.077 sec/batch)
2017-06-02 07:53:13.563477: step 215820, loss = 0.30 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:53:14.432908: step 215830, loss = 0.33 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:53:15.312272: step 215840, loss = 0.28 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:53:16.181482: step 215850, loss = 0.31 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:53:17.065141: step 215860, loss = 0.26 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:53:17.934390: step 215870, loss = 0.28 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:53:18.810740: step 215880, loss = 0.33 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:53:19.702344: step 215890, loss = 0.29 (1435.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:53:20.658609: step 215900, loss = 0.27 (1338.5 examples/sec; 0.096 sec/batch)
2017-06-02 07:53:21.426353: step 215910, loss = 0.28 (1667.2 examples/sec; 0.077 sec/batch)
2017-06-02 07:53:22.280113: step 215920, loss = 0.30 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:53:23.182402: step 215930, loss = 0.32 (1418.6 examples/sec; 0.090 sec/batch)
2017-06-02 07:53:24.035345: step 215940, loss = 0.25 (1500.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:53:24.891856: step 215950, loss = 0.21 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:53:25.764185: step 215960, loss = 0.26 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:53:26.629289: step 215970, loss = 0.38 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:53:27.504131: step 215980, loss = 0.24 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:53:28.380549: step 215990, loss = 0.26 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:53:29.387303: step 216000, loss = 0.29 (1271.4 examples/sec; 0.101 sec/batch)
2017-06-02 07:53:30.129248: step 216010, loss = 0.34 (1725.2 examples/sec; 0.074 sec/batch)
2017-06-02 07:53:30.981062: step 216020, loss = 0.26 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:53:31.852144: step 216030, loss = 0.22 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:53:32.692028: step 216040, loss = 0.28 (1524.0 examples/sec; 0.084 sec/batch)
2017-06-02 07:53:33.548919: step 216050, loss = 0.28 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:53:34.426054: step 216060, loss = 0.32 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:53:35.282380: step 216070, loss = 0.22 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:53:36.150482: step 216080, loss = 0.30 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:53:37.015835: step 216090, loss = 0.30 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:53:37.994422: step 216100, loss = 0.41 (1308.0 examples/sec; 0.098 sec/batch)
2017-06-02 07:53:38.756482: step 216110, loss = 0.29 (1679.7 examples/sec; 0.076 sec/batch)
2017-06-02 07:53:39.642207: step 216120, loss = 0.34 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 07:53:40.519416: step 216130, loss = 0.27 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:53:41.392556: step 216140, loss = 0.35 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:53:42.259044: step 216150, loss = 0.23 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:53:43.133647: step 216160, loss = 0.26 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:53:43.994188: step 216170, loss = 0.31 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:53:44.876103: step 216180, loss = 0.27 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:53:45.741473: step 216190, loss = 0.28 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:53:46.779813: step 216200, loss = 0.32 (1232.7 examples/sec; 0.104 sec/batch)
2017-06-02 07:53:47.466603: step 216210, loss = 0.38 (1863.8 examples/sec; 0.069 sec/batch)
2017-06-02 07:53:48.334365: step 216220, loss = 0.26 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:53:49.198938: step 216230, loss = 0.28 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:53:50.050943: step 216240, loss = 0.23 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:53:50.926089: step 216250, loss = 0.30 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:53:51.782785: step 216260, loss = 0.32 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:53:52.656655: step 216270, loss = 0.32 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:53:53.508168: step 216280, loss = 0.36 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:53:54.367080: step 216290, loss = 0.28 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:53:55.327160: step 216300, loss = 0.33 (1333.2 examples/sec; 0.096 sec/batch)
2017-06-02 07:53:56.079912: step 216310, loss = 0.35 (1700.4 examples/sec; 0.075 sec/batch)
2017-06-02 07:53:56.897131: step 216320, loss = 0.29 (1566.3 examples/sec; 0.082 sec/batch)
2017-06-02 07:53:57.758507: step 216330, loss = 0.38 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:53:58.619434: step 216340, loss = 0.29 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:53:59.474380: step 216350, loss = 0.30 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:54:00.310183: step 216360, loss = 0.36 (1531.5 examples/sec; 0.084 sec/batch)
2017-06-02 07:54:01.161280: step 216370, loss = 0.40 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:54:02.032654: step 216380, loss = 0.33 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:54:02.920507: step 216390, loss = 0.32 (1441.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:54:03.880466: step 216400, loss = 0.32 (1333.4 examples/sec; 0.096 sec/batch)
2017-06-02 07:54:04.613317: step 216410, loss = 0.27 (1746.6 examples/sec; 0.073 sec/batch)
2017-06-02 07:54:05.495454: step 216420, loss = 0.31 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:54:06.351551: step 216430, loss = 0.31 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:54:07.225399: step 216440, loss = 0.32 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:54:08.102675: step 216450, loss = 0.29 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:54:08.977493: step 216460, loss = 0.26 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:54:09.844895: step 216470, loss = 0.34 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:54:10.722335: step 216480, loss = 0.36 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:54:11.599950: step 216490, loss = 0.39 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:54:12.551857: step 216500, loss = 0.33 (1344.7 examples/sec; 0.095 sec/batch)
2017-06-02 07:54:13.311609: step 216510, loss = 0.35 (1684.8 examples/sec; 0.076 sec/batch)
2017-06-02 07:54:14.174986: step 216520, loss = 0.28 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:54:15.050654: step 216530, loss = 0.32 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:54:15.900330: step 216540, loss = 0.32 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:54:16.785111: step 216550, loss = 0.26 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:54:17.645365: step 216560, loss = 0.33 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:54:18.511026: step 216570, loss = 0.39 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:54:19.385391: step 216580, loss = 0.37 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:54:20.247772: step 216590, loss = 0.30 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:54:21.222551: step 216600, loss = 0.23 (1313.1 examples/sec; 0.097 sec/batch)
2017-06-02 07:54:21.968018: step 216610, loss = 0.37 (1717.1 examples/sec; 0.075 sec/batch)
2017-06-02 07:54:22.817022: step 216620, loss = 0.34 (1507.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:54:23.677193: step 216630, loss = 0.28 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:54:24.557941: step 216640, loss = 0.24 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:54:25.398991: step 216650, loss = 0.29 (1521.9 examples/sec; 0.084 sec/batch)
2017-06-02 07:54:26.283456: step 216660, loss = 0.29 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:54:27.141304: step 216670, loss = 0.28 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:54:28.020386: step 216680, loss = 0.28 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:54:28.882035: step 216690, loss = 0.33 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:54:29.855338: step 216700, loss = 0.35 (1315.1 examples/sec; 0.097 sec/batch)
2017-06-02 07:54:30.603159: step 216710, loss = 0.37 (1711.6 examples/sec; 0.075 sec/batch)
2017-06-02 07:54:31.478458: step 216720, loss = 0.21 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:54:32.343389: step 216730, loss = 0.25 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:54:33.225255: step 216740, loss = 0.31 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:54:34.114372: step 216750, loss = 0.34 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:54:34.968606: step 216760, loss = 0.33 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:54:35.855152: step 216770, loss = 0.24 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:54:36.690781: step 216780, loss = 0.22 (1531.8 examples/sec; 0.084 sec/batch)
2017-06-02 07:54:37.560258: step 216790, loss = 0.37 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:54:38.521670: step 216800, loss = 0.35 (1331.4 examples/sec; 0.096 sec/batch)
2017-06-02 07:54:39.241382: step 216810, loss = 0.31 (1778.5 examples/sec; 0.072 sec/batch)
2017-06-02 07:54:40.107019: step 216820, loss = 0.33 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:54:40.956740: step 216830, loss = 0.33 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:54:41.796104: step 216840, loss = 0.32 (1524.9 examples/sec; 0.084 sec/batch)
2017-06-02 07:54:42.645882: step 216850, loss = 0.28 (1506.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:54:43.504605: step 216860, loss = 0.30 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:54:44.358826: step 216870, loss = 0.25 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:54:45.200159: step 216880, loss = 0.26 (1521.4 examples/sec; 0.084 sec/batch)
2017-06-02 07:54:46.067997: step 216890, loss = 0.30 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:54:47.018824: step 216900, loss = 0.35 (1346.2 examples/sec; 0.095 sec/batch)
2017-06-02 07:54:47.811657: step 216910, loss = 0.32 (1614.5 examples/sec; 0.079 sec/batch)
2017-06-02 07:54:48.681467: step 216920, loss = 0.29 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:54:49.557501: step 216930, loss = 0.40 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:54:50.433860: step 216940, loss = 0.30 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:54:51.309223: step 216950, loss = 0.34 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:54:52.167355: step 216960, loss = 0.28 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:54:53.052545: step 216970, loss = 0.38 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:54:53.915449: step 216980, loss = 0.29 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:54:54.761132: step 216990, loss = 0.30 (1513.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:54:55.782081: step 217000, loss = 0.25 (1253.7 examples/sec; 0.102 sec/batch)
2017-06-02 07:54:56.512269: step 217010, loss = 0.31 (1753.0 examples/sec; 0.073 sec/batch)
2017-06-02 07:54:57.383777: step 217020, loss = 0.30 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:54:58.252805: step 217030, loss = 0.43 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:54:59.121596: step 217040, loss = 0.29 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:54:59.986460: step 217050, loss = 0.28 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:55:00.840430: step 217060, loss = 0.31 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:55:01.711713: step 217070, loss = 0.22 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:55:02.579248: step 217080, loss = 0.20 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:55:03.435049: step 217090, loss = 0.28 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:55:04.395412: step 217100, loss = 0.35 (1332.8 examples/sec; 0.096 sec/batch)
2017-06-02 07:55:05.174470: step 217110, loss = 0.30 (1643.0 examples/sec; 0.078 sec/batch)
2017-06-02 07:55:06.045845: step 217120, loss = 0.27 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:55:06.920743: step 217130, loss = 0.27 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:55:07.805604: step 217140, loss = 0.30 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:55:08.683550: step 217150, loss = 0.22 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:55:09.564466: step 217160, loss = 0.27 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:55:10.455433: step 217170, loss = 0.26 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:55:11.348106: step 217180, loss = 0.27 (1433.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:55:12.248489: step 217190, loss = 0.31 (1421.6 examples/sec; 0.090 sec/batch)
2017-06-02 07:55:13.228709: step 217200, loss = 0.30 (1305.8 examples/sec; 0.098 sec/batch)
2017-06-02 07:55:14.004948: step 217210, loss = 0.29 (1649.0 examples/sec; 0.078 sec/batch)
2017-06-02 07:55:14.866688: step 217220, loss = 0.34 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:55:15.704573: step 217230, loss = 0.27 (1527.6 examples/sec; 0.084 sec/batch)
2017-06-02 07:55:16.591187: step 217240, loss = 0.28 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:55:17.456589: step 217250, loss = 0.37 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:55:18.280142: step 217260, loss = 0.30 (1554.3 examples/sec; 0.082 sec/batch)
2017-06-02 07:55:19.133648: step 217270, loss = 0.29 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:55:19.994563: step 217280, loss = 0.26 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:55:20.837149: step 217290, loss = 0.29 (1519.1 examples/sec; 0.084 sec/batch)
2017-06-02 07:55:21.791655: step 217300, loss = 0.21 (1341.0 examples/sec; 0.095 sec/batch)
2017-06-02 07:55:22.547821: step 217310, loss = 0.28 (1692.8 examples/sec; 0.076 sec/batch)
2017-06-02 07:55:23.422250: step 217320, loss = 0.33 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:55:24.276793: step 217330, loss = 0.30 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:55:25.115366: step 217340, loss = 0.30 (1526.4 examples/sec; 0.084 sec/batch)
2017-06-02 07:55:25.967086: step 217350, loss = 0.21 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:55:26.812764: step 217360, loss = 0.32 (1513.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:55:27.676747: step 217370, loss = 0.26 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:55:28.547523: step 217380, loss = 0.33 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:55:29.439000: step 217390, loss = 0.37 (1435.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:55:30.395438: step 217400, loss = 0.30 (1338.3 examples/sec; 0.096 sec/batch)
2017-06-02 07:55:31.187562: step 217410, loss = 0.23 (1615.9 examples/sec; 0.079 sec/batch)
2017-06-02 07:55:32.053573: step 217420, loss = 0.28 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:55:32.910175: step 217430, loss = 0.27 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:55:33.774370: step 217440, loss = 0.31 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:55:34.648424: step 217450, loss = 0.32 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:55:35.502198: step 217460, loss = 0.32 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:55:36.362791: step 217470, loss = 0.35 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:55:37.265915: step 217480, loss = 0.30 (1417.3 examples/sec; 0.090 sec/batch)
2017-06-02 07:55:38.118755: step 217490, loss = 0.26 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:55:39.106989: step 217500, loss = 0.33 (1295.2 examples/sec; 0.099 sec/batch)
2017-06-02 07:55:39.867585: step 217510, loss = 0.33 (1682.9 examples/sec; 0.076 sec/batch)
2017-06-02 07:55:40.761802: step 217520, loss = 0.34 (1431.4 examples/sec; 0.089 sec/batch)
2017-06-02 07:55:41.635847: step 217530, loss = 0.25 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:55:42.527472: step 217540, loss = 0.25 (1435.6 examples/sec; 0.089 sec/batch)
2017-06-02 07:55:43.412574: step 217550, loss = 0.30 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:55:44.280105: step 217560, loss = 0.40 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:55:45.144561: step 217570, loss = 0.35 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:55:46.026477: step 217580, loss = 0.31 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:55:46.917190: step 217590, loss = 0.34 (1437.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:55:47.894447: step 217600, loss = 0.30 (1309.8 examples/sec; 0.098 sec/batch)
2017-06-02 07:55:48.666799: step 217610, loss = 0.32 (1657.3 examples/sec; 0.077 sec/batch)
2017-06-02 07:55:49.550465: step 217620, loss = 0.35 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:55:50.409611: step 217630, loss = 0.25 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:55:51.280663: step 217640, loss = 0.25 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:55:52.145361: step 217650, loss = 0.24 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:55:53.010865: step 217660, loss = 0.24 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:55:53.847978: step 217670, loss = 0.35 (1529.1 examples/sec; 0.084 sec/batch)
2017-06-02 07:55:54.707793: step 217680, loss = 0.29 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:55:55.580478: step 217690, loss = 0.28 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:55:56.548347: step 217700, loss = 0.26 (1322.5 examples/sec; 0.097 sec/batch)
2017-06-02 07:55:57.322549: step 217710, loss = 0.29 (1653.8 examples/sec; 0.077 sec/batch)
2017-06-02 07:55:58.193094: step 217720, loss = 0.30 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:55:59.062583: step 217730, loss = 0.29 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:55:59.924912: step 217740, loss = 0.28 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:56:00.803331: step 217750, loss = 0.27 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:56:01.665405: step 217760, loss = 0.32 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:56:02.517841: step 217770, loss = 0.31 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:56:03.374353: step 217780, loss = 0.25 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:56:04.232678: step 217790, loss = 0.28 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:56:05.195279: step 217800, loss = 0.34 (1329.7 examples/sec; 0.096 sec/batch)
2017-06-02 07:56:05.983122: step 217810, loss = 0.36 (1624.7 examples/sec; 0.079 sec/batch)
2017-06-02 07:56:06.843242: step 217820, loss = 0.23 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:56:07.724104: step 217830, loss = 0.23 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:56:08.584163: step 217840, loss = 0.27 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:56:09.459973: step 217850, loss = 0.25 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:56:10.283845: step 217860, loss = 0.26 (1553.6 examples/sec; 0.082 sec/batch)
2017-06-02 07:56:11.133424: step 217870, loss = 0.30 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:56:11.979579: step 217880, loss = 0.38 (1512.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:56:12.837599: step 217890, loss = 0.25 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:56:13.801969: step 217900, loss = 0.31 (1327.3 examples/sec; 0.096 sec/batch)
2017-06-02 07:56:14.565305: step 217910, loss = 0.32 (1676.9 examples/sec; 0.076 sec/batch)
2017-06-02 07:56:15.414604: step 217920, loss = 0.26 (1507.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:56:16.262028: step 217930, loss = 0.24 (1510.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:56:17.140915: step 217940, loss = 0.35 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:56:18.013220: step 217950, loss = 0.26 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:56:18.901172: step 217960, loss = 0.28 (1441.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:56:19.768209: step 217970, loss = 0.28 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:56:20.653866: step 217980, loss = 0.28 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:56:21.541629: step 217990, loss = 0.31 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:56:22.515237: step 218000, loss = 0.30 (1314.7 examples/sec; 0.097 sec/batch)
2017-06-02 07:56:23.289634: step 218010, loss = 0.32 (1652.9 examples/sec; 0.077 sec/batch)
2017-06-02 07:56:24.152163: step 218020, loss = 0.30 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:56:25.055478: step 218030, loss = 0.21 (1417.0 examples/sec; 0.090 sec/batch)
2017-06-02 07:56:25.900591: step 218040, loss = 0.26 (1514.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:56:26.752561: step 218050, loss = 0.25 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:56:27.615239: step 218060, loss = 0.30 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:56:28.487045: step 218070, loss = 0.31 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:56:29.348742: step 218080, loss = 0.27 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:56:30.193782: step 218090, loss = 0.28 (1514.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:56:31.142521: step 218100, loss = 0.26 (1349.2 examples/sec; 0.095 sec/batch)
2017-06-02 07:56:31.904473: step 218110, loss = 0.25 (1679.9 examples/sec; 0.076 sec/batch)
2017-06-02 07:56:32.758847: step 218120, loss = 0.26 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:56:33.615495: step 218130, loss = 0.25 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:56:34.493104: step 218140, loss = 0.27 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:56:35.350769: step 218150, loss = 0.38 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:56:36.205279: step 218160, loss = 0.29 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:56:37.057610: step 218170, loss = 0.36 (1501.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:56:37.907815: step 218180, loss = 0.26 (1505.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:56:38.768043: step 218190, loss = 0.27 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:56:39.724372: step 218200, loss = 0.32 (1338.4 examples/sec; 0.096 sec/batch)
2017-06-02 07:56:40.495817: step 218210, loss = 0.32 (1659.2 examples/sec; 0.077 sec/batch)
2017-06-02 07:56:41.361532: step 218220, loss = 0.24 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:56:42.200679: step 218230, loss = 0.29 (1525.4 examples/sec; 0.084 sec/batch)
2017-06-02 07:56:43.071779: step 218240, loss = 0.24 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:56:43.953133: step 218250, loss = 0.28 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:56:44.819592: step 218260, loss = 0.31 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:56:45.680623: step 218270, loss = 0.26 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:56:46.567680: step 218280, loss = 0.29 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:56:47.439512: step 218290, loss = 0.31 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:56:48.431344: step 218300, loss = 0.25 (1290.5 examples/sec; 0.099 sec/batch)
2017-06-02 07:56:49.195897: step 218310, loss = 0.36 (1674.2 examples/sec; 0.076 sec/batch)
2017-06-02 07:56:50.073247: step 218320, loss = 0.31 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:56:50.942847: step 218330, loss = 0.32 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:56:51.821303: step 218340, loss = 0.28 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:56:52.691675: step 218350, loss = 0.24 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:56:53.533483: step 218360, loss = 0.28 (1520.5 examples/sec; 0.084 sec/batch)
2017-06-02 07:56:54.391923: step 218370, loss = 0.23 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:56:55.264864: step 218380, loss = 0.31 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:56:56.139176: step 218390, loss = 0.31 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:56:57.101940: step 218400, loss = 0.44 (1329.5 examples/sec; 0.096 sec/batch)
2017-06-02 07:56:57.861727: step 218410, loss = 0.28 (1684.7 examples/sec; 0.076 sec/batch)
2017-06-02 07:56:58.725189: step 218420, loss = 0.30 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:56:59.597088: step 218430, loss = 0.26 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:57:00.422704: step 218440, loss = 0.29 (1550.4 examples/sec; 0.083 sec/batch)
2017-06-02 07:57:01.274142: step 218450, loss = 0.28 (1503.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:57:02.102811: step 218460, loss = 0.24 (1544.6 examples/sec; 0.083 sec/batch)
2017-06-02 07:57:02.954309: step 218470, loss = 0.27 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:57:03.831662: step 218480, loss = 0.32 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:57:04.655054: step 218490, loss = 0.28 (1554.5 examples/sec; 0.082 sec/batch)
2017-06-02 07:57:05.622838: step 218500, loss = 0.34 (1322.6 examples/sec; 0.097 sec/batch)
2017-06-02 07:57:06.381510: step 218510, loss = 0.24 (1687.2 examples/sec; 0.076 sec/batch)
2017-06-02 07:57:07.211743: step 218520, loss = 0.27 (1541.8 examples/sec; 0.083 sec/batch)
2017-06-02 07:57:08.070664: step 218530, loss = 0.27 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:57:08.927464: step 218540, loss = 0.28 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:57:09.758147: step 218550, loss = 0.25 (1540.9 examples/sec; 0.083 sec/batch)
2017-06-02 07:57:10.612961: step 218560, loss = 0.32 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:57:11.465172: step 218570, loss = 0.30 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:57:12.325568: step 218580, loss = 0.30 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:57:13.155438: step 218590, loss = 0.38 (1542.4 examples/sec; 0.083 sec/batch)
2017-06-02 07:57:14.104290: step 218600, loss = 0.18 (1349.0 examples/sec; 0.095 sec/batch)
2017-06-02 07:57:14.860619: step 218610, loss = 0.37 (1692.4 examples/sec; 0.076 sec/batch)
2017-06-02 07:57:15.709526: step 218620, loss = 0.29 (1507.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:57:16.586270: step 218630, loss = 0.30 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 07:57:17.434178: step 218640, loss = 0.29 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:57:18.269514: step 218650, loss = 0.24 (1532.3 examples/sec; 0.084 sec/batch)
2017-06-02 07:57:19.123686: step 218660, loss = 0.42 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:57:20.021577: step 218670, loss = 0.35 (1425.5 examples/sec; 0.090 sec/batch)
2017-06-02 07:57:20.859107: step 218680, loss = 0.23 (1528.3 examples/sec; 0.084 sec/batch)
2017-06-02 07:57:21.721797: step 218690, loss = 0.27 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:57:22.747687: step 218700, loss = 0.31 (1247.7 examples/sec; 0.103 sec/batch)
2017-06-02 07:57:23.479364: step 218710, loss = 0.32 (1749.4 examples/sec; 0.073 sec/batch)
2017-06-02 07:57:24.343343: step 218720, loss = 0.24 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:57:25.208659: step 218730, loss = 0.29 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:57:26.078200: step 218740, loss = 0.28 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:57:26.943766: step 218750, loss = 0.28 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:57:27.817423: step 218760, loss = 0.29 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:57:28.685949: step 218770, loss = 0.28 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:57:29.551717: step 218780, loss = 0.30 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:57:30.426933: step 218790, loss = 0.23 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:57:31.386670: step 218800, loss = 0.29 (1333.7 examples/sec; 0.096 sec/batch)
2017-06-02 07:57:32.155275: step 218810, loss = 0.21 (1665.4 examples/sec; 0.077 sec/batch)
2017-06-02 07:57:33.044500: step 218820, loss = 0.40 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:57:33.936751: step 218830, loss = 0.36 (1434.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:57:34.810757: step 218840, loss = 0.28 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:57:35.696911: step 218850, loss = 0.39 (1444.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:57:36.556507: step 218860, loss = 0.25 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:57:37.440981: step 218870, loss = 0.31 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:57:38.283271: step 218880, loss = 0.28 (1519.7 examples/sec; 0.084 sec/batch)
2017-06-02 07:57:39.150147: step 218890, loss = 0.31 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:57:40.131883: step 218900, loss = 0.31 (1303.8 examples/sec; 0.098 sec/batch)
2017-06-02 07:57:40.874117: step 218910, loss = 0.29 (1724.5 examples/sec; 0.074 sec/batch)
2017-06-02 07:57:41.740005: step 218920, loss = 0.33 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:57:42.580764: step 218930, loss = 0.26 (1522.4 examples/sec; 0.084 sec/batch)
2017-06-02 07:57:43.424374: step 218940, loss = 0.23 (1517.3 examples/sec; 0.084 sec/batch)
2017-06-02 07:57:44.268236: step 218950, loss = 0.34 (1516.9 examples/sec; 0.084 sec/batch)
2017-06-02 07:57:45.100915: step 218960, loss = 0.31 (1537.2 examples/sec; 0.083 sec/batch)
2017-06-02 07:57:45.964708: step 218970, loss = 0.29 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:57:46.851753: step 218980, loss = 0.33 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 07:57:47.712098: step 218990, loss = 0.25 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:57:48.694146: step 219000, loss = 0.34 (1303.4 examples/sec; 0.098 sec/batch)
2017-06-02 07:57:49.483681: step 219010, loss = 0.29 (1621.2 examples/sec; 0.079 sec/batch)
2017-06-02 07:57:50.336391: step 219020, loss = 0.32 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:57:51.187573: step 219030, loss = 0.28 (1503.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:57:52.040173: step 219040, loss = 0.23 (1501.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:57:52.895396: step 219050, loss = 0.40 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 07:57:53.730010: step 219060, loss = 0.30 (1533.6 examples/sec; 0.083 sec/batch)
2017-06-02 07:57:54.578270: step 219070, loss = 0.37 (1509.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:57:55.417304: step 219080, loss = 0.29 (1525.6 examples/sec; 0.084 sec/batch)
2017-06-02 07:57:56.262784: step 219090, loss = 0.27 (1513.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:57:57.201572: step 219100, loss = 0.37 (1363.4 examples/sec; 0.094 sec/batch)
2017-06-02 07:57:57.968119: step 219110, loss = 0.25 (1669.8 examples/sec; 0.077 sec/batch)
2017-06-02 07:57:58.819179: step 219120, loss = 0.23 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 07:57:59.668470: step 219130, loss = 0.26 (1507.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:58:00.547195: step 219140, loss = 0.28 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:58:01.414237: step 219150, loss = 0.24 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:58:02.286742: step 219160, loss = 0.33 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:58:03.170662: step 219170, loss = 0.32 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:58:04.063306: step 219180, loss = 0.33 (1433.9 examples/sec; 0.089 sec/batch)
2017-06-02 07:58:04.924581: step 219190, loss = 0.35 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:58:05.899998: step 219200, loss = 0.38 (1312.3 examples/sec; 0.098 sec/batch)
2017-06-02 07:58:06.639299: step 219210, loss = 0.33 (1731.4 examples/sec; 0.074 sec/batch)
2017-06-02 07:58:07.485208: step 219220, loss = 0.24 (1513.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:58:08.348744: step 219230, loss = 0.45 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:58:09.214810: step 219240, loss = 0.25 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:58:10.062791: step 219250, loss = 0.36 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:58:10.940311: step 219260, loss = 0.33 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:58:11.780322: step 219270, loss = 0.36 (1523.8 examples/sec; 0.084 sec/batch)
2017-06-02 07:58:12.654080: step 219280, loss = 0.30 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:58:13.496856: step 219290, loss = 0.38 (1518.8 examples/sec; 0.084 sec/batch)
2017-06-02 07:58:14.459027: step 219300, loss = 0.33 (1330.3 examples/sec; 0.096 sec/batch)
2017-06-02 07:58:15.213943: step 219310, loss = 0.33 (1695.5 examples/sec; 0.075 sec/batch)
2017-06-02 07:58:16.090355: step 219320, loss = 0.35 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:58:16.931962: step 219330, loss = 0.27 (1520.9 examples/sec; 0.084 sec/batch)
2017-06-02 07:58:17.771002: step 219340, loss = 0.32 (1525.6 examples/sec; 0.084 sec/batch)
2017-06-02 07:58:18.622165: step 219350, loss = 0.32 (1503.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:58:19.486537: step 219360, loss = 0.21 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:58:20.373420: step 219370, loss = 0.33 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:58:21.228116: step 219380, loss = 0.31 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 07:58:22.085927: step 219390, loss = 0.36 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:58:23.043556: step 219400, loss = 0.35 (1336.6 examples/sec; 0.096 sec/batch)
2017-06-02 07:58:23.814725: step 219410, loss = 0.37 (1659.8 examples/sec; 0.077 sec/batch)
2017-06-02 07:58:24.694981: step 219420, loss = 0.37 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:58:25.560914: step 219430, loss = 0.27 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:58:26.435434: step 219440, loss = 0.26 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:58:27.298046: step 219450, loss = 0.31 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:58:28.163358: step 219460, loss = 0.26 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:58:29.053609: step 219470, loss = 0.27 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:58:29.921956: step 219480, loss = 0.26 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:58:30.790403: step 219490, loss = 0.23 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:58:31.783932: step 219500, loss = 0.28 (1288.3 examples/sec; 0.099 sec/batch)
2017-06-02 07:58:32.545648: step 219510, loss = 0.26 (1680.4 examples/sec; 0.076 sec/batch)
2017-06-02 07:58:33.408549: step 219520, loss = 0.26 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 07:58:34.269997: step 219530, loss = 0.25 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:58:35.139705: step 219540, loss = 0.27 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:58:35.999587: step 219550, loss = 0.24 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:58:36.859932: step 219560, loss = 0.27 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:58:37.747126: step 219570, loss = 0.36 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 07:58:38.621934: step 219580, loss = 0.27 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:58:39.473462: step 219590, loss = 0.31 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:58:40.453004: step 219600, loss = 0.27 (1306.7 examples/sec; 0.098 sec/batch)
2017-06-02 07:58:41.235044: step 219610, loss = 0.30 (1636.7 examples/sec; 0.078 sec/batch)
2017-06-02 07:58:42.101773: step 219620, loss = 0.35 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:58:42.980489: step 219630, loss = 0.26 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 07:58:43.852038: step 219640, loss = 0.36 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:58:44.707317: step 219650, loss = 0.29 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 07:58:45.585868: step 219660, loss = 0.24 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 07:58:46.468463: step 219670, loss = 0.30 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 07:58:47.346276: step 219680, loss = 0.39 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:58:48.210648: step 219690, loss = 0.22 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:58:49.175448: step 219700, loss = 0.26 (1326.7 examples/sec; 0.096 sec/batch)
2017-06-02 07:58:49.986138: step 219710, loss = 0.37 (1578.9 examples/sec; 0.081 sec/batch)
2017-06-02 07:58:50.852422: step 219720, loss = 0.27 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:58:51.726249: step 219730, loss = 0.24 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 07:58:52.607537: step 219740, loss = 0.32 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 07:58:53.470010: step 219750, loss = 0.33 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:58:54.350303: step 219760, loss = 0.23 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 07:58:55.239488: step 219770, loss = 0.24 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:58:56.074056: step 219780, loss = 0.32 (1533.7 examples/sec; 0.083 sec/batch)
2017-06-02 07:58:56.928232: step 219790, loss = 0.39 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:58:57.957342: step 219800, loss = 0.37 (1243.8 examples/sec; 0.103 sec/batch)
2017-06-02 07:58:58.657801: step 219810, loss = 0.27 (1827.4 examples/sec; 0.070 sec/batch)
2017-06-02 07:58:59.513939: step 219820, loss = 0.32 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:59:00.359827: step 219830, loss = 0.25 (1513.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:59:01.231847: step 219840, loss = 0.38 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 07:59:02.085089: step 219850, loss = 0.25 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 07:59:02.956748: step 219860, loss = 0.29 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:59:03.809112: step 219870, loss = 0.28 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:59:04.661017: step 219880, loss = 0.30 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 07:59:05.517507: step 219890, loss = 0.31 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 07:59:06.465631: step 219900, loss = 0.20 (1350.0 examples/sec; 0.095 sec/batch)
2017-06-02 07:59:07.239718: step 219910, loss = 0.32 (1653.6 examples/sec; 0.077 sec/batch)
2017-06-02 07:59:08.081546: step 219920, loss = 0.24 (1520.5 examples/sec; 0.084 sec/batch)
2017-06-02 07:59:08.951019: step 219930, loss = 0.23 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:59:09.812828: step 219940, loss = 0.34 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:59:10.664297: step 219950, loss = 0.38 (1503.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:59:11.520877: step 219960, loss = 0.22 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 07:59:12.352798: step 219970, loss = 0.31 (1538.6 examples/sec; 0.083 sec/batch)
2017-06-02 07:59:13.210664: step 219980, loss = 0.36 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 07:59:14.087611: step 219990, loss = 0.32 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:59:15.055385: step 220000, loss = 0.29 (1322.6 examples/sec; 0.097 sec/batch)
2017-06-02 07:59:15.832415: step 220010, loss = 0.22 (1647.3 examples/sec; 0.078 sec/batch)
2017-06-02 07:59:16.724078: step 220020, loss = 0.44 (1435.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:59:17.604256: step 220030, loss = 0.25 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 07:59:18.461720: step 220040, loss = 0.30 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 07:59:19.336217: step 220050, loss = 0.29 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:59:20.203434: step 220060, loss = 0.28 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 07:59:21.071460: step 220070, loss = 0.29 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:59:21.923498: step 220080, loss = 0.33 (1502.3 examples/sec; 0.085 sec/batch)
2017-06-02 07:59:22.767032: step 220090, loss = 0.28 (1517.5 examples/sec; 0.084 sec/batch)
2017-06-02 07:59:23.744822: step 220100, loss = 0.49 (1309.0 examples/sec; 0.098 sec/batch)
2017-06-02 07:59:24.515964: step 220110, loss = 0.27 (1659.9 examples/sec; 0.077 sec/batch)
2017-06-02 07:59:25.402797: step 220120, loss = 0.31 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:59:26.265214: step 220130, loss = 0.28 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 07:59:27.119059: step 220140, loss = 0.35 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:59:27.990363: step 220150, loss = 0.33 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:59:28.844890: step 220160, loss = 0.27 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:59:29.714921: step 220170, loss = 0.35 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:59:30.587999: step 220180, loss = 0.23 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 07:59:31.431870: step 220190, loss = 0.28 (1516.8 examples/sec; 0.084 sec/batch)
2017-06-02 07:59:32.421287: step 220200, loss = 0.32 (1293.7 examples/sec; 0.099 sec/batch)
2017-06-02 07:59:33.193409: step 220210, loss = 0.35 (1657.8 examples/sec; 0.077 sec/batch)
2017-06-02 07:59:34.045722: step 220220, loss = 0.28 (1501.8 examples/sec; 0.085 sec/batch)
2017-06-02 07:59:34.911914: step 220230, loss = 0.35 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 07:59:35.753930: step 220240, loss = 0.28 (1520.2 examples/sec; 0.084 sec/batch)
2017-06-02 07:59:36.622684: step 220250, loss = 0.28 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 07:59:37.496249: step 220260, loss = 0.29 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 07:59:38.389289: step 220270, loss = 0.27 (1433.3 examples/sec; 0.089 sec/batch)
2017-06-02 07:59:39.271703: step 220280, loss = 0.30 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:59:40.158232: step 220290, loss = 0.28 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 07:59:41.144249: step 220300, loss = 0.27 (1298.2 examples/sec; 0.099 sec/batch)
2017-06-02 07:59:41.931315: step 220310, loss = 0.24 (1626.3 examples/sec; 0.079 sec/batch)
2017-06-02 07:59:42.785824: step 220320, loss = 0.22 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 07:59:43.675219: step 220330, loss = 0.28 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 07:59:44.550364: step 220340, loss = 0.34 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 07:59:45.414080: step 220350, loss = 0.30 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 07:59:46.267182: step 220360, loss = 0.27 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 07:59:47.129144: step 220370, loss = 0.27 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 07:59:47.979047: step 220380, loss = 0.23 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 07:59:48.850652: step 220390, loss = 0.25 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 07:59:49.809390: step 220400, loss = 0.34 (1335.1 examples/sec; 0.096 sec/batch)
2017-06-02 07:59:50.584427: step 220410, loss = 0.26 (1651.6 examples/sec; 0.078 sec/batch)
2017-06-02 07:59:51.435069: step 220420, loss = 0.30 (1504.7 examples/sec; 0.085 sec/batch)
2017-06-02 07:59:52.380867: step 220430, loss = 0.23 (1353.4 examples/sec; 0.095 sec/batch)
2017-06-02 07:59:53.262103: step 220440, loss = 0.30 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 07:59:54.133358: step 220450, loss = 0.31 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:59:55.010184: step 220460, loss = 0.27 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 07:59:55.875938: step 220470, loss = 0.24 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 07:59:56.766957: step 220480, loss = 0.33 (1436.5 examples/sec; 0.089 sec/batch)
2017-06-02 07:59:57.640561: step 220490, loss = 0.25 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 07:59:58.593797: step 220500, loss = 0.24 (1342.8 examples/sec; 0.095 sec/batch)
2017-06-02 07:59:59.382173: step 220510, loss = 0.25 (1623.6 examples/sec; 0.079 sec/batch)
2017-06-02 08:00:00.262605: step 220520, loss = 0.35 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:00:01.131180: step 220530, loss = 0.29 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:00:01.993392: step 220540, loss = 0.32 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:00:02.836508: step 220550, loss = 0.35 (1518.2 examples/sec; 0.084 sec/batch)
2017-06-02 08:00:03.729258: step 220560, loss = 0.27 (1433.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:00:04.569932: step 220570, loss = 0.39 (1522.6 examples/sec; 0.084 sec/batch)
2017-06-02 08:00:05.464239: step 220580, loss = 0.26 (1431.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:00:06.315895: step 220590, loss = 0.26 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:00:07.304420: step 220600, loss = 0.27 (1294.9 examples/sec; 0.099 sec/batch)
2017-06-02 08:00:08.056652: step 220610, loss = 0.25 (1701.6 examples/sec; 0.075 sec/batch)
2017-06-02 08:00:08.916763: step 220620, loss = 0.28 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:00:09.786675: step 220630, loss = 0.34 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:00:10.656164: step 220640, loss = 0.23 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:00:11.522081: step 220650, loss = 0.32 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:00:12.381778: step 220660, loss = 0.31 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:00:13.225373: step 220670, loss = 0.29 (1517.3 examples/sec; 0.084 sec/batch)
2017-06-02 08:00:14.083319: step 220680, loss = 0.34 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:00:14.937152: step 220690, loss = 0.27 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:00:15.901741: step 220700, loss = 0.21 (1327.0 examples/sec; 0.096 sec/batch)
2017-06-02 08:00:16.652003: step 220710, loss = 0.36 (1706.0 examples/sec; 0.075 sec/batch)
2017-06-02 08:00:17.528691: step 220720, loss = 0.23 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:00:18.376269: step 220730, loss = 0.31 (1510.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:00:19.256759: step 220740, loss = 0.31 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:00:20.117126: step 220750, loss = 0.32 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:00:20.955876: step 220760, loss = 0.27 (1526.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:00:21.805615: step 220770, loss = 0.32 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:00:22.655930: step 220780, loss = 0.25 (1505.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:00:23.502165: step 220790, loss = 0.29 (1512.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:00:24.470484: step 220800, loss = 0.24 (1321.9 examples/sec; 0.097 sec/batch)
2017-06-02 08:00:25.250876: step 220810, loss = 0.30 (1640.2 examples/sec; 0.078 sec/batch)
2017-06-02 08:00:26.123230: step 220820, loss = 0.45 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:00:27.001188: step 220830, loss = 0.31 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:00:27.836302: step 220840, loss = 0.32 (1532.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:00:28.705667: step 220850, loss = 0.21 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:00:29.572653: step 220860, loss = 0.27 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:00:30.439654: step 220870, loss = 0.28 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:00:31.322306: step 220880, loss = 0.26 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:00:32.197172: step 220890, loss = 0.38 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:00:33.152492: step 220900, loss = 0.28 (1339.9 examples/sec; 0.096 sec/batch)
2017-06-02 08:00:33.903699: step 220910, loss = 0.27 (1703.9 examples/sec; 0.075 sec/batch)
2017-06-02 08:00:34.782437: step 220920, loss = 0.26 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:00:35.637043: step 220930, loss = 0.34 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:00:36.471824: step 220940, loss = 0.33 (1533.4 examples/sec; 0.083 sec/batch)
2017-06-02 08:00:37.321570: step 220950, loss = 0.26 (1506.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:00:38.176506: step 220960, loss = 0.35 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:00:39.047492: step 220970, loss = 0.23 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:00:39.912984: step 220980, loss = 0.42 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:00:40.775402: step 220990, loss = 0.26 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:00:41.758248: step 221000, loss = 0.24 (1302.3 examples/sec; 0.098 sec/batch)
2017-06-02 08:00:42.513799: step 221010, loss = 0.32 (1694.1 examples/sec; 0.076 sec/batch)
2017-06-02 08:00:43.388347: step 221020, loss = 0.27 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:00:44.256414: step 221030, loss = 0.25 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:00:45.106227: step 221040, loss = 0.31 (1506.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:00:45.952472: step 221050, loss = 0.34 (1512.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:00:46.807359: step 221060, loss = 0.28 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:00:47.662765: step 221070, loss = 0.32 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:00:48.505330: step 221080, loss = 0.26 (1519.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:00:49.329158: step 221090, loss = 0.30 (1553.7 examples/sec; 0.082 sec/batch)
2017-06-02 08:00:50.295833: step 221100, loss = 0.38 (1324.1 examples/sec; 0.097 sec/batch)
2017-06-02 08:00:51.080527: step 221110, loss = 0.44 (1631.2 examples/sec; 0.078 sec/batch)
2017-06-02 08:00:51.946901: step 221120, loss = 0.27 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:00:52.848089: step 221130, loss = 0.31 (1420.3 examples/sec; 0.090 sec/batch)
2017-06-02 08:00:53.707247: step 221140, loss = 0.31 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:00:54.558113: step 221150, loss = 0.24 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:00:55.418275: step 221160, loss = 0.24 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:00:56.261871: step 221170, loss = 0.27 (1517.3 examples/sec; 0.084 sec/batch)
2017-06-02 08:00:57.135508: step 221180, loss = 0.35 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:00:58.033700: step 221190, loss = 0.29 (1425.1 examples/sec; 0.090 sec/batch)
2017-06-02 08:00:59.012675: step 221200, loss = 0.29 (1307.5 examples/sec; 0.098 sec/batch)
2017-06-02 08:00:59.788501: step 221210, loss = 0.39 (1649.9 examples/sec; 0.078 sec/batch)
2017-06-02 08:01:00.662346: step 221220, loss = 0.30 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:01:01.541262: step 221230, loss = 0.30 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:01:02.418067: step 221240, loss = 0.28 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:01:03.299679: step 221250, loss = 0.27 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:01:04.154051: step 221260, loss = 0.23 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:01:05.043167: step 221270, loss = 0.36 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 08:01:05.927909: step 221280, loss = 0.26 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:01:06.837597: step 221290, loss = 0.24 (1407.1 examples/sec; 0.091 sec/batch)
2017-06-02 08:01:07.819035: step 221300, loss = 0.28 (1304.2 examples/sec; 0.098 sec/batch)
2017-06-02 08:01:08.595918: step 221310, loss = 0.27 (1647.6 examples/sec; 0.078 sec/batch)
2017-06-02 08:01:09.487493: step 221320, loss = 0.33 (1435.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:01:10.347929: step 221330, loss = 0.33 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:01:11.227759: step 221340, loss = 0.33 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:01:12.093199: step 221350, loss = 0.30 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:01:12.947712: step 221360, loss = 0.27 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:01:13.812641: step 221370, loss = 0.23 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:01:14.671243: step 221380, loss = 0.22 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:01:15.525679: step 221390, loss = 0.34 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:01:16.482928: step 221400, loss = 0.23 (1337.2 examples/sec; 0.096 sec/batch)
2017-06-02 08:01:17.247905: step 221410, loss = 0.27 (1673.2 examples/sec; 0.076 sec/batch)
2017-06-02 08:01:18.103569: step 221420, loss = 0.27 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:01:18.950338: step 221430, loss = 0.27 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:01:19.819641: step 221440, loss = 0.29 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:01:20.678351: step 221450, loss = 0.28 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:01:21.532547: step 221460, loss = 0.25 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:01:22.386248: step 221470, loss = 0.25 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:01:23.256623: step 221480, loss = 0.29 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:01:24.131110: step 221490, loss = 0.25 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:01:25.112593: step 221500, loss = 0.27 (1304.1 examples/sec; 0.098 sec/batch)
2017-06-02 08:01:25.891980: step 221510, loss = 0.30 (1642.3 examples/sec; 0.078 sec/batch)
2017-06-02 08:01:26.787083: step 221520, loss = 0.38 (1430.0 examples/sec; 0.090 sec/batch)
2017-06-02 08:01:27.660799: step 221530, loss = 0.23 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:01:28.521442: step 221540, loss = 0.23 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:01:29.395826: step 221550, loss = 0.34 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:01:30.272343: step 221560, loss = 0.32 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:01:31.137063: step 221570, loss = 0.33 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:01:32.021241: step 221580, loss = 0.29 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:01:32.881515: step 221590, loss = 0.36 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:01:33.886084: step 221600, loss = 0.29 (1274.2 examples/sec; 0.100 sec/batch)
2017-06-02 08:01:34.657907: step 221610, loss = 0.28 (1658.4 examples/sec; 0.077 sec/batch)
2017-06-02 08:01:35.526510: step 221620, loss = 0.29 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:01:36.406641: step 221630, loss = 0.38 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:01:37.255425: step 221640, loss = 0.39 (1508.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:01:38.110349: step 221650, loss = 0.30 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:01:38.969284: step 221660, loss = 0.27 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:01:39.827407: step 221670, loss = 0.20 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:01:40.696062: step 221680, loss = 0.35 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:01:41.555597: step 221690, loss = 0.27 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:01:42.532814: step 221700, loss = 0.25 (1309.8 examples/sec; 0.098 sec/batch)
2017-06-02 08:01:43.302074: step 221710, loss = 0.22 (1663.9 examples/sec; 0.077 sec/batch)
2017-06-02 08:01:44.165411: step 221720, loss = 0.33 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:01:45.032611: step 221730, loss = 0.33 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:01:45.901584: step 221740, loss = 0.27 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:01:46.761234: step 221750, loss = 0.27 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:01:47.623675: step 221760, loss = 0.33 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:01:48.493438: step 221770, loss = 0.30 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:01:49.367544: step 221780, loss = 0.27 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:01:50.198520: step 221790, loss = 0.23 (1540.4 examples/sec; 0.083 sec/batch)
2017-06-02 08:01:51.143999: step 221800, loss = 0.24 (1353.8 examples/sec; 0.095 sec/batch)
2017-06-02 08:01:51.908574: step 221810, loss = 0.22 (1674.1 examples/sec; 0.076 sec/batch)
2017-06-02 08:01:52.786229: step 221820, loss = 0.32 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:01:53.638912: step 221830, loss = 0.23 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:01:54.499592: step 221840, loss = 0.29 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:01:55.358285: step 221850, loss = 0.33 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:01:56.232813: step 221860, loss = 0.18 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:01:57.105852: step 221870, loss = 0.31 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:01:57.968180: step 221880, loss = 0.39 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:01:58.819678: step 221890, loss = 0.36 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:01:59.789660: step 221900, loss = 0.32 (1319.6 examples/sec; 0.097 sec/batch)
2017-06-02 08:02:00.553060: step 221910, loss = 0.38 (1676.7 examples/sec; 0.076 sec/batch)
2017-06-02 08:02:01.396706: step 221920, loss = 0.25 (1517.2 examples/sec; 0.084 sec/batch)
2017-06-02 08:02:02.249332: step 221930, loss = 0.36 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:02:03.111926: step 221940, loss = 0.30 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:02:03.993953: step 221950, loss = 0.23 (1451.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:02:04.838455: step 221960, loss = 0.29 (1515.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:02:05.676039: step 221970, loss = 0.22 (1528.2 examples/sec; 0.084 sec/batch)
2017-06-02 08:02:06.552002: step 221980, loss = 0.27 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:02:07.411585: step 221990, loss = 0.27 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:02:08.380335: step 222000, loss = 0.34 (1321.3 examples/sec; 0.097 sec/batch)
2017-06-02 08:02:09.139552: step 222010, loss = 0.26 (1685.9 examples/sec; 0.076 sec/batch)
2017-06-02 08:02:09.997000: step 222020, loss = 0.33 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:02:10.846804: step 222030, loss = 0.26 (1506.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:02:11.700490: step 222040, loss = 0.35 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:02:12.561007: step 222050, loss = 0.30 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:02:13.437541: step 222060, loss = 0.37 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:02:14.318870: step 222070, loss = 0.31 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:02:15.177011: step 222080, loss = 0.24 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:02:16.039618: step 222090, loss = 0.21 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:02:17.013837: step 222100, loss = 0.26 (1313.9 examples/sec; 0.097 sec/batch)
2017-06-02 08:02:17.798847: step 222110, loss = 0.36 (1630.5 examples/sec; 0.079 sec/batch)
2017-06-02 08:02:18.691209: step 222120, loss = 0.23 (1434.4 examples/sec; 0.089 sec/batch)
2017-06-02 08:02:19.563805: step 222130, loss = 0.24 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:02:20.450035: step 222140, loss = 0.30 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:02:21.311484: step 222150, loss = 0.30 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:02:22.184110: step 222160, loss = 0.32 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:02:23.072857: step 222170, loss = 0.32 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:02:23.944204: step 222180, loss = 0.25 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:02:24.790760: step 222190, loss = 0.40 (1512.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:02:25.776510: step 222200, loss = 0.28 (1298.5 examples/sec; 0.099 sec/batch)
2017-06-02 08:02:26.524089: step 222210, loss = 0.33 (1712.2 examples/sec; 0.075 sec/batch)
2017-06-02 08:02:27.396577: step 222220, loss = 0.27 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:02:28.230269: step 222230, loss = 0.28 (1535.3 examples/sec; 0.083 sec/batch)
2017-06-02 08:02:29.073385: step 222240, loss = 0.29 (1518.2 examples/sec; 0.084 sec/batch)
2017-06-02 08:02:29.919311: step 222250, loss = 0.25 (1513.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:02:30.790406: step 222260, loss = 0.21 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:02:31.641674: step 222270, loss = 0.29 (1503.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:02:32.513597: step 222280, loss = 0.32 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:02:33.380419: step 222290, loss = 0.37 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:02:34.364940: step 222300, loss = 0.26 (1300.1 examples/sec; 0.098 sec/batch)
2017-06-02 08:02:35.138321: step 222310, loss = 0.23 (1655.1 examples/sec; 0.077 sec/batch)
2017-06-02 08:02:35.999565: step 222320, loss = 0.27 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:02:36.886603: step 222330, loss = 0.29 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:02:37.758111: step 222340, loss = 0.22 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:02:38.636386: step 222350, loss = 0.24 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:02:39.496861: step 222360, loss = 0.29 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:02:40.379153: step 222370, loss = 0.30 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:02:41.240603: step 222380, loss = 0.30 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:02:42.120246: step 222390, loss = 0.34 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:02:43.070093: step 222400, loss = 0.32 (1347.6 examples/sec; 0.095 sec/batch)
2017-06-02 08:02:43.805394: step 222410, loss = 0.34 (1740.8 examples/sec; 0.074 sec/batch)
2017-06-02 08:02:44.688456: step 222420, loss = 0.38 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:02:45.539291: step 222430, loss = 0.33 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:02:46.400781: step 222440, loss = 0.39 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:02:47.272788: step 222450, loss = 0.40 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:02:48.119338: step 222460, loss = 0.38 (1512.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:02:48.978299: step 222470, loss = 0.28 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:02:49.811620: step 222480, loss = 0.28 (1536.0 examples/sec; 0.083 sec/batch)
2017-06-02 08:02:50.676237: step 222490, loss = 0.26 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:02:51.689154: step 222500, loss = 0.27 (1263.7 examples/sec; 0.101 sec/batch)
2017-06-02 08:02:52.421665: step 222510, loss = 0.34 (1747.4 examples/sec; 0.073 sec/batch)
2017-06-02 08:02:53.285810: step 222520, loss = 0.25 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:02:54.160653: step 222530, loss = 0.31 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:02:54.990439: step 222540, loss = 0.34 (1542.5 examples/sec; 0.083 sec/batch)
2017-06-02 08:02:55.832082: step 222550, loss = 0.27 (1520.8 examples/sec; 0.084 sec/batch)
2017-06-02 08:02:56.721284: step 222560, loss = 0.31 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:02:57.583530: step 222570, loss = 0.27 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:02:58.453504: step 222580, loss = 0.34 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:02:59.322277: step 222590, loss = 0.21 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:03:00.295347: step 222600, loss = 0.24 (1315.4 examples/sec; 0.097 sec/batch)
2017-06-02 08:03:01.076598: step 222610, loss = 0.26 (1638.4 examples/sec; 0.078 sec/batch)
2017-06-02 08:03:01.923719: step 222620, loss = 0.27 (1511.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:03:02.794472: step 222630, loss = 0.27 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:03:03.671945: step 222640, loss = 0.38 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:03:04.532711: step 222650, loss = 0.20 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:03:05.409622: step 222660, loss = 0.34 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:03:06.273658: step 222670, loss = 0.21 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:03:07.102240: step 222680, loss = 0.23 (1544.8 examples/sec; 0.083 sec/batch)
2017-06-02 08:03:07.973334: step 222690, loss = 0.35 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:03:08.931625: step 222700, loss = 0.24 (1335.7 examples/sec; 0.096 sec/batch)
2017-06-02 08:03:09.691110: step 222710, loss = 0.25 (1685.4 examples/sec; 0.076 sec/batch)
2017-06-02 08:03:10.521204: step 222720, loss = 0.24 (1542.0 examples/sec; 0.083 sec/batch)
2017-06-02 08:03:11.355242: step 222730, loss = 0.34 (1534.7 examples/sec; 0.083 sec/batch)
2017-06-02 08:03:12.226759: step 222740, loss = 0.31 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:03:13.074474: step 222750, loss = 0.26 (1509.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:03:13.929106: step 222760, loss = 0.32 (1497.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:03:14.768742: step 222770, loss = 0.29 (1524.5 examples/sec; 0.084 sec/batch)
2017-06-02 08:03:15.617305: step 222780, loss = 0.31 (1508.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:03:16.472429: step 222790, loss = 0.30 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:03:17.403117: step 222800, loss = 0.38 (1375.3 examples/sec; 0.093 sec/batch)
2017-06-02 08:03:18.162800: step 222810, loss = 0.26 (1684.9 examples/sec; 0.076 sec/batch)
2017-06-02 08:03:19.035497: step 222820, loss = 0.34 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:03:19.879945: step 222830, loss = 0.26 (1515.8 examples/sec; 0.084 sec/batch)
2017-06-02 08:03:20.776114: step 222840, loss = 0.27 (1428.3 examples/sec; 0.090 sec/batch)
2017-06-02 08:03:21.651351: step 222850, loss = 0.26 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:03:22.515285: step 222860, loss = 0.26 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:03:23.386812: step 222870, loss = 0.34 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:03:24.254572: step 222880, loss = 0.32 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:03:25.104187: step 222890, loss = 0.29 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:03:26.076822: step 222900, loss = 0.26 (1316.0 examples/sec; 0.097 sec/batch)
2017-06-02 08:03:26.866914: step 222910, loss = 0.27 (1620.1 examples/sec; 0.079 sec/batch)
2017-06-02 08:03:27.711458: step 222920, loss = 0.26 (1515.6 examples/sec; 0.084 sec/batch)
2017-06-02 08:03:28.586645: step 222930, loss = 0.26 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:03:29.464071: step 222940, loss = 0.25 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:03:30.306478: step 222950, loss = 0.30 (1519.5 examples/sec; 0.084 sec/batch)
2017-06-02 08:03:31.164574: step 222960, loss = 0.35 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:03:32.048100: step 222970, loss = 0.28 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:03:32.912129: step 222980, loss = 0.28 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:03:33.768158: step 222990, loss = 0.27 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:03:34.739426: step 223000, loss = 0.25 (1317.9 examples/sec; 0.097 sec/batch)
2017-06-02 08:03:35.523513: step 223010, loss = 0.29 (1632.5 examples/sec; 0.078 sec/batch)
2017-06-02 08:03:36.383804: step 223020, loss = 0.25 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:03:37.249191: step 223030, loss = 0.26 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:03:38.097120: step 223040, loss = 0.21 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:03:38.955601: step 223050, loss = 0.24 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:03:39.831917: step 223060, loss = 0.21 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:03:40.714122: step 223070, loss = 0.30 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:03:41.596230: step 223080, loss = 0.33 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:03:42.453524: step 223090, loss = 0.30 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:03:43.413143: step 223100, loss = 0.28 (1333.8 examples/sec; 0.096 sec/batch)
2017-06-02 08:03:44.196467: step 223110, loss = 0.42 (1634.1 examples/sec; 0.078 sec/batch)
2017-06-02 08:03:45.060153: step 223120, loss = 0.29 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:03:45.905322: step 223130, loss = 0.26 (1514.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:03:46.782727: step 223140, loss = 0.30 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:03:47.671426: step 223150, loss = 0.29 (1440.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:03:48.544495: step 223160, loss = 0.25 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:03:49.378616: step 223170, loss = 0.32 (1534.5 examples/sec; 0.083 sec/batch)
2017-06-02 08:03:50.233301: step 223180, loss = 0.43 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:03:51.112967: step 223190, loss = 0.28 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:03:52.100380: step 223200, loss = 0.27 (1296.3 examples/sec; 0.099 sec/batch)
2017-06-02 08:03:52.884174: step 223210, loss = 0.24 (1633.1 examples/sec; 0.078 sec/batch)
2017-06-02 08:03:53.775722: step 223220, loss = 0.29 (1435.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:03:54.631014: step 223230, loss = 0.23 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:03:55.486665: step 223240, loss = 0.28 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:03:56.366384: step 223250, loss = 0.28 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:03:57.243059: step 223260, loss = 0.25 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:03:58.097809: step 223270, loss = 0.23 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:03:58.937482: step 223280, loss = 0.33 (1524.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:03:59.798415: step 223290, loss = 0.32 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:04:00.764276: step 223300, loss = 0.36 (1325.3 examples/sec; 0.097 sec/batch)
2017-06-02 08:04:01.511100: step 223310, loss = 0.33 (1713.9 examples/sec; 0.075 sec/batch)
2017-06-02 08:04:02.393928: step 223320, loss = 0.37 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:04:03.254504: step 223330, loss = 0.27 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:04:04.104494: step 223340, loss = 0.24 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:04:04.950335: step 223350, loss = 0.37 (1513.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:04:05.817391: step 223360, loss = 0.25 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:04:06.665395: step 223370, loss = 0.38 (1509.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:04:07.514095: step 223380, loss = 0.36 (1508.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:04:08.375268: step 223390, loss = 0.38 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:04:09.350423: step 223400, loss = 0.26 (1312.6 examples/sec; 0.098 sec/batch)
2017-06-02 08:04:10.089373: step 223410, loss = 0.29 (1732.2 examples/sec; 0.074 sec/batch)
2017-06-02 08:04:10.935506: step 223420, loss = 0.25 (1512.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:04:11.809023: step 223430, loss = 0.22 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:04:12.665802: step 223440, loss = 0.35 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:04:13.519080: step 223450, loss = 0.33 (1500.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:04:14.378269: step 223460, loss = 0.34 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:04:15.237678: step 223470, loss = 0.30 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:04:16.097526: step 223480, loss = 0.37 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:04:16.957075: step 223490, loss = 0.23 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:04:17.904910: step 223500, loss = 0.25 (1350.4 examples/sec; 0.095 sec/batch)
2017-06-02 08:04:18.662150: step 223510, loss = 0.28 (1690.4 examples/sec; 0.076 sec/batch)
2017-06-02 08:04:19.489841: step 223520, loss = 0.34 (1546.5 examples/sec; 0.083 sec/batch)
2017-06-02 08:04:20.325388: step 223530, loss = 0.32 (1531.9 examples/sec; 0.084 sec/batch)
2017-06-02 08:04:21.205097: step 223540, loss = 0.34 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:04:22.063748: step 223550, loss = 0.34 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:04:22.947610: step 223560, loss = 0.32 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:04:23.786305: step 223570, loss = 0.34 (1526.2 examples/sec; 0.084 sec/batch)
2017-06-02 08:04:24.654853: step 223580, loss = 0.34 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:04:25.486947: step 223590, loss = 0.23 (1538.3 examples/sec; 0.083 sec/batch)
2017-06-02 08:04:26.490911: step 223600, loss = 0.21 (1274.9 examples/sec; 0.100 sec/batch)
2017-06-02 08:04:27.217390: step 223610, loss = 0.27 (1761.9 examples/sec; 0.073 sec/batch)
2017-06-02 08:04:28.058402: step 223620, loss = 0.29 (1522.0 examples/sec; 0.084 sec/batch)
2017-06-02 08:04:28.887198: step 223630, loss = 0.30 (1544.4 examples/sec; 0.083 sec/batch)
2017-06-02 08:04:29.738463: step 223640, loss = 0.28 (1503.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:04:30.568150: step 223650, loss = 0.26 (1542.8 examples/sec; 0.083 sec/batch)
2017-06-02 08:04:31.425600: step 223660, loss = 0.27 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:04:32.302584: step 223670, loss = 0.32 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:04:33.183428: step 223680, loss = 0.32 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:04:34.059018: step 223690, loss = 0.30 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:04:35.052111: step 223700, loss = 0.39 (1288.9 examples/sec; 0.099 sec/batch)
2017-06-02 08:04:35.765291: step 223710, loss = 0.29 (1794.8 examples/sec; 0.071 sec/batch)
2017-06-02 08:04:36.643189: step 223720, loss = 0.30 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:04:37.517879: step 223730, loss = 0.31 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:04:38.385835: step 223740, loss = 0.41 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:04:39.252796: step 223750, loss = 0.25 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:04:40.114142: step 223760, loss = 0.35 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:04:40.968908: step 223770, loss = 0.25 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:04:41.819985: step 223780, loss = 0.26 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:04:42.664814: step 223790, loss = 0.22 (1515.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:04:43.618978: step 223800, loss = 0.30 (1341.5 examples/sec; 0.095 sec/batch)
2017-06-02 08:04:44.378649: step 223810, loss = 0.28 (1684.9 examples/sec; 0.076 sec/batch)
2017-06-02 08:04:45.228020: step 223820, loss = 0.31 (1507.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:04:46.102292: step 223830, loss = 0.23 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:04:46.966289: step 223840, loss = 0.28 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:04:47.830551: step 223850, loss = 0.29 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:04:48.694577: step 223860, loss = 0.27 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:04:49.556756: step 223870, loss = 0.26 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:04:50.420264: step 223880, loss = 0.31 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:04:51.311494: step 223890, loss = 0.26 (1436.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:04:52.294678: step 223900, loss = 0.29 (1301.9 examples/sec; 0.098 sec/batch)
2017-06-02 08:04:53.066816: step 223910, loss = 0.30 (1657.7 examples/sec; 0.077 sec/batch)
2017-06-02 08:04:53.948269: step 223920, loss = 0.29 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:04:54.844168: step 223930, loss = 0.37 (1428.7 examples/sec; 0.090 sec/batch)
2017-06-02 08:04:55.718176: step 223940, loss = 0.29 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:04:56.575512: step 223950, loss = 0.41 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:04:57.438312: step 223960, loss = 0.22 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:04:58.297842: step 223970, loss = 0.40 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:04:59.150788: step 223980, loss = 0.21 (1500.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:05:00.029226: step 223990, loss = 0.28 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:05:00.997428: step 224000, loss = 0.31 (1322.0 examples/sec; 0.097 sec/batch)
2017-06-02 08:05:01.746095: step 224010, loss = 0.25 (1709.7 examples/sec; 0.075 sec/batch)
2017-06-02 08:05:02.603409: step 224020, loss = 0.30 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:05:03.462640: step 224030, loss = 0.36 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:05:04.314647: step 224040, loss = 0.24 (1502.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:05:05.191057: step 224050, loss = 0.32 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:05:06.044761: step 224060, loss = 0.29 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:05:06.910027: step 224070, loss = 0.21 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:05:07.775982: step 224080, loss = 0.35 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:05:08.607173: step 224090, loss = 0.25 (1540.0 examples/sec; 0.083 sec/batch)
2017-06-02 08:05:09.586744: step 224100, loss = 0.31 (1306.7 examples/sec; 0.098 sec/batch)
2017-06-02 08:05:10.346723: step 224110, loss = 0.26 (1684.3 examples/sec; 0.076 sec/batch)
2017-06-02 08:05:11.200034: step 224120, loss = 0.31 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:05:12.045746: step 224130, loss = 0.28 (1513.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:05:12.913707: step 224140, loss = 0.29 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:05:13.795143: step 224150, loss = 0.22 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:05:14.664684: step 224160, loss = 0.21 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:05:15.527926: step 224170, loss = 0.29 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:05:16.399019: step 224180, loss = 0.30 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:05:17.278233: step 224190, loss = 0.22 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:05:18.246336: step 224200, loss = 0.23 (1322.2 examples/sec; 0.097 sec/batch)
2017-06-02 08:05:19.031271: step 224210, loss = 0.32 (1630.7 examples/sec; 0.078 sec/batch)
2017-06-02 08:05:19.897587: step 224220, loss = 0.22 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:05:20.783823: step 224230, loss = 0.29 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:05:21.664783: step 224240, loss = 0.26 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:05:22.538556: step 224250, loss = 0.33 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:05:23.430507: step 224260, loss = 0.32 (1435.1 examples/sec; 0.089 sec/batch)
2017-06-02 08:05:24.274052: step 224270, loss = 0.41 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:05:25.150867: step 224280, loss = 0.24 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:05:26.043953: step 224290, loss = 0.26 (1433.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:05:27.034962: step 224300, loss = 0.23 (1291.6 examples/sec; 0.099 sec/batch)
2017-06-02 08:05:27.818838: step 224310, loss = 0.28 (1632.9 examples/sec; 0.078 sec/batch)
2017-06-02 08:05:28.704506: step 224320, loss = 0.25 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:05:29.603531: step 224330, loss = 0.33 (1423.8 examples/sec; 0.090 sec/batch)
2017-06-02 08:05:30.470143: step 224340, loss = 0.34 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:05:31.345004: step 224350, loss = 0.38 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:05:32.208918: step 224360, loss = 0.23 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:05:33.095885: step 224370, loss = 0.19 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 08:05:33.950608: step 224380, loss = 0.22 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:05:34.831802: step 224390, loss = 0.36 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:05:35.812396: step 224400, loss = 0.30 (1305.3 examples/sec; 0.098 sec/batch)
2017-06-02 08:05:36.604700: step 224410, loss = 0.29 (1615.6 examples/sec; 0.079 sec/batch)
2017-06-02 08:05:37.479509: step 224420, loss = 0.25 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:05:38.353477: step 224430, loss = 0.27 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:05:39.227298: step 224440, loss = 0.26 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:05:40.091155: step 224450, loss = 0.28 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:05:40.965656: step 224460, loss = 0.31 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:05:41.828777: step 224470, loss = 0.29 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:05:42.683133: step 224480, loss = 0.28 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:05:43.518748: step 224490, loss = 0.23 (1531.8 examples/sec; 0.084 sec/batch)
2017-06-02 08:05:44.448461: step 224500, loss = 0.33 (1376.7 examples/sec; 0.093 sec/batch)
2017-06-02 08:05:45.199916: step 224510, loss = 0.28 (1703.4 examples/sec; 0.075 sec/batch)
2017-06-02 08:05:46.056912: step 224520, loss = 0.32 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:05:46.903130: step 224530, loss = 0.31 (1512.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:05:47.771962: step 224540, loss = 0.32 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:05:48.608145: step 224550, loss = 0.29 (1530.8 examples/sec; 0.084 sec/batch)
2017-06-02 08:05:49.442604: step 224560, loss = 0.32 (1533.9 examples/sec; 0.083 sec/batch)
2017-06-02 08:05:50.273725: step 224570, loss = 0.24 (1540.1 examples/sec; 0.083 sec/batch)
2017-06-02 08:05:51.124541: step 224580, loss = 0.41 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:05:51.968767: step 224590, loss = 0.33 (1516.2 examples/sec; 0.084 sec/batch)
2017-06-02 08:05:52.911324: step 224600, loss = 0.33 (1358.0 examples/sec; 0.094 sec/batch)
2017-06-02 08:05:53.680824: step 224610, loss = 0.24 (1663.5 examples/sec; 0.077 sec/batch)
2017-06-02 08:05:54.527630: step 224620, loss = 0.31 (1511.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:05:55.387109: step 224630, loss = 0.28 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:05:56.243837: step 224640, loss = 0.30 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:05:57.093687: step 224650, loss = 0.30 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:05:57.970057: step 224660, loss = 0.28 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:05:58.848207: step 224670, loss = 0.28 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:05:59.701787: step 224680, loss = 0.30 (1499.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:06:00.593929: step 224690, loss = 0.37 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:06:01.551278: step 224700, loss = 0.23 (1337.0 examples/sec; 0.096 sec/batch)
2017-06-02 08:06:02.308910: step 224710, loss = 0.25 (1689.5 examples/sec; 0.076 sec/batch)
2017-06-02 08:06:03.191444: step 224720, loss = 0.40 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:06:04.043842: step 224730, loss = 0.41 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:06:04.920211: step 224740, loss = 0.30 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:06:05.775562: step 224750, loss = 0.30 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:06:06.655206: step 224760, loss = 0.27 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:06:07.511206: step 224770, loss = 0.33 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:06:08.367114: step 224780, loss = 0.33 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:06:09.228043: step 224790, loss = 0.30 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:06:10.171887: step 224800, loss = 0.31 (1356.2 examples/sec; 0.094 sec/batch)
2017-06-02 08:06:10.935760: step 224810, loss = 0.25 (1675.7 examples/sec; 0.076 sec/batch)
2017-06-02 08:06:11.810193: step 224820, loss = 0.31 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:06:12.685342: step 224830, loss = 0.24 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:06:13.553376: step 224840, loss = 0.24 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:06:14.427202: step 224850, loss = 0.35 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:06:15.284966: step 224860, loss = 0.28 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:06:16.151712: step 224870, loss = 0.28 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:06:17.030667: step 224880, loss = 0.39 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:06:17.909154: step 224890, loss = 0.30 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:06:18.885142: step 224900, loss = 0.34 (1311.5 examples/sec; 0.098 sec/batch)
2017-06-02 08:06:19.656487: step 224910, loss = 0.33 (1659.5 examples/sec; 0.077 sec/batch)
2017-06-02 08:06:20.488867: step 224920, loss = 0.35 (1537.8 examples/sec; 0.083 sec/batch)
2017-06-02 08:06:21.338963: step 224930, loss = 0.40 (1505.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:06:22.184481: step 224940, loss = 0.27 (1513.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:06:23.036648: step 224950, loss = 0.30 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:06:23.926725: step 224960, loss = 0.33 (1438.1 examples/sec; 0.089 sec/batch)
2017-06-02 08:06:24.823373: step 224970, loss = 0.28 (1427.5 examples/sec; 0.090 sec/batch)
2017-06-02 08:06:25.694672: step 224980, loss = 0.26 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:06:26.534108: step 224990, loss = 0.31 (1524.8 examples/sec; 0.084 sec/batch)
2017-06-02 08:06:27.503692: step 225000, loss = 0.29 (1320.2 examples/sec; 0.097 sec/batch)
2017-06-02 08:06:28.306191: step 225010, loss = 0.31 (1595.0 examples/sec; 0.080 sec/batch)
2017-06-02 08:06:29.162799: step 225020, loss = 0.20 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:06:30.028617: step 225030, loss = 0.28 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:06:30.897953: step 225040, loss = 0.29 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:06:31.748296: step 225050, loss = 0.36 (1505.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:06:32.635024: step 225060, loss = 0.29 (1443.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:06:33.510290: step 225070, loss = 0.35 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:06:34.385500: step 225080, loss = 0.25 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:06:35.209616: step 225090, loss = 0.24 (1553.2 examples/sec; 0.082 sec/batch)
2017-06-02 08:06:36.187426: step 225100, loss = 0.30 (1309.0 examples/sec; 0.098 sec/batch)
2017-06-02 08:06:36.948101: step 225110, loss = 0.26 (1682.7 examples/sec; 0.076 sec/batch)
2017-06-02 08:06:37.780967: step 225120, loss = 0.40 (1536.9 examples/sec; 0.083 sec/batch)
2017-06-02 08:06:38.642343: step 225130, loss = 0.25 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:06:39.503550: step 225140, loss = 0.25 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:06:40.384906: step 225150, loss = 0.29 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:06:41.239715: step 225160, loss = 0.28 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:06:42.093872: step 225170, loss = 0.27 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:06:42.984812: step 225180, loss = 0.29 (1436.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:06:43.860850: step 225190, loss = 0.26 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:06:44.822447: step 225200, loss = 0.23 (1331.1 examples/sec; 0.096 sec/batch)
2017-06-02 08:06:45.595199: step 225210, loss = 0.29 (1656.4 examples/sec; 0.077 sec/batch)
2017-06-02 08:06:46.466842: step 225220, loss = 0.28 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:06:47.336065: step 225230, loss = 0.28 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:06:48.185641: step 225240, loss = 0.32 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:06:49.037682: step 225250, loss = 0.33 (1502.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:06:49.860132: step 225260, loss = 0.22 (1556.3 examples/sec; 0.082 sec/batch)
2017-06-02 08:06:50.750739: step 225270, loss = 0.30 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:06:51.618738: step 225280, loss = 0.28 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:06:52.507089: step 225290, loss = 0.35 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:06:53.473704: step 225300, loss = 0.29 (1324.2 examples/sec; 0.097 sec/batch)
2017-06-02 08:06:54.268495: step 225310, loss = 0.24 (1610.5 examples/sec; 0.079 sec/batch)
2017-06-02 08:06:55.135643: step 225320, loss = 0.28 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:06:55.992144: step 225330, loss = 0.29 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:06:56.849383: step 225340, loss = 0.29 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:06:57.705444: step 225350, loss = 0.37 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:06:58.566651: step 225360, loss = 0.33 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:06:59.432540: step 225370, loss = 0.24 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:07:00.286001: step 225380, loss = 0.28 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:07:01.134033: step 225390, loss = 0.35 (1509.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:07:02.115440: step 225400, loss = 0.31 (1304.3 examples/sec; 0.098 sec/batch)
2017-06-02 08:07:02.885636: step 225410, loss = 0.28 (1661.9 examples/sec; 0.077 sec/batch)
2017-06-02 08:07:03.743693: step 225420, loss = 0.27 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:07:04.606440: step 225430, loss = 0.31 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:07:05.483318: step 225440, loss = 0.44 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:07:06.331964: step 225450, loss = 0.21 (1508.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:07:07.188493: step 225460, loss = 0.26 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:07:08.031976: step 225470, loss = 0.29 (1517.5 examples/sec; 0.084 sec/batch)
2017-06-02 08:07:08.896490: step 225480, loss = 0.24 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:07:09.761317: step 225490, loss = 0.25 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:07:10.729364: step 225500, loss = 0.32 (1322.2 examples/sec; 0.097 sec/batch)
2017-06-02 08:07:11.491849: step 225510, loss = 0.29 (1678.7 examples/sec; 0.076 sec/batch)
2017-06-02 08:07:12.335337: step 225520, loss = 0.25 (1517.5 examples/sec; 0.084 sec/batch)
2017-06-02 08:07:13.180866: step 225530, loss = 0.38 (1513.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:07:14.064145: step 225540, loss = 0.30 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:07:14.924153: step 225550, loss = 0.30 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:07:15.788439: step 225560, loss = 0.34 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:07:16.673298: step 225570, loss = 0.35 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:07:17.531572: step 225580, loss = 0.27 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:07:18.376399: step 225590, loss = 0.34 (1515.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:07:19.341953: step 225600, loss = 0.33 (1325.7 examples/sec; 0.097 sec/batch)
2017-06-02 08:07:20.099104: step 225610, loss = 0.36 (1690.5 examples/sec; 0.076 sec/batch)
2017-06-02 08:07:20.965619: step 225620, loss = 0.31 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:07:21.817774: step 225630, loss = 0.31 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:07:22.683806: step 225640, loss = 0.24 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:07:23.579026: step 225650, loss = 0.28 (1429.8 examples/sec; 0.090 sec/batch)
2017-06-02 08:07:24.434946: step 225660, loss = 0.34 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:07:25.281757: step 225670, loss = 0.26 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:07:26.158580: step 225680, loss = 0.27 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:07:27.032573: step 225690, loss = 0.29 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:07:28.007294: step 225700, loss = 0.42 (1313.2 examples/sec; 0.097 sec/batch)
2017-06-02 08:07:28.755983: step 225710, loss = 0.29 (1709.7 examples/sec; 0.075 sec/batch)
2017-06-02 08:07:29.603259: step 225720, loss = 0.31 (1510.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:07:30.496461: step 225730, loss = 0.22 (1433.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:07:31.381988: step 225740, loss = 0.26 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:07:32.228216: step 225750, loss = 0.31 (1512.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:07:33.090156: step 225760, loss = 0.25 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:07:33.958148: step 225770, loss = 0.33 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:07:34.820057: step 225780, loss = 0.23 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:07:35.695758: step 225790, loss = 0.28 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:07:36.671093: step 225800, loss = 0.25 (1312.4 examples/sec; 0.098 sec/batch)
2017-06-02 08:07:37.428142: step 225810, loss = 0.26 (1690.8 examples/sec; 0.076 sec/batch)
2017-06-02 08:07:38.285873: step 225820, loss = 0.28 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:07:39.126272: step 225830, loss = 0.29 (1523.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:07:39.994389: step 225840, loss = 0.29 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:07:40.841179: step 225850, loss = 0.41 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:07:41.703491: step 225860, loss = 0.33 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:07:42.562904: step 225870, loss = 0.25 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:07:43.427583: step 225880, loss = 0.23 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:07:44.273116: step 225890, loss = 0.34 (1513.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:07:45.226507: step 225900, loss = 0.28 (1342.6 examples/sec; 0.095 sec/batch)
2017-06-02 08:07:45.967496: step 225910, loss = 0.38 (1727.4 examples/sec; 0.074 sec/batch)
2017-06-02 08:07:46.848723: step 225920, loss = 0.25 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:07:47.705171: step 225930, loss = 0.43 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:07:48.556868: step 225940, loss = 0.26 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:07:49.426789: step 225950, loss = 0.26 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:07:50.296496: step 225960, loss = 0.34 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:07:51.147335: step 225970, loss = 0.26 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:07:51.991006: step 225980, loss = 0.31 (1517.2 examples/sec; 0.084 sec/batch)
2017-06-02 08:07:52.866603: step 225990, loss = 0.23 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:07:53.828184: step 226000, loss = 0.24 (1331.1 examples/sec; 0.096 sec/batch)
2017-06-02 08:07:54.607351: step 226010, loss = 0.33 (1642.8 examples/sec; 0.078 sec/batch)
2017-06-02 08:07:55.508242: step 226020, loss = 0.31 (1420.8 examples/sec; 0.090 sec/batch)
2017-06-02 08:07:56.397487: step 226030, loss = 0.30 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 08:07:57.269001: step 226040, loss = 0.33 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:07:58.137069: step 226050, loss = 0.24 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:07:59.029704: step 226060, loss = 0.35 (1434.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:07:59.878583: step 226070, loss = 0.26 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:08:00.743609: step 226080, loss = 0.24 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:08:01.609811: step 226090, loss = 0.40 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:08:02.583399: step 226100, loss = 0.40 (1314.7 examples/sec; 0.097 sec/batch)
2017-06-02 08:08:03.335432: step 226110, loss = 0.33 (1702.1 examples/sec; 0.075 sec/batch)
2017-06-02 08:08:04.192411: step 226120, loss = 0.24 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:08:05.049523: step 226130, loss = 0.26 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:08:05.935842: step 226140, loss = 0.28 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:08:06.788649: step 226150, loss = 0.28 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:08:07.676061: step 226160, loss = 0.30 (1442.4 examples/sec; 0.089 sec/batch)
2017-06-02 08:08:08.576855: step 226170, loss = 0.42 (1421.0 examples/sec; 0.090 sec/batch)
2017-06-02 08:08:09.454664: step 226180, loss = 0.23 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:08:10.327793: step 226190, loss = 0.27 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:08:11.306991: step 226200, loss = 0.23 (1307.2 examples/sec; 0.098 sec/batch)
2017-06-02 08:08:12.069644: step 226210, loss = 0.34 (1678.3 examples/sec; 0.076 sec/batch)
2017-06-02 08:08:12.926914: step 226220, loss = 0.25 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:08:13.807154: step 226230, loss = 0.30 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:08:14.640661: step 226240, loss = 0.35 (1535.7 examples/sec; 0.083 sec/batch)
2017-06-02 08:08:15.499569: step 226250, loss = 0.31 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:08:16.381707: step 226260, loss = 0.30 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:08:17.251750: step 226270, loss = 0.27 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:08:18.113352: step 226280, loss = 0.28 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:08:19.012434: step 226290, loss = 0.26 (1423.7 examples/sec; 0.090 sec/batch)
2017-06-02 08:08:19.970214: step 226300, loss = 0.24 (1336.4 examples/sec; 0.096 sec/batch)
2017-06-02 08:08:20.741693: step 226310, loss = 0.31 (1659.2 examples/sec; 0.077 sec/batch)
2017-06-02 08:08:21.629277: step 226320, loss = 0.31 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 08:08:22.506049: step 226330, loss = 0.39 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:08:23.369368: step 226340, loss = 0.36 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:08:24.241012: step 226350, loss = 0.25 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:08:25.101968: step 226360, loss = 0.28 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:08:25.977122: step 226370, loss = 0.25 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:08:26.859608: step 226380, loss = 0.37 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:08:27.758783: step 226390, loss = 0.28 (1423.5 examples/sec; 0.090 sec/batch)
2017-06-02 08:08:28.739081: step 226400, loss = 0.28 (1305.7 examples/sec; 0.098 sec/batch)
2017-06-02 08:08:29.515136: step 226410, loss = 0.39 (1649.4 examples/sec; 0.078 sec/batch)
2017-06-02 08:08:30.365267: step 226420, loss = 0.33 (1505.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:08:31.239319: step 226430, loss = 0.30 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:08:32.118885: step 226440, loss = 0.27 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:08:32.973690: step 226450, loss = 0.34 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:08:33.827418: step 226460, loss = 0.28 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:08:34.667354: step 226470, loss = 0.26 (1523.9 examples/sec; 0.084 sec/batch)
2017-06-02 08:08:35.543111: step 226480, loss = 0.20 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:08:36.410676: step 226490, loss = 0.27 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:08:37.415314: step 226500, loss = 0.30 (1274.1 examples/sec; 0.100 sec/batch)
2017-06-02 08:08:38.167405: step 226510, loss = 0.22 (1701.9 examples/sec; 0.075 sec/batch)
2017-06-02 08:08:39.037947: step 226520, loss = 0.27 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:08:39.900345: step 226530, loss = 0.35 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:08:40.758549: step 226540, loss = 0.40 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:08:41.642759: step 226550, loss = 0.29 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:08:42.516378: step 226560, loss = 0.29 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:08:43.387574: step 226570, loss = 0.29 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:08:44.295131: step 226580, loss = 0.25 (1410.4 examples/sec; 0.091 sec/batch)
2017-06-02 08:08:45.179856: step 226590, loss = 0.23 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:08:46.141164: step 226600, loss = 0.27 (1331.5 examples/sec; 0.096 sec/batch)
2017-06-02 08:08:46.926554: step 226610, loss = 0.25 (1629.8 examples/sec; 0.079 sec/batch)
2017-06-02 08:08:47.785663: step 226620, loss = 0.26 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:08:48.672401: step 226630, loss = 0.28 (1443.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:08:49.553469: step 226640, loss = 0.26 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:08:50.439198: step 226650, loss = 0.19 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:08:51.311150: step 226660, loss = 0.29 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:08:52.189138: step 226670, loss = 0.26 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:08:53.054167: step 226680, loss = 0.29 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:08:53.950280: step 226690, loss = 0.30 (1428.4 examples/sec; 0.090 sec/batch)
2017-06-02 08:08:54.928413: step 226700, loss = 0.23 (1308.6 examples/sec; 0.098 sec/batch)
2017-06-02 08:08:55.705998: step 226710, loss = 0.31 (1646.1 examples/sec; 0.078 sec/batch)
2017-06-02 08:08:56.565945: step 226720, loss = 0.30 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:08:57.425951: step 226730, loss = 0.47 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:08:58.302266: step 226740, loss = 0.34 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:08:59.145640: step 226750, loss = 0.33 (1517.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:09:00.029500: step 226760, loss = 0.29 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:09:00.906635: step 226770, loss = 0.33 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:09:01.806722: step 226780, loss = 0.27 (1422.1 examples/sec; 0.090 sec/batch)
2017-06-02 08:09:02.683383: step 226790, loss = 0.29 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:09:03.648428: step 226800, loss = 0.28 (1326.4 examples/sec; 0.097 sec/batch)
2017-06-02 08:09:04.409375: step 226810, loss = 0.26 (1682.1 examples/sec; 0.076 sec/batch)
2017-06-02 08:09:05.264713: step 226820, loss = 0.36 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:09:06.151875: step 226830, loss = 0.32 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:09:07.005532: step 226840, loss = 0.26 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:09:07.868577: step 226850, loss = 0.25 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:09:08.745038: step 226860, loss = 0.27 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:09:09.611946: step 226870, loss = 0.35 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:09:10.453010: step 226880, loss = 0.23 (1521.9 examples/sec; 0.084 sec/batch)
2017-06-02 08:09:11.321260: step 226890, loss = 0.33 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:09:12.294273: step 226900, loss = 0.32 (1315.5 examples/sec; 0.097 sec/batch)
2017-06-02 08:09:13.072252: step 226910, loss = 0.30 (1645.3 examples/sec; 0.078 sec/batch)
2017-06-02 08:09:13.942913: step 226920, loss = 0.32 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:09:14.801274: step 226930, loss = 0.32 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:09:15.649226: step 226940, loss = 0.25 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:09:16.512803: step 226950, loss = 0.23 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:09:17.382952: step 226960, loss = 0.25 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:09:18.237871: step 226970, loss = 0.32 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:09:19.082256: step 226980, loss = 0.38 (1515.9 examples/sec; 0.084 sec/batch)
2017-06-02 08:09:19.946054: step 226990, loss = 0.29 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:09:20.911728: step 227000, loss = 0.26 (1325.5 examples/sec; 0.097 sec/batch)
2017-06-02 08:09:21.682444: step 227010, loss = 0.29 (1660.8 examples/sec; 0.077 sec/batch)
2017-06-02 08:09:22.553419: step 227020, loss = 0.34 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:09:23.418470: step 227030, loss = 0.34 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:09:24.262684: step 227040, loss = 0.23 (1516.2 examples/sec; 0.084 sec/batch)
2017-06-02 08:09:25.104559: step 227050, loss = 0.25 (1520.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:09:25.960139: step 227060, loss = 0.30 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:09:26.836849: step 227070, loss = 0.31 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:09:27.698339: step 227080, loss = 0.27 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:09:28.563285: step 227090, loss = 0.20 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:09:29.537740: step 227100, loss = 0.28 (1313.6 examples/sec; 0.097 sec/batch)
2017-06-02 08:09:30.310846: step 227110, loss = 0.41 (1655.7 examples/sec; 0.077 sec/batch)
2017-06-02 08:09:31.161193: step 227120, loss = 0.40 (1505.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:09:32.010877: step 227130, loss = 0.27 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:09:32.872862: step 227140, loss = 0.37 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:09:33.745640: step 227150, loss = 0.33 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:09:34.630276: step 227160, loss = 0.22 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:09:35.517369: step 227170, loss = 0.36 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:09:36.373140: step 227180, loss = 0.25 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:09:37.214570: step 227190, loss = 0.28 (1521.2 examples/sec; 0.084 sec/batch)
2017-06-02 08:09:38.176344: step 227200, loss = 0.29 (1330.9 examples/sec; 0.096 sec/batch)
2017-06-02 08:09:38.953213: step 227210, loss = 0.22 (1647.6 examples/sec; 0.078 sec/batch)
2017-06-02 08:09:39.819832: step 227220, loss = 0.34 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:09:40.680510: step 227230, loss = 0.33 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:09:41.538673: step 227240, loss = 0.36 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:09:42.405805: step 227250, loss = 0.23 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:09:43.274611: step 227260, loss = 0.28 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:09:44.121609: step 227270, loss = 0.26 (1511.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:09:44.986321: step 227280, loss = 0.26 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:09:45.846438: step 227290, loss = 0.24 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:09:46.792479: step 227300, loss = 0.29 (1353.0 examples/sec; 0.095 sec/batch)
2017-06-02 08:09:47.557217: step 227310, loss = 0.27 (1673.8 examples/sec; 0.076 sec/batch)
2017-06-02 08:09:48.429350: step 227320, loss = 0.32 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:09:49.323167: step 227330, loss = 0.27 (1432.1 examples/sec; 0.089 sec/batch)
2017-06-02 08:09:50.154949: step 227340, loss = 0.31 (1538.9 examples/sec; 0.083 sec/batch)
2017-06-02 08:09:51.014835: step 227350, loss = 0.36 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:09:51.882776: step 227360, loss = 0.37 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:09:52.794697: step 227370, loss = 0.41 (1403.6 examples/sec; 0.091 sec/batch)
2017-06-02 08:09:53.658601: step 227380, loss = 0.27 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:09:54.534425: step 227390, loss = 0.29 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:09:55.500975: step 227400, loss = 0.35 (1324.3 examples/sec; 0.097 sec/batch)
2017-06-02 08:09:56.252640: step 227410, loss = 0.36 (1702.9 examples/sec; 0.075 sec/batch)
2017-06-02 08:09:57.136511: step 227420, loss = 0.28 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:09:57.981644: step 227430, loss = 0.22 (1514.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:09:58.846631: step 227440, loss = 0.33 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:09:59.741884: step 227450, loss = 0.24 (1429.8 examples/sec; 0.090 sec/batch)
2017-06-02 08:10:00.618430: step 227460, loss = 0.23 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:10:01.465212: step 227470, loss = 0.27 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:10:02.338786: step 227480, loss = 0.25 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:10:03.218660: step 227490, loss = 0.31 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:10:04.199792: step 227500, loss = 0.28 (1304.6 examples/sec; 0.098 sec/batch)
2017-06-02 08:10:04.986157: step 227510, loss = 0.35 (1627.7 examples/sec; 0.079 sec/batch)
2017-06-02 08:10:05.866372: step 227520, loss = 0.27 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:10:06.727467: step 227530, loss = 0.27 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:10:07.620652: step 227540, loss = 0.33 (1433.1 examples/sec; 0.089 sec/batch)
2017-06-02 08:10:08.509555: step 227550, loss = 0.25 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:10:09.404978: step 227560, loss = 0.25 (1429.5 examples/sec; 0.090 sec/batch)
2017-06-02 08:10:10.267521: step 227570, loss = 0.31 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:10:11.125083: step 227580, loss = 0.27 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:10:11.959553: step 227590, loss = 0.25 (1533.9 examples/sec; 0.083 sec/batch)
2017-06-02 08:10:12.954234: step 227600, loss = 0.27 (1286.8 examples/sec; 0.099 sec/batch)
2017-06-02 08:10:13.724726: step 227610, loss = 0.31 (1661.3 examples/sec; 0.077 sec/batch)
2017-06-02 08:10:14.585452: step 227620, loss = 0.25 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:10:15.458286: step 227630, loss = 0.32 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:10:16.350466: step 227640, loss = 0.26 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:10:17.221241: step 227650, loss = 0.28 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:10:18.108923: step 227660, loss = 0.25 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:10:18.948155: step 227670, loss = 0.32 (1525.2 examples/sec; 0.084 sec/batch)
2017-06-02 08:10:19.816452: step 227680, loss = 0.27 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:10:20.683427: step 227690, loss = 0.30 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:10:21.651899: step 227700, loss = 0.37 (1321.7 examples/sec; 0.097 sec/batch)
2017-06-02 08:10:22.390848: step 227710, loss = 0.30 (1732.2 examples/sec; 0.074 sec/batch)
2017-06-02 08:10:23.258507: step 227720, loss = 0.22 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:10:24.122837: step 227730, loss = 0.25 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:10:24.982671: step 227740, loss = 0.25 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:10:25.836555: step 227750, loss = 0.29 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:10:26.694895: step 227760, loss = 0.33 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:10:27.559887: step 227770, loss = 0.40 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:10:28.429913: step 227780, loss = 0.34 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:10:29.287417: step 227790, loss = 0.28 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:10:30.248993: step 227800, loss = 0.26 (1331.2 examples/sec; 0.096 sec/batch)
2017-06-02 08:10:30.994112: step 227810, loss = 0.23 (1717.8 examples/sec; 0.075 sec/batch)
2017-06-02 08:10:31.852713: step 227820, loss = 0.24 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:10:32.718447: step 227830, loss = 0.27 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:10:33.542971: step 227840, loss = 0.33 (1552.4 examples/sec; 0.082 sec/batch)
2017-06-02 08:10:34.421321: step 227850, loss = 0.29 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:10:35.291727: step 227860, loss = 0.32 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:10:36.136167: step 227870, loss = 0.37 (1515.8 examples/sec; 0.084 sec/batch)
2017-06-02 08:10:36.984115: step 227880, loss = 0.34 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:10:37.846127: step 227890, loss = 0.41 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:10:38.809579: step 227900, loss = 0.23 (1328.6 examples/sec; 0.096 sec/batch)
2017-06-02 08:10:39.586799: step 227910, loss = 0.23 (1646.9 examples/sec; 0.078 sec/batch)
2017-06-02 08:10:40.429046: step 227920, loss = 0.21 (1519.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:10:41.283638: step 227930, loss = 0.36 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:10:42.146803: step 227940, loss = 0.24 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:10:43.019146: step 227950, loss = 0.32 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:10:43.903209: step 227960, loss = 0.27 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:10:44.766460: step 227970, loss = 0.29 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:10:45.602293: step 227980, loss = 0.37 (1531.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:10:46.486230: step 227990, loss = 0.36 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:10:47.502495: step 228000, loss = 0.30 (1259.5 examples/sec; 0.102 sec/batch)
2017-06-02 08:10:48.231528: step 228010, loss = 0.27 (1755.8 examples/sec; 0.073 sec/batch)
2017-06-02 08:10:49.075315: step 228020, loss = 0.36 (1517.0 examples/sec; 0.084 sec/batch)
2017-06-02 08:10:49.934562: step 228030, loss = 0.34 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:10:50.806336: step 228040, loss = 0.33 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:10:51.700083: step 228050, loss = 0.31 (1432.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:10:52.568151: step 228060, loss = 0.25 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:10:53.436389: step 228070, loss = 0.24 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:10:54.306335: step 228080, loss = 0.25 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:10:55.182035: step 228090, loss = 0.22 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:10:56.175091: step 228100, loss = 0.28 (1288.9 examples/sec; 0.099 sec/batch)
2017-06-02 08:10:56.924027: step 228110, loss = 0.31 (1709.1 examples/sec; 0.075 sec/batch)
2017-06-02 08:10:57.784981: step 228120, loss = 0.20 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:10:58.655806: step 228130, loss = 0.32 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:10:59.536474: step 228140, loss = 0.31 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:11:00.401610: step 228150, loss = 0.31 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:11:01.261276: step 228160, loss = 0.36 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:11:02.126824: step 228170, loss = 0.22 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:11:02.981110: step 228180, loss = 0.32 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:11:03.859181: step 228190, loss = 0.30 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:11:04.807566: step 228200, loss = 0.33 (1349.7 examples/sec; 0.095 sec/batch)
2017-06-02 08:11:05.582266: step 228210, loss = 0.30 (1652.3 examples/sec; 0.077 sec/batch)
2017-06-02 08:11:06.467542: step 228220, loss = 0.29 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:11:07.325795: step 228230, loss = 0.32 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:11:08.189699: step 228240, loss = 0.37 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:11:09.050344: step 228250, loss = 0.27 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:11:09.890983: step 228260, loss = 0.27 (1522.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:11:10.761221: step 228270, loss = 0.23 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:11:11.625876: step 228280, loss = 0.40 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:11:12.489327: step 228290, loss = 0.32 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:11:13.446511: step 228300, loss = 0.28 (1337.2 examples/sec; 0.096 sec/batch)
2017-06-02 08:11:14.215054: step 228310, loss = 0.31 (1665.5 examples/sec; 0.077 sec/batch)
2017-06-02 08:11:15.064864: step 228320, loss = 0.28 (1506.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:11:15.922918: step 228330, loss = 0.35 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:11:16.764497: step 228340, loss = 0.30 (1521.0 examples/sec; 0.084 sec/batch)
2017-06-02 08:11:17.638954: step 228350, loss = 0.32 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:11:18.500961: step 228360, loss = 0.29 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:11:19.372285: step 228370, loss = 0.29 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:11:20.243479: step 228380, loss = 0.31 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:11:21.101314: step 228390, loss = 0.40 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:11:22.088471: step 228400, loss = 0.30 (1296.7 examples/sec; 0.099 sec/batch)
2017-06-02 08:11:22.841008: step 228410, loss = 0.32 (1701.0 examples/sec; 0.075 sec/batch)
2017-06-02 08:11:23.700563: step 228420, loss = 0.28 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:11:24.546492: step 228430, loss = 0.34 (1513.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:11:25.412351: step 228440, loss = 0.30 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:11:26.281580: step 228450, loss = 0.32 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:11:27.152894: step 228460, loss = 0.29 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:11:28.006616: step 228470, loss = 0.30 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:11:28.871889: step 228480, loss = 0.31 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:11:29.752520: step 228490, loss = 0.24 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:11:30.739962: step 228500, loss = 0.39 (1296.3 examples/sec; 0.099 sec/batch)
2017-06-02 08:11:31.497750: step 228510, loss = 0.32 (1689.1 examples/sec; 0.076 sec/batch)
2017-06-02 08:11:32.350027: step 228520, loss = 0.29 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:11:33.216294: step 228530, loss = 0.28 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:11:34.077090: step 228540, loss = 0.31 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:11:34.921577: step 228550, loss = 0.27 (1515.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:11:35.798499: step 228560, loss = 0.27 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:11:36.661914: step 228570, loss = 0.33 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:11:37.506758: step 228580, loss = 0.33 (1515.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:11:38.378231: step 228590, loss = 0.35 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:11:39.327048: step 228600, loss = 0.20 (1349.0 examples/sec; 0.095 sec/batch)
2017-06-02 08:11:40.086552: step 228610, loss = 0.24 (1685.3 examples/sec; 0.076 sec/batch)
2017-06-02 08:11:40.946907: step 228620, loss = 0.30 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:11:41.832584: step 228630, loss = 0.33 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:11:42.695757: step 228640, loss = 0.27 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:11:43.566980: step 228650, loss = 0.32 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:11:44.455966: step 228660, loss = 0.25 (1439.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:11:45.335497: step 228670, loss = 0.25 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:11:46.209568: step 228680, loss = 0.27 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:11:47.092485: step 228690, loss = 0.28 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:11:48.102369: step 228700, loss = 0.26 (1267.5 examples/sec; 0.101 sec/batch)
2017-06-02 08:11:48.852497: step 228710, loss = 0.24 (1706.4 examples/sec; 0.075 sec/batch)
2017-06-02 08:11:49.716130: step 228720, loss = 0.31 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:11:50.584322: step 228730, loss = 0.29 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:11:51.426136: step 228740, loss = 0.28 (1520.6 examples/sec; 0.084 sec/batch)
2017-06-02 08:11:52.289472: step 228750, loss = 0.24 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:11:53.168276: step 228760, loss = 0.18 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:11:54.042208: step 228770, loss = 0.28 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:11:54.904418: step 228780, loss = 0.35 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:11:55.764294: step 228790, loss = 0.29 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:11:56.709299: step 228800, loss = 0.29 (1354.5 examples/sec; 0.095 sec/batch)
2017-06-02 08:11:57.474182: step 228810, loss = 0.29 (1673.5 examples/sec; 0.076 sec/batch)
2017-06-02 08:11:58.331301: step 228820, loss = 0.25 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:11:59.189286: step 228830, loss = 0.30 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:12:00.037648: step 228840, loss = 0.25 (1508.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:12:00.877107: step 228850, loss = 0.33 (1524.8 examples/sec; 0.084 sec/batch)
2017-06-02 08:12:01.741299: step 228860, loss = 0.26 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:12:02.609241: step 228870, loss = 0.26 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:12:03.465417: step 228880, loss = 0.24 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:12:04.320380: step 228890, loss = 0.33 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:12:05.267427: step 228900, loss = 0.24 (1351.6 examples/sec; 0.095 sec/batch)
2017-06-02 08:12:06.036573: step 228910, loss = 0.42 (1664.2 examples/sec; 0.077 sec/batch)
2017-06-02 08:12:06.903297: step 228920, loss = 0.29 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:12:07.743417: step 228930, loss = 0.28 (1523.6 examples/sec; 0.084 sec/batch)
2017-06-02 08:12:08.592884: step 228940, loss = 0.29 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:12:09.449905: step 228950, loss = 0.37 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:12:10.280712: step 228960, loss = 0.27 (1540.7 examples/sec; 0.083 sec/batch)
2017-06-02 08:12:11.136339: step 228970, loss = 0.25 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:12:11.997025: step 228980, loss = 0.32 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:12:12.851140: step 228990, loss = 0.29 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:12:13.843663: step 229000, loss = 0.23 (1289.6 examples/sec; 0.099 sec/batch)
2017-06-02 08:12:14.578863: step 229010, loss = 0.26 (1741.1 examples/sec; 0.074 sec/batch)
2017-06-02 08:12:15.446624: step 229020, loss = 0.27 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:12:16.296060: step 229030, loss = 0.28 (1506.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:12:17.168119: step 229040, loss = 0.28 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:12:18.035386: step 229050, loss = 0.26 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:12:18.873746: step 229060, loss = 0.28 (1526.8 examples/sec; 0.084 sec/batch)
2017-06-02 08:12:19.720291: step 229070, loss = 0.34 (1512.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:12:20.557822: step 229080, loss = 0.34 (1528.3 examples/sec; 0.084 sec/batch)
2017-06-02 08:12:21.411722: step 229090, loss = 0.30 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:12:22.373555: step 229100, loss = 0.27 (1330.8 examples/sec; 0.096 sec/batch)
2017-06-02 08:12:23.137651: step 229110, loss = 0.26 (1675.2 examples/sec; 0.076 sec/batch)
2017-06-02 08:12:23.994070: step 229120, loss = 0.32 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:12:24.864569: step 229130, loss = 0.36 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:12:25.727154: step 229140, loss = 0.31 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:12:26.591537: step 229150, loss = 0.34 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:12:27.432947: step 229160, loss = 0.33 (1521.3 examples/sec; 0.084 sec/batch)
2017-06-02 08:12:28.282624: step 229170, loss = 0.33 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:12:29.146896: step 229180, loss = 0.23 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:12:30.023386: step 229190, loss = 0.30 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:12:30.973562: step 229200, loss = 0.39 (1347.1 examples/sec; 0.095 sec/batch)
2017-06-02 08:12:31.737943: step 229210, loss = 0.23 (1674.6 examples/sec; 0.076 sec/batch)
2017-06-02 08:12:32.607922: step 229220, loss = 0.32 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:12:33.482439: step 229230, loss = 0.25 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:12:34.351445: step 229240, loss = 0.28 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:12:35.210451: step 229250, loss = 0.26 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:12:36.087618: step 229260, loss = 0.25 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:12:36.936470: step 229270, loss = 0.25 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:12:37.793537: step 229280, loss = 0.27 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:12:38.667611: step 229290, loss = 0.35 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:12:39.632745: step 229300, loss = 0.32 (1326.2 examples/sec; 0.097 sec/batch)
2017-06-02 08:12:40.427057: step 229310, loss = 0.31 (1611.5 examples/sec; 0.079 sec/batch)
2017-06-02 08:12:41.298309: step 229320, loss = 0.29 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:12:42.163910: step 229330, loss = 0.35 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:12:43.053307: step 229340, loss = 0.27 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:12:43.939858: step 229350, loss = 0.23 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:12:44.825199: step 229360, loss = 0.29 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:12:45.711311: step 229370, loss = 0.35 (1444.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:12:46.590298: step 229380, loss = 0.28 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:12:47.451831: step 229390, loss = 0.25 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:12:48.406770: step 229400, loss = 0.31 (1340.4 examples/sec; 0.095 sec/batch)
2017-06-02 08:12:49.156816: step 229410, loss = 0.29 (1706.5 examples/sec; 0.075 sec/batch)
2017-06-02 08:12:50.021114: step 229420, loss = 0.36 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:12:50.889142: step 229430, loss = 0.30 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:12:51.757405: step 229440, loss = 0.22 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:12:52.605284: step 229450, loss = 0.25 (1509.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:12:53.488597: step 229460, loss = 0.20 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:12:54.358706: step 229470, loss = 0.31 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:12:55.186998: step 229480, loss = 0.25 (1545.3 examples/sec; 0.083 sec/batch)
2017-06-02 08:12:56.048592: step 229490, loss = 0.37 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:12:57.034131: step 229500, loss = 0.29 (1298.8 examples/sec; 0.099 sec/batch)
2017-06-02 08:12:57.835114: step 229510, loss = 0.31 (1598.0 examples/sec; 0.080 sec/batch)
2017-06-02 08:12:58.682961: step 229520, loss = 0.29 (1509.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:12:59.522386: step 229530, loss = 0.28 (1524.9 examples/sec; 0.084 sec/batch)
2017-06-02 08:13:00.396238: step 229540, loss = 0.26 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:13:01.292949: step 229550, loss = 0.31 (1427.5 examples/sec; 0.090 sec/batch)
2017-06-02 08:13:02.159476: step 229560, loss = 0.40 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:13:02.998556: step 229570, loss = 0.31 (1525.5 examples/sec; 0.084 sec/batch)
2017-06-02 08:13:03.862144: step 229580, loss = 0.25 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:13:04.705615: step 229590, loss = 0.29 (1517.5 examples/sec; 0.084 sec/batch)
2017-06-02 08:13:05.708187: step 229600, loss = 0.33 (1276.7 examples/sec; 0.100 sec/batch)
2017-06-02 08:13:06.421854: step 229610, loss = 0.26 (1793.6 examples/sec; 0.071 sec/batch)
2017-06-02 08:13:07.265447: step 229620, loss = 0.29 (1517.3 examples/sec; 0.084 sec/batch)
2017-06-02 08:13:08.183427: step 229630, loss = 0.43 (1394.4 examples/sec; 0.092 sec/batch)
2017-06-02 08:13:09.042600: step 229640, loss = 0.25 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:13:09.901518: step 229650, loss = 0.25 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:13:10.734470: step 229660, loss = 0.27 (1536.7 examples/sec; 0.083 sec/batch)
2017-06-02 08:13:11.639228: step 229670, loss = 0.28 (1414.8 examples/sec; 0.090 sec/batch)
2017-06-02 08:13:12.489916: step 229680, loss = 0.25 (1504.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:13:13.351506: step 229690, loss = 0.39 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:13:14.341243: step 229700, loss = 0.29 (1293.3 examples/sec; 0.099 sec/batch)
2017-06-02 08:13:15.063271: step 229710, loss = 0.25 (1772.8 examples/sec; 0.072 sec/batch)
2017-06-02 08:13:15.914985: step 229720, loss = 0.27 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:13:16.798670: step 229730, loss = 0.27 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:13:17.641708: step 229740, loss = 0.21 (1518.3 examples/sec; 0.084 sec/batch)
2017-06-02 08:13:18.495490: step 229750, loss = 0.26 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:13:19.321860: step 229760, loss = 0.36 (1549.0 examples/sec; 0.083 sec/batch)
2017-06-02 08:13:20.168862: step 229770, loss = 0.34 (1511.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:13:21.030083: step 229780, loss = 0.31 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:13:21.889832: step 229790, loss = 0.30 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:13:22.839816: step 229800, loss = 0.31 (1347.4 examples/sec; 0.095 sec/batch)
2017-06-02 08:13:23.604559: step 229810, loss = 0.29 (1673.8 examples/sec; 0.076 sec/batch)
2017-06-02 08:13:24.446139: step 229820, loss = 0.35 (1520.9 examples/sec; 0.084 sec/batch)
2017-06-02 08:13:25.300598: step 229830, loss = 0.24 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:13:26.168539: step 229840, loss = 0.27 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:13:27.014847: step 229850, loss = 0.32 (1512.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:13:27.880173: step 229860, loss = 0.26 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:13:28.748499: step 229870, loss = 0.21 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:13:29.611570: step 229880, loss = 0.36 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:13:30.484609: step 229890, loss = 0.33 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:13:31.450175: step 229900, loss = 0.31 (1325.6 examples/sec; 0.097 sec/batch)
2017-06-02 08:13:32.210626: step 229910, loss = 0.21 (1683.2 examples/sec; 0.076 sec/batch)
2017-06-02 08:13:33.068474: step 229920, loss = 0.28 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:13:33.912160: step 229930, loss = 0.30 (1517.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:13:34.792004: step 229940, loss = 0.27 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:13:35.648975: step 229950, loss = 0.22 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:13:36.494569: step 229960, loss = 0.31 (1513.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:13:37.362695: step 229970, loss = 0.28 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:13:38.239107: step 229980, loss = 0.26 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:13:39.078662: step 229990, loss = 0.37 (1524.6 examples/sec; 0.084 sec/batch)
2017-06-02 08:13:40.078503: step 230000, loss = 0.24 (1280.2 examples/sec; 0.100 sec/batch)
2017-06-02 08:13:40.845242: step 230010, loss = 0.25 (1669.4 examples/sec; 0.077 sec/batch)
2017-06-02 08:13:41.673894: step 230020, loss = 0.33 (1544.7 examples/sec; 0.083 sec/batch)
2017-06-02 08:13:42.489555: step 230030, loss = 0.51 (1569.3 examples/sec; 0.082 sec/batch)
2017-06-02 08:13:43.361255: step 230040, loss = 0.36 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:13:44.218490: step 230050, loss = 0.21 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:13:45.093769: step 230060, loss = 0.33 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:13:45.958442: step 230070, loss = 0.28 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:13:46.820833: step 230080, loss = 0.33 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:13:47.669130: step 230090, loss = 0.25 (1508.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:13:48.652021: step 230100, loss = 0.29 (1302.3 examples/sec; 0.098 sec/batch)
2017-06-02 08:13:49.426830: step 230110, loss = 0.26 (1652.0 examples/sec; 0.077 sec/batch)
2017-06-02 08:13:50.297300: step 230120, loss = 0.28 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:13:51.143188: step 230130, loss = 0.30 (1513.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:13:52.004647: step 230140, loss = 0.34 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:13:52.852096: step 230150, loss = 0.33 (1510.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:13:53.723936: step 230160, loss = 0.31 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:13:54.605574: step 230170, loss = 0.27 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:13:55.467085: step 230180, loss = 0.24 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:13:56.330278: step 230190, loss = 0.23 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:13:57.272028: step 230200, loss = 0.32 (1359.2 examples/sec; 0.094 sec/batch)
2017-06-02 08:13:58.040201: step 230210, loss = 0.33 (1666.3 examples/sec; 0.077 sec/batch)
2017-06-02 08:13:58.916468: step 230220, loss = 0.26 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:13:59.768898: step 230230, loss = 0.32 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:14:00.646019: step 230240, loss = 0.31 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:14:01.520434: step 230250, loss = 0.26 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:14:02.374166: step 230260, loss = 0.41 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:14:03.236371: step 230270, loss = 0.26 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:14:04.117065: step 230280, loss = 0.30 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:14:04.966308: step 230290, loss = 0.23 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:14:05.910477: step 230300, loss = 0.19 (1355.7 examples/sec; 0.094 sec/batch)
2017-06-02 08:14:06.693440: step 230310, loss = 0.37 (1634.8 examples/sec; 0.078 sec/batch)
2017-06-02 08:14:07.567457: step 230320, loss = 0.36 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:14:08.426984: step 230330, loss = 0.29 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:14:09.302720: step 230340, loss = 0.27 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:14:10.157624: step 230350, loss = 0.29 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:14:11.034211: step 230360, loss = 0.36 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:14:11.879189: step 230370, loss = 0.20 (1514.9 examples/sec; 0.084 sec/batch)
2017-06-02 08:14:12.737415: step 230380, loss = 0.37 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:14:13.586637: step 230390, loss = 0.24 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:14:14.564897: step 230400, loss = 0.25 (1308.4 examples/sec; 0.098 sec/batch)
2017-06-02 08:14:15.345780: step 230410, loss = 0.36 (1639.2 examples/sec; 0.078 sec/batch)
2017-06-02 08:14:16.211489: step 230420, loss = 0.31 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:14:17.062265: step 230430, loss = 0.26 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:14:17.923013: step 230440, loss = 0.25 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:14:18.791101: step 230450, loss = 0.25 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:14:19.668259: step 230460, loss = 0.24 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:14:20.509439: step 230470, loss = 0.29 (1521.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:14:21.373228: step 230480, loss = 0.32 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:14:22.253880: step 230490, loss = 0.24 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:14:23.203363: step 230500, loss = 0.41 (1348.1 examples/sec; 0.095 sec/batch)
2017-06-02 08:14:23.961502: step 230510, loss = 0.27 (1688.3 examples/sec; 0.076 sec/batch)
2017-06-02 08:14:24.795350: step 230520, loss = 0.24 (1535.0 examples/sec; 0.083 sec/batch)
2017-06-02 08:14:25.638328: step 230530, loss = 0.25 (1518.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:14:26.508799: step 230540, loss = 0.31 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:14:27.338099: step 230550, loss = 0.32 (1543.5 examples/sec; 0.083 sec/batch)
2017-06-02 08:14:28.191776: step 230560, loss = 0.35 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:14:29.053698: step 230570, loss = 0.35 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:14:29.936958: step 230580, loss = 0.35 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:14:30.824911: step 230590, loss = 0.32 (1441.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:14:31.817799: step 230600, loss = 0.37 (1289.2 examples/sec; 0.099 sec/batch)
2017-06-02 08:14:32.585070: step 230610, loss = 0.37 (1668.2 examples/sec; 0.077 sec/batch)
2017-06-02 08:14:33.459608: step 230620, loss = 0.23 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:14:34.340638: step 230630, loss = 0.27 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:14:35.177628: step 230640, loss = 0.27 (1529.3 examples/sec; 0.084 sec/batch)
2017-06-02 08:14:36.068440: step 230650, loss = 0.25 (1436.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:14:36.942408: step 230660, loss = 0.27 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:14:37.828388: step 230670, loss = 0.30 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:14:38.708605: step 230680, loss = 0.23 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:14:39.559672: step 230690, loss = 0.34 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:14:40.529500: step 230700, loss = 0.21 (1319.8 examples/sec; 0.097 sec/batch)
2017-06-02 08:14:41.292444: step 230710, loss = 0.27 (1677.7 examples/sec; 0.076 sec/batch)
2017-06-02 08:14:42.144485: step 230720, loss = 0.31 (1502.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:14:42.993825: step 230730, loss = 0.41 (1507.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:14:43.849618: step 230740, loss = 0.27 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:14:44.705184: step 230750, loss = 0.22 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:14:45.572859: step 230760, loss = 0.33 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:14:46.429614: step 230770, loss = 0.26 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:14:47.292999: step 230780, loss = 0.25 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:14:48.156913: step 230790, loss = 0.23 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:14:49.138264: step 230800, loss = 0.28 (1304.3 examples/sec; 0.098 sec/batch)
2017-06-02 08:14:49.883261: step 230810, loss = 0.21 (1718.1 examples/sec; 0.074 sec/batch)
2017-06-02 08:14:50.753277: step 230820, loss = 0.31 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:14:51.608342: step 230830, loss = 0.31 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:14:52.437055: step 230840, loss = 0.36 (1544.6 examples/sec; 0.083 sec/batch)
2017-06-02 08:14:53.295451: step 230850, loss = 0.21 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:14:54.178989: step 230860, loss = 0.29 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:14:55.066028: step 230870, loss = 0.33 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:14:55.911430: step 230880, loss = 0.28 (1514.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:14:56.803551: step 230890, loss = 0.28 (1434.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:14:57.768259: step 230900, loss = 0.35 (1326.8 examples/sec; 0.096 sec/batch)
2017-06-02 08:14:58.544130: step 230910, loss = 0.27 (1649.7 examples/sec; 0.078 sec/batch)
2017-06-02 08:14:59.406171: step 230920, loss = 0.32 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:15:00.269945: step 230930, loss = 0.22 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:15:01.164416: step 230940, loss = 0.33 (1431.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:15:02.054136: step 230950, loss = 0.33 (1438.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:15:02.935489: step 230960, loss = 0.39 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:15:03.799563: step 230970, loss = 0.24 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:15:04.676229: step 230980, loss = 0.31 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:15:05.556054: step 230990, loss = 0.23 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:15:06.508830: step 231000, loss = 0.28 (1343.4 examples/sec; 0.095 sec/batch)
2017-06-02 08:15:07.296927: step 231010, loss = 0.22 (1624.2 examples/sec; 0.079 sec/batch)
2017-06-02 08:15:08.167538: step 231020, loss = 0.33 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:15:09.047727: step 231030, loss = 0.31 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:15:09.907017: step 231040, loss = 0.27 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:15:10.785305: step 231050, loss = 0.28 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:15:11.663198: step 231060, loss = 0.22 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:15:12.512419: step 231070, loss = 0.23 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:15:13.369883: step 231080, loss = 0.32 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:15:14.250458: step 231090, loss = 0.37 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:15:15.223282: step 231100, loss = 0.24 (1315.7 examples/sec; 0.097 sec/batch)
2017-06-02 08:15:15.957804: step 231110, loss = 0.25 (1742.6 examples/sec; 0.073 sec/batch)
2017-06-02 08:15:16.840198: step 231120, loss = 0.32 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:15:17.721218: step 231130, loss = 0.35 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:15:18.601576: step 231140, loss = 0.27 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:15:19.495563: step 231150, loss = 0.30 (1431.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:15:20.361661: step 231160, loss = 0.30 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:15:21.229933: step 231170, loss = 0.26 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:15:22.119299: step 231180, loss = 0.28 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:15:23.020101: step 231190, loss = 0.29 (1421.0 examples/sec; 0.090 sec/batch)
2017-06-02 08:15:23.975043: step 231200, loss = 0.35 (1340.4 examples/sec; 0.095 sec/batch)
2017-06-02 08:15:24.722508: step 231210, loss = 0.20 (1712.5 examples/sec; 0.075 sec/batch)
2017-06-02 08:15:25.593654: step 231220, loss = 0.32 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:15:26.466410: step 231230, loss = 0.24 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:15:27.341376: step 231240, loss = 0.31 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:15:28.228103: step 231250, loss = 0.29 (1443.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:15:29.101035: step 231260, loss = 0.30 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:15:29.979722: step 231270, loss = 0.27 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:15:30.854689: step 231280, loss = 0.33 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:15:31.738390: step 231290, loss = 0.38 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:15:32.720945: step 231300, loss = 0.36 (1302.7 examples/sec; 0.098 sec/batch)
2017-06-02 08:15:33.463090: step 231310, loss = 0.26 (1724.7 examples/sec; 0.074 sec/batch)
2017-06-02 08:15:34.328690: step 231320, loss = 0.27 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:15:35.209611: step 231330, loss = 0.24 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:15:36.053542: step 231340, loss = 0.31 (1516.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:15:36.923767: step 231350, loss = 0.30 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:15:37.786550: step 231360, loss = 0.29 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:15:38.612797: step 231370, loss = 0.28 (1549.2 examples/sec; 0.083 sec/batch)
2017-06-02 08:15:39.452971: step 231380, loss = 0.37 (1523.5 examples/sec; 0.084 sec/batch)
2017-06-02 08:15:40.307016: step 231390, loss = 0.24 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:15:41.276883: step 231400, loss = 0.31 (1319.8 examples/sec; 0.097 sec/batch)
2017-06-02 08:15:42.036099: step 231410, loss = 0.29 (1686.0 examples/sec; 0.076 sec/batch)
2017-06-02 08:15:42.914253: step 231420, loss = 0.27 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:15:43.768611: step 231430, loss = 0.32 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:15:44.645555: step 231440, loss = 0.25 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:15:45.489172: step 231450, loss = 0.30 (1517.3 examples/sec; 0.084 sec/batch)
2017-06-02 08:15:46.335461: step 231460, loss = 0.29 (1512.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:15:47.209660: step 231470, loss = 0.25 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:15:48.068635: step 231480, loss = 0.21 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:15:48.937882: step 231490, loss = 0.29 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:15:49.886752: step 231500, loss = 0.28 (1349.0 examples/sec; 0.095 sec/batch)
2017-06-02 08:15:50.641930: step 231510, loss = 0.28 (1695.0 examples/sec; 0.076 sec/batch)
2017-06-02 08:15:51.504334: step 231520, loss = 0.36 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:15:52.362803: step 231530, loss = 0.30 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:15:53.230084: step 231540, loss = 0.24 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:15:54.080147: step 231550, loss = 0.32 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:15:54.952303: step 231560, loss = 0.30 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:15:55.805226: step 231570, loss = 0.32 (1500.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:15:56.659616: step 231580, loss = 0.48 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:15:57.485326: step 231590, loss = 0.29 (1550.2 examples/sec; 0.083 sec/batch)
2017-06-02 08:15:58.452931: step 231600, loss = 0.36 (1322.8 examples/sec; 0.097 sec/batch)
2017-06-02 08:15:59.205386: step 231610, loss = 0.26 (1701.1 examples/sec; 0.075 sec/batch)
2017-06-02 08:16:00.056377: step 231620, loss = 0.25 (1504.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:16:00.912202: step 231630, loss = 0.29 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:16:01.772606: step 231640, loss = 0.29 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:16:02.612985: step 231650, loss = 0.34 (1523.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:16:03.483773: step 231660, loss = 0.25 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:16:04.347903: step 231670, loss = 0.23 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:16:05.200062: step 231680, loss = 0.23 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:16:06.075197: step 231690, loss = 0.26 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:16:07.036907: step 231700, loss = 0.29 (1331.0 examples/sec; 0.096 sec/batch)
2017-06-02 08:16:07.809310: step 231710, loss = 0.29 (1657.2 examples/sec; 0.077 sec/batch)
2017-06-02 08:16:08.670237: step 231720, loss = 0.34 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:16:09.520762: step 231730, loss = 0.26 (1505.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:16:10.413582: step 231740, loss = 0.28 (1433.6 examples/sec; 0.089 sec/batch)
2017-06-02 08:16:11.244089: step 231750, loss = 0.35 (1541.2 examples/sec; 0.083 sec/batch)
2017-06-02 08:16:12.116790: step 231760, loss = 0.28 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:16:12.996074: step 231770, loss = 0.25 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:16:13.870311: step 231780, loss = 0.36 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:16:14.745703: step 231790, loss = 0.31 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:16:15.709149: step 231800, loss = 0.32 (1328.5 examples/sec; 0.096 sec/batch)
2017-06-02 08:16:16.496674: step 231810, loss = 0.28 (1625.3 examples/sec; 0.079 sec/batch)
2017-06-02 08:16:17.382115: step 231820, loss = 0.30 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 08:16:18.238011: step 231830, loss = 0.24 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:16:19.114775: step 231840, loss = 0.33 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:16:19.986448: step 231850, loss = 0.35 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:16:20.845009: step 231860, loss = 0.38 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:16:21.720713: step 231870, loss = 0.31 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:16:22.564411: step 231880, loss = 0.37 (1517.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:16:23.411932: step 231890, loss = 0.27 (1510.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:16:24.426463: step 231900, loss = 0.32 (1261.6 examples/sec; 0.101 sec/batch)
2017-06-02 08:16:25.164125: step 231910, loss = 0.35 (1735.2 examples/sec; 0.074 sec/batch)
2017-06-02 08:16:26.010774: step 231920, loss = 0.31 (1511.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:16:26.870191: step 231930, loss = 0.25 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:16:27.745594: step 231940, loss = 0.30 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:16:28.632218: step 231950, loss = 0.25 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:16:29.493466: step 231960, loss = 0.38 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:16:30.366649: step 231970, loss = 0.29 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:16:31.220657: step 231980, loss = 0.28 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:16:32.121068: step 231990, loss = 0.33 (1421.6 examples/sec; 0.090 sec/batch)
2017-06-02 08:16:33.057241: step 232000, loss = 0.33 (1367.3 examples/sec; 0.094 sec/batch)
2017-06-02 08:16:33.848761: step 232010, loss = 0.29 (1617.1 examples/sec; 0.079 sec/batch)
2017-06-02 08:16:34.723954: step 232020, loss = 0.22 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:16:35.568861: step 232030, loss = 0.46 (1515.0 examples/sec; 0.084 sec/batch)
2017-06-02 08:16:36.416091: step 232040, loss = 0.25 (1510.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:16:37.262875: step 232050, loss = 0.26 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:16:38.151611: step 232060, loss = 0.26 (1440.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:16:39.017517: step 232070, loss = 0.25 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:16:39.920419: step 232080, loss = 0.30 (1417.6 examples/sec; 0.090 sec/batch)
2017-06-02 08:16:40.798496: step 232090, loss = 0.29 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:16:41.770190: step 232100, loss = 0.29 (1317.3 examples/sec; 0.097 sec/batch)
2017-06-02 08:16:42.529560: step 232110, loss = 0.24 (1685.6 examples/sec; 0.076 sec/batch)
2017-06-02 08:16:43.390249: step 232120, loss = 0.28 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:16:44.250659: step 232130, loss = 0.35 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:16:45.106619: step 232140, loss = 0.29 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:16:45.927874: step 232150, loss = 0.26 (1558.6 examples/sec; 0.082 sec/batch)
2017-06-02 08:16:46.814052: step 232160, loss = 0.25 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 08:16:47.645815: step 232170, loss = 0.31 (1538.9 examples/sec; 0.083 sec/batch)
2017-06-02 08:16:48.527886: step 232180, loss = 0.39 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:16:49.383154: step 232190, loss = 0.40 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:16:50.365878: step 232200, loss = 0.35 (1302.5 examples/sec; 0.098 sec/batch)
2017-06-02 08:16:51.109832: step 232210, loss = 0.27 (1720.5 examples/sec; 0.074 sec/batch)
2017-06-02 08:16:51.978612: step 232220, loss = 0.29 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:16:52.857382: step 232230, loss = 0.31 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:16:53.760839: step 232240, loss = 0.26 (1416.8 examples/sec; 0.090 sec/batch)
2017-06-02 08:16:54.633665: step 232250, loss = 0.29 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:16:55.472640: step 232260, loss = 0.25 (1525.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:16:56.351713: step 232270, loss = 0.28 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:16:57.215466: step 232280, loss = 0.26 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:16:58.051630: step 232290, loss = 0.28 (1530.8 examples/sec; 0.084 sec/batch)
2017-06-02 08:16:59.014669: step 232300, loss = 0.23 (1329.1 examples/sec; 0.096 sec/batch)
2017-06-02 08:16:59.774219: step 232310, loss = 0.29 (1685.2 examples/sec; 0.076 sec/batch)
2017-06-02 08:17:00.650681: step 232320, loss = 0.39 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:17:01.502132: step 232330, loss = 0.25 (1503.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:17:02.368170: step 232340, loss = 0.34 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:17:03.229623: step 232350, loss = 0.23 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:17:04.105657: step 232360, loss = 0.31 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:17:04.953516: step 232370, loss = 0.28 (1509.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:17:05.805290: step 232380, loss = 0.31 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:17:06.644193: step 232390, loss = 0.25 (1525.8 examples/sec; 0.084 sec/batch)
2017-06-02 08:17:07.605707: step 232400, loss = 0.28 (1331.2 examples/sec; 0.096 sec/batch)
2017-06-02 08:17:08.395135: step 232410, loss = 0.31 (1621.4 examples/sec; 0.079 sec/batch)
2017-06-02 08:17:09.244498: step 232420, loss = 0.37 (1507.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:17:10.107469: step 232430, loss = 0.29 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:17:10.934785: step 232440, loss = 0.28 (1547.2 examples/sec; 0.083 sec/batch)
2017-06-02 08:17:11.787092: step 232450, loss = 0.33 (1501.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:17:12.646957: step 232460, loss = 0.24 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:17:13.520612: step 232470, loss = 0.24 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:17:14.374392: step 232480, loss = 0.24 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:17:15.228802: step 232490, loss = 0.29 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:17:16.180210: step 232500, loss = 0.32 (1345.4 examples/sec; 0.095 sec/batch)
2017-06-02 08:17:16.942257: step 232510, loss = 0.34 (1679.7 examples/sec; 0.076 sec/batch)
2017-06-02 08:17:17.804100: step 232520, loss = 0.32 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:17:18.641970: step 232530, loss = 0.25 (1527.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:17:19.502833: step 232540, loss = 0.29 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:17:20.373634: step 232550, loss = 0.27 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:17:21.250290: step 232560, loss = 0.26 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:17:22.095192: step 232570, loss = 0.22 (1514.9 examples/sec; 0.084 sec/batch)
2017-06-02 08:17:22.947988: step 232580, loss = 0.27 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:17:23.820049: step 232590, loss = 0.27 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:17:24.836639: step 232600, loss = 0.28 (1259.1 examples/sec; 0.102 sec/batch)
2017-06-02 08:17:25.559586: step 232610, loss = 0.27 (1770.5 examples/sec; 0.072 sec/batch)
2017-06-02 08:17:26.430355: step 232620, loss = 0.34 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:17:27.291306: step 232630, loss = 0.35 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:17:28.142966: step 232640, loss = 0.34 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:17:29.003332: step 232650, loss = 0.36 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:17:29.871507: step 232660, loss = 0.39 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:17:30.739280: step 232670, loss = 0.32 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:17:31.592698: step 232680, loss = 0.23 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:17:32.443710: step 232690, loss = 0.24 (1504.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:17:33.418342: step 232700, loss = 0.22 (1313.3 examples/sec; 0.097 sec/batch)
2017-06-02 08:17:34.182867: step 232710, loss = 0.23 (1674.2 examples/sec; 0.076 sec/batch)
2017-06-02 08:17:35.075163: step 232720, loss = 0.29 (1434.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:17:35.933878: step 232730, loss = 0.28 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:17:36.777802: step 232740, loss = 0.33 (1516.8 examples/sec; 0.084 sec/batch)
2017-06-02 08:17:37.659117: step 232750, loss = 0.35 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:17:38.541395: step 232760, loss = 0.31 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:17:39.401497: step 232770, loss = 0.27 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:17:40.249079: step 232780, loss = 0.24 (1510.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:17:41.115941: step 232790, loss = 0.31 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:17:42.075146: step 232800, loss = 0.24 (1334.4 examples/sec; 0.096 sec/batch)
2017-06-02 08:17:42.867096: step 232810, loss = 0.29 (1616.3 examples/sec; 0.079 sec/batch)
2017-06-02 08:17:43.731360: step 232820, loss = 0.30 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:17:44.590746: step 232830, loss = 0.27 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:17:45.483006: step 232840, loss = 0.21 (1434.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:17:46.334629: step 232850, loss = 0.26 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:17:47.202864: step 232860, loss = 0.31 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:17:48.062200: step 232870, loss = 0.27 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:17:48.923389: step 232880, loss = 0.31 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:17:49.788962: step 232890, loss = 0.24 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:17:50.774636: step 232900, loss = 0.29 (1298.6 examples/sec; 0.099 sec/batch)
2017-06-02 08:17:51.532662: step 232910, loss = 0.35 (1688.6 examples/sec; 0.076 sec/batch)
2017-06-02 08:17:52.411557: step 232920, loss = 0.30 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:17:53.258390: step 232930, loss = 0.31 (1511.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:17:54.128667: step 232940, loss = 0.22 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:17:55.001369: step 232950, loss = 0.36 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:17:55.884978: step 232960, loss = 0.21 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:17:56.718896: step 232970, loss = 0.32 (1534.9 examples/sec; 0.083 sec/batch)
2017-06-02 08:17:57.571928: step 232980, loss = 0.27 (1500.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:17:58.437553: step 232990, loss = 0.30 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:17:59.415001: step 233000, loss = 0.30 (1309.5 examples/sec; 0.098 sec/batch)
2017-06-02 08:18:00.142362: step 233010, loss = 0.38 (1759.8 examples/sec; 0.073 sec/batch)
2017-06-02 08:18:01.000844: step 233020, loss = 0.27 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:18:01.866266: step 233030, loss = 0.36 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:18:02.709021: step 233040, loss = 0.27 (1518.8 examples/sec; 0.084 sec/batch)
2017-06-02 08:18:03.573469: step 233050, loss = 0.29 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:18:04.420255: step 233060, loss = 0.31 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:18:05.252104: step 233070, loss = 0.28 (1538.7 examples/sec; 0.083 sec/batch)
2017-06-02 08:18:06.116985: step 233080, loss = 0.31 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:18:06.987305: step 233090, loss = 0.28 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:18:07.958809: step 233100, loss = 0.27 (1317.5 examples/sec; 0.097 sec/batch)
2017-06-02 08:18:08.696942: step 233110, loss = 0.36 (1734.1 examples/sec; 0.074 sec/batch)
2017-06-02 08:18:09.582465: step 233120, loss = 0.30 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:18:10.411759: step 233130, loss = 0.30 (1543.5 examples/sec; 0.083 sec/batch)
2017-06-02 08:18:11.269685: step 233140, loss = 0.34 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:18:12.142299: step 233150, loss = 0.25 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:18:12.998667: step 233160, loss = 0.34 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:18:13.844668: step 233170, loss = 0.32 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:18:14.733267: step 233180, loss = 0.25 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:18:15.600877: step 233190, loss = 0.28 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:18:16.560368: step 233200, loss = 0.34 (1334.0 examples/sec; 0.096 sec/batch)
2017-06-02 08:18:17.330621: step 233210, loss = 0.25 (1661.8 examples/sec; 0.077 sec/batch)
2017-06-02 08:18:18.174894: step 233220, loss = 0.35 (1516.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:18:19.032909: step 233230, loss = 0.22 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:18:19.880523: step 233240, loss = 0.32 (1510.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:18:20.760980: step 233250, loss = 0.29 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:18:21.648721: step 233260, loss = 0.28 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:18:22.482094: step 233270, loss = 0.25 (1536.0 examples/sec; 0.083 sec/batch)
2017-06-02 08:18:23.323837: step 233280, loss = 0.28 (1520.6 examples/sec; 0.084 sec/batch)
2017-06-02 08:18:24.168020: step 233290, loss = 0.29 (1516.3 examples/sec; 0.084 sec/batch)
2017-06-02 08:18:25.138727: step 233300, loss = 0.39 (1318.6 examples/sec; 0.097 sec/batch)
2017-06-02 08:18:25.885613: step 233310, loss = 0.30 (1713.8 examples/sec; 0.075 sec/batch)
2017-06-02 08:18:26.711287: step 233320, loss = 0.34 (1550.3 examples/sec; 0.083 sec/batch)
2017-06-02 08:18:27.572309: step 233330, loss = 0.40 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:18:28.429074: step 233340, loss = 0.29 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:18:29.270365: step 233350, loss = 0.25 (1521.5 examples/sec; 0.084 sec/batch)
2017-06-02 08:18:30.104266: step 233360, loss = 0.23 (1535.0 examples/sec; 0.083 sec/batch)
2017-06-02 08:18:30.989924: step 233370, loss = 0.37 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:18:31.838855: step 233380, loss = 0.34 (1507.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:18:32.695345: step 233390, loss = 0.27 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:18:33.668337: step 233400, loss = 0.30 (1315.5 examples/sec; 0.097 sec/batch)
2017-06-02 08:18:34.442787: step 233410, loss = 0.35 (1652.8 examples/sec; 0.077 sec/batch)
2017-06-02 08:18:35.311885: step 233420, loss = 0.26 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:18:36.173854: step 233430, loss = 0.25 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:18:37.052718: step 233440, loss = 0.28 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:18:37.902920: step 233450, loss = 0.22 (1505.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:18:38.776689: step 233460, loss = 0.35 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:18:39.664646: step 233470, loss = 0.33 (1441.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:18:40.534280: step 233480, loss = 0.29 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:18:41.393656: step 233490, loss = 0.28 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:18:42.339900: step 233500, loss = 0.24 (1352.7 examples/sec; 0.095 sec/batch)
2017-06-02 08:18:43.110752: step 233510, loss = 0.21 (1660.5 examples/sec; 0.077 sec/batch)
2017-06-02 08:18:43.977805: step 233520, loss = 0.29 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:18:44.842704: step 233530, loss = 0.28 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:18:45.703097: step 233540, loss = 0.30 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:18:46.587097: step 233550, loss = 0.23 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:18:47.445533: step 233560, loss = 0.33 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:18:48.281639: step 233570, loss = 0.30 (1530.9 examples/sec; 0.084 sec/batch)
2017-06-02 08:18:49.140098: step 233580, loss = 0.39 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:18:50.020031: step 233590, loss = 0.23 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:18:50.987144: step 233600, loss = 0.30 (1323.5 examples/sec; 0.097 sec/batch)
2017-06-02 08:18:51.756540: step 233610, loss = 0.26 (1663.6 examples/sec; 0.077 sec/batch)
2017-06-02 08:18:52.615177: step 233620, loss = 0.26 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:18:53.524931: step 233630, loss = 0.28 (1407.0 examples/sec; 0.091 sec/batch)
2017-06-02 08:18:54.411069: step 233640, loss = 0.31 (1444.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:18:55.274351: step 233650, loss = 0.29 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:18:56.129667: step 233660, loss = 0.27 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:18:57.005396: step 233670, loss = 0.32 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:18:57.858220: step 233680, loss = 0.26 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:18:58.706845: step 233690, loss = 0.30 (1508.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:18:59.686286: step 233700, loss = 0.34 (1306.9 examples/sec; 0.098 sec/batch)
2017-06-02 08:19:00.483976: step 233710, loss = 0.29 (1604.6 examples/sec; 0.080 sec/batch)
2017-06-02 08:19:01.348249: step 233720, loss = 0.26 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:19:02.213147: step 233730, loss = 0.33 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:19:03.077920: step 233740, loss = 0.29 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:19:03.945115: step 233750, loss = 0.24 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:19:04.799214: step 233760, loss = 0.26 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:19:05.642044: step 233770, loss = 0.35 (1518.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:19:06.495569: step 233780, loss = 0.23 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:19:07.363934: step 233790, loss = 0.27 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:19:08.345813: step 233800, loss = 0.35 (1303.6 examples/sec; 0.098 sec/batch)
2017-06-02 08:19:09.069333: step 233810, loss = 0.24 (1769.1 examples/sec; 0.072 sec/batch)
2017-06-02 08:19:09.948634: step 233820, loss = 0.35 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:19:10.807403: step 233830, loss = 0.25 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:19:11.677178: step 233840, loss = 0.30 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:19:12.522676: step 233850, loss = 0.30 (1513.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:19:13.406933: step 233860, loss = 0.36 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:19:14.287202: step 233870, loss = 0.24 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:19:15.146445: step 233880, loss = 0.35 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:19:16.026655: step 233890, loss = 0.31 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:19:16.966296: step 233900, loss = 0.28 (1362.2 examples/sec; 0.094 sec/batch)
2017-06-02 08:19:17.740198: step 233910, loss = 0.29 (1654.0 examples/sec; 0.077 sec/batch)
2017-06-02 08:19:18.612204: step 233920, loss = 0.26 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:19:19.477620: step 233930, loss = 0.25 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:19:20.357783: step 233940, loss = 0.34 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:19:21.226049: step 233950, loss = 0.35 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:19:22.091586: step 233960, loss = 0.29 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:19:22.952049: step 233970, loss = 0.23 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:19:23.785998: step 233980, loss = 0.23 (1534.9 examples/sec; 0.083 sec/batch)
2017-06-02 08:19:24.641877: step 233990, loss = 0.30 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:19:25.606293: step 234000, loss = 0.39 (1327.2 examples/sec; 0.096 sec/batch)
2017-06-02 08:19:26.365474: step 234010, loss = 0.33 (1686.0 examples/sec; 0.076 sec/batch)
2017-06-02 08:19:27.248893: step 234020, loss = 0.25 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:19:28.129129: step 234030, loss = 0.34 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:19:28.999643: step 234040, loss = 0.34 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:19:29.848947: step 234050, loss = 0.25 (1507.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:19:30.727984: step 234060, loss = 0.26 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:19:31.575647: step 234070, loss = 0.24 (1510.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:19:32.430441: step 234080, loss = 0.27 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:19:33.295919: step 234090, loss = 0.32 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:19:34.237930: step 234100, loss = 0.38 (1358.8 examples/sec; 0.094 sec/batch)
2017-06-02 08:19:35.011424: step 234110, loss = 0.27 (1654.8 examples/sec; 0.077 sec/batch)
2017-06-02 08:19:35.878149: step 234120, loss = 0.35 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:19:36.721986: step 234130, loss = 0.28 (1516.9 examples/sec; 0.084 sec/batch)
2017-06-02 08:19:37.572414: step 234140, loss = 0.32 (1505.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:19:38.450818: step 234150, loss = 0.29 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:19:39.312866: step 234160, loss = 0.24 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:19:40.166751: step 234170, loss = 0.30 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:19:41.027827: step 234180, loss = 0.29 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:19:41.887344: step 234190, loss = 0.25 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:19:42.863751: step 234200, loss = 0.32 (1310.9 examples/sec; 0.098 sec/batch)
2017-06-02 08:19:43.645176: step 234210, loss = 0.30 (1638.0 examples/sec; 0.078 sec/batch)
2017-06-02 08:19:44.469235: step 234220, loss = 0.26 (1553.3 examples/sec; 0.082 sec/batch)
2017-06-02 08:19:45.332688: step 234230, loss = 0.31 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:19:46.192352: step 234240, loss = 0.31 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:19:47.041705: step 234250, loss = 0.26 (1507.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:19:47.900156: step 234260, loss = 0.30 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:19:48.758611: step 234270, loss = 0.46 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:19:49.589241: step 234280, loss = 0.34 (1541.0 examples/sec; 0.083 sec/batch)
2017-06-02 08:19:50.432673: step 234290, loss = 0.34 (1517.6 examples/sec; 0.084 sec/batch)
2017-06-02 08:19:51.391985: step 234300, loss = 0.25 (1334.3 examples/sec; 0.096 sec/batch)
2017-06-02 08:19:52.210185: step 234310, loss = 0.37 (1564.4 examples/sec; 0.082 sec/batch)
2017-06-02 08:19:53.087771: step 234320, loss = 0.26 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:19:53.941231: step 234330, loss = 0.28 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:19:54.797467: step 234340, loss = 0.25 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:19:55.649255: step 234350, loss = 0.35 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:19:56.524706: step 234360, loss = 0.29 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:19:57.377081: step 234370, loss = 0.36 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:19:58.211064: step 234380, loss = 0.24 (1534.8 examples/sec; 0.083 sec/batch)
2017-06-02 08:19:59.078935: step 234390, loss = 0.25 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:20:00.081961: step 234400, loss = 0.26 (1276.1 examples/sec; 0.100 sec/batch)
2017-06-02 08:20:00.790289: step 234410, loss = 0.22 (1807.1 examples/sec; 0.071 sec/batch)
2017-06-02 08:20:01.662756: step 234420, loss = 0.34 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:20:02.539408: step 234430, loss = 0.30 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:20:03.401476: step 234440, loss = 0.26 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:20:04.201560: step 234450, loss = 0.41 (1599.8 examples/sec; 0.080 sec/batch)
2017-06-02 08:20:05.043464: step 234460, loss = 0.28 (1520.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:20:05.910599: step 234470, loss = 0.31 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:20:06.759448: step 234480, loss = 0.33 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:20:07.617444: step 234490, loss = 0.29 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:20:08.612721: step 234500, loss = 0.30 (1286.1 examples/sec; 0.100 sec/batch)
2017-06-02 08:20:09.351284: step 234510, loss = 0.25 (1733.1 examples/sec; 0.074 sec/batch)
2017-06-02 08:20:10.198415: step 234520, loss = 0.35 (1511.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:20:11.035613: step 234530, loss = 0.22 (1528.9 examples/sec; 0.084 sec/batch)
2017-06-02 08:20:11.915633: step 234540, loss = 0.37 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:20:12.798670: step 234550, loss = 0.31 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:20:13.662807: step 234560, loss = 0.35 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:20:14.557720: step 234570, loss = 0.24 (1430.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:20:15.437219: step 234580, loss = 0.29 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:20:16.325004: step 234590, loss = 0.31 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:20:17.300870: step 234600, loss = 0.24 (1311.7 examples/sec; 0.098 sec/batch)
2017-06-02 08:20:18.051645: step 234610, loss = 0.35 (1704.9 examples/sec; 0.075 sec/batch)
2017-06-02 08:20:18.915010: step 234620, loss = 0.27 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:20:19.774795: step 234630, loss = 0.27 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:20:20.650791: step 234640, loss = 0.25 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:20:21.528176: step 234650, loss = 0.42 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:20:22.382786: step 234660, loss = 0.38 (1497.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:20:23.225381: step 234670, loss = 0.34 (1519.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:20:24.102923: step 234680, loss = 0.32 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:20:24.966140: step 234690, loss = 0.32 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:20:25.922101: step 234700, loss = 0.27 (1339.0 examples/sec; 0.096 sec/batch)
2017-06-02 08:20:26.706034: step 234710, loss = 0.25 (1632.8 examples/sec; 0.078 sec/batch)
2017-06-02 08:20:27.586927: step 234720, loss = 0.31 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:20:28.443996: step 234730, loss = 0.21 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:20:29.307723: step 234740, loss = 0.20 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:20:30.151444: step 234750, loss = 0.20 (1517.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:20:31.018425: step 234760, loss = 0.28 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:20:31.874583: step 234770, loss = 0.28 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:20:32.713411: step 234780, loss = 0.32 (1525.9 examples/sec; 0.084 sec/batch)
2017-06-02 08:20:33.572147: step 234790, loss = 0.28 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:20:34.530480: step 234800, loss = 0.32 (1335.7 examples/sec; 0.096 sec/batch)
2017-06-02 08:20:35.267103: step 234810, loss = 0.31 (1737.7 examples/sec; 0.074 sec/batch)
2017-06-02 08:20:36.130436: step 234820, loss = 0.21 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:20:36.987156: step 234830, loss = 0.23 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:20:37.837691: step 234840, loss = 0.32 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:20:38.680305: step 234850, loss = 0.33 (1519.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:20:39.534350: step 234860, loss = 0.24 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:20:40.394287: step 234870, loss = 0.24 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:20:41.269244: step 234880, loss = 0.35 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:20:42.143648: step 234890, loss = 0.28 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:20:43.123133: step 234900, loss = 0.24 (1306.8 examples/sec; 0.098 sec/batch)
2017-06-02 08:20:43.904956: step 234910, loss = 0.24 (1637.2 examples/sec; 0.078 sec/batch)
2017-06-02 08:20:44.770962: step 234920, loss = 0.29 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:20:45.641821: step 234930, loss = 0.20 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:20:46.507424: step 234940, loss = 0.35 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:20:47.392294: step 234950, loss = 0.34 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:20:48.258846: step 234960, loss = 0.40 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:20:49.123773: step 234970, loss = 0.30 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:20:49.976971: step 234980, loss = 0.26 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:20:50.854915: step 234990, loss = 0.32 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:20:51.800904: step 235000, loss = 0.29 (1352.5 examples/sec; 0.095 sec/batch)
2017-06-02 08:20:52.558109: step 235010, loss = 0.34 (1690.5 examples/sec; 0.076 sec/batch)
2017-06-02 08:20:53.411028: step 235020, loss = 0.31 (1500.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:20:54.261668: step 235030, loss = 0.27 (1504.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:20:55.134467: step 235040, loss = 0.25 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:20:55.987101: step 235050, loss = 0.30 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:20:56.839541: step 235060, loss = 0.40 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:20:57.724401: step 235070, loss = 0.30 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:20:58.590275: step 235080, loss = 0.23 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:20:59.447700: step 235090, loss = 0.35 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:21:00.483894: step 235100, loss = 0.26 (1235.3 examples/sec; 0.104 sec/batch)
2017-06-02 08:21:01.160147: step 235110, loss = 0.30 (1892.8 examples/sec; 0.068 sec/batch)
2017-06-02 08:21:02.029537: step 235120, loss = 0.31 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:21:02.897542: step 235130, loss = 0.26 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:21:03.757577: step 235140, loss = 0.25 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:21:04.623691: step 235150, loss = 0.25 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:21:05.505244: step 235160, loss = 0.26 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:21:06.367951: step 235170, loss = 0.29 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:21:07.223393: step 235180, loss = 0.30 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:21:08.104243: step 235190, loss = 0.24 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:21:09.038058: step 235200, loss = 0.27 (1370.7 examples/sec; 0.093 sec/batch)
2017-06-02 08:21:09.805940: step 235210, loss = 0.34 (1666.9 examples/sec; 0.077 sec/batch)
2017-06-02 08:21:10.685687: step 235220, loss = 0.22 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:21:11.535982: step 235230, loss = 0.27 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:21:12.419381: step 235240, loss = 0.30 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:21:13.273890: step 235250, loss = 0.38 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:21:14.140918: step 235260, loss = 0.27 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:21:15.006992: step 235270, loss = 0.38 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:21:15.861058: step 235280, loss = 0.30 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:21:16.729246: step 235290, loss = 0.34 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:21:17.702203: step 235300, loss = 0.31 (1315.6 examples/sec; 0.097 sec/batch)
2017-06-02 08:21:18.465212: step 235310, loss = 0.27 (1677.6 examples/sec; 0.076 sec/batch)
2017-06-02 08:21:19.316915: step 235320, loss = 0.29 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:21:20.175519: step 235330, loss = 0.32 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:21:21.029264: step 235340, loss = 0.30 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:21:21.849074: step 235350, loss = 0.26 (1561.3 examples/sec; 0.082 sec/batch)
2017-06-02 08:21:22.722236: step 235360, loss = 0.22 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:21:23.566795: step 235370, loss = 0.27 (1515.6 examples/sec; 0.084 sec/batch)
2017-06-02 08:21:24.436610: step 235380, loss = 0.35 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:21:25.292465: step 235390, loss = 0.31 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:21:26.274188: step 235400, loss = 0.22 (1303.8 examples/sec; 0.098 sec/batch)
2017-06-02 08:21:27.025697: step 235410, loss = 0.30 (1703.2 examples/sec; 0.075 sec/batch)
2017-06-02 08:21:27.916125: step 235420, loss = 0.33 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:21:28.777076: step 235430, loss = 0.36 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:21:29.634209: step 235440, loss = 0.30 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:21:30.496920: step 235450, loss = 0.27 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:21:31.354221: step 235460, loss = 0.30 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:21:32.194504: step 235470, loss = 0.27 (1523.3 examples/sec; 0.084 sec/batch)
2017-06-02 08:21:33.054941: step 235480, loss = 0.28 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:21:33.905897: step 235490, loss = 0.26 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:21:34.876310: step 235500, loss = 0.32 (1319.0 examples/sec; 0.097 sec/batch)
2017-06-02 08:21:35.641096: step 235510, loss = 0.31 (1673.7 examples/sec; 0.076 sec/batch)
2017-06-02 08:21:36.502773: step 235520, loss = 0.34 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:21:37.373229: step 235530, loss = 0.33 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:21:38.260691: step 235540, loss = 0.23 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:21:39.126755: step 235550, loss = 0.36 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:21:39.989022: step 235560, loss = 0.36 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:21:40.828873: step 235570, loss = 0.21 (1524.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:21:41.684594: step 235580, loss = 0.39 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:21:42.528669: step 235590, loss = 0.26 (1516.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:21:43.491858: step 235600, loss = 0.23 (1328.9 examples/sec; 0.096 sec/batch)
2017-06-02 08:21:44.230766: step 235610, loss = 0.30 (1732.3 examples/sec; 0.074 sec/batch)
2017-06-02 08:21:45.079780: step 235620, loss = 0.26 (1507.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:21:45.937009: step 235630, loss = 0.31 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:21:46.798816: step 235640, loss = 0.26 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:21:47.661365: step 235650, loss = 0.29 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:21:48.514721: step 235660, loss = 0.34 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:21:49.338819: step 235670, loss = 0.23 (1553.2 examples/sec; 0.082 sec/batch)
2017-06-02 08:21:50.192495: step 235680, loss = 0.25 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:21:51.046703: step 235690, loss = 0.23 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:21:52.018312: step 235700, loss = 0.36 (1317.4 examples/sec; 0.097 sec/batch)
2017-06-02 08:21:52.772091: step 235710, loss = 0.26 (1698.2 examples/sec; 0.075 sec/batch)
2017-06-02 08:21:53.617951: step 235720, loss = 0.31 (1513.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:21:54.481487: step 235730, loss = 0.27 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:21:55.371094: step 235740, loss = 0.34 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:21:56.243427: step 235750, loss = 0.24 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:21:57.089836: step 235760, loss = 0.29 (1512.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:21:57.949142: step 235770, loss = 0.40 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:21:58.804781: step 235780, loss = 0.32 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:21:59.658116: step 235790, loss = 0.32 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:22:00.651231: step 235800, loss = 0.32 (1288.9 examples/sec; 0.099 sec/batch)
2017-06-02 08:22:01.447655: step 235810, loss = 0.28 (1607.2 examples/sec; 0.080 sec/batch)
2017-06-02 08:22:02.309954: step 235820, loss = 0.25 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:22:03.154522: step 235830, loss = 0.21 (1515.6 examples/sec; 0.084 sec/batch)
2017-06-02 08:22:04.004824: step 235840, loss = 0.30 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:22:04.851480: step 235850, loss = 0.24 (1511.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:22:05.719147: step 235860, loss = 0.24 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:22:06.575976: step 235870, loss = 0.33 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:22:07.469121: step 235880, loss = 0.28 (1433.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:22:08.326495: step 235890, loss = 0.33 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:22:09.346403: step 235900, loss = 0.38 (1255.0 examples/sec; 0.102 sec/batch)
2017-06-02 08:22:10.078049: step 235910, loss = 0.28 (1749.5 examples/sec; 0.073 sec/batch)
2017-06-02 08:22:10.924796: step 235920, loss = 0.37 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:22:11.792589: step 235930, loss = 0.22 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:22:12.663092: step 235940, loss = 0.30 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:22:13.522892: step 235950, loss = 0.28 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:22:14.388497: step 235960, loss = 0.29 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:22:15.255008: step 235970, loss = 0.27 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:22:16.124832: step 235980, loss = 0.41 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:22:16.956794: step 235990, loss = 0.31 (1538.5 examples/sec; 0.083 sec/batch)
2017-06-02 08:22:17.909354: step 236000, loss = 0.34 (1343.7 examples/sec; 0.095 sec/batch)
2017-06-02 08:22:18.660838: step 236010, loss = 0.39 (1703.3 examples/sec; 0.075 sec/batch)
2017-06-02 08:22:19.509923: step 236020, loss = 0.23 (1507.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:22:20.390535: step 236030, loss = 0.36 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:22:21.223775: step 236040, loss = 0.32 (1536.2 examples/sec; 0.083 sec/batch)
2017-06-02 08:22:22.105913: step 236050, loss = 0.28 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:22:22.958187: step 236060, loss = 0.31 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:22:23.817783: step 236070, loss = 0.31 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:22:24.663711: step 236080, loss = 0.33 (1513.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:22:25.497596: step 236090, loss = 0.29 (1535.0 examples/sec; 0.083 sec/batch)
2017-06-02 08:22:26.453985: step 236100, loss = 0.28 (1338.4 examples/sec; 0.096 sec/batch)
2017-06-02 08:22:27.212071: step 236110, loss = 0.30 (1688.5 examples/sec; 0.076 sec/batch)
2017-06-02 08:22:28.084350: step 236120, loss = 0.38 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:22:28.968952: step 236130, loss = 0.37 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:22:29.835393: step 236140, loss = 0.26 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:22:30.680470: step 236150, loss = 0.31 (1514.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:22:31.542500: step 236160, loss = 0.26 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:22:32.388584: step 236170, loss = 0.29 (1512.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:22:33.260089: step 236180, loss = 0.31 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:22:34.110372: step 236190, loss = 0.31 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:22:35.068878: step 236200, loss = 0.33 (1335.4 examples/sec; 0.096 sec/batch)
2017-06-02 08:22:35.815771: step 236210, loss = 0.30 (1713.8 examples/sec; 0.075 sec/batch)
2017-06-02 08:22:36.661419: step 236220, loss = 0.30 (1513.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:22:37.548355: step 236230, loss = 0.30 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:22:38.385944: step 236240, loss = 0.32 (1528.2 examples/sec; 0.084 sec/batch)
2017-06-02 08:22:39.248690: step 236250, loss = 0.28 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:22:40.112085: step 236260, loss = 0.27 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:22:40.961263: step 236270, loss = 0.31 (1507.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:22:41.818157: step 236280, loss = 0.44 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:22:42.672295: step 236290, loss = 0.30 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:22:43.621906: step 236300, loss = 0.37 (1347.9 examples/sec; 0.095 sec/batch)
2017-06-02 08:22:44.385352: step 236310, loss = 0.23 (1676.6 examples/sec; 0.076 sec/batch)
2017-06-02 08:22:45.229691: step 236320, loss = 0.24 (1516.0 examples/sec; 0.084 sec/batch)
2017-06-02 08:22:46.099116: step 236330, loss = 0.24 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:22:46.971338: step 236340, loss = 0.23 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:22:47.812041: step 236350, loss = 0.26 (1522.6 examples/sec; 0.084 sec/batch)
2017-06-02 08:22:48.642364: step 236360, loss = 0.26 (1541.6 examples/sec; 0.083 sec/batch)
2017-06-02 08:22:49.501023: step 236370, loss = 0.25 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:22:50.340321: step 236380, loss = 0.25 (1525.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:22:51.220325: step 236390, loss = 0.34 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:22:52.238073: step 236400, loss = 0.35 (1257.7 examples/sec; 0.102 sec/batch)
2017-06-02 08:22:52.977750: step 236410, loss = 0.32 (1730.5 examples/sec; 0.074 sec/batch)
2017-06-02 08:22:53.832768: step 236420, loss = 0.32 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:22:54.690488: step 236430, loss = 0.33 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:22:55.544165: step 236440, loss = 0.26 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:22:56.406325: step 236450, loss = 0.32 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:22:57.299610: step 236460, loss = 0.30 (1432.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:22:58.173637: step 236470, loss = 0.26 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:22:59.049259: step 236480, loss = 0.22 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:22:59.931263: step 236490, loss = 0.26 (1451.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:23:00.896198: step 236500, loss = 0.34 (1326.5 examples/sec; 0.096 sec/batch)
2017-06-02 08:23:01.696903: step 236510, loss = 0.37 (1598.6 examples/sec; 0.080 sec/batch)
2017-06-02 08:23:02.581270: step 236520, loss = 0.24 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:23:03.436183: step 236530, loss = 0.30 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:23:04.298236: step 236540, loss = 0.28 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:23:05.169185: step 236550, loss = 0.34 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:23:06.050750: step 236560, loss = 0.25 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:23:06.915783: step 236570, loss = 0.27 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:23:07.787363: step 236580, loss = 0.33 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:23:08.642591: step 236590, loss = 0.39 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:23:09.601894: step 236600, loss = 0.27 (1334.3 examples/sec; 0.096 sec/batch)
2017-06-02 08:23:10.350225: step 236610, loss = 0.23 (1710.5 examples/sec; 0.075 sec/batch)
2017-06-02 08:23:11.222162: step 236620, loss = 0.25 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:23:12.096529: step 236630, loss = 0.30 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:23:12.972931: step 236640, loss = 0.33 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:23:13.843043: step 236650, loss = 0.35 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:23:14.706316: step 236660, loss = 0.33 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:23:15.559881: step 236670, loss = 0.27 (1499.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:23:16.428743: step 236680, loss = 0.34 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:23:17.288748: step 236690, loss = 0.28 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:23:18.271878: step 236700, loss = 0.29 (1302.0 examples/sec; 0.098 sec/batch)
2017-06-02 08:23:19.045769: step 236710, loss = 0.27 (1654.0 examples/sec; 0.077 sec/batch)
2017-06-02 08:23:19.899618: step 236720, loss = 0.32 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:23:20.750086: step 236730, loss = 0.33 (1505.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:23:21.624853: step 236740, loss = 0.36 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:23:22.498302: step 236750, loss = 0.33 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:23:23.348315: step 236760, loss = 0.32 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:23:24.189605: step 236770, loss = 0.25 (1521.5 examples/sec; 0.084 sec/batch)
2017-06-02 08:23:25.065608: step 236780, loss = 0.28 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:23:25.929403: step 236790, loss = 0.27 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:23:26.876049: step 236800, loss = 0.26 (1352.1 examples/sec; 0.095 sec/batch)
2017-06-02 08:23:27.645032: step 236810, loss = 0.28 (1664.5 examples/sec; 0.077 sec/batch)
2017-06-02 08:23:28.514687: step 236820, loss = 0.32 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:23:29.373502: step 236830, loss = 0.25 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:23:30.239653: step 236840, loss = 0.26 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:23:31.081517: step 236850, loss = 0.35 (1520.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:23:31.935192: step 236860, loss = 0.29 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:23:32.812353: step 236870, loss = 0.30 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:23:33.674636: step 236880, loss = 0.34 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:23:34.536685: step 236890, loss = 0.31 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:23:35.510478: step 236900, loss = 0.28 (1314.4 examples/sec; 0.097 sec/batch)
2017-06-02 08:23:36.283849: step 236910, loss = 0.24 (1655.1 examples/sec; 0.077 sec/batch)
2017-06-02 08:23:37.147181: step 236920, loss = 0.30 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:23:38.018668: step 236930, loss = 0.28 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:23:38.873596: step 236940, loss = 0.26 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:23:39.738681: step 236950, loss = 0.33 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:23:40.605268: step 236960, loss = 0.33 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:23:41.475810: step 236970, loss = 0.31 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:23:42.346919: step 236980, loss = 0.25 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:23:43.205811: step 236990, loss = 0.28 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:23:44.241342: step 237000, loss = 0.38 (1236.1 examples/sec; 0.104 sec/batch)
2017-06-02 08:23:44.960960: step 237010, loss = 0.29 (1778.7 examples/sec; 0.072 sec/batch)
2017-06-02 08:23:45.810125: step 237020, loss = 0.30 (1507.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:23:46.665302: step 237030, loss = 0.30 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:23:47.504287: step 237040, loss = 0.22 (1525.6 examples/sec; 0.084 sec/batch)
2017-06-02 08:23:48.382354: step 237050, loss = 0.28 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:23:49.242279: step 237060, loss = 0.25 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:23:50.075989: step 237070, loss = 0.29 (1535.3 examples/sec; 0.083 sec/batch)
2017-06-02 08:23:50.937152: step 237080, loss = 0.25 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:23:51.810664: step 237090, loss = 0.26 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:23:52.765740: step 237100, loss = 0.34 (1340.2 examples/sec; 0.096 sec/batch)
2017-06-02 08:23:53.533056: step 237110, loss = 0.27 (1668.1 examples/sec; 0.077 sec/batch)
2017-06-02 08:23:54.395839: step 237120, loss = 0.31 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:23:55.264406: step 237130, loss = 0.28 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:23:56.090146: step 237140, loss = 0.27 (1550.1 examples/sec; 0.083 sec/batch)
2017-06-02 08:23:56.944130: step 237150, loss = 0.30 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:23:57.803817: step 237160, loss = 0.22 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:23:58.648051: step 237170, loss = 0.27 (1516.2 examples/sec; 0.084 sec/batch)
2017-06-02 08:23:59.490064: step 237180, loss = 0.27 (1520.2 examples/sec; 0.084 sec/batch)
2017-06-02 08:24:00.343690: step 237190, loss = 0.33 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:24:01.304805: step 237200, loss = 0.25 (1331.8 examples/sec; 0.096 sec/batch)
2017-06-02 08:24:02.096681: step 237210, loss = 0.35 (1616.4 examples/sec; 0.079 sec/batch)
2017-06-02 08:24:02.981143: step 237220, loss = 0.31 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:24:03.865871: step 237230, loss = 0.26 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:24:04.721534: step 237240, loss = 0.33 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:24:05.571192: step 237250, loss = 0.30 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:24:06.427237: step 237260, loss = 0.36 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:24:07.293193: step 237270, loss = 0.29 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:24:08.139920: step 237280, loss = 0.31 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:24:09.000928: step 237290, loss = 0.26 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:24:09.967155: step 237300, loss = 0.23 (1324.7 examples/sec; 0.097 sec/batch)
2017-06-02 08:24:10.755961: step 237310, loss = 0.25 (1622.7 examples/sec; 0.079 sec/batch)
2017-06-02 08:24:11.601772: step 237320, loss = 0.35 (1513.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:24:12.465638: step 237330, loss = 0.36 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:24:13.296963: step 237340, loss = 0.30 (1539.7 examples/sec; 0.083 sec/batch)
2017-06-02 08:24:14.187672: step 237350, loss = 0.26 (1437.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:24:15.055810: step 237360, loss = 0.28 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:24:15.924585: step 237370, loss = 0.29 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:24:16.817520: step 237380, loss = 0.29 (1433.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:24:17.653067: step 237390, loss = 0.28 (1531.9 examples/sec; 0.084 sec/batch)
2017-06-02 08:24:18.628736: step 237400, loss = 0.29 (1311.9 examples/sec; 0.098 sec/batch)
2017-06-02 08:24:19.427128: step 237410, loss = 0.29 (1603.2 examples/sec; 0.080 sec/batch)
2017-06-02 08:24:20.262541: step 237420, loss = 0.29 (1532.2 examples/sec; 0.084 sec/batch)
2017-06-02 08:24:21.116451: step 237430, loss = 0.27 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:24:21.989151: step 237440, loss = 0.31 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:24:22.886721: step 237450, loss = 0.26 (1426.1 examples/sec; 0.090 sec/batch)
2017-06-02 08:24:23.749845: step 237460, loss = 0.32 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:24:24.614263: step 237470, loss = 0.29 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:24:25.513890: step 237480, loss = 0.31 (1422.8 examples/sec; 0.090 sec/batch)
2017-06-02 08:24:26.386985: step 237490, loss = 0.46 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:24:27.360217: step 237500, loss = 0.45 (1315.2 examples/sec; 0.097 sec/batch)
2017-06-02 08:24:28.131998: step 237510, loss = 0.32 (1658.5 examples/sec; 0.077 sec/batch)
2017-06-02 08:24:28.990959: step 237520, loss = 0.23 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:24:29.860677: step 237530, loss = 0.31 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:24:30.707482: step 237540, loss = 0.23 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:24:31.588369: step 237550, loss = 0.27 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:24:32.449740: step 237560, loss = 0.33 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:24:33.319540: step 237570, loss = 0.36 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:24:34.162250: step 237580, loss = 0.29 (1518.9 examples/sec; 0.084 sec/batch)
2017-06-02 08:24:35.046707: step 237590, loss = 0.27 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:24:36.020946: step 237600, loss = 0.24 (1313.8 examples/sec; 0.097 sec/batch)
2017-06-02 08:24:36.795478: step 237610, loss = 0.29 (1652.6 examples/sec; 0.077 sec/batch)
2017-06-02 08:24:37.700939: step 237620, loss = 0.32 (1413.6 examples/sec; 0.091 sec/batch)
2017-06-02 08:24:38.580563: step 237630, loss = 0.23 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:24:39.444337: step 237640, loss = 0.23 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:24:40.293387: step 237650, loss = 0.21 (1507.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:24:41.097297: step 237660, loss = 0.35 (1592.2 examples/sec; 0.080 sec/batch)
2017-06-02 08:24:41.963124: step 237670, loss = 0.25 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:24:42.834150: step 237680, loss = 0.33 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:24:43.705143: step 237690, loss = 0.31 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:24:44.684620: step 237700, loss = 0.26 (1306.8 examples/sec; 0.098 sec/batch)
2017-06-02 08:24:45.481089: step 237710, loss = 0.37 (1607.1 examples/sec; 0.080 sec/batch)
2017-06-02 08:24:46.386440: step 237720, loss = 0.27 (1413.8 examples/sec; 0.091 sec/batch)
2017-06-02 08:24:47.251586: step 237730, loss = 0.25 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:24:48.128267: step 237740, loss = 0.39 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:24:49.015698: step 237750, loss = 0.24 (1442.4 examples/sec; 0.089 sec/batch)
2017-06-02 08:24:49.907661: step 237760, loss = 0.28 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:24:50.783042: step 237770, loss = 0.29 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:24:51.638335: step 237780, loss = 0.38 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:24:52.495529: step 237790, loss = 0.32 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:24:53.446821: step 237800, loss = 0.28 (1345.5 examples/sec; 0.095 sec/batch)
2017-06-02 08:24:54.213291: step 237810, loss = 0.23 (1670.0 examples/sec; 0.077 sec/batch)
2017-06-02 08:24:55.074813: step 237820, loss = 0.28 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:24:55.938745: step 237830, loss = 0.30 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:24:56.809529: step 237840, loss = 0.27 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:24:57.672982: step 237850, loss = 0.23 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:24:58.537605: step 237860, loss = 0.29 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:24:59.398512: step 237870, loss = 0.29 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:25:00.263788: step 237880, loss = 0.34 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:25:01.139184: step 237890, loss = 0.27 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:25:02.102233: step 237900, loss = 0.23 (1329.1 examples/sec; 0.096 sec/batch)
2017-06-02 08:25:02.879705: step 237910, loss = 0.29 (1646.4 examples/sec; 0.078 sec/batch)
2017-06-02 08:25:03.743567: step 237920, loss = 0.25 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:25:04.589602: step 237930, loss = 0.32 (1512.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:25:05.424959: step 237940, loss = 0.26 (1532.3 examples/sec; 0.084 sec/batch)
2017-06-02 08:25:06.280594: step 237950, loss = 0.29 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:25:07.175478: step 237960, loss = 0.32 (1430.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:25:08.035483: step 237970, loss = 0.38 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:25:08.900504: step 237980, loss = 0.27 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:25:09.749973: step 237990, loss = 0.32 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:25:10.737625: step 238000, loss = 0.33 (1296.0 examples/sec; 0.099 sec/batch)
2017-06-02 08:25:11.507943: step 238010, loss = 0.23 (1661.7 examples/sec; 0.077 sec/batch)
2017-06-02 08:25:12.368714: step 238020, loss = 0.25 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:25:13.203148: step 238030, loss = 0.27 (1534.0 examples/sec; 0.083 sec/batch)
2017-06-02 08:25:14.043852: step 238040, loss = 0.26 (1522.5 examples/sec; 0.084 sec/batch)
2017-06-02 08:25:14.910387: step 238050, loss = 0.28 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:25:15.795165: step 238060, loss = 0.33 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:25:16.633351: step 238070, loss = 0.38 (1527.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:25:17.522068: step 238080, loss = 0.26 (1440.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:25:18.383303: step 238090, loss = 0.42 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:25:19.356985: step 238100, loss = 0.23 (1314.6 examples/sec; 0.097 sec/batch)
2017-06-02 08:25:20.106924: step 238110, loss = 0.30 (1706.8 examples/sec; 0.075 sec/batch)
2017-06-02 08:25:20.963271: step 238120, loss = 0.29 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:25:21.849203: step 238130, loss = 0.32 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:25:22.733440: step 238140, loss = 0.33 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:25:23.601896: step 238150, loss = 0.36 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:25:24.454063: step 238160, loss = 0.34 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:25:25.311231: step 238170, loss = 0.20 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:25:26.175273: step 238180, loss = 0.39 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:25:27.037515: step 238190, loss = 0.35 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:25:28.066188: step 238200, loss = 0.27 (1244.3 examples/sec; 0.103 sec/batch)
2017-06-02 08:25:28.787808: step 238210, loss = 0.32 (1773.8 examples/sec; 0.072 sec/batch)
2017-06-02 08:25:29.643278: step 238220, loss = 0.24 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:25:30.512764: step 238230, loss = 0.25 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:25:31.383678: step 238240, loss = 0.37 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:25:32.256685: step 238250, loss = 0.29 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:25:33.115546: step 238260, loss = 0.23 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:25:33.989186: step 238270, loss = 0.29 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:25:34.849693: step 238280, loss = 0.34 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:25:35.700174: step 238290, loss = 0.25 (1505.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:25:36.651715: step 238300, loss = 0.30 (1345.2 examples/sec; 0.095 sec/batch)
2017-06-02 08:25:37.409304: step 238310, loss = 0.33 (1689.6 examples/sec; 0.076 sec/batch)
2017-06-02 08:25:38.295272: step 238320, loss = 0.34 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:25:39.159386: step 238330, loss = 0.34 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:25:40.008851: step 238340, loss = 0.27 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:25:40.858999: step 238350, loss = 0.28 (1505.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:25:41.739534: step 238360, loss = 0.29 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:25:42.578666: step 238370, loss = 0.35 (1525.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:25:43.441017: step 238380, loss = 0.30 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:25:44.323192: step 238390, loss = 0.29 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:25:45.296440: step 238400, loss = 0.38 (1315.2 examples/sec; 0.097 sec/batch)
2017-06-02 08:25:46.046525: step 238410, loss = 0.29 (1706.5 examples/sec; 0.075 sec/batch)
2017-06-02 08:25:46.904872: step 238420, loss = 0.26 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:25:47.763193: step 238430, loss = 0.31 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:25:48.623374: step 238440, loss = 0.23 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:25:49.492421: step 238450, loss = 0.29 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:25:50.352340: step 238460, loss = 0.27 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:25:51.222525: step 238470, loss = 0.30 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:25:52.099032: step 238480, loss = 0.38 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:25:52.954033: step 238490, loss = 0.20 (1497.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:25:53.922452: step 238500, loss = 0.35 (1321.7 examples/sec; 0.097 sec/batch)
2017-06-02 08:25:54.681971: step 238510, loss = 0.40 (1685.3 examples/sec; 0.076 sec/batch)
2017-06-02 08:25:55.567180: step 238520, loss = 0.28 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:25:56.441488: step 238530, loss = 0.29 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:25:57.313153: step 238540, loss = 0.32 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:25:58.191887: step 238550, loss = 0.25 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:25:59.056874: step 238560, loss = 0.28 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:25:59.924342: step 238570, loss = 0.25 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:26:00.786807: step 238580, loss = 0.37 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:26:01.655962: step 238590, loss = 0.31 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:26:02.659227: step 238600, loss = 0.29 (1275.8 examples/sec; 0.100 sec/batch)
2017-06-02 08:26:03.356776: step 238610, loss = 0.37 (1835.0 examples/sec; 0.070 sec/batch)
2017-06-02 08:26:04.193952: step 238620, loss = 0.26 (1529.0 examples/sec; 0.084 sec/batch)
2017-06-02 08:26:05.045197: step 238630, loss = 0.37 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:26:05.929287: step 238640, loss = 0.33 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:26:06.793982: step 238650, loss = 0.25 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:26:07.661533: step 238660, loss = 0.35 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:26:08.515061: step 238670, loss = 0.23 (1499.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:26:09.366765: step 238680, loss = 0.35 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:26:10.226813: step 238690, loss = 0.21 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:26:11.207652: step 238700, loss = 0.30 (1305.0 examples/sec; 0.098 sec/batch)
2017-06-02 08:26:11.968381: step 238710, loss = 0.24 (1682.6 examples/sec; 0.076 sec/batch)
2017-06-02 08:26:12.813020: step 238720, loss = 0.30 (1515.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:26:13.674321: step 238730, loss = 0.42 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:26:14.512992: step 238740, loss = 0.32 (1526.2 examples/sec; 0.084 sec/batch)
2017-06-02 08:26:15.348655: step 238750, loss = 0.34 (1531.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:26:16.199695: step 238760, loss = 0.22 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:26:17.059062: step 238770, loss = 0.36 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:26:17.920381: step 238780, loss = 0.25 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:26:18.779398: step 238790, loss = 0.27 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:26:19.770118: step 238800, loss = 0.27 (1292.0 examples/sec; 0.099 sec/batch)
2017-06-02 08:26:20.515000: step 238810, loss = 0.33 (1718.4 examples/sec; 0.074 sec/batch)
2017-06-02 08:26:21.420086: step 238820, loss = 0.22 (1414.2 examples/sec; 0.091 sec/batch)
2017-06-02 08:26:22.288280: step 238830, loss = 0.31 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:26:23.131826: step 238840, loss = 0.39 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:26:23.981946: step 238850, loss = 0.25 (1505.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:26:24.858463: step 238860, loss = 0.28 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:26:25.747442: step 238870, loss = 0.26 (1439.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:26:26.616675: step 238880, loss = 0.30 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:26:27.478628: step 238890, loss = 0.27 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:26:28.458291: step 238900, loss = 0.21 (1306.6 examples/sec; 0.098 sec/batch)
2017-06-02 08:26:29.228433: step 238910, loss = 0.41 (1662.0 examples/sec; 0.077 sec/batch)
2017-06-02 08:26:30.086432: step 238920, loss = 0.25 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:26:30.958902: step 238930, loss = 0.33 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:26:31.819534: step 238940, loss = 0.26 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:26:32.715507: step 238950, loss = 0.37 (1428.6 examples/sec; 0.090 sec/batch)
2017-06-02 08:26:33.601454: step 238960, loss = 0.27 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:26:34.471777: step 238970, loss = 0.36 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:26:35.343832: step 238980, loss = 0.32 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:26:36.212786: step 238990, loss = 0.32 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:26:37.167370: step 239000, loss = 0.35 (1340.9 examples/sec; 0.095 sec/batch)
2017-06-02 08:26:37.922585: step 239010, loss = 0.25 (1694.9 examples/sec; 0.076 sec/batch)
2017-06-02 08:26:38.777393: step 239020, loss = 0.29 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:26:39.634408: step 239030, loss = 0.28 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:26:40.465317: step 239040, loss = 0.36 (1540.5 examples/sec; 0.083 sec/batch)
2017-06-02 08:26:41.323331: step 239050, loss = 0.34 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:26:42.181902: step 239060, loss = 0.38 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:26:43.029745: step 239070, loss = 0.35 (1509.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:26:43.882862: step 239080, loss = 0.36 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:26:44.727582: step 239090, loss = 0.28 (1515.3 examples/sec; 0.084 sec/batch)
2017-06-02 08:26:45.672143: step 239100, loss = 0.34 (1355.1 examples/sec; 0.094 sec/batch)
2017-06-02 08:26:46.423442: step 239110, loss = 0.24 (1703.7 examples/sec; 0.075 sec/batch)
2017-06-02 08:26:47.296242: step 239120, loss = 0.29 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:26:48.171545: step 239130, loss = 0.35 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:26:49.012843: step 239140, loss = 0.35 (1521.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:26:49.887665: step 239150, loss = 0.24 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:26:50.733940: step 239160, loss = 0.22 (1512.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:26:51.573457: step 239170, loss = 0.28 (1524.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:26:52.426107: step 239180, loss = 0.22 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:26:53.287072: step 239190, loss = 0.26 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:26:54.248187: step 239200, loss = 0.25 (1331.8 examples/sec; 0.096 sec/batch)
2017-06-02 08:26:54.999242: step 239210, loss = 0.27 (1704.3 examples/sec; 0.075 sec/batch)
2017-06-02 08:26:55.860201: step 239220, loss = 0.34 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:26:56.712967: step 239230, loss = 0.34 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:26:57.588390: step 239240, loss = 0.35 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:26:58.437724: step 239250, loss = 0.33 (1507.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:26:59.308307: step 239260, loss = 0.35 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:27:00.167425: step 239270, loss = 0.27 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:27:01.015774: step 239280, loss = 0.26 (1508.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:27:01.855977: step 239290, loss = 0.26 (1523.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:27:02.821123: step 239300, loss = 0.32 (1326.2 examples/sec; 0.097 sec/batch)
2017-06-02 08:27:03.583368: step 239310, loss = 0.25 (1679.3 examples/sec; 0.076 sec/batch)
2017-06-02 08:27:04.435416: step 239320, loss = 0.26 (1502.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:27:05.308217: step 239330, loss = 0.27 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:27:06.182452: step 239340, loss = 0.30 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:27:07.032767: step 239350, loss = 0.27 (1505.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:27:07.914802: step 239360, loss = 0.28 (1451.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:27:08.818436: step 239370, loss = 0.35 (1416.5 examples/sec; 0.090 sec/batch)
2017-06-02 08:27:09.646408: step 239380, loss = 0.25 (1546.0 examples/sec; 0.083 sec/batch)
2017-06-02 08:27:10.504761: step 239390, loss = 0.24 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:27:11.457090: step 239400, loss = 0.29 (1344.1 examples/sec; 0.095 sec/batch)
2017-06-02 08:27:12.198341: step 239410, loss = 0.33 (1726.8 examples/sec; 0.074 sec/batch)
2017-06-02 08:27:13.054605: step 239420, loss = 0.27 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:27:13.915321: step 239430, loss = 0.26 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:27:14.762048: step 239440, loss = 0.25 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:27:15.632747: step 239450, loss = 0.26 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:27:16.492584: step 239460, loss = 0.26 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:27:17.322124: step 239470, loss = 0.28 (1543.0 examples/sec; 0.083 sec/batch)
2017-06-02 08:27:18.184192: step 239480, loss = 0.38 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:27:19.032891: step 239490, loss = 0.39 (1508.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:27:20.061007: step 239500, loss = 0.27 (1245.0 examples/sec; 0.103 sec/batch)
2017-06-02 08:27:20.798616: step 239510, loss = 0.23 (1735.4 examples/sec; 0.074 sec/batch)
2017-06-02 08:27:21.670930: step 239520, loss = 0.33 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:27:22.547688: step 239530, loss = 0.28 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:27:23.416368: step 239540, loss = 0.26 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:27:24.290588: step 239550, loss = 0.30 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:27:25.146774: step 239560, loss = 0.31 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:27:25.982293: step 239570, loss = 0.28 (1532.0 examples/sec; 0.084 sec/batch)
2017-06-02 08:27:26.831126: step 239580, loss = 0.24 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:27:27.688301: step 239590, loss = 0.30 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:27:28.635680: step 239600, loss = 0.29 (1351.1 examples/sec; 0.095 sec/batch)
2017-06-02 08:27:29.413318: step 239610, loss = 0.29 (1646.0 examples/sec; 0.078 sec/batch)
2017-06-02 08:27:30.276781: step 239620, loss = 0.31 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:27:31.138918: step 239630, loss = 0.28 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:27:32.004458: step 239640, loss = 0.30 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:27:32.892269: step 239650, loss = 0.27 (1441.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:27:33.754219: step 239660, loss = 0.33 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:27:34.577207: step 239670, loss = 0.38 (1555.3 examples/sec; 0.082 sec/batch)
2017-06-02 08:27:35.430485: step 239680, loss = 0.28 (1500.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:27:36.303224: step 239690, loss = 0.37 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:27:37.248552: step 239700, loss = 0.36 (1354.0 examples/sec; 0.095 sec/batch)
2017-06-02 08:27:38.026570: step 239710, loss = 0.29 (1645.2 examples/sec; 0.078 sec/batch)
2017-06-02 08:27:38.865929: step 239720, loss = 0.27 (1525.0 examples/sec; 0.084 sec/batch)
2017-06-02 08:27:39.733640: step 239730, loss = 0.26 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:27:40.609661: step 239740, loss = 0.31 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:27:41.461889: step 239750, loss = 0.28 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:27:42.331042: step 239760, loss = 0.33 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:27:43.206677: step 239770, loss = 0.30 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:27:44.087549: step 239780, loss = 0.27 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:27:44.972836: step 239790, loss = 0.36 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:27:45.962138: step 239800, loss = 0.33 (1295.0 examples/sec; 0.099 sec/batch)
2017-06-02 08:27:46.663246: step 239810, loss = 0.26 (1823.3 examples/sec; 0.070 sec/batch)
2017-06-02 08:27:47.551133: step 239820, loss = 0.21 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 08:27:48.400003: step 239830, loss = 0.25 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:27:49.265126: step 239840, loss = 0.30 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:27:50.132464: step 239850, loss = 0.35 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:27:50.976284: step 239860, loss = 0.22 (1516.9 examples/sec; 0.084 sec/batch)
2017-06-02 08:27:51.844139: step 239870, loss = 0.33 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:27:52.723394: step 239880, loss = 0.23 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:27:53.570110: step 239890, loss = 0.36 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:27:54.542862: step 239900, loss = 0.27 (1315.9 examples/sec; 0.097 sec/batch)
2017-06-02 08:27:55.304489: step 239910, loss = 0.25 (1680.6 examples/sec; 0.076 sec/batch)
2017-06-02 08:27:56.155872: step 239920, loss = 0.28 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:27:57.017697: step 239930, loss = 0.36 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:27:57.889688: step 239940, loss = 0.37 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:27:58.732040: step 239950, loss = 0.30 (1519.6 examples/sec; 0.084 sec/batch)
2017-06-02 08:27:59.594039: step 239960, loss = 0.26 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:28:00.458188: step 239970, loss = 0.28 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:28:01.313954: step 239980, loss = 0.27 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:28:02.149021: step 239990, loss = 0.28 (1532.8 examples/sec; 0.084 sec/batch)
2017-06-02 08:28:03.104281: step 240000, loss = 0.25 (1340.0 examples/sec; 0.096 sec/batch)
2017-06-02 08:28:03.874527: step 240010, loss = 0.25 (1661.8 examples/sec; 0.077 sec/batch)
2017-06-02 08:28:04.746057: step 240020, loss = 0.36 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:28:05.613098: step 240030, loss = 0.24 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:28:06.505674: step 240040, loss = 0.19 (1434.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:28:07.374213: step 240050, loss = 0.32 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:28:08.232728: step 240060, loss = 0.26 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:28:09.092525: step 240070, loss = 0.29 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:28:09.940200: step 240080, loss = 0.26 (1510.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:28:10.812397: step 240090, loss = 0.25 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:28:11.779006: step 240100, loss = 0.28 (1324.2 examples/sec; 0.097 sec/batch)
2017-06-02 08:28:12.545118: step 240110, loss = 0.30 (1670.8 examples/sec; 0.077 sec/batch)
2017-06-02 08:28:13.400133: step 240120, loss = 0.25 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:28:14.255662: step 240130, loss = 0.35 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:28:15.130111: step 240140, loss = 0.34 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:28:15.993866: step 240150, loss = 0.25 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:28:16.862853: step 240160, loss = 0.39 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:28:17.694915: step 240170, loss = 0.30 (1538.4 examples/sec; 0.083 sec/batch)
2017-06-02 08:28:18.560584: step 240180, loss = 0.31 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:28:19.415143: step 240190, loss = 0.24 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:28:20.359677: step 240200, loss = 0.29 (1355.2 examples/sec; 0.094 sec/batch)
2017-06-02 08:28:21.098301: step 240210, loss = 0.32 (1733.0 examples/sec; 0.074 sec/batch)
2017-06-02 08:28:21.971051: step 240220, loss = 0.40 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:28:22.820577: step 240230, loss = 0.28 (1506.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:28:23.671087: step 240240, loss = 0.21 (1505.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:28:24.556274: step 240250, loss = 0.31 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:28:25.392147: step 240260, loss = 0.33 (1531.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:28:26.238626: step 240270, loss = 0.28 (1512.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:28:27.142000: step 240280, loss = 0.29 (1416.9 examples/sec; 0.090 sec/batch)
2017-06-02 08:28:28.033979: step 240290, loss = 0.28 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:28:29.028972: step 240300, loss = 0.24 (1286.4 examples/sec; 0.099 sec/batch)
2017-06-02 08:28:29.805795: step 240310, loss = 0.33 (1647.7 examples/sec; 0.078 sec/batch)
2017-06-02 08:28:30.687472: step 240320, loss = 0.25 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:28:31.533155: step 240330, loss = 0.27 (1513.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:28:32.419972: step 240340, loss = 0.29 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 08:28:33.273451: step 240350, loss = 0.26 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:28:34.125442: step 240360, loss = 0.22 (1502.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:28:34.970259: step 240370, loss = 0.31 (1515.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:28:35.823835: step 240380, loss = 0.24 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:28:36.699230: step 240390, loss = 0.34 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:28:37.674665: step 240400, loss = 0.32 (1312.2 examples/sec; 0.098 sec/batch)
2017-06-02 08:28:38.416370: step 240410, loss = 0.30 (1725.7 examples/sec; 0.074 sec/batch)
2017-06-02 08:28:39.291985: step 240420, loss = 0.25 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:28:40.184121: step 240430, loss = 0.26 (1434.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:28:41.035838: step 240440, loss = 0.29 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:28:41.867925: step 240450, loss = 0.31 (1538.3 examples/sec; 0.083 sec/batch)
2017-06-02 08:28:42.733325: step 240460, loss = 0.26 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:28:43.600451: step 240470, loss = 0.27 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:28:44.440585: step 240480, loss = 0.27 (1523.6 examples/sec; 0.084 sec/batch)
2017-06-02 08:28:45.302963: step 240490, loss = 0.34 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:28:46.264608: step 240500, loss = 0.30 (1331.1 examples/sec; 0.096 sec/batch)
2017-06-02 08:28:47.031944: step 240510, loss = 0.31 (1668.1 examples/sec; 0.077 sec/batch)
2017-06-02 08:28:47.884982: step 240520, loss = 0.26 (1500.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:28:48.765905: step 240530, loss = 0.32 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:28:49.637447: step 240540, loss = 0.35 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:28:50.472220: step 240550, loss = 0.34 (1533.4 examples/sec; 0.083 sec/batch)
2017-06-02 08:28:51.365744: step 240560, loss = 0.34 (1432.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:28:52.227816: step 240570, loss = 0.25 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:28:53.099791: step 240580, loss = 0.32 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:28:53.975346: step 240590, loss = 0.30 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:28:54.933891: step 240600, loss = 0.39 (1335.4 examples/sec; 0.096 sec/batch)
2017-06-02 08:28:55.707299: step 240610, loss = 0.38 (1655.0 examples/sec; 0.077 sec/batch)
2017-06-02 08:28:56.550297: step 240620, loss = 0.31 (1518.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:28:57.413726: step 240630, loss = 0.34 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:28:58.293240: step 240640, loss = 0.25 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:28:59.178477: step 240650, loss = 0.33 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:29:00.057175: step 240660, loss = 0.39 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:29:00.921058: step 240670, loss = 0.32 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:29:01.798948: step 240680, loss = 0.28 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:29:02.675008: step 240690, loss = 0.26 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:29:03.677869: step 240700, loss = 0.27 (1276.3 examples/sec; 0.100 sec/batch)
2017-06-02 08:29:04.424506: step 240710, loss = 0.28 (1714.3 examples/sec; 0.075 sec/batch)
2017-06-02 08:29:05.286844: step 240720, loss = 0.35 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:29:06.158461: step 240730, loss = 0.24 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:29:07.037210: step 240740, loss = 0.25 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:29:07.909277: step 240750, loss = 0.24 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:29:08.786637: step 240760, loss = 0.32 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:29:09.643864: step 240770, loss = 0.30 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:29:10.508086: step 240780, loss = 0.27 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:29:11.416831: step 240790, loss = 0.27 (1408.6 examples/sec; 0.091 sec/batch)
2017-06-02 08:29:12.411929: step 240800, loss = 0.27 (1286.3 examples/sec; 0.100 sec/batch)
2017-06-02 08:29:13.143413: step 240810, loss = 0.38 (1749.9 examples/sec; 0.073 sec/batch)
2017-06-02 08:29:14.022938: step 240820, loss = 0.30 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:29:14.865221: step 240830, loss = 0.28 (1519.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:29:15.705205: step 240840, loss = 0.25 (1523.8 examples/sec; 0.084 sec/batch)
2017-06-02 08:29:16.565470: step 240850, loss = 0.33 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:29:17.422301: step 240860, loss = 0.39 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:29:18.285378: step 240870, loss = 0.34 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:29:19.125364: step 240880, loss = 0.20 (1523.9 examples/sec; 0.084 sec/batch)
2017-06-02 08:29:20.018854: step 240890, loss = 0.26 (1432.6 examples/sec; 0.089 sec/batch)
2017-06-02 08:29:20.975684: step 240900, loss = 0.24 (1337.7 examples/sec; 0.096 sec/batch)
2017-06-02 08:29:21.749901: step 240910, loss = 0.22 (1653.3 examples/sec; 0.077 sec/batch)
2017-06-02 08:29:22.623463: step 240920, loss = 0.26 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:29:23.483126: step 240930, loss = 0.28 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:29:24.359222: step 240940, loss = 0.27 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:29:25.229864: step 240950, loss = 0.20 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:29:26.085797: step 240960, loss = 0.33 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:29:26.942855: step 240970, loss = 0.32 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:29:27.798113: step 240980, loss = 0.31 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:29:28.685969: step 240990, loss = 0.39 (1441.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:29:29.679157: step 241000, loss = 0.41 (1288.8 examples/sec; 0.099 sec/batch)
2017-06-02 08:29:30.431171: step 241010, loss = 0.27 (1702.1 examples/sec; 0.075 sec/batch)
2017-06-02 08:29:31.269974: step 241020, loss = 0.21 (1526.0 examples/sec; 0.084 sec/batch)
2017-06-02 08:29:32.134610: step 241030, loss = 0.29 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:29:33.017087: step 241040, loss = 0.23 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:29:33.896721: step 241050, loss = 0.29 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:29:34.742790: step 241060, loss = 0.33 (1512.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:29:35.613832: step 241070, loss = 0.31 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:29:36.478930: step 241080, loss = 0.31 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:29:37.337884: step 241090, loss = 0.19 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:29:38.304945: step 241100, loss = 0.35 (1323.6 examples/sec; 0.097 sec/batch)
2017-06-02 08:29:39.066642: step 241110, loss = 0.29 (1680.5 examples/sec; 0.076 sec/batch)
2017-06-02 08:29:39.923073: step 241120, loss = 0.31 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:29:40.779191: step 241130, loss = 0.39 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:29:41.640778: step 241140, loss = 0.20 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:29:42.495713: step 241150, loss = 0.28 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:29:43.372275: step 241160, loss = 0.32 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:29:44.230006: step 241170, loss = 0.30 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:29:45.088218: step 241180, loss = 0.33 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:29:45.949256: step 241190, loss = 0.33 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:29:46.894782: step 241200, loss = 0.31 (1353.7 examples/sec; 0.095 sec/batch)
2017-06-02 08:29:47.668862: step 241210, loss = 0.31 (1653.6 examples/sec; 0.077 sec/batch)
2017-06-02 08:29:48.517760: step 241220, loss = 0.29 (1507.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:29:49.389184: step 241230, loss = 0.29 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:29:50.264067: step 241240, loss = 0.31 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:29:51.133265: step 241250, loss = 0.31 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:29:51.987765: step 241260, loss = 0.27 (1498.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:29:52.903384: step 241270, loss = 0.24 (1398.0 examples/sec; 0.092 sec/batch)
2017-06-02 08:29:53.758754: step 241280, loss = 0.30 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:29:54.594179: step 241290, loss = 0.27 (1532.2 examples/sec; 0.084 sec/batch)
2017-06-02 08:29:55.559945: step 241300, loss = 0.33 (1325.4 examples/sec; 0.097 sec/batch)
2017-06-02 08:29:56.321059: step 241310, loss = 0.27 (1681.7 examples/sec; 0.076 sec/batch)
2017-06-02 08:29:57.175027: step 241320, loss = 0.35 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:29:58.040652: step 241330, loss = 0.26 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:29:58.900158: step 241340, loss = 0.32 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:29:59.752662: step 241350, loss = 0.29 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:30:00.611039: step 241360, loss = 0.25 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:30:01.503095: step 241370, loss = 0.38 (1434.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:30:02.377663: step 241380, loss = 0.26 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:30:03.239165: step 241390, loss = 0.30 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:30:04.218897: step 241400, loss = 0.29 (1306.5 examples/sec; 0.098 sec/batch)
2017-06-02 08:30:04.966814: step 241410, loss = 0.26 (1711.4 examples/sec; 0.075 sec/batch)
2017-06-02 08:30:05.812528: step 241420, loss = 0.36 (1513.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:30:06.684251: step 241430, loss = 0.29 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:30:07.556785: step 241440, loss = 0.26 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:30:08.404275: step 241450, loss = 0.37 (1510.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:30:09.288336: step 241460, loss = 0.30 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:30:10.147142: step 241470, loss = 0.34 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:30:11.012516: step 241480, loss = 0.36 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:30:11.885590: step 241490, loss = 0.44 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:30:12.874609: step 241500, loss = 0.32 (1294.2 examples/sec; 0.099 sec/batch)
2017-06-02 08:30:13.648973: step 241510, loss = 0.28 (1653.0 examples/sec; 0.077 sec/batch)
2017-06-02 08:30:14.496978: step 241520, loss = 0.25 (1509.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:30:15.351555: step 241530, loss = 0.24 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:30:16.218905: step 241540, loss = 0.31 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:30:17.092199: step 241550, loss = 0.41 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:30:17.956215: step 241560, loss = 0.25 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:30:18.838743: step 241570, loss = 0.27 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:30:19.703992: step 241580, loss = 0.22 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:30:20.580468: step 241590, loss = 0.28 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:30:21.558246: step 241600, loss = 0.22 (1309.1 examples/sec; 0.098 sec/batch)
2017-06-02 08:30:22.301415: step 241610, loss = 0.37 (1722.3 examples/sec; 0.074 sec/batch)
2017-06-02 08:30:23.201049: step 241620, loss = 0.28 (1422.8 examples/sec; 0.090 sec/batch)
2017-06-02 08:30:24.056204: step 241630, loss = 0.38 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:30:24.915320: step 241640, loss = 0.43 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:30:25.805295: step 241650, loss = 0.24 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:30:26.655346: step 241660, loss = 0.38 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:30:27.525292: step 241670, loss = 0.32 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:30:28.405508: step 241680, loss = 0.31 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:30:29.268354: step 241690, loss = 0.34 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:30:30.220279: step 241700, loss = 0.25 (1344.6 examples/sec; 0.095 sec/batch)
2017-06-02 08:30:30.996797: step 241710, loss = 0.31 (1648.4 examples/sec; 0.078 sec/batch)
2017-06-02 08:30:31.828631: step 241720, loss = 0.32 (1538.8 examples/sec; 0.083 sec/batch)
2017-06-02 08:30:32.696561: step 241730, loss = 0.32 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:30:33.551438: step 241740, loss = 0.30 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:30:34.421564: step 241750, loss = 0.28 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:30:35.296966: step 241760, loss = 0.26 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:30:36.150003: step 241770, loss = 0.39 (1500.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:30:36.995454: step 241780, loss = 0.36 (1514.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:30:37.880302: step 241790, loss = 0.31 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:30:38.856242: step 241800, loss = 0.29 (1311.6 examples/sec; 0.098 sec/batch)
2017-06-02 08:30:39.615013: step 241810, loss = 0.29 (1686.9 examples/sec; 0.076 sec/batch)
2017-06-02 08:30:40.497914: step 241820, loss = 0.28 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:30:41.373476: step 241830, loss = 0.26 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:30:42.227173: step 241840, loss = 0.26 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:30:43.080810: step 241850, loss = 0.32 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:30:43.952883: step 241860, loss = 0.27 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:30:44.818904: step 241870, loss = 0.29 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:30:45.685921: step 241880, loss = 0.26 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:30:46.530464: step 241890, loss = 0.36 (1515.6 examples/sec; 0.084 sec/batch)
2017-06-02 08:30:47.460914: step 241900, loss = 0.31 (1375.7 examples/sec; 0.093 sec/batch)
2017-06-02 08:30:48.211366: step 241910, loss = 0.38 (1705.7 examples/sec; 0.075 sec/batch)
2017-06-02 08:30:49.073539: step 241920, loss = 0.30 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:30:49.942999: step 241930, loss = 0.28 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:30:50.790575: step 241940, loss = 0.31 (1510.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:30:51.661516: step 241950, loss = 0.27 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:30:52.527303: step 241960, loss = 0.26 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:30:53.381479: step 241970, loss = 0.25 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:30:54.261374: step 241980, loss = 0.36 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:30:55.143768: step 241990, loss = 0.28 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:30:56.152331: step 242000, loss = 0.25 (1269.1 examples/sec; 0.101 sec/batch)
2017-06-02 08:30:56.870212: step 242010, loss = 0.34 (1783.0 examples/sec; 0.072 sec/batch)
2017-06-02 08:30:57.757804: step 242020, loss = 0.32 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 08:30:58.630530: step 242030, loss = 0.28 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:30:59.500008: step 242040, loss = 0.28 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:31:00.384639: step 242050, loss = 0.30 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:31:01.260392: step 242060, loss = 0.25 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:31:02.113975: step 242070, loss = 0.26 (1499.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:31:02.976266: step 242080, loss = 0.31 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:31:03.855091: step 242090, loss = 0.30 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:31:04.839087: step 242100, loss = 0.33 (1300.8 examples/sec; 0.098 sec/batch)
2017-06-02 08:31:05.613869: step 242110, loss = 0.27 (1652.1 examples/sec; 0.077 sec/batch)
2017-06-02 08:31:06.487562: step 242120, loss = 0.34 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:31:07.331859: step 242130, loss = 0.37 (1516.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:31:08.210151: step 242140, loss = 0.21 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:31:09.075091: step 242150, loss = 0.32 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:31:09.928693: step 242160, loss = 0.19 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:31:10.782856: step 242170, loss = 0.25 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:31:11.670479: step 242180, loss = 0.26 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 08:31:12.526822: step 242190, loss = 0.26 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:31:13.510206: step 242200, loss = 0.31 (1301.6 examples/sec; 0.098 sec/batch)
2017-06-02 08:31:14.282442: step 242210, loss = 0.36 (1657.5 examples/sec; 0.077 sec/batch)
2017-06-02 08:31:15.155832: step 242220, loss = 0.35 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:31:16.001787: step 242230, loss = 0.35 (1513.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:31:16.866754: step 242240, loss = 0.32 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:31:17.714206: step 242250, loss = 0.31 (1510.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:31:18.572361: step 242260, loss = 0.33 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:31:19.428138: step 242270, loss = 0.38 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:31:20.296939: step 242280, loss = 0.33 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:31:21.173681: step 242290, loss = 0.21 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:31:22.135351: step 242300, loss = 0.28 (1331.0 examples/sec; 0.096 sec/batch)
2017-06-02 08:31:22.907301: step 242310, loss = 0.24 (1658.2 examples/sec; 0.077 sec/batch)
2017-06-02 08:31:23.771993: step 242320, loss = 0.25 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:31:24.651446: step 242330, loss = 0.27 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:31:25.527566: step 242340, loss = 0.24 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:31:26.369322: step 242350, loss = 0.29 (1520.6 examples/sec; 0.084 sec/batch)
2017-06-02 08:31:27.240359: step 242360, loss = 0.30 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:31:28.104148: step 242370, loss = 0.28 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:31:28.937153: step 242380, loss = 0.28 (1536.6 examples/sec; 0.083 sec/batch)
2017-06-02 08:31:29.790388: step 242390, loss = 0.33 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:31:30.756838: step 242400, loss = 0.42 (1324.4 examples/sec; 0.097 sec/batch)
2017-06-02 08:31:31.525834: step 242410, loss = 0.34 (1664.5 examples/sec; 0.077 sec/batch)
2017-06-02 08:31:32.396485: step 242420, loss = 0.26 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:31:33.260325: step 242430, loss = 0.33 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:31:34.108026: step 242440, loss = 0.22 (1510.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:31:34.949077: step 242450, loss = 0.30 (1521.9 examples/sec; 0.084 sec/batch)
2017-06-02 08:31:35.831026: step 242460, loss = 0.23 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:31:36.677051: step 242470, loss = 0.29 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:31:37.573445: step 242480, loss = 0.33 (1427.9 examples/sec; 0.090 sec/batch)
2017-06-02 08:31:38.416900: step 242490, loss = 0.25 (1517.6 examples/sec; 0.084 sec/batch)
2017-06-02 08:31:39.392513: step 242500, loss = 0.34 (1312.0 examples/sec; 0.098 sec/batch)
2017-06-02 08:31:40.152617: step 242510, loss = 0.31 (1684.0 examples/sec; 0.076 sec/batch)
2017-06-02 08:31:40.996767: step 242520, loss = 0.35 (1516.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:31:41.863483: step 242530, loss = 0.26 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:31:42.759263: step 242540, loss = 0.37 (1429.0 examples/sec; 0.090 sec/batch)
2017-06-02 08:31:43.619603: step 242550, loss = 0.28 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:31:44.481511: step 242560, loss = 0.25 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:31:45.363847: step 242570, loss = 0.34 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:31:46.234713: step 242580, loss = 0.30 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:31:47.115703: step 242590, loss = 0.40 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:31:48.099601: step 242600, loss = 0.33 (1300.9 examples/sec; 0.098 sec/batch)
2017-06-02 08:31:48.856331: step 242610, loss = 0.30 (1691.5 examples/sec; 0.076 sec/batch)
2017-06-02 08:31:49.734258: step 242620, loss = 0.28 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:31:50.591911: step 242630, loss = 0.28 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:31:51.436879: step 242640, loss = 0.35 (1514.9 examples/sec; 0.084 sec/batch)
2017-06-02 08:31:52.308611: step 242650, loss = 0.28 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:31:53.186703: step 242660, loss = 0.26 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:31:54.053407: step 242670, loss = 0.31 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:31:54.917850: step 242680, loss = 0.27 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:31:55.769829: step 242690, loss = 0.31 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:31:56.728705: step 242700, loss = 0.27 (1334.9 examples/sec; 0.096 sec/batch)
2017-06-02 08:31:57.513749: step 242710, loss = 0.31 (1630.5 examples/sec; 0.079 sec/batch)
2017-06-02 08:31:58.386895: step 242720, loss = 0.36 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:31:59.236644: step 242730, loss = 0.30 (1506.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:32:00.113874: step 242740, loss = 0.34 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:32:00.979428: step 242750, loss = 0.29 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:32:01.838668: step 242760, loss = 0.30 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:32:02.703872: step 242770, loss = 0.26 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:32:03.591870: step 242780, loss = 0.32 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 08:32:04.467376: step 242790, loss = 0.29 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:32:05.464656: step 242800, loss = 0.34 (1283.5 examples/sec; 0.100 sec/batch)
2017-06-02 08:32:06.252977: step 242810, loss = 0.27 (1623.7 examples/sec; 0.079 sec/batch)
2017-06-02 08:32:07.132463: step 242820, loss = 0.25 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:32:07.991200: step 242830, loss = 0.38 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:32:08.841272: step 242840, loss = 0.29 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:32:09.731485: step 242850, loss = 0.23 (1437.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:32:10.583646: step 242860, loss = 0.25 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:32:11.486439: step 242870, loss = 0.33 (1417.8 examples/sec; 0.090 sec/batch)
2017-06-02 08:32:12.303157: step 242880, loss = 0.26 (1567.3 examples/sec; 0.082 sec/batch)
2017-06-02 08:32:13.157313: step 242890, loss = 0.28 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:32:14.201393: step 242900, loss = 0.23 (1225.9 examples/sec; 0.104 sec/batch)
2017-06-02 08:32:14.949727: step 242910, loss = 0.24 (1710.5 examples/sec; 0.075 sec/batch)
2017-06-02 08:32:15.836346: step 242920, loss = 0.32 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:32:16.686243: step 242930, loss = 0.36 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:32:17.561864: step 242940, loss = 0.26 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:32:18.424287: step 242950, loss = 0.26 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:32:19.266417: step 242960, loss = 0.35 (1520.0 examples/sec; 0.084 sec/batch)
2017-06-02 08:32:20.090982: step 242970, loss = 0.32 (1552.3 examples/sec; 0.082 sec/batch)
2017-06-02 08:32:20.958827: step 242980, loss = 0.22 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:32:21.791684: step 242990, loss = 0.26 (1536.9 examples/sec; 0.083 sec/batch)
2017-06-02 08:32:22.749693: step 243000, loss = 0.34 (1336.1 examples/sec; 0.096 sec/batch)
2017-06-02 08:32:23.502546: step 243010, loss = 0.39 (1700.2 examples/sec; 0.075 sec/batch)
2017-06-02 08:32:24.368552: step 243020, loss = 0.27 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:32:25.245443: step 243030, loss = 0.37 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:32:26.100621: step 243040, loss = 0.27 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:32:26.981985: step 243050, loss = 0.24 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:32:27.876042: step 243060, loss = 0.29 (1431.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:32:28.738092: step 243070, loss = 0.26 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:32:29.598085: step 243080, loss = 0.30 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:32:30.458918: step 243090, loss = 0.27 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:32:31.489425: step 243100, loss = 0.28 (1242.1 examples/sec; 0.103 sec/batch)
2017-06-02 08:32:32.206669: step 243110, loss = 0.27 (1784.6 examples/sec; 0.072 sec/batch)
2017-06-02 08:32:33.077700: step 243120, loss = 0.29 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:32:33.928856: step 243130, loss = 0.27 (1503.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:32:34.781060: step 243140, loss = 0.35 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:32:35.659202: step 243150, loss = 0.28 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:32:36.516864: step 243160, loss = 0.29 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:32:37.372877: step 243170, loss = 0.36 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:32:38.236761: step 243180, loss = 0.33 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:32:39.104202: step 243190, loss = 0.22 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:32:40.065146: step 243200, loss = 0.35 (1332.0 examples/sec; 0.096 sec/batch)
2017-06-02 08:32:40.827163: step 243210, loss = 0.30 (1679.8 examples/sec; 0.076 sec/batch)
2017-06-02 08:32:41.693474: step 243220, loss = 0.34 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:32:42.560156: step 243230, loss = 0.27 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:32:43.402723: step 243240, loss = 0.24 (1519.2 examples/sec; 0.084 sec/batch)
2017-06-02 08:32:44.277374: step 243250, loss = 0.29 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:32:45.133721: step 243260, loss = 0.29 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:32:45.988700: step 243270, loss = 0.40 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:32:46.851475: step 243280, loss = 0.28 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:32:47.695107: step 243290, loss = 0.28 (1517.3 examples/sec; 0.084 sec/batch)
2017-06-02 08:32:48.680110: step 243300, loss = 0.27 (1299.5 examples/sec; 0.099 sec/batch)
2017-06-02 08:32:49.452903: step 243310, loss = 0.30 (1656.3 examples/sec; 0.077 sec/batch)
2017-06-02 08:32:50.332674: step 243320, loss = 0.27 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:32:51.191599: step 243330, loss = 0.26 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:32:52.062784: step 243340, loss = 0.22 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:32:52.933443: step 243350, loss = 0.25 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:32:53.784215: step 243360, loss = 0.32 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:32:54.655510: step 243370, loss = 0.36 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:32:55.524330: step 243380, loss = 0.31 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:32:56.397214: step 243390, loss = 0.29 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:32:57.388293: step 243400, loss = 0.30 (1291.5 examples/sec; 0.099 sec/batch)
2017-06-02 08:32:58.166956: step 243410, loss = 0.26 (1643.8 examples/sec; 0.078 sec/batch)
2017-06-02 08:32:59.034246: step 243420, loss = 0.28 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:32:59.901940: step 243430, loss = 0.27 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:33:00.756634: step 243440, loss = 0.32 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:33:01.615841: step 243450, loss = 0.27 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:33:02.475511: step 243460, loss = 0.31 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:33:03.338399: step 243470, loss = 0.26 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:33:04.212320: step 243480, loss = 0.32 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:33:05.083174: step 243490, loss = 0.30 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:33:06.030193: step 243500, loss = 0.32 (1351.6 examples/sec; 0.095 sec/batch)
2017-06-02 08:33:06.769420: step 243510, loss = 0.35 (1731.5 examples/sec; 0.074 sec/batch)
2017-06-02 08:33:07.634280: step 243520, loss = 0.28 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:33:08.500334: step 243530, loss = 0.29 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:33:09.359003: step 243540, loss = 0.28 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:33:10.225425: step 243550, loss = 0.41 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:33:11.108669: step 243560, loss = 0.24 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:33:11.980351: step 243570, loss = 0.29 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:33:12.859501: step 243580, loss = 0.31 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:33:13.751838: step 243590, loss = 0.33 (1434.4 examples/sec; 0.089 sec/batch)
2017-06-02 08:33:14.722261: step 243600, loss = 0.33 (1319.0 examples/sec; 0.097 sec/batch)
2017-06-02 08:33:15.509301: step 243610, loss = 0.35 (1626.3 examples/sec; 0.079 sec/batch)
2017-06-02 08:33:16.405091: step 243620, loss = 0.38 (1428.9 examples/sec; 0.090 sec/batch)
2017-06-02 08:33:17.270248: step 243630, loss = 0.21 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:33:18.135512: step 243640, loss = 0.38 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:33:19.008030: step 243650, loss = 0.26 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:33:19.884475: step 243660, loss = 0.22 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:33:20.730376: step 243670, loss = 0.28 (1513.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:33:21.614742: step 243680, loss = 0.29 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:33:22.460897: step 243690, loss = 0.22 (1512.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:33:23.447051: step 243700, loss = 0.29 (1298.0 examples/sec; 0.099 sec/batch)
2017-06-02 08:33:24.225143: step 243710, loss = 0.31 (1645.0 examples/sec; 0.078 sec/batch)
2017-06-02 08:33:25.074583: step 243720, loss = 0.36 (1506.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:33:25.966427: step 243730, loss = 0.23 (1435.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:33:26.834535: step 243740, loss = 0.32 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:33:27.728206: step 243750, loss = 0.30 (1432.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:33:28.587720: step 243760, loss = 0.24 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:33:29.428454: step 243770, loss = 0.19 (1522.5 examples/sec; 0.084 sec/batch)
2017-06-02 08:33:30.314127: step 243780, loss = 0.27 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:33:31.164235: step 243790, loss = 0.46 (1505.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:33:32.126358: step 243800, loss = 0.27 (1330.4 examples/sec; 0.096 sec/batch)
2017-06-02 08:33:32.857616: step 243810, loss = 0.28 (1750.4 examples/sec; 0.073 sec/batch)
2017-06-02 08:33:33.677524: step 243820, loss = 0.34 (1561.2 examples/sec; 0.082 sec/batch)
2017-06-02 08:33:34.549988: step 243830, loss = 0.31 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:33:35.431802: step 243840, loss = 0.25 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:33:36.312370: step 243850, loss = 0.27 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:33:37.193835: step 243860, loss = 0.21 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:33:38.076980: step 243870, loss = 0.22 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:33:38.960965: step 243880, loss = 0.24 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:33:39.829453: step 243890, loss = 0.27 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:33:40.802821: step 243900, loss = 0.23 (1315.0 examples/sec; 0.097 sec/batch)
2017-06-02 08:33:41.601402: step 243910, loss = 0.31 (1602.9 examples/sec; 0.080 sec/batch)
2017-06-02 08:33:42.483276: step 243920, loss = 0.40 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:33:43.370319: step 243930, loss = 0.39 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:33:44.232047: step 243940, loss = 0.42 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:33:45.129465: step 243950, loss = 0.29 (1426.3 examples/sec; 0.090 sec/batch)
2017-06-02 08:33:46.000634: step 243960, loss = 0.30 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:33:46.882990: step 243970, loss = 0.30 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:33:47.726149: step 243980, loss = 0.31 (1518.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:33:48.613660: step 243990, loss = 0.28 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:33:49.621150: step 244000, loss = 0.28 (1270.5 examples/sec; 0.101 sec/batch)
2017-06-02 08:33:50.338165: step 244010, loss = 0.27 (1785.2 examples/sec; 0.072 sec/batch)
2017-06-02 08:33:51.212924: step 244020, loss = 0.30 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:33:52.081899: step 244030, loss = 0.29 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:33:52.940451: step 244040, loss = 0.31 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:33:53.804946: step 244050, loss = 0.26 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:33:54.664857: step 244060, loss = 0.32 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:33:55.540574: step 244070, loss = 0.34 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:33:56.394408: step 244080, loss = 0.25 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:33:57.247467: step 244090, loss = 0.22 (1500.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:33:58.229316: step 244100, loss = 0.32 (1303.7 examples/sec; 0.098 sec/batch)
2017-06-02 08:33:58.994226: step 244110, loss = 0.23 (1673.4 examples/sec; 0.076 sec/batch)
2017-06-02 08:33:59.848006: step 244120, loss = 0.26 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:34:00.710634: step 244130, loss = 0.24 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:34:01.588802: step 244140, loss = 0.38 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:34:02.432345: step 244150, loss = 0.25 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:34:03.295611: step 244160, loss = 0.28 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:34:04.186378: step 244170, loss = 0.22 (1437.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:34:05.062956: step 244180, loss = 0.27 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:34:05.914085: step 244190, loss = 0.25 (1503.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:34:06.885012: step 244200, loss = 0.38 (1318.3 examples/sec; 0.097 sec/batch)
2017-06-02 08:34:07.649919: step 244210, loss = 0.26 (1673.4 examples/sec; 0.076 sec/batch)
2017-06-02 08:34:08.522255: step 244220, loss = 0.30 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:34:09.370440: step 244230, loss = 0.27 (1509.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:34:10.238914: step 244240, loss = 0.29 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:34:11.116655: step 244250, loss = 0.27 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:34:11.960723: step 244260, loss = 0.35 (1516.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:34:12.799107: step 244270, loss = 0.19 (1526.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:34:13.658211: step 244280, loss = 0.27 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:34:14.518955: step 244290, loss = 0.26 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:34:15.551866: step 244300, loss = 0.21 (1239.2 examples/sec; 0.103 sec/batch)
2017-06-02 08:34:16.248540: step 244310, loss = 0.29 (1837.3 examples/sec; 0.070 sec/batch)
2017-06-02 08:34:17.102720: step 244320, loss = 0.33 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:34:17.957506: step 244330, loss = 0.28 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:34:18.782330: step 244340, loss = 0.29 (1551.8 examples/sec; 0.082 sec/batch)
2017-06-02 08:34:19.629250: step 244350, loss = 0.31 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:34:20.517907: step 244360, loss = 0.34 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 08:34:21.388418: step 244370, loss = 0.36 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:34:22.245239: step 244380, loss = 0.30 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:34:23.101414: step 244390, loss = 0.32 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:34:24.084253: step 244400, loss = 0.29 (1302.3 examples/sec; 0.098 sec/batch)
2017-06-02 08:34:24.829525: step 244410, loss = 0.28 (1717.6 examples/sec; 0.075 sec/batch)
2017-06-02 08:34:25.712053: step 244420, loss = 0.25 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:34:26.581428: step 244430, loss = 0.31 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:34:27.426067: step 244440, loss = 0.26 (1515.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:34:28.270113: step 244450, loss = 0.28 (1516.5 examples/sec; 0.084 sec/batch)
2017-06-02 08:34:29.143751: step 244460, loss = 0.31 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:34:29.985495: step 244470, loss = 0.32 (1520.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:34:30.836484: step 244480, loss = 0.31 (1504.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:34:31.717969: step 244490, loss = 0.26 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:34:32.682657: step 244500, loss = 0.30 (1326.8 examples/sec; 0.096 sec/batch)
2017-06-02 08:34:33.458116: step 244510, loss = 0.21 (1650.6 examples/sec; 0.078 sec/batch)
2017-06-02 08:34:34.316629: step 244520, loss = 0.24 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:34:35.179258: step 244530, loss = 0.31 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:34:36.036347: step 244540, loss = 0.24 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:34:36.916331: step 244550, loss = 0.32 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:34:37.764556: step 244560, loss = 0.24 (1509.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:34:38.606812: step 244570, loss = 0.26 (1519.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:34:39.459326: step 244580, loss = 0.37 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:34:40.313608: step 244590, loss = 0.38 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:34:41.262492: step 244600, loss = 0.21 (1348.9 examples/sec; 0.095 sec/batch)
2017-06-02 08:34:42.046015: step 244610, loss = 0.25 (1633.7 examples/sec; 0.078 sec/batch)
2017-06-02 08:34:42.905576: step 244620, loss = 0.27 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:34:43.773333: step 244630, loss = 0.31 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:34:44.667815: step 244640, loss = 0.34 (1431.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:34:45.531435: step 244650, loss = 0.28 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:34:46.401139: step 244660, loss = 0.27 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:34:47.281406: step 244670, loss = 0.27 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:34:48.146224: step 244680, loss = 0.37 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:34:49.014880: step 244690, loss = 0.35 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:34:49.978903: step 244700, loss = 0.27 (1327.8 examples/sec; 0.096 sec/batch)
2017-06-02 08:34:50.755707: step 244710, loss = 0.31 (1647.8 examples/sec; 0.078 sec/batch)
2017-06-02 08:34:51.625295: step 244720, loss = 0.30 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:34:52.501475: step 244730, loss = 0.29 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:34:53.374037: step 244740, loss = 0.27 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:34:54.261310: step 244750, loss = 0.32 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 08:34:55.145327: step 244760, loss = 0.41 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:34:56.042684: step 244770, loss = 0.24 (1426.4 examples/sec; 0.090 sec/batch)
2017-06-02 08:34:56.940993: step 244780, loss = 0.21 (1424.9 examples/sec; 0.090 sec/batch)
2017-06-02 08:34:57.835430: step 244790, loss = 0.35 (1431.1 examples/sec; 0.089 sec/batch)
2017-06-02 08:34:58.820558: step 244800, loss = 0.32 (1299.3 examples/sec; 0.099 sec/batch)
2017-06-02 08:34:59.588688: step 244810, loss = 0.21 (1666.4 examples/sec; 0.077 sec/batch)
2017-06-02 08:35:00.464317: step 244820, loss = 0.28 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:35:01.351272: step 244830, loss = 0.28 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 08:35:02.259857: step 244840, loss = 0.21 (1408.8 examples/sec; 0.091 sec/batch)
2017-06-02 08:35:03.128482: step 244850, loss = 0.24 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:35:04.035911: step 244860, loss = 0.28 (1410.6 examples/sec; 0.091 sec/batch)
2017-06-02 08:35:04.910010: step 244870, loss = 0.31 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:35:05.787222: step 244880, loss = 0.26 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:35:06.636427: step 244890, loss = 0.26 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:35:07.595725: step 244900, loss = 0.29 (1334.3 examples/sec; 0.096 sec/batch)
2017-06-02 08:35:08.382338: step 244910, loss = 0.25 (1627.2 examples/sec; 0.079 sec/batch)
2017-06-02 08:35:09.245277: step 244920, loss = 0.32 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:35:10.087484: step 244930, loss = 0.33 (1519.8 examples/sec; 0.084 sec/batch)
2017-06-02 08:35:10.979328: step 244940, loss = 0.39 (1435.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:35:11.829747: step 244950, loss = 0.32 (1505.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:35:12.706687: step 244960, loss = 0.25 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:35:13.580726: step 244970, loss = 0.23 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:35:14.440951: step 244980, loss = 0.33 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:35:15.319500: step 244990, loss = 0.39 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:35:16.289689: step 245000, loss = 0.24 (1319.3 examples/sec; 0.097 sec/batch)
2017-06-02 08:35:17.063879: step 245010, loss = 0.38 (1653.3 examples/sec; 0.077 sec/batch)
2017-06-02 08:35:17.939202: step 245020, loss = 0.27 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:35:18.825196: step 245030, loss = 0.34 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:35:19.706426: step 245040, loss = 0.29 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:35:20.569430: step 245050, loss = 0.22 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:35:21.465041: step 245060, loss = 0.30 (1429.2 examples/sec; 0.090 sec/batch)
2017-06-02 08:35:22.335744: step 245070, loss = 0.27 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:35:23.205508: step 245080, loss = 0.31 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:35:24.092760: step 245090, loss = 0.26 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:35:25.077204: step 245100, loss = 0.27 (1300.2 examples/sec; 0.098 sec/batch)
2017-06-02 08:35:25.870423: step 245110, loss = 0.32 (1613.7 examples/sec; 0.079 sec/batch)
2017-06-02 08:35:26.734475: step 245120, loss = 0.33 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:35:27.612141: step 245130, loss = 0.32 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:35:28.451429: step 245140, loss = 0.31 (1525.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:35:29.333699: step 245150, loss = 0.21 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:35:30.201521: step 245160, loss = 0.28 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:35:31.065251: step 245170, loss = 0.32 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:35:31.923434: step 245180, loss = 0.23 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:35:32.804961: step 245190, loss = 0.22 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:35:33.792275: step 245200, loss = 0.27 (1296.4 examples/sec; 0.099 sec/batch)
2017-06-02 08:35:34.544490: step 245210, loss = 0.26 (1701.6 examples/sec; 0.075 sec/batch)
2017-06-02 08:35:35.402769: step 245220, loss = 0.29 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:35:36.269491: step 245230, loss = 0.23 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:35:37.140397: step 245240, loss = 0.31 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:35:38.023608: step 245250, loss = 0.23 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:35:38.871863: step 245260, loss = 0.42 (1509.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:35:39.756965: step 245270, loss = 0.30 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:35:40.621238: step 245280, loss = 0.22 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:35:41.493662: step 245290, loss = 0.27 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:35:42.467864: step 245300, loss = 0.31 (1313.9 examples/sec; 0.097 sec/batch)
2017-06-02 08:35:43.231819: step 245310, loss = 0.33 (1675.5 examples/sec; 0.076 sec/batch)
2017-06-02 08:35:44.084535: step 245320, loss = 0.26 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:35:44.937940: step 245330, loss = 0.32 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:35:45.768081: step 245340, loss = 0.26 (1541.9 examples/sec; 0.083 sec/batch)
2017-06-02 08:35:46.619287: step 245350, loss = 0.39 (1503.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:35:47.473253: step 245360, loss = 0.38 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:35:48.313559: step 245370, loss = 0.30 (1523.3 examples/sec; 0.084 sec/batch)
2017-06-02 08:35:49.186489: step 245380, loss = 0.29 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:35:50.048008: step 245390, loss = 0.41 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:35:50.998405: step 245400, loss = 0.29 (1346.8 examples/sec; 0.095 sec/batch)
2017-06-02 08:35:51.784094: step 245410, loss = 0.29 (1629.1 examples/sec; 0.079 sec/batch)
2017-06-02 08:35:52.641267: step 245420, loss = 0.32 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:35:53.524998: step 245430, loss = 0.28 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:35:54.382102: step 245440, loss = 0.32 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:35:55.236144: step 245450, loss = 0.31 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:35:56.103306: step 245460, loss = 0.35 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:35:56.982356: step 245470, loss = 0.28 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:35:57.876972: step 245480, loss = 0.25 (1430.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:35:58.699158: step 245490, loss = 0.33 (1556.8 examples/sec; 0.082 sec/batch)
2017-06-02 08:35:59.641798: step 245500, loss = 0.35 (1357.9 examples/sec; 0.094 sec/batch)
2017-06-02 08:36:00.380653: step 245510, loss = 0.40 (1732.4 examples/sec; 0.074 sec/batch)
2017-06-02 08:36:01.269305: step 245520, loss = 0.29 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 08:36:02.147900: step 245530, loss = 0.26 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:36:02.988311: step 245540, loss = 0.26 (1523.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:36:03.835641: step 245550, loss = 0.29 (1510.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:36:04.699984: step 245560, loss = 0.25 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:36:05.566576: step 245570, loss = 0.31 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:36:06.413925: step 245580, loss = 0.26 (1510.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:36:07.278422: step 245590, loss = 0.44 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:36:08.261032: step 245600, loss = 0.31 (1302.6 examples/sec; 0.098 sec/batch)
2017-06-02 08:36:08.997930: step 245610, loss = 0.32 (1737.0 examples/sec; 0.074 sec/batch)
2017-06-02 08:36:09.850848: step 245620, loss = 0.22 (1500.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:36:10.719635: step 245630, loss = 0.27 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:36:11.564706: step 245640, loss = 0.26 (1514.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:36:12.439011: step 245650, loss = 0.21 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:36:13.297196: step 245660, loss = 0.31 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:36:14.139623: step 245670, loss = 0.25 (1519.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:36:15.012859: step 245680, loss = 0.27 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:36:15.893507: step 245690, loss = 0.32 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:36:16.856712: step 245700, loss = 0.32 (1328.9 examples/sec; 0.096 sec/batch)
2017-06-02 08:36:17.637672: step 245710, loss = 0.28 (1639.0 examples/sec; 0.078 sec/batch)
2017-06-02 08:36:18.494206: step 245720, loss = 0.36 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:36:19.370833: step 245730, loss = 0.24 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:36:20.219448: step 245740, loss = 0.32 (1508.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:36:21.111072: step 245750, loss = 0.31 (1435.6 examples/sec; 0.089 sec/batch)
2017-06-02 08:36:21.960562: step 245760, loss = 0.32 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:36:22.810751: step 245770, loss = 0.31 (1505.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:36:23.670399: step 245780, loss = 0.31 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:36:24.537185: step 245790, loss = 0.33 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:36:25.511697: step 245800, loss = 0.25 (1313.5 examples/sec; 0.097 sec/batch)
2017-06-02 08:36:26.252203: step 245810, loss = 0.33 (1728.5 examples/sec; 0.074 sec/batch)
2017-06-02 08:36:27.100830: step 245820, loss = 0.46 (1508.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:36:27.960118: step 245830, loss = 0.37 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:36:28.843143: step 245840, loss = 0.28 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:36:29.694216: step 245850, loss = 0.30 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:36:30.567237: step 245860, loss = 0.32 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:36:31.447441: step 245870, loss = 0.25 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:36:32.302790: step 245880, loss = 0.26 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:36:33.151122: step 245890, loss = 0.26 (1508.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:36:34.115930: step 245900, loss = 0.27 (1326.7 examples/sec; 0.096 sec/batch)
2017-06-02 08:36:34.887774: step 245910, loss = 0.29 (1658.4 examples/sec; 0.077 sec/batch)
2017-06-02 08:36:35.769309: step 245920, loss = 0.29 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:36:36.584344: step 245930, loss = 0.24 (1570.5 examples/sec; 0.082 sec/batch)
2017-06-02 08:36:37.430906: step 245940, loss = 0.34 (1512.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:36:38.300435: step 245950, loss = 0.29 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:36:39.155589: step 245960, loss = 0.31 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:36:40.021908: step 245970, loss = 0.25 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:36:40.879153: step 245980, loss = 0.48 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:36:41.759311: step 245990, loss = 0.33 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:36:42.729681: step 246000, loss = 0.32 (1319.1 examples/sec; 0.097 sec/batch)
2017-06-02 08:36:43.492434: step 246010, loss = 0.29 (1678.1 examples/sec; 0.076 sec/batch)
2017-06-02 08:36:44.365026: step 246020, loss = 0.24 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:36:45.254059: step 246030, loss = 0.33 (1439.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:36:46.089371: step 246040, loss = 0.28 (1532.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:36:46.962801: step 246050, loss = 0.30 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:36:47.821074: step 246060, loss = 0.26 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:36:48.712013: step 246070, loss = 0.28 (1436.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:36:49.557291: step 246080, loss = 0.34 (1514.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:36:50.428571: step 246090, loss = 0.29 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:36:51.417495: step 246100, loss = 0.23 (1294.4 examples/sec; 0.099 sec/batch)
2017-06-02 08:36:52.193424: step 246110, loss = 0.17 (1649.6 examples/sec; 0.078 sec/batch)
2017-06-02 08:36:53.055123: step 246120, loss = 0.31 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:36:53.919449: step 246130, loss = 0.29 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:36:54.782191: step 246140, loss = 0.30 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:36:55.672299: step 246150, loss = 0.30 (1438.1 examples/sec; 0.089 sec/batch)
2017-06-02 08:36:56.513911: step 246160, loss = 0.24 (1520.9 examples/sec; 0.084 sec/batch)
2017-06-02 08:36:57.397337: step 246170, loss = 0.24 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:36:58.269031: step 246180, loss = 0.38 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:36:59.140677: step 246190, loss = 0.31 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:37:00.129224: step 246200, loss = 0.25 (1294.8 examples/sec; 0.099 sec/batch)
2017-06-02 08:37:00.888709: step 246210, loss = 0.23 (1685.4 examples/sec; 0.076 sec/batch)
2017-06-02 08:37:01.787610: step 246220, loss = 0.31 (1424.0 examples/sec; 0.090 sec/batch)
2017-06-02 08:37:02.643260: step 246230, loss = 0.27 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:37:03.514183: step 246240, loss = 0.24 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:37:04.392019: step 246250, loss = 0.25 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:37:05.278775: step 246260, loss = 0.27 (1443.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:37:06.153182: step 246270, loss = 0.29 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:37:07.025245: step 246280, loss = 0.25 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:37:07.901872: step 246290, loss = 0.41 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:37:08.894702: step 246300, loss = 0.32 (1289.2 examples/sec; 0.099 sec/batch)
2017-06-02 08:37:09.653957: step 246310, loss = 0.35 (1685.9 examples/sec; 0.076 sec/batch)
2017-06-02 08:37:10.521086: step 246320, loss = 0.35 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:37:11.401144: step 246330, loss = 0.23 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:37:12.272880: step 246340, loss = 0.28 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:37:13.134529: step 246350, loss = 0.28 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:37:14.008301: step 246360, loss = 0.28 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:37:14.865548: step 246370, loss = 0.36 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:37:15.721997: step 246380, loss = 0.25 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:37:16.573176: step 246390, loss = 0.30 (1503.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:37:17.534830: step 246400, loss = 0.32 (1331.0 examples/sec; 0.096 sec/batch)
2017-06-02 08:37:18.285608: step 246410, loss = 0.27 (1704.9 examples/sec; 0.075 sec/batch)
2017-06-02 08:37:19.152722: step 246420, loss = 0.33 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:37:20.013899: step 246430, loss = 0.30 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:37:20.876183: step 246440, loss = 0.27 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:37:21.766783: step 246450, loss = 0.30 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:37:22.599173: step 246460, loss = 0.39 (1537.8 examples/sec; 0.083 sec/batch)
2017-06-02 08:37:23.455811: step 246470, loss = 0.45 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:37:24.327766: step 246480, loss = 0.24 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:37:25.188359: step 246490, loss = 0.30 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:37:26.174296: step 246500, loss = 0.30 (1298.3 examples/sec; 0.099 sec/batch)
2017-06-02 08:37:26.909254: step 246510, loss = 0.31 (1741.6 examples/sec; 0.073 sec/batch)
2017-06-02 08:37:27.791919: step 246520, loss = 0.31 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:37:28.664446: step 246530, loss = 0.33 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:37:29.525527: step 246540, loss = 0.32 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:37:30.405065: step 246550, loss = 0.29 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:37:31.283250: step 246560, loss = 0.27 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:37:32.153002: step 246570, loss = 0.30 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:37:33.043967: step 246580, loss = 0.27 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 08:37:33.925643: step 246590, loss = 0.33 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:37:34.900943: step 246600, loss = 0.27 (1312.4 examples/sec; 0.098 sec/batch)
2017-06-02 08:37:35.667002: step 246610, loss = 0.26 (1670.9 examples/sec; 0.077 sec/batch)
2017-06-02 08:37:36.546651: step 246620, loss = 0.35 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:37:37.400637: step 246630, loss = 0.33 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:37:38.271150: step 246640, loss = 0.27 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:37:39.135227: step 246650, loss = 0.28 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:37:40.003700: step 246660, loss = 0.34 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:37:40.892758: step 246670, loss = 0.25 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:37:41.766680: step 246680, loss = 0.30 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:37:42.592367: step 246690, loss = 0.25 (1550.2 examples/sec; 0.083 sec/batch)
2017-06-02 08:37:43.560303: step 246700, loss = 0.27 (1322.4 examples/sec; 0.097 sec/batch)
2017-06-02 08:37:44.323307: step 246710, loss = 0.31 (1677.6 examples/sec; 0.076 sec/batch)
2017-06-02 08:37:45.169399: step 246720, loss = 0.23 (1512.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:37:46.058038: step 246730, loss = 0.29 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 08:37:46.958673: step 246740, loss = 0.28 (1421.2 examples/sec; 0.090 sec/batch)
2017-06-02 08:37:47.847096: step 246750, loss = 0.23 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:37:48.714704: step 246760, loss = 0.31 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:37:49.612921: step 246770, loss = 0.31 (1425.0 examples/sec; 0.090 sec/batch)
2017-06-02 08:37:50.484014: step 246780, loss = 0.35 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:37:51.377341: step 246790, loss = 0.25 (1432.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:37:52.371627: step 246800, loss = 0.36 (1287.4 examples/sec; 0.099 sec/batch)
2017-06-02 08:37:53.137765: step 246810, loss = 0.29 (1670.7 examples/sec; 0.077 sec/batch)
2017-06-02 08:37:54.001936: step 246820, loss = 0.23 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:37:54.900132: step 246830, loss = 0.31 (1425.1 examples/sec; 0.090 sec/batch)
2017-06-02 08:37:55.805406: step 246840, loss = 0.26 (1414.0 examples/sec; 0.091 sec/batch)
2017-06-02 08:37:56.692089: step 246850, loss = 0.33 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 08:37:57.573723: step 246860, loss = 0.28 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:37:58.451594: step 246870, loss = 0.30 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:37:59.290765: step 246880, loss = 0.40 (1525.3 examples/sec; 0.084 sec/batch)
2017-06-02 08:38:00.137679: step 246890, loss = 0.31 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:38:01.106697: step 246900, loss = 0.30 (1320.9 examples/sec; 0.097 sec/batch)
2017-06-02 08:38:01.877707: step 246910, loss = 0.29 (1660.1 examples/sec; 0.077 sec/batch)
2017-06-02 08:38:02.751863: step 246920, loss = 0.28 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:38:03.636653: step 246930, loss = 0.25 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:38:04.506358: step 246940, loss = 0.31 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:38:05.381948: step 246950, loss = 0.31 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:38:06.266046: step 246960, loss = 0.34 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:38:07.147249: step 246970, loss = 0.30 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:38:08.047101: step 246980, loss = 0.22 (1422.4 examples/sec; 0.090 sec/batch)
2017-06-02 08:38:08.917144: step 246990, loss = 0.26 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:38:09.970561: step 247000, loss = 0.43 (1215.1 examples/sec; 0.105 sec/batch)
2017-06-02 08:38:10.716117: step 247010, loss = 0.24 (1716.8 examples/sec; 0.075 sec/batch)
2017-06-02 08:38:11.561053: step 247020, loss = 0.31 (1514.9 examples/sec; 0.084 sec/batch)
2017-06-02 08:38:12.430384: step 247030, loss = 0.26 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:38:13.307288: step 247040, loss = 0.30 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:38:14.161669: step 247050, loss = 0.22 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:38:15.019692: step 247060, loss = 0.29 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:38:15.893569: step 247070, loss = 0.36 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:38:16.728598: step 247080, loss = 0.21 (1532.9 examples/sec; 0.084 sec/batch)
2017-06-02 08:38:17.614935: step 247090, loss = 0.26 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 08:38:18.572884: step 247100, loss = 0.38 (1336.2 examples/sec; 0.096 sec/batch)
2017-06-02 08:38:19.353765: step 247110, loss = 0.33 (1639.1 examples/sec; 0.078 sec/batch)
2017-06-02 08:38:20.240236: step 247120, loss = 0.26 (1443.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:38:21.096161: step 247130, loss = 0.28 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:38:21.968110: step 247140, loss = 0.29 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:38:22.830096: step 247150, loss = 0.31 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:38:23.697180: step 247160, loss = 0.27 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:38:24.585826: step 247170, loss = 0.27 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:38:25.459413: step 247180, loss = 0.22 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:38:26.305827: step 247190, loss = 0.32 (1512.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:38:27.280689: step 247200, loss = 0.33 (1313.0 examples/sec; 0.097 sec/batch)
2017-06-02 08:38:28.029454: step 247210, loss = 0.31 (1709.5 examples/sec; 0.075 sec/batch)
2017-06-02 08:38:28.912684: step 247220, loss = 0.22 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:38:29.788062: step 247230, loss = 0.31 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:38:30.650367: step 247240, loss = 0.27 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:38:31.518603: step 247250, loss = 0.35 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:38:32.399622: step 247260, loss = 0.27 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:38:33.253709: step 247270, loss = 0.35 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:38:34.128872: step 247280, loss = 0.27 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:38:35.007373: step 247290, loss = 0.30 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:38:35.977506: step 247300, loss = 0.26 (1319.4 examples/sec; 0.097 sec/batch)
2017-06-02 08:38:36.761871: step 247310, loss = 0.34 (1631.9 examples/sec; 0.078 sec/batch)
2017-06-02 08:38:37.630575: step 247320, loss = 0.35 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:38:38.495773: step 247330, loss = 0.28 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:38:39.389147: step 247340, loss = 0.25 (1432.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:38:40.284428: step 247350, loss = 0.31 (1429.7 examples/sec; 0.090 sec/batch)
2017-06-02 08:38:41.179803: step 247360, loss = 0.32 (1429.6 examples/sec; 0.090 sec/batch)
2017-06-02 08:38:42.062577: step 247370, loss = 0.30 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:38:42.954206: step 247380, loss = 0.32 (1435.6 examples/sec; 0.089 sec/batch)
2017-06-02 08:38:43.844378: step 247390, loss = 0.28 (1437.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:38:44.815585: step 247400, loss = 0.35 (1317.9 examples/sec; 0.097 sec/batch)
2017-06-02 08:38:45.564828: step 247410, loss = 0.30 (1708.4 examples/sec; 0.075 sec/batch)
2017-06-02 08:38:46.443795: step 247420, loss = 0.24 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:38:47.322604: step 247430, loss = 0.28 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:38:48.208670: step 247440, loss = 0.31 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 08:38:49.081563: step 247450, loss = 0.26 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:38:49.949608: step 247460, loss = 0.23 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:38:50.831683: step 247470, loss = 0.27 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:38:51.675657: step 247480, loss = 0.30 (1516.6 examples/sec; 0.084 sec/batch)
2017-06-02 08:38:52.539633: step 247490, loss = 0.28 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:38:53.507349: step 247500, loss = 0.29 (1322.7 examples/sec; 0.097 sec/batch)
2017-06-02 08:38:54.243426: step 247510, loss = 0.32 (1739.0 examples/sec; 0.074 sec/batch)
2017-06-02 08:38:55.140067: step 247520, loss = 0.21 (1427.5 examples/sec; 0.090 sec/batch)
2017-06-02 08:38:56.018908: step 247530, loss = 0.35 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:38:56.892954: step 247540, loss = 0.25 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:38:57.760040: step 247550, loss = 0.39 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:38:58.656627: step 247560, loss = 0.38 (1427.6 examples/sec; 0.090 sec/batch)
2017-06-02 08:38:59.548974: step 247570, loss = 0.32 (1434.4 examples/sec; 0.089 sec/batch)
2017-06-02 08:39:00.416106: step 247580, loss = 0.30 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:39:01.295221: step 247590, loss = 0.24 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:39:02.327531: step 247600, loss = 0.24 (1239.9 examples/sec; 0.103 sec/batch)
2017-06-02 08:39:03.069263: step 247610, loss = 0.27 (1725.7 examples/sec; 0.074 sec/batch)
2017-06-02 08:39:03.951871: step 247620, loss = 0.25 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:39:04.834528: step 247630, loss = 0.32 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:39:05.721389: step 247640, loss = 0.28 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:39:06.586751: step 247650, loss = 0.38 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:39:07.465157: step 247660, loss = 0.30 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:39:08.340939: step 247670, loss = 0.32 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:39:09.237871: step 247680, loss = 0.31 (1427.1 examples/sec; 0.090 sec/batch)
2017-06-02 08:39:10.122497: step 247690, loss = 0.26 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:39:11.093304: step 247700, loss = 0.30 (1318.5 examples/sec; 0.097 sec/batch)
2017-06-02 08:39:11.849070: step 247710, loss = 0.28 (1693.7 examples/sec; 0.076 sec/batch)
2017-06-02 08:39:12.711022: step 247720, loss = 0.21 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:39:13.605684: step 247730, loss = 0.32 (1430.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:39:14.490937: step 247740, loss = 0.31 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:39:15.375756: step 247750, loss = 0.25 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:39:16.263514: step 247760, loss = 0.30 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:39:17.141088: step 247770, loss = 0.25 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:39:18.021049: step 247780, loss = 0.23 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:39:18.883213: step 247790, loss = 0.40 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:39:19.849422: step 247800, loss = 0.32 (1324.8 examples/sec; 0.097 sec/batch)
2017-06-02 08:39:20.624422: step 247810, loss = 0.30 (1651.6 examples/sec; 0.077 sec/batch)
2017-06-02 08:39:21.504476: step 247820, loss = 0.28 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:39:22.357967: step 247830, loss = 0.21 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:39:23.207838: step 247840, loss = 0.32 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:39:24.068476: step 247850, loss = 0.31 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:39:24.932590: step 247860, loss = 0.25 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:39:25.804175: step 247870, loss = 0.31 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:39:26.652117: step 247880, loss = 0.25 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:39:27.512470: step 247890, loss = 0.33 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:39:28.492003: step 247900, loss = 0.36 (1306.7 examples/sec; 0.098 sec/batch)
2017-06-02 08:39:29.230437: step 247910, loss = 0.37 (1733.4 examples/sec; 0.074 sec/batch)
2017-06-02 08:39:30.096096: step 247920, loss = 0.30 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:39:30.962007: step 247930, loss = 0.34 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:39:31.838003: step 247940, loss = 0.31 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:39:32.707325: step 247950, loss = 0.30 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:39:33.587365: step 247960, loss = 0.36 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:39:34.418613: step 247970, loss = 0.33 (1539.9 examples/sec; 0.083 sec/batch)
2017-06-02 08:39:35.278622: step 247980, loss = 0.33 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:39:36.151670: step 247990, loss = 0.24 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:39:37.112426: step 248000, loss = 0.21 (1332.3 examples/sec; 0.096 sec/batch)
2017-06-02 08:39:37.873419: step 248010, loss = 0.42 (1682.0 examples/sec; 0.076 sec/batch)
2017-06-02 08:39:38.735318: step 248020, loss = 0.30 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:39:39.580703: step 248030, loss = 0.35 (1514.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:39:40.450711: step 248040, loss = 0.22 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:39:41.284680: step 248050, loss = 0.24 (1534.8 examples/sec; 0.083 sec/batch)
2017-06-02 08:39:42.144440: step 248060, loss = 0.30 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:39:42.991545: step 248070, loss = 0.29 (1511.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:39:43.864896: step 248080, loss = 0.29 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:39:44.716769: step 248090, loss = 0.29 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:39:45.682557: step 248100, loss = 0.36 (1325.3 examples/sec; 0.097 sec/batch)
2017-06-02 08:39:46.431517: step 248110, loss = 0.31 (1709.0 examples/sec; 0.075 sec/batch)
2017-06-02 08:39:47.314793: step 248120, loss = 0.33 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:39:48.188628: step 248130, loss = 0.28 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:39:49.057997: step 248140, loss = 0.29 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:39:49.930123: step 248150, loss = 0.33 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:39:50.806266: step 248160, loss = 0.27 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:39:51.690603: step 248170, loss = 0.29 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:39:52.612905: step 248180, loss = 0.26 (1387.8 examples/sec; 0.092 sec/batch)
2017-06-02 08:39:53.483808: step 248190, loss = 0.26 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:39:54.497923: step 248200, loss = 0.27 (1262.2 examples/sec; 0.101 sec/batch)
2017-06-02 08:39:55.234309: step 248210, loss = 0.30 (1738.2 examples/sec; 0.074 sec/batch)
2017-06-02 08:39:56.075793: step 248220, loss = 0.26 (1521.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:39:56.955397: step 248230, loss = 0.31 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:39:57.833073: step 248240, loss = 0.33 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:39:58.731870: step 248250, loss = 0.31 (1424.1 examples/sec; 0.090 sec/batch)
2017-06-02 08:39:59.571928: step 248260, loss = 0.29 (1523.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:40:00.462502: step 248270, loss = 0.32 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:40:01.308527: step 248280, loss = 0.25 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:40:02.186988: step 248290, loss = 0.39 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:40:03.148268: step 248300, loss = 0.23 (1331.6 examples/sec; 0.096 sec/batch)
2017-06-02 08:40:03.932554: step 248310, loss = 0.30 (1632.1 examples/sec; 0.078 sec/batch)
2017-06-02 08:40:04.805126: step 248320, loss = 0.34 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:40:05.668794: step 248330, loss = 0.27 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:40:06.559198: step 248340, loss = 0.33 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:40:07.438867: step 248350, loss = 0.32 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:40:08.305595: step 248360, loss = 0.33 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:40:09.198892: step 248370, loss = 0.22 (1432.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:40:10.081200: step 248380, loss = 0.29 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:40:10.940134: step 248390, loss = 0.30 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:40:11.946958: step 248400, loss = 0.24 (1271.3 examples/sec; 0.101 sec/batch)
2017-06-02 08:40:12.727168: step 248410, loss = 0.29 (1640.6 examples/sec; 0.078 sec/batch)
2017-06-02 08:40:13.578155: step 248420, loss = 0.28 (1504.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:40:14.454881: step 248430, loss = 0.27 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:40:15.340541: step 248440, loss = 0.26 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:40:16.210709: step 248450, loss = 0.35 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:40:17.097829: step 248460, loss = 0.29 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:40:17.971441: step 248470, loss = 0.31 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:40:18.832672: step 248480, loss = 0.23 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:40:19.731819: step 248490, loss = 0.31 (1423.6 examples/sec; 0.090 sec/batch)
2017-06-02 08:40:20.726081: step 248500, loss = 0.26 (1287.4 examples/sec; 0.099 sec/batch)
2017-06-02 08:40:21.500409: step 248510, loss = 0.23 (1653.1 examples/sec; 0.077 sec/batch)
2017-06-02 08:40:22.372271: step 248520, loss = 0.27 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:40:23.222181: step 248530, loss = 0.29 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:40:24.095341: step 248540, loss = 0.28 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:40:24.987300: step 248550, loss = 0.25 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:40:25.882824: step 248560, loss = 0.40 (1429.4 examples/sec; 0.090 sec/batch)
2017-06-02 08:40:26.767178: step 248570, loss = 0.39 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:40:27.663530: step 248580, loss = 0.24 (1428.0 examples/sec; 0.090 sec/batch)
2017-06-02 08:40:28.520358: step 248590, loss = 0.31 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:40:29.485294: step 248600, loss = 0.34 (1326.5 examples/sec; 0.096 sec/batch)
2017-06-02 08:40:30.264643: step 248610, loss = 0.24 (1642.4 examples/sec; 0.078 sec/batch)
2017-06-02 08:40:31.147718: step 248620, loss = 0.28 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:40:32.011459: step 248630, loss = 0.27 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:40:32.883300: step 248640, loss = 0.25 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:40:33.740159: step 248650, loss = 0.31 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:40:34.605242: step 248660, loss = 0.26 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:40:35.498489: step 248670, loss = 0.26 (1433.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:40:36.391163: step 248680, loss = 0.26 (1433.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:40:37.288322: step 248690, loss = 0.29 (1426.7 examples/sec; 0.090 sec/batch)
2017-06-02 08:40:38.348084: step 248700, loss = 0.27 (1207.8 examples/sec; 0.106 sec/batch)
2017-06-02 08:40:39.097635: step 248710, loss = 0.33 (1707.7 examples/sec; 0.075 sec/batch)
2017-06-02 08:40:39.979319: step 248720, loss = 0.28 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:40:40.860687: step 248730, loss = 0.25 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:40:41.753263: step 248740, loss = 0.26 (1434.1 examples/sec; 0.089 sec/batch)
2017-06-02 08:40:42.634541: step 248750, loss = 0.33 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:40:43.523426: step 248760, loss = 0.25 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:40:44.418320: step 248770, loss = 0.25 (1430.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:40:45.274583: step 248780, loss = 0.38 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:40:46.128037: step 248790, loss = 0.37 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:40:47.065240: step 248800, loss = 0.36 (1368.0 examples/sec; 0.094 sec/batch)
2017-06-02 08:40:47.803490: step 248810, loss = 0.36 (1730.3 examples/sec; 0.074 sec/batch)
2017-06-02 08:40:48.653135: step 248820, loss = 0.31 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:40:49.511696: step 248830, loss = 0.27 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:40:50.392151: step 248840, loss = 0.33 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:40:51.270694: step 248850, loss = 0.22 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:40:52.157363: step 248860, loss = 0.30 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 08:40:53.031309: step 248870, loss = 0.26 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:40:53.899072: step 248880, loss = 0.27 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:40:54.800608: step 248890, loss = 0.25 (1419.8 examples/sec; 0.090 sec/batch)
2017-06-02 08:40:55.761030: step 248900, loss = 0.30 (1332.8 examples/sec; 0.096 sec/batch)
2017-06-02 08:40:56.533625: step 248910, loss = 0.30 (1656.8 examples/sec; 0.077 sec/batch)
2017-06-02 08:40:57.390897: step 248920, loss = 0.27 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:40:58.260568: step 248930, loss = 0.26 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:40:59.143284: step 248940, loss = 0.25 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:41:00.017445: step 248950, loss = 0.25 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:41:00.891392: step 248960, loss = 0.26 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:41:01.761091: step 248970, loss = 0.29 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:41:02.637632: step 248980, loss = 0.28 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:41:03.498579: step 248990, loss = 0.32 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:41:04.477508: step 249000, loss = 0.31 (1307.5 examples/sec; 0.098 sec/batch)
2017-06-02 08:41:05.278040: step 249010, loss = 0.29 (1598.9 examples/sec; 0.080 sec/batch)
2017-06-02 08:41:06.139705: step 249020, loss = 0.22 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:41:07.026862: step 249030, loss = 0.23 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:41:07.917513: step 249040, loss = 0.31 (1437.1 examples/sec; 0.089 sec/batch)
2017-06-02 08:41:08.770558: step 249050, loss = 0.25 (1500.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:41:09.619133: step 249060, loss = 0.28 (1508.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:41:10.472363: step 249070, loss = 0.26 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:41:11.336829: step 249080, loss = 0.32 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:41:12.226258: step 249090, loss = 0.26 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 08:41:13.188098: step 249100, loss = 0.31 (1330.8 examples/sec; 0.096 sec/batch)
2017-06-02 08:41:13.964702: step 249110, loss = 0.26 (1648.2 examples/sec; 0.078 sec/batch)
2017-06-02 08:41:14.838034: step 249120, loss = 0.23 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:41:15.735825: step 249130, loss = 0.26 (1425.7 examples/sec; 0.090 sec/batch)
2017-06-02 08:41:16.596291: step 249140, loss = 0.35 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:41:17.450715: step 249150, loss = 0.25 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:41:18.317574: step 249160, loss = 0.30 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:41:19.176750: step 249170, loss = 0.26 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:41:20.033879: step 249180, loss = 0.24 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:41:20.925178: step 249190, loss = 0.31 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 08:41:21.875400: step 249200, loss = 0.26 (1347.1 examples/sec; 0.095 sec/batch)
2017-06-02 08:41:22.616618: step 249210, loss = 0.25 (1726.9 examples/sec; 0.074 sec/batch)
2017-06-02 08:41:23.499257: step 249220, loss = 0.28 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:41:24.347739: step 249230, loss = 0.28 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:41:25.207136: step 249240, loss = 0.25 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:41:26.055213: step 249250, loss = 0.27 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:41:26.917645: step 249260, loss = 0.29 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:41:27.778795: step 249270, loss = 0.31 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:41:28.619982: step 249280, loss = 0.25 (1521.6 examples/sec; 0.084 sec/batch)
2017-06-02 08:41:29.482309: step 249290, loss = 0.32 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:41:30.461486: step 249300, loss = 0.27 (1307.2 examples/sec; 0.098 sec/batch)
2017-06-02 08:41:31.245233: step 249310, loss = 0.34 (1633.2 examples/sec; 0.078 sec/batch)
2017-06-02 08:41:32.131085: step 249320, loss = 0.26 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:41:33.016603: step 249330, loss = 0.31 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:41:33.895461: step 249340, loss = 0.28 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:41:34.752508: step 249350, loss = 0.28 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:41:35.612270: step 249360, loss = 0.25 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:41:36.468651: step 249370, loss = 0.23 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:41:37.328367: step 249380, loss = 0.31 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:41:38.182145: step 249390, loss = 0.27 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:41:39.167397: step 249400, loss = 0.32 (1299.2 examples/sec; 0.099 sec/batch)
2017-06-02 08:41:39.922226: step 249410, loss = 0.23 (1695.7 examples/sec; 0.075 sec/batch)
2017-06-02 08:41:40.783851: step 249420, loss = 0.25 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:41:41.649136: step 249430, loss = 0.26 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:41:42.503992: step 249440, loss = 0.29 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:41:43.376550: step 249450, loss = 0.35 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:41:44.233086: step 249460, loss = 0.23 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:41:45.094444: step 249470, loss = 0.34 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:41:45.972032: step 249480, loss = 0.30 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:41:46.834385: step 249490, loss = 0.23 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:41:47.799647: step 249500, loss = 0.36 (1326.1 examples/sec; 0.097 sec/batch)
2017-06-02 08:41:48.556073: step 249510, loss = 0.28 (1692.2 examples/sec; 0.076 sec/batch)
2017-06-02 08:41:49.399578: step 249520, loss = 0.32 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:41:50.258662: step 249530, loss = 0.30 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:41:51.140467: step 249540, loss = 0.25 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:41:52.015927: step 249550, loss = 0.38 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:41:52.861001: step 249560, loss = 0.34 (1514.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:41:53.728985: step 249570, loss = 0.20 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:41:54.565791: step 249580, loss = 0.36 (1529.6 examples/sec; 0.084 sec/batch)
2017-06-02 08:41:55.418421: step 249590, loss = 0.23 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:41:56.394278: step 249600, loss = 0.30 (1311.7 examples/sec; 0.098 sec/batch)
2017-06-02 08:41:57.148614: step 249610, loss = 0.30 (1696.8 examples/sec; 0.075 sec/batch)
2017-06-02 08:41:58.006087: step 249620, loss = 0.28 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:41:58.882023: step 249630, loss = 0.34 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:41:59.765457: step 249640, loss = 0.33 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:42:00.628667: step 249650, loss = 0.42 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:42:01.477533: step 249660, loss = 0.20 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:42:02.323266: step 249670, loss = 0.36 (1513.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:42:03.190582: step 249680, loss = 0.34 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:42:04.065363: step 249690, loss = 0.29 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:42:05.066482: step 249700, loss = 0.24 (1278.6 examples/sec; 0.100 sec/batch)
2017-06-02 08:42:05.788970: step 249710, loss = 0.33 (1771.7 examples/sec; 0.072 sec/batch)
2017-06-02 08:42:06.619641: step 249720, loss = 0.40 (1540.9 examples/sec; 0.083 sec/batch)
2017-06-02 08:42:07.502330: step 249730, loss = 0.30 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:42:08.352189: step 249740, loss = 0.26 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:42:09.213659: step 249750, loss = 0.39 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:42:10.092590: step 249760, loss = 0.24 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:42:10.976081: step 249770, loss = 0.19 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:42:11.841398: step 249780, loss = 0.25 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:42:12.732569: step 249790, loss = 0.27 (1436.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:42:13.725261: step 249800, loss = 0.24 (1289.4 examples/sec; 0.099 sec/batch)
2017-06-02 08:42:14.500688: step 249810, loss = 0.39 (1650.7 examples/sec; 0.078 sec/batch)
2017-06-02 08:42:15.344852: step 249820, loss = 0.34 (1516.3 examples/sec; 0.084 sec/batch)
2017-06-02 08:42:16.206582: step 249830, loss = 0.34 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:42:17.085115: step 249840, loss = 0.32 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:42:17.956062: step 249850, loss = 0.28 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:42:18.822175: step 249860, loss = 0.33 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:42:19.689420: step 249870, loss = 0.31 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:42:20.572610: step 249880, loss = 0.27 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:42:21.442094: step 249890, loss = 0.27 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:42:22.410400: step 249900, loss = 0.32 (1321.9 examples/sec; 0.097 sec/batch)
2017-06-02 08:42:23.189131: step 249910, loss = 0.25 (1643.7 examples/sec; 0.078 sec/batch)
2017-06-02 08:42:24.034016: step 249920, loss = 0.28 (1515.0 examples/sec; 0.084 sec/batch)
2017-06-02 08:42:24.913133: step 249930, loss = 0.23 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:42:25.788558: step 249940, loss = 0.30 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:42:26.649702: step 249950, loss = 0.37 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:42:27.512612: step 249960, loss = 0.34 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:42:28.356820: step 249970, loss = 0.27 (1516.2 examples/sec; 0.084 sec/batch)
2017-06-02 08:42:29.215877: step 249980, loss = 0.25 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:42:30.068618: step 249990, loss = 0.31 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:42:31.033053: step 250000, loss = 0.35 (1327.2 examples/sec; 0.096 sec/batch)
2017-06-02 08:42:31.802295: step 250010, loss = 0.37 (1664.0 examples/sec; 0.077 sec/batch)
2017-06-02 08:42:32.661223: step 250020, loss = 0.34 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:42:33.529690: step 250030, loss = 0.25 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:42:34.396898: step 250040, loss = 0.26 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:42:35.270819: step 250050, loss = 0.23 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:42:36.133993: step 250060, loss = 0.22 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:42:37.026440: step 250070, loss = 0.29 (1434.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:42:37.898769: step 250080, loss = 0.27 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:42:38.771563: step 250090, loss = 0.25 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:42:39.754860: step 250100, loss = 0.27 (1301.7 examples/sec; 0.098 sec/batch)
2017-06-02 08:42:40.516829: step 250110, loss = 0.29 (1679.9 examples/sec; 0.076 sec/batch)
2017-06-02 08:42:41.377130: step 250120, loss = 0.21 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:42:42.242415: step 250130, loss = 0.33 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:42:43.099257: step 250140, loss = 0.29 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:42:43.961452: step 250150, loss = 0.29 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:42:44.819135: step 250160, loss = 0.25 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:42:45.660414: step 250170, loss = 0.25 (1521.5 examples/sec; 0.084 sec/batch)
2017-06-02 08:42:46.521383: step 250180, loss = 0.26 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:42:47.393691: step 250190, loss = 0.24 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:42:48.356550: step 250200, loss = 0.31 (1329.4 examples/sec; 0.096 sec/batch)
2017-06-02 08:42:49.103122: step 250210, loss = 0.31 (1714.5 examples/sec; 0.075 sec/batch)
2017-06-02 08:42:49.964885: step 250220, loss = 0.32 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:42:50.803380: step 250230, loss = 0.31 (1526.6 examples/sec; 0.084 sec/batch)
2017-06-02 08:42:51.681665: step 250240, loss = 0.38 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:42:52.550458: step 250250, loss = 0.27 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:42:53.416606: step 250260, loss = 0.27 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:42:54.262322: step 250270, loss = 0.30 (1513.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:42:55.116989: step 250280, loss = 0.30 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:42:55.996598: step 250290, loss = 0.31 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:42:56.953608: step 250300, loss = 0.35 (1337.5 examples/sec; 0.096 sec/batch)
2017-06-02 08:42:57.729338: step 250310, loss = 0.29 (1650.0 examples/sec; 0.078 sec/batch)
2017-06-02 08:42:58.578435: step 250320, loss = 0.26 (1507.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:42:59.409257: step 250330, loss = 0.28 (1540.6 examples/sec; 0.083 sec/batch)
2017-06-02 08:43:00.293486: step 250340, loss = 0.28 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:43:01.157392: step 250350, loss = 0.32 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:43:02.028828: step 250360, loss = 0.22 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:43:02.878940: step 250370, loss = 0.34 (1505.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:43:03.744481: step 250380, loss = 0.25 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:43:04.607243: step 250390, loss = 0.38 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:43:05.559524: step 250400, loss = 0.27 (1344.1 examples/sec; 0.095 sec/batch)
2017-06-02 08:43:06.343326: step 250410, loss = 0.23 (1633.1 examples/sec; 0.078 sec/batch)
2017-06-02 08:43:07.204624: step 250420, loss = 0.32 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:43:08.058155: step 250430, loss = 0.32 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:43:08.933854: step 250440, loss = 0.29 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:43:09.796438: step 250450, loss = 0.26 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:43:10.650786: step 250460, loss = 0.25 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:43:11.514060: step 250470, loss = 0.28 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:43:12.367494: step 250480, loss = 0.27 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:43:13.218619: step 250490, loss = 0.25 (1503.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:43:14.169936: step 250500, loss = 0.29 (1345.5 examples/sec; 0.095 sec/batch)
2017-06-02 08:43:14.910131: step 250510, loss = 0.30 (1729.3 examples/sec; 0.074 sec/batch)
2017-06-02 08:43:15.760687: step 250520, loss = 0.23 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:43:16.634372: step 250530, loss = 0.33 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:43:17.491803: step 250540, loss = 0.22 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:43:18.372611: step 250550, loss = 0.25 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:43:19.250387: step 250560, loss = 0.39 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:43:20.136552: step 250570, loss = 0.26 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 08:43:21.004348: step 250580, loss = 0.30 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:43:21.861919: step 250590, loss = 0.20 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:43:22.796636: step 250600, loss = 0.25 (1369.4 examples/sec; 0.093 sec/batch)
2017-06-02 08:43:23.556015: step 250610, loss = 0.29 (1685.6 examples/sec; 0.076 sec/batch)
2017-06-02 08:43:24.440680: step 250620, loss = 0.22 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:43:25.310307: step 250630, loss = 0.25 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:43:26.170943: step 250640, loss = 0.35 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:43:27.060359: step 250650, loss = 0.34 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 08:43:27.908256: step 250660, loss = 0.26 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:43:28.792648: step 250670, loss = 0.29 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:43:29.674480: step 250680, loss = 0.27 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:43:30.521618: step 250690, loss = 0.26 (1511.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:43:31.520346: step 250700, loss = 0.27 (1281.6 examples/sec; 0.100 sec/batch)
2017-06-02 08:43:32.246677: step 250710, loss = 0.34 (1762.3 examples/sec; 0.073 sec/batch)
2017-06-02 08:43:33.093793: step 250720, loss = 0.26 (1511.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:43:33.966681: step 250730, loss = 0.20 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:43:34.827035: step 250740, loss = 0.32 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:43:35.682465: step 250750, loss = 0.31 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:43:36.537768: step 250760, loss = 0.28 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:43:37.395725: step 250770, loss = 0.30 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:43:38.240385: step 250780, loss = 0.28 (1515.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:43:39.105632: step 250790, loss = 0.36 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:43:40.065929: step 250800, loss = 0.39 (1332.9 examples/sec; 0.096 sec/batch)
2017-06-02 08:43:40.855214: step 250810, loss = 0.22 (1621.7 examples/sec; 0.079 sec/batch)
2017-06-02 08:43:41.710287: step 250820, loss = 0.28 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:43:42.550380: step 250830, loss = 0.35 (1523.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:43:43.419439: step 250840, loss = 0.34 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:43:44.289525: step 250850, loss = 0.34 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:43:45.197033: step 250860, loss = 0.22 (1410.5 examples/sec; 0.091 sec/batch)
2017-06-02 08:43:46.051858: step 250870, loss = 0.23 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:43:46.897866: step 250880, loss = 0.39 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:43:47.762619: step 250890, loss = 0.31 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:43:48.753614: step 250900, loss = 0.31 (1291.6 examples/sec; 0.099 sec/batch)
2017-06-02 08:43:49.544891: step 250910, loss = 0.25 (1617.6 examples/sec; 0.079 sec/batch)
2017-06-02 08:43:50.413175: step 250920, loss = 0.19 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:43:51.278039: step 250930, loss = 0.37 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:43:52.148520: step 250940, loss = 0.32 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:43:53.013772: step 250950, loss = 0.22 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:43:53.882005: step 250960, loss = 0.33 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:43:54.738342: step 250970, loss = 0.34 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:43:55.618069: step 250980, loss = 0.27 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:43:56.503568: step 250990, loss = 0.25 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:43:57.457479: step 251000, loss = 0.36 (1341.8 examples/sec; 0.095 sec/batch)
2017-06-02 08:43:58.236161: step 251010, loss = 0.24 (1643.8 examples/sec; 0.078 sec/batch)
2017-06-02 08:43:59.107413: step 251020, loss = 0.23 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:43:59.976864: step 251030, loss = 0.32 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:44:00.840977: step 251040, loss = 0.38 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:44:01.719744: step 251050, loss = 0.31 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:44:02.595832: step 251060, loss = 0.23 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:44:03.450990: step 251070, loss = 0.34 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:44:04.324584: step 251080, loss = 0.28 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:44:05.171228: step 251090, loss = 0.25 (1511.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:44:06.143390: step 251100, loss = 0.24 (1316.6 examples/sec; 0.097 sec/batch)
2017-06-02 08:44:06.908534: step 251110, loss = 0.28 (1672.9 examples/sec; 0.077 sec/batch)
2017-06-02 08:44:07.784256: step 251120, loss = 0.29 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:44:08.668651: step 251130, loss = 0.28 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:44:09.512952: step 251140, loss = 0.31 (1516.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:44:10.397911: step 251150, loss = 0.32 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:44:11.238493: step 251160, loss = 0.27 (1522.8 examples/sec; 0.084 sec/batch)
2017-06-02 08:44:12.098175: step 251170, loss = 0.30 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:44:12.969028: step 251180, loss = 0.28 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:44:13.810903: step 251190, loss = 0.27 (1520.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:44:14.763351: step 251200, loss = 0.21 (1343.9 examples/sec; 0.095 sec/batch)
2017-06-02 08:44:15.520887: step 251210, loss = 0.22 (1689.7 examples/sec; 0.076 sec/batch)
2017-06-02 08:44:16.404391: step 251220, loss = 0.28 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:44:17.254896: step 251230, loss = 0.39 (1505.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:44:18.129417: step 251240, loss = 0.27 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:44:19.003816: step 251250, loss = 0.31 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:44:19.876149: step 251260, loss = 0.30 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:44:20.723626: step 251270, loss = 0.28 (1510.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:44:21.563694: step 251280, loss = 0.26 (1523.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:44:22.446030: step 251290, loss = 0.27 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:44:23.420448: step 251300, loss = 0.26 (1313.6 examples/sec; 0.097 sec/batch)
2017-06-02 08:44:24.153214: step 251310, loss = 0.39 (1746.8 examples/sec; 0.073 sec/batch)
2017-06-02 08:44:24.990187: step 251320, loss = 0.26 (1529.3 examples/sec; 0.084 sec/batch)
2017-06-02 08:44:25.853508: step 251330, loss = 0.35 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:44:26.703876: step 251340, loss = 0.27 (1505.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:44:27.547611: step 251350, loss = 0.33 (1517.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:44:28.434043: step 251360, loss = 0.35 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:44:29.288354: step 251370, loss = 0.27 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:44:30.150078: step 251380, loss = 0.24 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:44:31.004439: step 251390, loss = 0.24 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:44:31.995127: step 251400, loss = 0.29 (1292.0 examples/sec; 0.099 sec/batch)
2017-06-02 08:44:32.762222: step 251410, loss = 0.25 (1668.6 examples/sec; 0.077 sec/batch)
2017-06-02 08:44:33.626036: step 251420, loss = 0.33 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:44:34.491960: step 251430, loss = 0.31 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:44:35.352326: step 251440, loss = 0.23 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:44:36.207595: step 251450, loss = 0.33 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:44:37.060072: step 251460, loss = 0.30 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:44:37.923868: step 251470, loss = 0.28 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:44:38.791887: step 251480, loss = 0.26 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:44:39.642419: step 251490, loss = 0.24 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:44:40.596480: step 251500, loss = 0.29 (1341.6 examples/sec; 0.095 sec/batch)
2017-06-02 08:44:41.352074: step 251510, loss = 0.30 (1694.0 examples/sec; 0.076 sec/batch)
2017-06-02 08:44:42.201286: step 251520, loss = 0.32 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:44:43.067460: step 251530, loss = 0.32 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:44:43.909491: step 251540, loss = 0.32 (1520.0 examples/sec; 0.084 sec/batch)
2017-06-02 08:44:44.751431: step 251550, loss = 0.30 (1520.3 examples/sec; 0.084 sec/batch)
2017-06-02 08:44:45.603973: step 251560, loss = 0.34 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:44:46.462837: step 251570, loss = 0.26 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:44:47.327584: step 251580, loss = 0.31 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:44:48.175170: step 251590, loss = 0.36 (1510.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:44:49.150616: step 251600, loss = 0.36 (1312.2 examples/sec; 0.098 sec/batch)
2017-06-02 08:44:49.904105: step 251610, loss = 0.21 (1698.8 examples/sec; 0.075 sec/batch)
2017-06-02 08:44:50.729257: step 251620, loss = 0.29 (1551.2 examples/sec; 0.083 sec/batch)
2017-06-02 08:44:51.584977: step 251630, loss = 0.28 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:44:52.448030: step 251640, loss = 0.33 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:44:53.292547: step 251650, loss = 0.34 (1515.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:44:54.154435: step 251660, loss = 0.26 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:44:55.030248: step 251670, loss = 0.40 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:44:55.894503: step 251680, loss = 0.34 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:44:56.732158: step 251690, loss = 0.37 (1528.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:44:57.685505: step 251700, loss = 0.29 (1342.6 examples/sec; 0.095 sec/batch)
2017-06-02 08:44:58.450322: step 251710, loss = 0.37 (1673.6 examples/sec; 0.076 sec/batch)
2017-06-02 08:44:59.325907: step 251720, loss = 0.25 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:45:00.185420: step 251730, loss = 0.23 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:45:01.056959: step 251740, loss = 0.29 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:45:01.935359: step 251750, loss = 0.28 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:45:02.775517: step 251760, loss = 0.26 (1523.5 examples/sec; 0.084 sec/batch)
2017-06-02 08:45:03.625765: step 251770, loss = 0.30 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:45:04.472499: step 251780, loss = 0.31 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:45:05.318748: step 251790, loss = 0.26 (1512.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:45:06.266414: step 251800, loss = 0.23 (1350.7 examples/sec; 0.095 sec/batch)
2017-06-02 08:45:07.002011: step 251810, loss = 0.39 (1740.1 examples/sec; 0.074 sec/batch)
2017-06-02 08:45:07.842195: step 251820, loss = 0.26 (1523.5 examples/sec; 0.084 sec/batch)
2017-06-02 08:45:08.716166: step 251830, loss = 0.25 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:45:09.550114: step 251840, loss = 0.44 (1534.9 examples/sec; 0.083 sec/batch)
2017-06-02 08:45:10.409697: step 251850, loss = 0.30 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:45:11.286911: step 251860, loss = 0.28 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:45:12.166490: step 251870, loss = 0.23 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:45:13.049813: step 251880, loss = 0.28 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:45:13.910830: step 251890, loss = 0.40 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:45:14.880297: step 251900, loss = 0.28 (1320.3 examples/sec; 0.097 sec/batch)
2017-06-02 08:45:15.586508: step 251910, loss = 0.33 (1812.5 examples/sec; 0.071 sec/batch)
2017-06-02 08:45:16.428806: step 251920, loss = 0.28 (1519.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:45:17.285676: step 251930, loss = 0.25 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:45:18.149478: step 251940, loss = 0.34 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:45:19.018968: step 251950, loss = 0.41 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:45:19.890002: step 251960, loss = 0.35 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:45:20.734666: step 251970, loss = 0.35 (1515.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:45:21.607600: step 251980, loss = 0.28 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:45:22.456465: step 251990, loss = 0.28 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:45:23.422298: step 252000, loss = 0.54 (1325.3 examples/sec; 0.097 sec/batch)
2017-06-02 08:45:24.174867: step 252010, loss = 0.29 (1700.8 examples/sec; 0.075 sec/batch)
2017-06-02 08:45:25.048131: step 252020, loss = 0.31 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:45:25.934162: step 252030, loss = 0.36 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 08:45:26.833100: step 252040, loss = 0.29 (1423.9 examples/sec; 0.090 sec/batch)
2017-06-02 08:45:27.707417: step 252050, loss = 0.26 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:45:28.587834: step 252060, loss = 0.23 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:45:29.459316: step 252070, loss = 0.38 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:45:30.350548: step 252080, loss = 0.32 (1436.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:45:31.242379: step 252090, loss = 0.40 (1435.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:45:32.215046: step 252100, loss = 0.41 (1316.0 examples/sec; 0.097 sec/batch)
2017-06-02 08:45:32.980506: step 252110, loss = 0.27 (1672.2 examples/sec; 0.077 sec/batch)
2017-06-02 08:45:33.815000: step 252120, loss = 0.25 (1533.9 examples/sec; 0.083 sec/batch)
2017-06-02 08:45:34.689046: step 252130, loss = 0.30 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:45:35.535286: step 252140, loss = 0.23 (1512.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:45:36.361663: step 252150, loss = 0.28 (1548.9 examples/sec; 0.083 sec/batch)
2017-06-02 08:45:37.205752: step 252160, loss = 0.26 (1516.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:45:38.039164: step 252170, loss = 0.23 (1535.9 examples/sec; 0.083 sec/batch)
2017-06-02 08:45:38.914272: step 252180, loss = 0.25 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:45:39.765284: step 252190, loss = 0.28 (1504.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:45:40.769062: step 252200, loss = 0.31 (1275.2 examples/sec; 0.100 sec/batch)
2017-06-02 08:45:41.529361: step 252210, loss = 0.22 (1683.6 examples/sec; 0.076 sec/batch)
2017-06-02 08:45:42.380590: step 252220, loss = 0.23 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:45:43.259589: step 252230, loss = 0.27 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:45:44.108706: step 252240, loss = 0.27 (1507.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:45:44.945821: step 252250, loss = 0.27 (1529.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:45:45.807483: step 252260, loss = 0.28 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:45:46.685727: step 252270, loss = 0.31 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:45:47.557901: step 252280, loss = 0.27 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:45:48.399587: step 252290, loss = 0.24 (1520.8 examples/sec; 0.084 sec/batch)
2017-06-02 08:45:49.379926: step 252300, loss = 0.30 (1305.7 examples/sec; 0.098 sec/batch)
2017-06-02 08:45:50.173586: step 252310, loss = 0.29 (1612.8 examples/sec; 0.079 sec/batch)
2017-06-02 08:45:51.053971: step 252320, loss = 0.24 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:45:51.904380: step 252330, loss = 0.23 (1505.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:45:52.754923: step 252340, loss = 0.25 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:45:53.643100: step 252350, loss = 0.27 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:45:54.513376: step 252360, loss = 0.28 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:45:55.380310: step 252370, loss = 0.31 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:45:56.236542: step 252380, loss = 0.40 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:45:57.105349: step 252390, loss = 0.26 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:45:58.085224: step 252400, loss = 0.31 (1306.3 examples/sec; 0.098 sec/batch)
2017-06-02 08:45:58.826182: step 252410, loss = 0.33 (1727.5 examples/sec; 0.074 sec/batch)
2017-06-02 08:45:59.649163: step 252420, loss = 0.34 (1555.3 examples/sec; 0.082 sec/batch)
2017-06-02 08:46:00.526712: step 252430, loss = 0.26 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:46:01.373808: step 252440, loss = 0.22 (1511.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:46:02.225998: step 252450, loss = 0.27 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:46:03.073462: step 252460, loss = 0.29 (1510.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:46:03.941218: step 252470, loss = 0.30 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:46:04.791555: step 252480, loss = 0.41 (1505.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:46:05.673755: step 252490, loss = 0.34 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:46:06.631531: step 252500, loss = 0.24 (1336.4 examples/sec; 0.096 sec/batch)
2017-06-02 08:46:07.401998: step 252510, loss = 0.21 (1661.4 examples/sec; 0.077 sec/batch)
2017-06-02 08:46:08.270434: step 252520, loss = 0.28 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:46:09.139405: step 252530, loss = 0.28 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:46:09.991943: step 252540, loss = 0.40 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:46:10.864676: step 252550, loss = 0.28 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:46:11.727203: step 252560, loss = 0.32 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:46:12.606881: step 252570, loss = 0.23 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:46:13.452123: step 252580, loss = 0.21 (1514.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:46:14.306658: step 252590, loss = 0.29 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:46:15.274983: step 252600, loss = 0.28 (1321.9 examples/sec; 0.097 sec/batch)
2017-06-02 08:46:16.035172: step 252610, loss = 0.24 (1683.8 examples/sec; 0.076 sec/batch)
2017-06-02 08:46:16.900821: step 252620, loss = 0.22 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:46:17.779988: step 252630, loss = 0.21 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:46:18.657480: step 252640, loss = 0.31 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:46:19.525682: step 252650, loss = 0.45 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:46:20.383263: step 252660, loss = 0.25 (1492.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:46:21.264477: step 252670, loss = 0.29 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:46:22.109925: step 252680, loss = 0.25 (1514.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:46:22.985334: step 252690, loss = 0.28 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:46:23.968761: step 252700, loss = 0.25 (1301.6 examples/sec; 0.098 sec/batch)
2017-06-02 08:46:24.730764: step 252710, loss = 0.27 (1679.8 examples/sec; 0.076 sec/batch)
2017-06-02 08:46:25.620536: step 252720, loss = 0.26 (1438.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:46:26.477881: step 252730, loss = 0.31 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:46:27.342835: step 252740, loss = 0.34 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:46:28.230482: step 252750, loss = 0.30 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:46:29.103069: step 252760, loss = 0.27 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:46:29.976788: step 252770, loss = 0.26 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:46:30.827219: step 252780, loss = 0.32 (1505.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:46:31.710237: step 252790, loss = 0.22 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:46:32.704553: step 252800, loss = 0.24 (1287.3 examples/sec; 0.099 sec/batch)
2017-06-02 08:46:33.439111: step 252810, loss = 0.25 (1742.6 examples/sec; 0.073 sec/batch)
2017-06-02 08:46:34.314893: step 252820, loss = 0.27 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:46:35.170991: step 252830, loss = 0.37 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:46:36.018724: step 252840, loss = 0.35 (1509.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:46:36.886705: step 252850, loss = 0.26 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:46:37.775401: step 252860, loss = 0.28 (1440.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:46:38.647957: step 252870, loss = 0.29 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:46:39.496835: step 252880, loss = 0.33 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:46:40.378752: step 252890, loss = 0.37 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:46:41.343037: step 252900, loss = 0.20 (1327.4 examples/sec; 0.096 sec/batch)
2017-06-02 08:46:42.101123: step 252910, loss = 0.27 (1688.5 examples/sec; 0.076 sec/batch)
2017-06-02 08:46:42.967202: step 252920, loss = 0.29 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:46:43.825454: step 252930, loss = 0.25 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:46:44.680266: step 252940, loss = 0.32 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:46:45.541262: step 252950, loss = 0.32 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:46:46.395820: step 252960, loss = 0.34 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:46:47.274226: step 252970, loss = 0.41 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:46:48.153309: step 252980, loss = 0.32 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:46:49.024539: step 252990, loss = 0.35 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:46:49.987999: step 253000, loss = 0.31 (1328.5 examples/sec; 0.096 sec/batch)
2017-06-02 08:46:50.743086: step 253010, loss = 0.20 (1695.2 examples/sec; 0.076 sec/batch)
2017-06-02 08:46:51.640905: step 253020, loss = 0.27 (1425.7 examples/sec; 0.090 sec/batch)
2017-06-02 08:46:52.514819: step 253030, loss = 0.34 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:46:53.382467: step 253040, loss = 0.30 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:46:54.275845: step 253050, loss = 0.25 (1432.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:46:55.149251: step 253060, loss = 0.29 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:46:56.003458: step 253070, loss = 0.34 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:46:56.863653: step 253080, loss = 0.31 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:46:57.728838: step 253090, loss = 0.27 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:46:58.725120: step 253100, loss = 0.23 (1284.8 examples/sec; 0.100 sec/batch)
2017-06-02 08:46:59.493939: step 253110, loss = 0.36 (1664.9 examples/sec; 0.077 sec/batch)
2017-06-02 08:47:00.365899: step 253120, loss = 0.23 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:47:01.248709: step 253130, loss = 0.22 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:47:02.122652: step 253140, loss = 0.25 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:47:02.981816: step 253150, loss = 0.39 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:47:03.866380: step 253160, loss = 0.25 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:47:04.731412: step 253170, loss = 0.29 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:47:05.621239: step 253180, loss = 0.21 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:47:06.513278: step 253190, loss = 0.28 (1434.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:47:07.494211: step 253200, loss = 0.39 (1304.9 examples/sec; 0.098 sec/batch)
2017-06-02 08:47:08.251851: step 253210, loss = 0.31 (1689.5 examples/sec; 0.076 sec/batch)
2017-06-02 08:47:09.158676: step 253220, loss = 0.36 (1411.5 examples/sec; 0.091 sec/batch)
2017-06-02 08:47:10.025922: step 253230, loss = 0.24 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:47:10.891181: step 253240, loss = 0.26 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:47:11.764631: step 253250, loss = 0.25 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:47:12.628961: step 253260, loss = 0.29 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:47:13.470674: step 253270, loss = 0.25 (1520.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:47:14.325348: step 253280, loss = 0.48 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:47:15.191868: step 253290, loss = 0.25 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:47:16.141075: step 253300, loss = 0.31 (1348.5 examples/sec; 0.095 sec/batch)
2017-06-02 08:47:16.897288: step 253310, loss = 0.27 (1692.6 examples/sec; 0.076 sec/batch)
2017-06-02 08:47:17.733333: step 253320, loss = 0.32 (1531.0 examples/sec; 0.084 sec/batch)
2017-06-02 08:47:18.608120: step 253330, loss = 0.27 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:47:19.482052: step 253340, loss = 0.28 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:47:20.321769: step 253350, loss = 0.33 (1524.3 examples/sec; 0.084 sec/batch)
2017-06-02 08:47:21.166275: step 253360, loss = 0.27 (1515.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:47:22.031757: step 253370, loss = 0.25 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:47:22.915779: step 253380, loss = 0.29 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:47:23.792609: step 253390, loss = 0.29 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:47:24.738549: step 253400, loss = 0.36 (1353.1 examples/sec; 0.095 sec/batch)
2017-06-02 08:47:25.514164: step 253410, loss = 0.30 (1650.3 examples/sec; 0.078 sec/batch)
2017-06-02 08:47:26.378453: step 253420, loss = 0.29 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:47:27.255517: step 253430, loss = 0.26 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:47:28.156926: step 253440, loss = 0.33 (1420.0 examples/sec; 0.090 sec/batch)
2017-06-02 08:47:29.051209: step 253450, loss = 0.38 (1431.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:47:29.929464: step 253460, loss = 0.33 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:47:30.804648: step 253470, loss = 0.24 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:47:31.687553: step 253480, loss = 0.30 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:47:32.560268: step 253490, loss = 0.29 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:47:33.543022: step 253500, loss = 0.24 (1302.5 examples/sec; 0.098 sec/batch)
2017-06-02 08:47:34.308953: step 253510, loss = 0.35 (1671.2 examples/sec; 0.077 sec/batch)
2017-06-02 08:47:35.210254: step 253520, loss = 0.27 (1420.1 examples/sec; 0.090 sec/batch)
2017-06-02 08:47:36.106274: step 253530, loss = 0.30 (1428.6 examples/sec; 0.090 sec/batch)
2017-06-02 08:47:36.936460: step 253540, loss = 0.30 (1541.8 examples/sec; 0.083 sec/batch)
2017-06-02 08:47:37.804123: step 253550, loss = 0.27 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:47:38.681319: step 253560, loss = 0.29 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:47:39.576768: step 253570, loss = 0.32 (1429.5 examples/sec; 0.090 sec/batch)
2017-06-02 08:47:40.469869: step 253580, loss = 0.34 (1433.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:47:41.355984: step 253590, loss = 0.22 (1444.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:47:42.330021: step 253600, loss = 0.34 (1314.1 examples/sec; 0.097 sec/batch)
2017-06-02 08:47:43.127375: step 253610, loss = 0.33 (1605.3 examples/sec; 0.080 sec/batch)
2017-06-02 08:47:43.999493: step 253620, loss = 0.23 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:47:44.897381: step 253630, loss = 0.29 (1425.6 examples/sec; 0.090 sec/batch)
2017-06-02 08:47:45.763664: step 253640, loss = 0.30 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:47:46.644004: step 253650, loss = 0.34 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:47:47.506090: step 253660, loss = 0.22 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:47:48.353513: step 253670, loss = 0.35 (1510.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:47:49.241778: step 253680, loss = 0.27 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:47:50.141002: step 253690, loss = 0.23 (1423.4 examples/sec; 0.090 sec/batch)
2017-06-02 08:47:51.128756: step 253700, loss = 0.22 (1295.9 examples/sec; 0.099 sec/batch)
2017-06-02 08:47:51.888640: step 253710, loss = 0.36 (1684.5 examples/sec; 0.076 sec/batch)
2017-06-02 08:47:52.750016: step 253720, loss = 0.23 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:47:53.617253: step 253730, loss = 0.32 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:47:54.465983: step 253740, loss = 0.33 (1508.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:47:55.333718: step 253750, loss = 0.24 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:47:56.200000: step 253760, loss = 0.29 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:47:57.074442: step 253770, loss = 0.37 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:47:57.933946: step 253780, loss = 0.30 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:47:58.816528: step 253790, loss = 0.28 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:47:59.775693: step 253800, loss = 0.30 (1334.5 examples/sec; 0.096 sec/batch)
2017-06-02 08:48:00.585078: step 253810, loss = 0.28 (1581.4 examples/sec; 0.081 sec/batch)
2017-06-02 08:48:01.460308: step 253820, loss = 0.29 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:48:02.349221: step 253830, loss = 0.23 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:48:03.217546: step 253840, loss = 0.39 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:48:04.084533: step 253850, loss = 0.29 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:48:04.959060: step 253860, loss = 0.24 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:48:05.843535: step 253870, loss = 0.28 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:48:06.715817: step 253880, loss = 0.29 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:48:07.586184: step 253890, loss = 0.26 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:48:08.538156: step 253900, loss = 0.28 (1344.6 examples/sec; 0.095 sec/batch)
2017-06-02 08:48:09.292940: step 253910, loss = 0.20 (1695.8 examples/sec; 0.075 sec/batch)
2017-06-02 08:48:10.155076: step 253920, loss = 0.31 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:48:11.026616: step 253930, loss = 0.29 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:48:11.912326: step 253940, loss = 0.29 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:48:12.780848: step 253950, loss = 0.31 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:48:13.654821: step 253960, loss = 0.39 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:48:14.521802: step 253970, loss = 0.30 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:48:15.404879: step 253980, loss = 0.31 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:48:16.304812: step 253990, loss = 0.31 (1422.3 examples/sec; 0.090 sec/batch)
2017-06-02 08:48:17.342798: step 254000, loss = 0.35 (1233.2 examples/sec; 0.104 sec/batch)
2017-06-02 08:48:18.075408: step 254010, loss = 0.27 (1747.2 examples/sec; 0.073 sec/batch)
2017-06-02 08:48:18.940999: step 254020, loss = 0.23 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:48:19.838356: step 254030, loss = 0.42 (1426.4 examples/sec; 0.090 sec/batch)
2017-06-02 08:48:20.713687: step 254040, loss = 0.34 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:48:21.592044: step 254050, loss = 0.29 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:48:22.443088: step 254060, loss = 0.35 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:48:23.303874: step 254070, loss = 0.29 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:48:24.168813: step 254080, loss = 0.32 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:48:25.061060: step 254090, loss = 0.26 (1434.6 examples/sec; 0.089 sec/batch)
2017-06-02 08:48:26.003419: step 254100, loss = 0.33 (1358.3 examples/sec; 0.094 sec/batch)
2017-06-02 08:48:26.776175: step 254110, loss = 0.32 (1656.4 examples/sec; 0.077 sec/batch)
2017-06-02 08:48:27.653338: step 254120, loss = 0.26 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:48:28.518512: step 254130, loss = 0.25 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:48:29.377194: step 254140, loss = 0.31 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:48:30.271521: step 254150, loss = 0.27 (1431.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:48:31.152723: step 254160, loss = 0.37 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:48:32.034195: step 254170, loss = 0.23 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:48:32.907763: step 254180, loss = 0.33 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:48:33.786448: step 254190, loss = 0.29 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:48:34.756261: step 254200, loss = 0.29 (1319.8 examples/sec; 0.097 sec/batch)
2017-06-02 08:48:35.499387: step 254210, loss = 0.24 (1722.5 examples/sec; 0.074 sec/batch)
2017-06-02 08:48:36.372345: step 254220, loss = 0.26 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:48:37.274250: step 254230, loss = 0.28 (1419.2 examples/sec; 0.090 sec/batch)
2017-06-02 08:48:38.170676: step 254240, loss = 0.39 (1427.9 examples/sec; 0.090 sec/batch)
2017-06-02 08:48:39.047371: step 254250, loss = 0.27 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:48:39.915398: step 254260, loss = 0.37 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:48:40.780359: step 254270, loss = 0.21 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:48:41.632434: step 254280, loss = 0.30 (1502.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:48:42.485006: step 254290, loss = 0.28 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:48:43.445537: step 254300, loss = 0.31 (1332.6 examples/sec; 0.096 sec/batch)
2017-06-02 08:48:44.210655: step 254310, loss = 0.28 (1672.9 examples/sec; 0.077 sec/batch)
2017-06-02 08:48:45.081939: step 254320, loss = 0.28 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:48:45.957627: step 254330, loss = 0.29 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:48:46.817612: step 254340, loss = 0.23 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:48:47.681568: step 254350, loss = 0.26 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:48:48.542377: step 254360, loss = 0.27 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:48:49.401765: step 254370, loss = 0.32 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:48:50.269040: step 254380, loss = 0.23 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:48:51.130690: step 254390, loss = 0.29 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:48:52.102809: step 254400, loss = 0.20 (1316.7 examples/sec; 0.097 sec/batch)
2017-06-02 08:48:52.866876: step 254410, loss = 0.32 (1675.3 examples/sec; 0.076 sec/batch)
2017-06-02 08:48:53.721003: step 254420, loss = 0.37 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:48:54.555560: step 254430, loss = 0.26 (1533.8 examples/sec; 0.083 sec/batch)
2017-06-02 08:48:55.446221: step 254440, loss = 0.29 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:48:56.327010: step 254450, loss = 0.33 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:48:57.189274: step 254460, loss = 0.23 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:48:58.067245: step 254470, loss = 0.30 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:48:58.912726: step 254480, loss = 0.31 (1513.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:48:59.764997: step 254490, loss = 0.24 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:49:00.738365: step 254500, loss = 0.30 (1315.0 examples/sec; 0.097 sec/batch)
2017-06-02 08:49:01.514455: step 254510, loss = 0.32 (1649.3 examples/sec; 0.078 sec/batch)
2017-06-02 08:49:02.401279: step 254520, loss = 0.34 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 08:49:03.295818: step 254530, loss = 0.28 (1430.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:49:04.166881: step 254540, loss = 0.24 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:49:05.040110: step 254550, loss = 0.34 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:49:05.892249: step 254560, loss = 0.39 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:49:06.747992: step 254570, loss = 0.25 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:49:07.593858: step 254580, loss = 0.34 (1513.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:49:08.446091: step 254590, loss = 0.25 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:49:09.388728: step 254600, loss = 0.25 (1357.9 examples/sec; 0.094 sec/batch)
2017-06-02 08:49:10.180426: step 254610, loss = 0.29 (1616.8 examples/sec; 0.079 sec/batch)
2017-06-02 08:49:11.052682: step 254620, loss = 0.27 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:49:11.937576: step 254630, loss = 0.24 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:49:12.794649: step 254640, loss = 0.25 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:49:13.657257: step 254650, loss = 0.26 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:49:14.547787: step 254660, loss = 0.32 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:49:15.397214: step 254670, loss = 0.30 (1506.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:49:16.270747: step 254680, loss = 0.30 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:49:17.161454: step 254690, loss = 0.34 (1437.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:49:18.150821: step 254700, loss = 0.29 (1293.8 examples/sec; 0.099 sec/batch)
2017-06-02 08:49:18.910109: step 254710, loss = 0.22 (1685.8 examples/sec; 0.076 sec/batch)
2017-06-02 08:49:19.773542: step 254720, loss = 0.27 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:49:20.645714: step 254730, loss = 0.31 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:49:21.519602: step 254740, loss = 0.33 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:49:22.390546: step 254750, loss = 0.38 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:49:23.268974: step 254760, loss = 0.48 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:49:24.134182: step 254770, loss = 0.22 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:49:25.008221: step 254780, loss = 0.27 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:49:25.852276: step 254790, loss = 0.24 (1516.5 examples/sec; 0.084 sec/batch)
2017-06-02 08:49:26.870362: step 254800, loss = 0.28 (1257.2 examples/sec; 0.102 sec/batch)
2017-06-02 08:49:27.632545: step 254810, loss = 0.29 (1679.4 examples/sec; 0.076 sec/batch)
2017-06-02 08:49:28.528962: step 254820, loss = 0.25 (1427.9 examples/sec; 0.090 sec/batch)
2017-06-02 08:49:29.395782: step 254830, loss = 0.27 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:49:30.285391: step 254840, loss = 0.31 (1438.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:49:31.164754: step 254850, loss = 0.29 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:49:32.001070: step 254860, loss = 0.25 (1530.5 examples/sec; 0.084 sec/batch)
2017-06-02 08:49:32.878809: step 254870, loss = 0.26 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:49:33.762987: step 254880, loss = 0.29 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:49:34.610396: step 254890, loss = 0.31 (1510.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:49:35.639889: step 254900, loss = 0.39 (1243.3 examples/sec; 0.103 sec/batch)
2017-06-02 08:49:36.366687: step 254910, loss = 0.28 (1761.2 examples/sec; 0.073 sec/batch)
2017-06-02 08:49:37.247615: step 254920, loss = 0.25 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:49:38.102146: step 254930, loss = 0.38 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:49:38.974133: step 254940, loss = 0.27 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:49:39.852362: step 254950, loss = 0.22 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:49:40.734138: step 254960, loss = 0.31 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:49:41.616959: step 254970, loss = 0.49 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:49:42.484604: step 254980, loss = 0.29 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:49:43.325975: step 254990, loss = 0.27 (1521.3 examples/sec; 0.084 sec/batch)
2017-06-02 08:49:44.335422: step 255000, loss = 0.32 (1268.0 examples/sec; 0.101 sec/batch)
2017-06-02 08:49:45.064889: step 255010, loss = 0.28 (1754.7 examples/sec; 0.073 sec/batch)
2017-06-02 08:49:45.913185: step 255020, loss = 0.36 (1508.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:49:46.803596: step 255030, loss = 0.23 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:49:47.698770: step 255040, loss = 0.33 (1429.9 examples/sec; 0.090 sec/batch)
2017-06-02 08:49:48.579773: step 255050, loss = 0.25 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:49:49.468605: step 255060, loss = 0.22 (1440.1 examples/sec; 0.089 sec/batch)
2017-06-02 08:49:50.361375: step 255070, loss = 0.25 (1433.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:49:51.247976: step 255080, loss = 0.24 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:49:52.311353: step 255090, loss = 0.32 (1203.7 examples/sec; 0.106 sec/batch)
2017-06-02 08:49:53.246055: step 255100, loss = 0.31 (1369.4 examples/sec; 0.093 sec/batch)
2017-06-02 08:49:54.032377: step 255110, loss = 0.31 (1627.8 examples/sec; 0.079 sec/batch)
2017-06-02 08:49:54.895544: step 255120, loss = 0.30 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:49:55.751530: step 255130, loss = 0.23 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:49:56.611746: step 255140, loss = 0.32 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:49:57.478263: step 255150, loss = 0.28 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:49:58.325851: step 255160, loss = 0.21 (1510.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:49:59.183325: step 255170, loss = 0.32 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:50:00.062653: step 255180, loss = 0.32 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:50:00.904041: step 255190, loss = 0.32 (1521.3 examples/sec; 0.084 sec/batch)
2017-06-02 08:50:01.903465: step 255200, loss = 0.24 (1280.7 examples/sec; 0.100 sec/batch)
2017-06-02 08:50:02.626389: step 255210, loss = 0.27 (1770.6 examples/sec; 0.072 sec/batch)
2017-06-02 08:50:03.496746: step 255220, loss = 0.27 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:50:04.369641: step 255230, loss = 0.29 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:50:05.263491: step 255240, loss = 0.19 (1432.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:50:06.151193: step 255250, loss = 0.37 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:50:07.042916: step 255260, loss = 0.27 (1435.4 examples/sec; 0.089 sec/batch)
2017-06-02 08:50:07.939838: step 255270, loss = 0.27 (1427.1 examples/sec; 0.090 sec/batch)
2017-06-02 08:50:08.851595: step 255280, loss = 0.27 (1403.9 examples/sec; 0.091 sec/batch)
2017-06-02 08:50:09.722375: step 255290, loss = 0.32 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:50:10.712944: step 255300, loss = 0.38 (1292.2 examples/sec; 0.099 sec/batch)
2017-06-02 08:50:11.429300: step 255310, loss = 0.33 (1786.8 examples/sec; 0.072 sec/batch)
2017-06-02 08:50:12.306706: step 255320, loss = 0.26 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:50:13.190347: step 255330, loss = 0.24 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:50:14.069547: step 255340, loss = 0.32 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:50:14.935918: step 255350, loss = 0.35 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:50:15.795447: step 255360, loss = 0.26 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:50:16.656016: step 255370, loss = 0.20 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:50:17.511959: step 255380, loss = 0.31 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:50:18.372415: step 255390, loss = 0.32 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:50:19.350459: step 255400, loss = 0.31 (1308.7 examples/sec; 0.098 sec/batch)
2017-06-02 08:50:20.121008: step 255410, loss = 0.29 (1661.2 examples/sec; 0.077 sec/batch)
2017-06-02 08:50:20.980295: step 255420, loss = 0.25 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:50:21.850231: step 255430, loss = 0.20 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:50:22.691111: step 255440, loss = 0.27 (1522.2 examples/sec; 0.084 sec/batch)
2017-06-02 08:50:23.556256: step 255450, loss = 0.31 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:50:24.434633: step 255460, loss = 0.31 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:50:25.309536: step 255470, loss = 0.35 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:50:26.145358: step 255480, loss = 0.20 (1531.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:50:27.022845: step 255490, loss = 0.28 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:50:28.008123: step 255500, loss = 0.19 (1299.1 examples/sec; 0.099 sec/batch)
2017-06-02 08:50:28.740626: step 255510, loss = 0.25 (1747.4 examples/sec; 0.073 sec/batch)
2017-06-02 08:50:29.593406: step 255520, loss = 0.27 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:50:30.437216: step 255530, loss = 0.23 (1516.9 examples/sec; 0.084 sec/batch)
2017-06-02 08:50:31.325039: step 255540, loss = 0.31 (1441.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:50:32.172969: step 255550, loss = 0.28 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:50:33.041817: step 255560, loss = 0.28 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:50:33.870438: step 255570, loss = 0.28 (1544.7 examples/sec; 0.083 sec/batch)
2017-06-02 08:50:34.736100: step 255580, loss = 0.25 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:50:35.562022: step 255590, loss = 0.25 (1549.8 examples/sec; 0.083 sec/batch)
2017-06-02 08:50:36.581291: step 255600, loss = 0.31 (1255.8 examples/sec; 0.102 sec/batch)
2017-06-02 08:50:37.339019: step 255610, loss = 0.34 (1689.3 examples/sec; 0.076 sec/batch)
2017-06-02 08:50:38.201038: step 255620, loss = 0.33 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:50:39.060106: step 255630, loss = 0.31 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:50:39.923085: step 255640, loss = 0.30 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:50:40.787022: step 255650, loss = 0.26 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:50:41.649498: step 255660, loss = 0.27 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:50:42.522447: step 255670, loss = 0.35 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:50:43.400146: step 255680, loss = 0.27 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:50:44.268813: step 255690, loss = 0.45 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:50:45.229490: step 255700, loss = 0.33 (1332.4 examples/sec; 0.096 sec/batch)
2017-06-02 08:50:46.000452: step 255710, loss = 0.30 (1660.3 examples/sec; 0.077 sec/batch)
2017-06-02 08:50:46.879351: step 255720, loss = 0.32 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:50:47.752469: step 255730, loss = 0.28 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:50:48.648760: step 255740, loss = 0.27 (1428.1 examples/sec; 0.090 sec/batch)
2017-06-02 08:50:49.521700: step 255750, loss = 0.30 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:50:50.389983: step 255760, loss = 0.23 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:50:51.270999: step 255770, loss = 0.30 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:50:52.154003: step 255780, loss = 0.22 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:50:53.026995: step 255790, loss = 0.28 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:50:54.004942: step 255800, loss = 0.29 (1308.9 examples/sec; 0.098 sec/batch)
2017-06-02 08:50:54.780781: step 255810, loss = 0.31 (1649.8 examples/sec; 0.078 sec/batch)
2017-06-02 08:50:55.674784: step 255820, loss = 0.34 (1431.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:50:56.546592: step 255830, loss = 0.26 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:50:57.406728: step 255840, loss = 0.31 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:50:58.305836: step 255850, loss = 0.33 (1423.6 examples/sec; 0.090 sec/batch)
2017-06-02 08:50:59.201645: step 255860, loss = 0.31 (1428.9 examples/sec; 0.090 sec/batch)
2017-06-02 08:51:00.079388: step 255870, loss = 0.30 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:51:00.944354: step 255880, loss = 0.26 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:51:01.807921: step 255890, loss = 0.25 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:51:02.782345: step 255900, loss = 0.22 (1313.6 examples/sec; 0.097 sec/batch)
2017-06-02 08:51:03.561958: step 255910, loss = 0.27 (1641.8 examples/sec; 0.078 sec/batch)
2017-06-02 08:51:04.447311: step 255920, loss = 0.30 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:51:05.326599: step 255930, loss = 0.25 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:51:06.191043: step 255940, loss = 0.32 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:51:07.073229: step 255950, loss = 0.26 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:51:07.929495: step 255960, loss = 0.28 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:51:08.812471: step 255970, loss = 0.32 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:51:09.701891: step 255980, loss = 0.29 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 08:51:10.606992: step 255990, loss = 0.31 (1414.2 examples/sec; 0.091 sec/batch)
2017-06-02 08:51:11.637863: step 256000, loss = 0.26 (1241.7 examples/sec; 0.103 sec/batch)
2017-06-02 08:51:12.379928: step 256010, loss = 0.27 (1724.9 examples/sec; 0.074 sec/batch)
2017-06-02 08:51:13.270983: step 256020, loss = 0.29 (1436.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:51:14.146403: step 256030, loss = 0.29 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:51:14.983772: step 256040, loss = 0.31 (1528.6 examples/sec; 0.084 sec/batch)
2017-06-02 08:51:15.857240: step 256050, loss = 0.24 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:51:16.730282: step 256060, loss = 0.28 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:51:17.598854: step 256070, loss = 0.35 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:51:18.475924: step 256080, loss = 0.37 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:51:19.337487: step 256090, loss = 0.24 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:51:20.289535: step 256100, loss = 0.26 (1344.5 examples/sec; 0.095 sec/batch)
2017-06-02 08:51:21.051689: step 256110, loss = 0.29 (1679.5 examples/sec; 0.076 sec/batch)
2017-06-02 08:51:21.894387: step 256120, loss = 0.33 (1518.9 examples/sec; 0.084 sec/batch)
2017-06-02 08:51:22.780771: step 256130, loss = 0.29 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 08:51:23.642671: step 256140, loss = 0.26 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:51:24.486880: step 256150, loss = 0.33 (1516.2 examples/sec; 0.084 sec/batch)
2017-06-02 08:51:25.342623: step 256160, loss = 0.23 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:51:26.206685: step 256170, loss = 0.25 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:51:27.066364: step 256180, loss = 0.25 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:51:27.936017: step 256190, loss = 0.24 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:51:28.914976: step 256200, loss = 0.26 (1307.5 examples/sec; 0.098 sec/batch)
2017-06-02 08:51:29.692842: step 256210, loss = 0.31 (1645.5 examples/sec; 0.078 sec/batch)
2017-06-02 08:51:30.546387: step 256220, loss = 0.30 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:51:31.429369: step 256230, loss = 0.26 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:51:32.285663: step 256240, loss = 0.27 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:51:33.172387: step 256250, loss = 0.26 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:51:34.053076: step 256260, loss = 0.33 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:51:34.903519: step 256270, loss = 0.21 (1505.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:51:35.809647: step 256280, loss = 0.24 (1412.6 examples/sec; 0.091 sec/batch)
2017-06-02 08:51:36.686909: step 256290, loss = 0.23 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:51:37.649696: step 256300, loss = 0.28 (1329.5 examples/sec; 0.096 sec/batch)
2017-06-02 08:51:38.426236: step 256310, loss = 0.29 (1648.3 examples/sec; 0.078 sec/batch)
2017-06-02 08:51:39.294150: step 256320, loss = 0.28 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:51:40.193253: step 256330, loss = 0.30 (1423.6 examples/sec; 0.090 sec/batch)
2017-06-02 08:51:41.054010: step 256340, loss = 0.30 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:51:41.892121: step 256350, loss = 0.27 (1527.2 examples/sec; 0.084 sec/batch)
2017-06-02 08:51:42.756294: step 256360, loss = 0.31 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:51:43.614625: step 256370, loss = 0.28 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:51:44.477168: step 256380, loss = 0.29 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:51:45.350956: step 256390, loss = 0.27 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:51:46.349763: step 256400, loss = 0.32 (1281.5 examples/sec; 0.100 sec/batch)
2017-06-02 08:51:47.127068: step 256410, loss = 0.33 (1646.7 examples/sec; 0.078 sec/batch)
2017-06-02 08:51:47.996815: step 256420, loss = 0.31 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:51:48.870873: step 256430, loss = 0.29 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:51:49.726409: step 256440, loss = 0.24 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:51:50.587898: step 256450, loss = 0.30 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:51:51.474995: step 256460, loss = 0.30 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:51:52.345424: step 256470, loss = 0.24 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:51:53.189870: step 256480, loss = 0.38 (1515.8 examples/sec; 0.084 sec/batch)
2017-06-02 08:51:54.058745: step 256490, loss = 0.37 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:51:55.039206: step 256500, loss = 0.35 (1305.5 examples/sec; 0.098 sec/batch)
2017-06-02 08:51:55.834938: step 256510, loss = 0.24 (1608.6 examples/sec; 0.080 sec/batch)
2017-06-02 08:51:56.737040: step 256520, loss = 0.29 (1418.9 examples/sec; 0.090 sec/batch)
2017-06-02 08:51:57.625238: step 256530, loss = 0.35 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 08:51:58.495830: step 256540, loss = 0.29 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:51:59.388399: step 256550, loss = 0.32 (1434.1 examples/sec; 0.089 sec/batch)
2017-06-02 08:52:00.271108: step 256560, loss = 0.30 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:52:01.125209: step 256570, loss = 0.26 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:52:02.004364: step 256580, loss = 0.22 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:52:02.885627: step 256590, loss = 0.34 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:52:03.864183: step 256600, loss = 0.35 (1308.1 examples/sec; 0.098 sec/batch)
2017-06-02 08:52:04.646294: step 256610, loss = 0.26 (1636.6 examples/sec; 0.078 sec/batch)
2017-06-02 08:52:05.502814: step 256620, loss = 0.29 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:52:06.364104: step 256630, loss = 0.33 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:52:07.249200: step 256640, loss = 0.30 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 08:52:08.128432: step 256650, loss = 0.28 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:52:09.005826: step 256660, loss = 0.27 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:52:09.851210: step 256670, loss = 0.25 (1514.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:52:10.747858: step 256680, loss = 0.29 (1427.5 examples/sec; 0.090 sec/batch)
2017-06-02 08:52:11.600699: step 256690, loss = 0.29 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:52:12.583215: step 256700, loss = 0.28 (1302.8 examples/sec; 0.098 sec/batch)
2017-06-02 08:52:13.358806: step 256710, loss = 0.36 (1650.4 examples/sec; 0.078 sec/batch)
2017-06-02 08:52:14.241183: step 256720, loss = 0.21 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:52:15.123346: step 256730, loss = 0.23 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:52:16.018610: step 256740, loss = 0.27 (1429.7 examples/sec; 0.090 sec/batch)
2017-06-02 08:52:16.897364: step 256750, loss = 0.32 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:52:17.770656: step 256760, loss = 0.33 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:52:18.655277: step 256770, loss = 0.30 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:52:19.532600: step 256780, loss = 0.26 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:52:20.416496: step 256790, loss = 0.33 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:52:21.408451: step 256800, loss = 0.29 (1290.4 examples/sec; 0.099 sec/batch)
2017-06-02 08:52:22.177051: step 256810, loss = 0.25 (1665.4 examples/sec; 0.077 sec/batch)
2017-06-02 08:52:23.038576: step 256820, loss = 0.31 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:52:23.912625: step 256830, loss = 0.25 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:52:24.786233: step 256840, loss = 0.29 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:52:25.678311: step 256850, loss = 0.23 (1434.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:52:26.544870: step 256860, loss = 0.21 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:52:27.405925: step 256870, loss = 0.31 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:52:28.286611: step 256880, loss = 0.29 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:52:29.161441: step 256890, loss = 0.44 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:52:30.131563: step 256900, loss = 0.33 (1319.4 examples/sec; 0.097 sec/batch)
2017-06-02 08:52:30.871582: step 256910, loss = 0.30 (1729.7 examples/sec; 0.074 sec/batch)
2017-06-02 08:52:31.737459: step 256920, loss = 0.24 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:52:32.615233: step 256930, loss = 0.31 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:52:33.477430: step 256940, loss = 0.29 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:52:34.334566: step 256950, loss = 0.36 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:52:35.210470: step 256960, loss = 0.23 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:52:36.090095: step 256970, loss = 0.28 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:52:36.978599: step 256980, loss = 0.20 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:52:37.840806: step 256990, loss = 0.30 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:52:38.822561: step 257000, loss = 0.32 (1303.8 examples/sec; 0.098 sec/batch)
2017-06-02 08:52:39.605606: step 257010, loss = 0.27 (1634.7 examples/sec; 0.078 sec/batch)
2017-06-02 08:52:40.448465: step 257020, loss = 0.28 (1518.6 examples/sec; 0.084 sec/batch)
2017-06-02 08:52:41.323656: step 257030, loss = 0.26 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:52:42.185264: step 257040, loss = 0.42 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:52:43.064153: step 257050, loss = 0.30 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:52:43.956355: step 257060, loss = 0.24 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:52:44.834560: step 257070, loss = 0.26 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:52:45.699618: step 257080, loss = 0.30 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:52:46.571000: step 257090, loss = 0.25 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:52:47.563132: step 257100, loss = 0.33 (1290.1 examples/sec; 0.099 sec/batch)
2017-06-02 08:52:48.314621: step 257110, loss = 0.34 (1703.3 examples/sec; 0.075 sec/batch)
2017-06-02 08:52:49.210061: step 257120, loss = 0.27 (1429.5 examples/sec; 0.090 sec/batch)
2017-06-02 08:52:50.084355: step 257130, loss = 0.41 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:52:50.951754: step 257140, loss = 0.28 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:52:51.849032: step 257150, loss = 0.27 (1426.5 examples/sec; 0.090 sec/batch)
2017-06-02 08:52:52.733588: step 257160, loss = 0.25 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:52:53.626580: step 257170, loss = 0.23 (1433.4 examples/sec; 0.089 sec/batch)
2017-06-02 08:52:54.525755: step 257180, loss = 0.31 (1423.5 examples/sec; 0.090 sec/batch)
2017-06-02 08:52:55.418344: step 257190, loss = 0.26 (1434.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:52:56.388717: step 257200, loss = 0.34 (1319.1 examples/sec; 0.097 sec/batch)
2017-06-02 08:52:57.143931: step 257210, loss = 0.33 (1694.9 examples/sec; 0.076 sec/batch)
2017-06-02 08:52:58.006815: step 257220, loss = 0.33 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:52:58.855500: step 257230, loss = 0.25 (1508.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:52:59.709388: step 257240, loss = 0.33 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:53:00.564860: step 257250, loss = 0.37 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:53:01.429039: step 257260, loss = 0.24 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:53:02.277072: step 257270, loss = 0.31 (1509.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:53:03.132252: step 257280, loss = 0.28 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:53:03.990775: step 257290, loss = 0.36 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:53:04.950881: step 257300, loss = 0.30 (1333.2 examples/sec; 0.096 sec/batch)
2017-06-02 08:53:05.701588: step 257310, loss = 0.32 (1705.1 examples/sec; 0.075 sec/batch)
2017-06-02 08:53:06.543859: step 257320, loss = 0.32 (1519.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:53:07.396644: step 257330, loss = 0.18 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:53:08.286930: step 257340, loss = 0.31 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:53:09.139364: step 257350, loss = 0.24 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:53:10.006832: step 257360, loss = 0.21 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:53:10.843418: step 257370, loss = 0.33 (1530.0 examples/sec; 0.084 sec/batch)
2017-06-02 08:53:11.701440: step 257380, loss = 0.24 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:53:12.555407: step 257390, loss = 0.30 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:53:13.506227: step 257400, loss = 0.26 (1346.2 examples/sec; 0.095 sec/batch)
2017-06-02 08:53:14.248133: step 257410, loss = 0.28 (1725.3 examples/sec; 0.074 sec/batch)
2017-06-02 08:53:15.094903: step 257420, loss = 0.31 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:53:15.935881: step 257430, loss = 0.29 (1522.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:53:16.781384: step 257440, loss = 0.27 (1513.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:53:17.651627: step 257450, loss = 0.27 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:53:18.512169: step 257460, loss = 0.28 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:53:19.399841: step 257470, loss = 0.26 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:53:20.277224: step 257480, loss = 0.32 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:53:21.129587: step 257490, loss = 0.26 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:53:22.107677: step 257500, loss = 0.24 (1308.7 examples/sec; 0.098 sec/batch)
2017-06-02 08:53:22.902510: step 257510, loss = 0.39 (1610.4 examples/sec; 0.079 sec/batch)
2017-06-02 08:53:23.787850: step 257520, loss = 0.28 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:53:24.672193: step 257530, loss = 0.32 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:53:25.557657: step 257540, loss = 0.34 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 08:53:26.411885: step 257550, loss = 0.29 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:53:27.278802: step 257560, loss = 0.24 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:53:28.179569: step 257570, loss = 0.28 (1421.0 examples/sec; 0.090 sec/batch)
2017-06-02 08:53:29.046015: step 257580, loss = 0.21 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:53:29.919490: step 257590, loss = 0.27 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:53:30.888213: step 257600, loss = 0.22 (1321.3 examples/sec; 0.097 sec/batch)
2017-06-02 08:53:31.665620: step 257610, loss = 0.30 (1646.5 examples/sec; 0.078 sec/batch)
2017-06-02 08:53:32.530231: step 257620, loss = 0.30 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:53:33.405517: step 257630, loss = 0.32 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:53:34.291435: step 257640, loss = 0.31 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:53:35.174041: step 257650, loss = 0.30 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:53:36.066838: step 257660, loss = 0.31 (1433.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:53:36.951195: step 257670, loss = 0.24 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:53:37.843171: step 257680, loss = 0.26 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:53:38.707139: step 257690, loss = 0.27 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:53:39.687568: step 257700, loss = 0.28 (1305.5 examples/sec; 0.098 sec/batch)
2017-06-02 08:53:40.449899: step 257710, loss = 0.26 (1679.1 examples/sec; 0.076 sec/batch)
2017-06-02 08:53:41.326551: step 257720, loss = 0.43 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:53:42.216098: step 257730, loss = 0.26 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:53:43.089587: step 257740, loss = 0.30 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:53:43.968285: step 257750, loss = 0.35 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:53:44.840836: step 257760, loss = 0.34 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:53:45.725547: step 257770, loss = 0.25 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:53:46.608963: step 257780, loss = 0.27 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:53:47.483494: step 257790, loss = 0.31 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:53:48.472857: step 257800, loss = 0.26 (1293.8 examples/sec; 0.099 sec/batch)
2017-06-02 08:53:49.245585: step 257810, loss = 0.26 (1656.5 examples/sec; 0.077 sec/batch)
2017-06-02 08:53:50.129009: step 257820, loss = 0.23 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:53:50.984016: step 257830, loss = 0.29 (1497.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:53:51.848562: step 257840, loss = 0.25 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:53:52.685565: step 257850, loss = 0.31 (1529.3 examples/sec; 0.084 sec/batch)
2017-06-02 08:53:53.536608: step 257860, loss = 0.24 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:53:54.380137: step 257870, loss = 0.32 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:53:55.251639: step 257880, loss = 0.30 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:53:56.125727: step 257890, loss = 0.28 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:53:57.096704: step 257900, loss = 0.19 (1318.3 examples/sec; 0.097 sec/batch)
2017-06-02 08:53:57.851769: step 257910, loss = 0.27 (1695.3 examples/sec; 0.076 sec/batch)
2017-06-02 08:53:58.715420: step 257920, loss = 0.33 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:53:59.586124: step 257930, loss = 0.29 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:54:00.425412: step 257940, loss = 0.34 (1525.1 examples/sec; 0.084 sec/batch)
2017-06-02 08:54:01.296638: step 257950, loss = 0.31 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:54:02.159458: step 257960, loss = 0.33 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:54:03.022922: step 257970, loss = 0.26 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:54:03.886079: step 257980, loss = 0.30 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:54:04.737825: step 257990, loss = 0.27 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:54:05.693711: step 258000, loss = 0.31 (1339.1 examples/sec; 0.096 sec/batch)
2017-06-02 08:54:06.446117: step 258010, loss = 0.30 (1701.2 examples/sec; 0.075 sec/batch)
2017-06-02 08:54:07.301362: step 258020, loss = 0.25 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:54:08.164689: step 258030, loss = 0.33 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:54:09.029571: step 258040, loss = 0.32 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:54:09.901264: step 258050, loss = 0.26 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:54:10.753329: step 258060, loss = 0.27 (1502.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:54:11.591752: step 258070, loss = 0.27 (1526.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:54:12.464209: step 258080, loss = 0.45 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:54:13.335562: step 258090, loss = 0.27 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:54:14.296108: step 258100, loss = 0.28 (1332.6 examples/sec; 0.096 sec/batch)
2017-06-02 08:54:15.068056: step 258110, loss = 0.29 (1658.2 examples/sec; 0.077 sec/batch)
2017-06-02 08:54:15.939718: step 258120, loss = 0.26 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:54:16.807969: step 258130, loss = 0.24 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:54:17.667967: step 258140, loss = 0.32 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:54:18.560142: step 258150, loss = 0.45 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:54:19.423318: step 258160, loss = 0.22 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:54:20.321577: step 258170, loss = 0.29 (1425.0 examples/sec; 0.090 sec/batch)
2017-06-02 08:54:21.195825: step 258180, loss = 0.28 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:54:22.047571: step 258190, loss = 0.31 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:54:23.031518: step 258200, loss = 0.22 (1300.9 examples/sec; 0.098 sec/batch)
2017-06-02 08:54:23.814419: step 258210, loss = 0.31 (1634.9 examples/sec; 0.078 sec/batch)
2017-06-02 08:54:24.684283: step 258220, loss = 0.39 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:54:25.538184: step 258230, loss = 0.32 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:54:26.387827: step 258240, loss = 0.26 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:54:27.263982: step 258250, loss = 0.36 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:54:28.137162: step 258260, loss = 0.31 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:54:29.031186: step 258270, loss = 0.27 (1431.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:54:29.900097: step 258280, loss = 0.30 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:54:30.764499: step 258290, loss = 0.31 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:54:31.766695: step 258300, loss = 0.31 (1277.2 examples/sec; 0.100 sec/batch)
2017-06-02 08:54:32.474711: step 258310, loss = 0.24 (1807.9 examples/sec; 0.071 sec/batch)
2017-06-02 08:54:33.378780: step 258320, loss = 0.31 (1415.8 examples/sec; 0.090 sec/batch)
2017-06-02 08:54:34.234771: step 258330, loss = 0.35 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:54:35.081352: step 258340, loss = 0.32 (1512.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:54:35.931718: step 258350, loss = 0.29 (1505.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:54:36.791483: step 258360, loss = 0.27 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:54:37.647512: step 258370, loss = 0.30 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:54:38.532379: step 258380, loss = 0.26 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:54:39.402941: step 258390, loss = 0.22 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:54:40.373217: step 258400, loss = 0.28 (1319.2 examples/sec; 0.097 sec/batch)
2017-06-02 08:54:41.139470: step 258410, loss = 0.32 (1670.5 examples/sec; 0.077 sec/batch)
2017-06-02 08:54:42.006700: step 258420, loss = 0.27 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:54:42.886416: step 258430, loss = 0.33 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:54:43.744023: step 258440, loss = 0.29 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:54:44.620745: step 258450, loss = 0.37 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:54:45.469229: step 258460, loss = 0.31 (1508.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:54:46.328020: step 258470, loss = 0.29 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:54:47.195852: step 258480, loss = 0.25 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:54:48.084785: step 258490, loss = 0.27 (1439.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:54:49.105141: step 258500, loss = 0.28 (1254.5 examples/sec; 0.102 sec/batch)
2017-06-02 08:54:49.803529: step 258510, loss = 0.28 (1832.8 examples/sec; 0.070 sec/batch)
2017-06-02 08:54:50.696593: step 258520, loss = 0.31 (1433.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:54:51.573709: step 258530, loss = 0.29 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:54:52.447909: step 258540, loss = 0.27 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:54:53.334665: step 258550, loss = 0.31 (1443.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:54:54.206172: step 258560, loss = 0.35 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:54:55.083426: step 258570, loss = 0.27 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:54:55.965214: step 258580, loss = 0.27 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:54:56.838578: step 258590, loss = 0.24 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:54:57.839047: step 258600, loss = 0.36 (1279.4 examples/sec; 0.100 sec/batch)
2017-06-02 08:54:58.605174: step 258610, loss = 0.27 (1670.7 examples/sec; 0.077 sec/batch)
2017-06-02 08:54:59.466446: step 258620, loss = 0.29 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:55:00.337194: step 258630, loss = 0.33 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:55:01.194538: step 258640, loss = 0.37 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:55:02.069268: step 258650, loss = 0.27 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:55:02.895043: step 258660, loss = 0.32 (1550.0 examples/sec; 0.083 sec/batch)
2017-06-02 08:55:03.734150: step 258670, loss = 0.23 (1525.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:55:04.584653: step 258680, loss = 0.29 (1505.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:55:05.460895: step 258690, loss = 0.24 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:55:06.407007: step 258700, loss = 0.31 (1352.9 examples/sec; 0.095 sec/batch)
2017-06-02 08:55:07.198011: step 258710, loss = 0.26 (1618.2 examples/sec; 0.079 sec/batch)
2017-06-02 08:55:08.054965: step 258720, loss = 0.32 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:55:08.921267: step 258730, loss = 0.29 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:55:09.793670: step 258740, loss = 0.28 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:55:10.641717: step 258750, loss = 0.27 (1509.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:55:11.503173: step 258760, loss = 0.30 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:55:12.362975: step 258770, loss = 0.29 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:55:13.212667: step 258780, loss = 0.26 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:55:14.070729: step 258790, loss = 0.29 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:55:15.080395: step 258800, loss = 0.25 (1267.8 examples/sec; 0.101 sec/batch)
2017-06-02 08:55:15.819270: step 258810, loss = 0.26 (1732.4 examples/sec; 0.074 sec/batch)
2017-06-02 08:55:16.685189: step 258820, loss = 0.30 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:55:17.554242: step 258830, loss = 0.28 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:55:18.410952: step 258840, loss = 0.31 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:55:19.285248: step 258850, loss = 0.25 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:55:20.157350: step 258860, loss = 0.32 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:55:21.042570: step 258870, loss = 0.28 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:55:21.909187: step 258880, loss = 0.25 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:55:22.785751: step 258890, loss = 0.26 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:55:23.761960: step 258900, loss = 0.28 (1311.2 examples/sec; 0.098 sec/batch)
2017-06-02 08:55:24.533359: step 258910, loss = 0.27 (1659.3 examples/sec; 0.077 sec/batch)
2017-06-02 08:55:25.393569: step 258920, loss = 0.35 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:55:26.276194: step 258930, loss = 0.26 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:55:27.134070: step 258940, loss = 0.29 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:55:28.005016: step 258950, loss = 0.26 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:55:28.894608: step 258960, loss = 0.25 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:55:29.767310: step 258970, loss = 0.22 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:55:30.658331: step 258980, loss = 0.25 (1436.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:55:31.523288: step 258990, loss = 0.32 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:55:32.492666: step 259000, loss = 0.31 (1320.4 examples/sec; 0.097 sec/batch)
2017-06-02 08:55:33.254748: step 259010, loss = 0.30 (1679.6 examples/sec; 0.076 sec/batch)
2017-06-02 08:55:34.105557: step 259020, loss = 0.24 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:55:34.963113: step 259030, loss = 0.30 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:55:35.837143: step 259040, loss = 0.22 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:55:36.691710: step 259050, loss = 0.26 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 08:55:37.562618: step 259060, loss = 0.28 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:55:38.457347: step 259070, loss = 0.30 (1430.6 examples/sec; 0.089 sec/batch)
2017-06-02 08:55:39.351008: step 259080, loss = 0.35 (1432.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:55:40.226239: step 259090, loss = 0.23 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:55:41.212999: step 259100, loss = 0.36 (1297.2 examples/sec; 0.099 sec/batch)
2017-06-02 08:55:41.997315: step 259110, loss = 0.26 (1632.0 examples/sec; 0.078 sec/batch)
2017-06-02 08:55:42.872152: step 259120, loss = 0.28 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:55:43.762873: step 259130, loss = 0.27 (1437.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:55:44.634489: step 259140, loss = 0.29 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:55:45.507916: step 259150, loss = 0.29 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:55:46.381718: step 259160, loss = 0.33 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:55:47.268637: step 259170, loss = 0.27 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:55:48.150714: step 259180, loss = 0.22 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:55:49.052017: step 259190, loss = 0.22 (1420.2 examples/sec; 0.090 sec/batch)
2017-06-02 08:55:50.028293: step 259200, loss = 0.24 (1311.1 examples/sec; 0.098 sec/batch)
2017-06-02 08:55:50.801681: step 259210, loss = 0.24 (1655.0 examples/sec; 0.077 sec/batch)
2017-06-02 08:55:51.672365: step 259220, loss = 0.28 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:55:52.544156: step 259230, loss = 0.27 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:55:53.420581: step 259240, loss = 0.20 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:55:54.299721: step 259250, loss = 0.30 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:55:55.183140: step 259260, loss = 0.28 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:55:56.056452: step 259270, loss = 0.28 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:55:56.933864: step 259280, loss = 0.25 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:55:57.788843: step 259290, loss = 0.31 (1497.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:55:58.794450: step 259300, loss = 0.25 (1272.9 examples/sec; 0.101 sec/batch)
2017-06-02 08:55:59.551520: step 259310, loss = 0.28 (1690.8 examples/sec; 0.076 sec/batch)
2017-06-02 08:56:00.438187: step 259320, loss = 0.25 (1443.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:56:01.309583: step 259330, loss = 0.34 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:56:02.191209: step 259340, loss = 0.23 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:56:03.055836: step 259350, loss = 0.25 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:56:03.926026: step 259360, loss = 0.33 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:56:04.804233: step 259370, loss = 0.28 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:56:05.681377: step 259380, loss = 0.22 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:56:06.532344: step 259390, loss = 0.30 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:56:07.518957: step 259400, loss = 0.30 (1297.3 examples/sec; 0.099 sec/batch)
2017-06-02 08:56:08.233919: step 259410, loss = 0.28 (1790.3 examples/sec; 0.071 sec/batch)
2017-06-02 08:56:09.074087: step 259420, loss = 0.30 (1523.5 examples/sec; 0.084 sec/batch)
2017-06-02 08:56:09.953894: step 259430, loss = 0.31 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:56:10.811073: step 259440, loss = 0.42 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:56:11.645881: step 259450, loss = 0.25 (1533.3 examples/sec; 0.083 sec/batch)
2017-06-02 08:56:12.519997: step 259460, loss = 0.27 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:56:13.373319: step 259470, loss = 0.23 (1500.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:56:14.225423: step 259480, loss = 0.34 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:56:15.079909: step 259490, loss = 0.31 (1498.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:56:16.052649: step 259500, loss = 0.32 (1315.9 examples/sec; 0.097 sec/batch)
2017-06-02 08:56:16.809939: step 259510, loss = 0.32 (1690.2 examples/sec; 0.076 sec/batch)
2017-06-02 08:56:17.684026: step 259520, loss = 0.36 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:56:18.535345: step 259530, loss = 0.31 (1503.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:56:19.398595: step 259540, loss = 0.22 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:56:20.244483: step 259550, loss = 0.30 (1513.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:56:21.130077: step 259560, loss = 0.30 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:56:22.010462: step 259570, loss = 0.39 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:56:22.871958: step 259580, loss = 0.27 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:56:23.742415: step 259590, loss = 0.36 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:56:24.710713: step 259600, loss = 0.25 (1321.9 examples/sec; 0.097 sec/batch)
2017-06-02 08:56:25.484849: step 259610, loss = 0.37 (1653.5 examples/sec; 0.077 sec/batch)
2017-06-02 08:56:26.330830: step 259620, loss = 0.27 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:56:27.204018: step 259630, loss = 0.21 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:56:28.074510: step 259640, loss = 0.22 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:56:28.943636: step 259650, loss = 0.31 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:56:29.807890: step 259660, loss = 0.19 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:56:30.659182: step 259670, loss = 0.28 (1503.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:56:31.536132: step 259680, loss = 0.20 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:56:32.399605: step 259690, loss = 0.31 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:56:33.378862: step 259700, loss = 0.40 (1307.1 examples/sec; 0.098 sec/batch)
2017-06-02 08:56:34.145317: step 259710, loss = 0.27 (1670.0 examples/sec; 0.077 sec/batch)
2017-06-02 08:56:35.004645: step 259720, loss = 0.29 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:56:35.866136: step 259730, loss = 0.30 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:56:36.719876: step 259740, loss = 0.31 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:56:37.583892: step 259750, loss = 0.24 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:56:38.452912: step 259760, loss = 0.29 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:56:39.333728: step 259770, loss = 0.19 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:56:40.190255: step 259780, loss = 0.26 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:56:41.061262: step 259790, loss = 0.37 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:56:42.028559: step 259800, loss = 0.29 (1323.3 examples/sec; 0.097 sec/batch)
2017-06-02 08:56:42.797161: step 259810, loss = 0.24 (1665.4 examples/sec; 0.077 sec/batch)
2017-06-02 08:56:43.683244: step 259820, loss = 0.33 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 08:56:44.544390: step 259830, loss = 0.37 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:56:45.408787: step 259840, loss = 0.31 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 08:56:46.264290: step 259850, loss = 0.30 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:56:47.156324: step 259860, loss = 0.36 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:56:48.039476: step 259870, loss = 0.31 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 08:56:48.924813: step 259880, loss = 0.32 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:56:49.809287: step 259890, loss = 0.24 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:56:50.769503: step 259900, loss = 0.30 (1333.0 examples/sec; 0.096 sec/batch)
2017-06-02 08:56:51.523719: step 259910, loss = 0.29 (1697.1 examples/sec; 0.075 sec/batch)
2017-06-02 08:56:52.359042: step 259920, loss = 0.36 (1532.3 examples/sec; 0.084 sec/batch)
2017-06-02 08:56:53.227721: step 259930, loss = 0.25 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:56:54.099566: step 259940, loss = 0.26 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:56:54.965386: step 259950, loss = 0.27 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:56:55.829340: step 259960, loss = 0.33 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:56:56.715113: step 259970, loss = 0.26 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 08:56:57.589909: step 259980, loss = 0.24 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:56:58.448289: step 259990, loss = 0.26 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 08:56:59.457626: step 260000, loss = 0.34 (1268.2 examples/sec; 0.101 sec/batch)
2017-06-02 08:57:00.194486: step 260010, loss = 0.25 (1737.1 examples/sec; 0.074 sec/batch)
2017-06-02 08:57:01.095659: step 260020, loss = 0.24 (1420.4 examples/sec; 0.090 sec/batch)
2017-06-02 08:57:01.976817: step 260030, loss = 0.22 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:57:02.872210: step 260040, loss = 0.31 (1429.5 examples/sec; 0.090 sec/batch)
2017-06-02 08:57:03.763634: step 260050, loss = 0.31 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:57:04.652906: step 260060, loss = 0.32 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 08:57:05.535335: step 260070, loss = 0.28 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:57:06.428872: step 260080, loss = 0.33 (1432.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:57:07.303810: step 260090, loss = 0.38 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:57:08.295282: step 260100, loss = 0.24 (1291.0 examples/sec; 0.099 sec/batch)
2017-06-02 08:57:09.071928: step 260110, loss = 0.34 (1648.1 examples/sec; 0.078 sec/batch)
2017-06-02 08:57:09.959454: step 260120, loss = 0.23 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:57:10.841289: step 260130, loss = 0.27 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:57:11.727088: step 260140, loss = 0.28 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:57:12.629522: step 260150, loss = 0.28 (1418.4 examples/sec; 0.090 sec/batch)
2017-06-02 08:57:13.501710: step 260160, loss = 0.32 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:57:14.364033: step 260170, loss = 0.28 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:57:15.232985: step 260180, loss = 0.31 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:57:16.100792: step 260190, loss = 0.30 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:57:17.065896: step 260200, loss = 0.27 (1326.3 examples/sec; 0.097 sec/batch)
2017-06-02 08:57:17.835101: step 260210, loss = 0.23 (1664.1 examples/sec; 0.077 sec/batch)
2017-06-02 08:57:18.675808: step 260220, loss = 0.34 (1522.5 examples/sec; 0.084 sec/batch)
2017-06-02 08:57:19.563012: step 260230, loss = 0.29 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:57:20.422337: step 260240, loss = 0.26 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:57:21.297382: step 260250, loss = 0.26 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:57:22.166173: step 260260, loss = 0.29 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:57:23.056449: step 260270, loss = 0.37 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:57:23.904531: step 260280, loss = 0.27 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:57:24.777076: step 260290, loss = 0.26 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:57:25.738351: step 260300, loss = 0.29 (1331.6 examples/sec; 0.096 sec/batch)
2017-06-02 08:57:26.518570: step 260310, loss = 0.27 (1640.6 examples/sec; 0.078 sec/batch)
2017-06-02 08:57:27.363146: step 260320, loss = 0.28 (1515.5 examples/sec; 0.084 sec/batch)
2017-06-02 08:57:28.204931: step 260330, loss = 0.28 (1520.6 examples/sec; 0.084 sec/batch)
2017-06-02 08:57:29.075258: step 260340, loss = 0.33 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:57:29.940499: step 260350, loss = 0.28 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:57:30.814579: step 260360, loss = 0.24 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:57:31.690022: step 260370, loss = 0.28 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:57:32.562913: step 260380, loss = 0.37 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:57:33.425842: step 260390, loss = 0.34 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 08:57:34.403077: step 260400, loss = 0.32 (1309.8 examples/sec; 0.098 sec/batch)
2017-06-02 08:57:35.171150: step 260410, loss = 0.31 (1666.5 examples/sec; 0.077 sec/batch)
2017-06-02 08:57:36.029058: step 260420, loss = 0.27 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 08:57:36.884049: step 260430, loss = 0.38 (1497.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:57:37.777208: step 260440, loss = 0.25 (1433.1 examples/sec; 0.089 sec/batch)
2017-06-02 08:57:38.624777: step 260450, loss = 0.27 (1510.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:57:39.468881: step 260460, loss = 0.26 (1516.4 examples/sec; 0.084 sec/batch)
2017-06-02 08:57:40.339291: step 260470, loss = 0.28 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:57:41.223404: step 260480, loss = 0.26 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:57:42.079878: step 260490, loss = 0.32 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 08:57:43.023348: step 260500, loss = 0.28 (1356.7 examples/sec; 0.094 sec/batch)
2017-06-02 08:57:43.797278: step 260510, loss = 0.27 (1653.9 examples/sec; 0.077 sec/batch)
2017-06-02 08:57:44.645129: step 260520, loss = 0.29 (1509.7 examples/sec; 0.085 sec/batch)
2017-06-02 08:57:45.530365: step 260530, loss = 0.25 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:57:46.377996: step 260540, loss = 0.25 (1510.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:57:47.263521: step 260550, loss = 0.27 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:57:48.101948: step 260560, loss = 0.26 (1526.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:57:48.968822: step 260570, loss = 0.34 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:57:49.841482: step 260580, loss = 0.38 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:57:50.722849: step 260590, loss = 0.31 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:57:51.699659: step 260600, loss = 0.28 (1310.4 examples/sec; 0.098 sec/batch)
2017-06-02 08:57:52.456276: step 260610, loss = 0.20 (1691.7 examples/sec; 0.076 sec/batch)
2017-06-02 08:57:53.345252: step 260620, loss = 0.30 (1439.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:57:54.209129: step 260630, loss = 0.35 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:57:55.081843: step 260640, loss = 0.23 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:57:55.935655: step 260650, loss = 0.28 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:57:56.820833: step 260660, loss = 0.24 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 08:57:57.688629: step 260670, loss = 0.28 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:57:58.566907: step 260680, loss = 0.27 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:57:59.436344: step 260690, loss = 0.28 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:58:00.385355: step 260700, loss = 0.28 (1348.8 examples/sec; 0.095 sec/batch)
2017-06-02 08:58:01.150390: step 260710, loss = 0.26 (1673.1 examples/sec; 0.077 sec/batch)
2017-06-02 08:58:02.024128: step 260720, loss = 0.31 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:58:02.886975: step 260730, loss = 0.33 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:58:03.753263: step 260740, loss = 0.27 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:58:04.626751: step 260750, loss = 0.31 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:58:05.496072: step 260760, loss = 0.35 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:58:06.344577: step 260770, loss = 0.34 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:58:07.239275: step 260780, loss = 0.32 (1430.6 examples/sec; 0.089 sec/batch)
2017-06-02 08:58:08.123635: step 260790, loss = 0.36 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 08:58:09.142438: step 260800, loss = 0.24 (1256.4 examples/sec; 0.102 sec/batch)
2017-06-02 08:58:09.854776: step 260810, loss = 0.34 (1796.9 examples/sec; 0.071 sec/batch)
2017-06-02 08:58:10.729637: step 260820, loss = 0.27 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:58:11.605659: step 260830, loss = 0.28 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:58:12.468355: step 260840, loss = 0.23 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:58:13.341240: step 260850, loss = 0.38 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:58:14.203858: step 260860, loss = 0.19 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:58:15.049298: step 260870, loss = 0.29 (1514.0 examples/sec; 0.085 sec/batch)
2017-06-02 08:58:15.898303: step 260880, loss = 0.35 (1507.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:58:16.783338: step 260890, loss = 0.22 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:58:17.760658: step 260900, loss = 0.24 (1309.7 examples/sec; 0.098 sec/batch)
2017-06-02 08:58:18.552259: step 260910, loss = 0.26 (1617.0 examples/sec; 0.079 sec/batch)
2017-06-02 08:58:19.416714: step 260920, loss = 0.34 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:58:20.288982: step 260930, loss = 0.22 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 08:58:21.176056: step 260940, loss = 0.30 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:58:22.062274: step 260950, loss = 0.39 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:58:22.923937: step 260960, loss = 0.26 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:58:23.780073: step 260970, loss = 0.33 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:58:24.628159: step 260980, loss = 0.25 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 08:58:25.479566: step 260990, loss = 0.33 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:58:26.473391: step 261000, loss = 0.21 (1287.9 examples/sec; 0.099 sec/batch)
2017-06-02 08:58:27.251888: step 261010, loss = 0.24 (1644.2 examples/sec; 0.078 sec/batch)
2017-06-02 08:58:28.120779: step 261020, loss = 0.26 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:58:28.995766: step 261030, loss = 0.19 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:58:29.825472: step 261040, loss = 0.32 (1542.7 examples/sec; 0.083 sec/batch)
2017-06-02 08:58:30.692183: step 261050, loss = 0.38 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 08:58:31.568791: step 261060, loss = 0.29 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 08:58:32.446254: step 261070, loss = 0.32 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:58:33.337484: step 261080, loss = 0.27 (1436.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:58:34.191865: step 261090, loss = 0.34 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:58:35.174648: step 261100, loss = 0.27 (1302.4 examples/sec; 0.098 sec/batch)
2017-06-02 08:58:35.961500: step 261110, loss = 0.23 (1626.7 examples/sec; 0.079 sec/batch)
2017-06-02 08:58:36.849092: step 261120, loss = 0.25 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 08:58:37.738799: step 261130, loss = 0.19 (1438.7 examples/sec; 0.089 sec/batch)
2017-06-02 08:58:38.627824: step 261140, loss = 0.32 (1439.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:58:39.505314: step 261150, loss = 0.23 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:58:40.388626: step 261160, loss = 0.33 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:58:41.279675: step 261170, loss = 0.29 (1436.5 examples/sec; 0.089 sec/batch)
2017-06-02 08:58:42.082409: step 261180, loss = 0.23 (1594.5 examples/sec; 0.080 sec/batch)
2017-06-02 08:58:42.976977: step 261190, loss = 0.28 (1430.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:58:43.932093: step 261200, loss = 0.43 (1340.1 examples/sec; 0.096 sec/batch)
2017-06-02 08:58:44.662718: step 261210, loss = 0.21 (1751.9 examples/sec; 0.073 sec/batch)
2017-06-02 08:58:45.493652: step 261220, loss = 0.27 (1540.4 examples/sec; 0.083 sec/batch)
2017-06-02 08:58:46.345811: step 261230, loss = 0.26 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 08:58:47.190214: step 261240, loss = 0.30 (1515.9 examples/sec; 0.084 sec/batch)
2017-06-02 08:58:48.080190: step 261250, loss = 0.24 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:58:48.976703: step 261260, loss = 0.31 (1427.8 examples/sec; 0.090 sec/batch)
2017-06-02 08:58:49.855376: step 261270, loss = 0.27 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 08:58:50.707812: step 261280, loss = 0.30 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:58:51.588957: step 261290, loss = 0.30 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:58:52.551385: step 261300, loss = 0.36 (1330.0 examples/sec; 0.096 sec/batch)
2017-06-02 08:58:53.330416: step 261310, loss = 0.33 (1643.1 examples/sec; 0.078 sec/batch)
2017-06-02 08:58:54.191433: step 261320, loss = 0.28 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:58:55.058667: step 261330, loss = 0.29 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:58:55.935368: step 261340, loss = 0.31 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:58:56.812044: step 261350, loss = 0.27 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:58:57.678046: step 261360, loss = 0.25 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 08:58:58.546623: step 261370, loss = 0.38 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:58:59.427563: step 261380, loss = 0.33 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:59:00.329944: step 261390, loss = 0.21 (1418.5 examples/sec; 0.090 sec/batch)
2017-06-02 08:59:01.300853: step 261400, loss = 0.26 (1318.4 examples/sec; 0.097 sec/batch)
2017-06-02 08:59:02.095373: step 261410, loss = 0.27 (1611.0 examples/sec; 0.079 sec/batch)
2017-06-02 08:59:02.991611: step 261420, loss = 0.22 (1428.2 examples/sec; 0.090 sec/batch)
2017-06-02 08:59:03.851792: step 261430, loss = 0.23 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 08:59:04.700326: step 261440, loss = 0.35 (1508.5 examples/sec; 0.085 sec/batch)
2017-06-02 08:59:05.560608: step 261450, loss = 0.27 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 08:59:06.464983: step 261460, loss = 0.35 (1415.3 examples/sec; 0.090 sec/batch)
2017-06-02 08:59:07.339196: step 261470, loss = 0.34 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 08:59:08.223278: step 261480, loss = 0.25 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:59:09.132545: step 261490, loss = 0.30 (1407.7 examples/sec; 0.091 sec/batch)
2017-06-02 08:59:10.117146: step 261500, loss = 0.22 (1300.0 examples/sec; 0.098 sec/batch)
2017-06-02 08:59:10.906138: step 261510, loss = 0.27 (1622.3 examples/sec; 0.079 sec/batch)
2017-06-02 08:59:11.817283: step 261520, loss = 0.32 (1404.8 examples/sec; 0.091 sec/batch)
2017-06-02 08:59:12.718217: step 261530, loss = 0.25 (1420.8 examples/sec; 0.090 sec/batch)
2017-06-02 08:59:13.594372: step 261540, loss = 0.32 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 08:59:14.445473: step 261550, loss = 0.23 (1503.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:59:15.302442: step 261560, loss = 0.29 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:59:16.169162: step 261570, loss = 0.22 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:59:17.060962: step 261580, loss = 0.35 (1435.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:59:17.941818: step 261590, loss = 0.30 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 08:59:18.897652: step 261600, loss = 0.26 (1339.1 examples/sec; 0.096 sec/batch)
2017-06-02 08:59:19.665319: step 261610, loss = 0.38 (1667.4 examples/sec; 0.077 sec/batch)
2017-06-02 08:59:20.562935: step 261620, loss = 0.26 (1426.0 examples/sec; 0.090 sec/batch)
2017-06-02 08:59:21.444107: step 261630, loss = 0.34 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:59:22.316776: step 261640, loss = 0.34 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 08:59:23.199664: step 261650, loss = 0.31 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 08:59:24.084228: step 261660, loss = 0.25 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:59:24.983756: step 261670, loss = 0.32 (1423.0 examples/sec; 0.090 sec/batch)
2017-06-02 08:59:25.869930: step 261680, loss = 0.28 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 08:59:26.709221: step 261690, loss = 0.30 (1525.2 examples/sec; 0.084 sec/batch)
2017-06-02 08:59:27.692452: step 261700, loss = 0.23 (1301.8 examples/sec; 0.098 sec/batch)
2017-06-02 08:59:28.463739: step 261710, loss = 0.35 (1659.6 examples/sec; 0.077 sec/batch)
2017-06-02 08:59:29.321931: step 261720, loss = 0.21 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:59:30.197681: step 261730, loss = 0.26 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 08:59:31.041086: step 261740, loss = 0.28 (1517.7 examples/sec; 0.084 sec/batch)
2017-06-02 08:59:31.887964: step 261750, loss = 0.23 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 08:59:32.764100: step 261760, loss = 0.24 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:59:33.649220: step 261770, loss = 0.29 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:59:34.494717: step 261780, loss = 0.26 (1513.9 examples/sec; 0.085 sec/batch)
2017-06-02 08:59:35.335005: step 261790, loss = 0.34 (1523.3 examples/sec; 0.084 sec/batch)
2017-06-02 08:59:36.293977: step 261800, loss = 0.31 (1334.8 examples/sec; 0.096 sec/batch)
2017-06-02 08:59:37.075231: step 261810, loss = 0.26 (1638.4 examples/sec; 0.078 sec/batch)
2017-06-02 08:59:37.920545: step 261820, loss = 0.22 (1514.2 examples/sec; 0.085 sec/batch)
2017-06-02 08:59:38.807079: step 261830, loss = 0.35 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 08:59:39.667465: step 261840, loss = 0.30 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 08:59:40.557632: step 261850, loss = 0.26 (1437.9 examples/sec; 0.089 sec/batch)
2017-06-02 08:59:41.448925: step 261860, loss = 0.23 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 08:59:42.323076: step 261870, loss = 0.30 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 08:59:43.198557: step 261880, loss = 0.25 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:59:44.073478: step 261890, loss = 0.52 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:59:45.064905: step 261900, loss = 0.26 (1291.1 examples/sec; 0.099 sec/batch)
2017-06-02 08:59:45.820960: step 261910, loss = 0.30 (1693.0 examples/sec; 0.076 sec/batch)
2017-06-02 08:59:46.692532: step 261920, loss = 0.27 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 08:59:47.567122: step 261930, loss = 0.32 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 08:59:48.458985: step 261940, loss = 0.21 (1435.2 examples/sec; 0.089 sec/batch)
2017-06-02 08:59:49.315983: step 261950, loss = 0.25 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 08:59:50.200540: step 261960, loss = 0.23 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 08:59:51.090887: step 261970, loss = 0.33 (1437.6 examples/sec; 0.089 sec/batch)
2017-06-02 08:59:51.919432: step 261980, loss = 0.21 (1544.9 examples/sec; 0.083 sec/batch)
2017-06-02 08:59:52.809338: step 261990, loss = 0.33 (1438.3 examples/sec; 0.089 sec/batch)
2017-06-02 08:59:53.819954: step 262000, loss = 0.22 (1266.6 examples/sec; 0.101 sec/batch)
2017-06-02 08:59:54.541708: step 262010, loss = 0.30 (1773.5 examples/sec; 0.072 sec/batch)
2017-06-02 08:59:55.403350: step 262020, loss = 0.26 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 08:59:56.268825: step 262030, loss = 0.22 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 08:59:57.142714: step 262040, loss = 0.25 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 08:59:58.025149: step 262050, loss = 0.30 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 08:59:58.877591: step 262060, loss = 0.26 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 08:59:59.764528: step 262070, loss = 0.32 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:00:00.634390: step 262080, loss = 0.25 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:00:01.501060: step 262090, loss = 0.27 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:00:02.460212: step 262100, loss = 0.27 (1334.5 examples/sec; 0.096 sec/batch)
2017-06-02 09:00:03.215306: step 262110, loss = 0.27 (1695.1 examples/sec; 0.076 sec/batch)
2017-06-02 09:00:04.087401: step 262120, loss = 0.29 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:00:04.951434: step 262130, loss = 0.28 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:00:05.810899: step 262140, loss = 0.32 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:00:06.672477: step 262150, loss = 0.26 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:00:07.535970: step 262160, loss = 0.28 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:00:08.387879: step 262170, loss = 0.32 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:00:09.268927: step 262180, loss = 0.32 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:00:10.118930: step 262190, loss = 0.28 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:00:11.090847: step 262200, loss = 0.30 (1317.0 examples/sec; 0.097 sec/batch)
2017-06-02 09:00:11.863046: step 262210, loss = 0.23 (1657.6 examples/sec; 0.077 sec/batch)
2017-06-02 09:00:12.737508: step 262220, loss = 0.32 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:00:13.581067: step 262230, loss = 0.26 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:00:14.408145: step 262240, loss = 0.27 (1547.6 examples/sec; 0.083 sec/batch)
2017-06-02 09:00:15.268967: step 262250, loss = 0.33 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:00:16.127035: step 262260, loss = 0.25 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:00:17.004393: step 262270, loss = 0.34 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:00:17.859672: step 262280, loss = 0.33 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:00:18.738263: step 262290, loss = 0.27 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:00:19.776656: step 262300, loss = 0.30 (1232.7 examples/sec; 0.104 sec/batch)
2017-06-02 09:00:20.523078: step 262310, loss = 0.27 (1714.9 examples/sec; 0.075 sec/batch)
2017-06-02 09:00:21.408844: step 262320, loss = 0.27 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:00:22.285357: step 262330, loss = 0.32 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:00:23.176507: step 262340, loss = 0.22 (1436.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:00:24.046486: step 262350, loss = 0.35 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:00:24.914476: step 262360, loss = 0.30 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:00:25.797326: step 262370, loss = 0.24 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:00:26.656098: step 262380, loss = 0.32 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:00:27.532042: step 262390, loss = 0.29 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:00:28.524452: step 262400, loss = 0.24 (1289.8 examples/sec; 0.099 sec/batch)
2017-06-02 09:00:29.262890: step 262410, loss = 0.26 (1733.4 examples/sec; 0.074 sec/batch)
2017-06-02 09:00:30.124173: step 262420, loss = 0.31 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:00:30.985385: step 262430, loss = 0.31 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:00:31.838569: step 262440, loss = 0.21 (1500.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:00:32.705745: step 262450, loss = 0.37 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:00:33.574677: step 262460, loss = 0.21 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:00:34.429376: step 262470, loss = 0.29 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:00:35.316282: step 262480, loss = 0.31 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:00:36.208255: step 262490, loss = 0.29 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:00:37.215240: step 262500, loss = 0.33 (1271.1 examples/sec; 0.101 sec/batch)
2017-06-02 09:00:37.956196: step 262510, loss = 0.36 (1727.5 examples/sec; 0.074 sec/batch)
2017-06-02 09:00:38.846307: step 262520, loss = 0.31 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:00:39.741778: step 262530, loss = 0.25 (1429.4 examples/sec; 0.090 sec/batch)
2017-06-02 09:00:40.630563: step 262540, loss = 0.35 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:00:41.496429: step 262550, loss = 0.33 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:00:42.379511: step 262560, loss = 0.33 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:00:43.254256: step 262570, loss = 0.28 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:00:44.127931: step 262580, loss = 0.27 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:00:45.030354: step 262590, loss = 0.33 (1418.4 examples/sec; 0.090 sec/batch)
2017-06-02 09:00:46.002042: step 262600, loss = 0.37 (1317.3 examples/sec; 0.097 sec/batch)
2017-06-02 09:00:46.772588: step 262610, loss = 0.32 (1661.2 examples/sec; 0.077 sec/batch)
2017-06-02 09:00:47.642402: step 262620, loss = 0.29 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:00:48.530342: step 262630, loss = 0.24 (1441.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:00:49.401843: step 262640, loss = 0.24 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:00:50.297422: step 262650, loss = 0.29 (1429.2 examples/sec; 0.090 sec/batch)
2017-06-02 09:00:51.162995: step 262660, loss = 0.34 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:00:52.023219: step 262670, loss = 0.30 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:00:52.903923: step 262680, loss = 0.31 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:00:53.785064: step 262690, loss = 0.27 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:00:54.776574: step 262700, loss = 0.38 (1290.9 examples/sec; 0.099 sec/batch)
2017-06-02 09:00:55.521469: step 262710, loss = 0.45 (1718.4 examples/sec; 0.074 sec/batch)
2017-06-02 09:00:56.389464: step 262720, loss = 0.34 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:00:57.251006: step 262730, loss = 0.32 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:00:58.129862: step 262740, loss = 0.33 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:00:58.981202: step 262750, loss = 0.22 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:00:59.813656: step 262760, loss = 0.41 (1537.6 examples/sec; 0.083 sec/batch)
2017-06-02 09:01:00.642827: step 262770, loss = 0.34 (1543.7 examples/sec; 0.083 sec/batch)
2017-06-02 09:01:01.527994: step 262780, loss = 0.33 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:01:02.393996: step 262790, loss = 0.27 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:01:03.380353: step 262800, loss = 0.26 (1297.7 examples/sec; 0.099 sec/batch)
2017-06-02 09:01:04.138537: step 262810, loss = 0.28 (1688.2 examples/sec; 0.076 sec/batch)
2017-06-02 09:01:05.003170: step 262820, loss = 0.30 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:01:05.852166: step 262830, loss = 0.32 (1507.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:01:06.715780: step 262840, loss = 0.31 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:01:07.571167: step 262850, loss = 0.32 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:01:08.431481: step 262860, loss = 0.27 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:01:09.308259: step 262870, loss = 0.30 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:01:10.157251: step 262880, loss = 0.26 (1507.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:01:11.010477: step 262890, loss = 0.29 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:01:12.051864: step 262900, loss = 0.31 (1229.1 examples/sec; 0.104 sec/batch)
2017-06-02 09:01:12.789617: step 262910, loss = 0.34 (1735.0 examples/sec; 0.074 sec/batch)
2017-06-02 09:01:13.679079: step 262920, loss = 0.35 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:01:14.530646: step 262930, loss = 0.31 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:01:15.413175: step 262940, loss = 0.29 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:01:16.265588: step 262950, loss = 0.35 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:01:17.113913: step 262960, loss = 0.39 (1508.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:01:17.940462: step 262970, loss = 0.29 (1548.6 examples/sec; 0.083 sec/batch)
2017-06-02 09:01:18.773393: step 262980, loss = 0.34 (1536.7 examples/sec; 0.083 sec/batch)
2017-06-02 09:01:19.643176: step 262990, loss = 0.28 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:01:20.621444: step 263000, loss = 0.27 (1308.4 examples/sec; 0.098 sec/batch)
2017-06-02 09:01:21.380642: step 263010, loss = 0.30 (1686.0 examples/sec; 0.076 sec/batch)
2017-06-02 09:01:22.224333: step 263020, loss = 0.35 (1517.1 examples/sec; 0.084 sec/batch)
2017-06-02 09:01:23.093869: step 263030, loss = 0.30 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:01:23.978192: step 263040, loss = 0.23 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:01:24.845358: step 263050, loss = 0.27 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:01:25.710462: step 263060, loss = 0.25 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:01:26.583240: step 263070, loss = 0.32 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:01:27.434125: step 263080, loss = 0.31 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:01:28.279021: step 263090, loss = 0.32 (1515.0 examples/sec; 0.084 sec/batch)
2017-06-02 09:01:29.239109: step 263100, loss = 0.26 (1333.2 examples/sec; 0.096 sec/batch)
2017-06-02 09:01:30.000963: step 263110, loss = 0.35 (1680.1 examples/sec; 0.076 sec/batch)
2017-06-02 09:01:30.867184: step 263120, loss = 0.25 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:01:31.743163: step 263130, loss = 0.31 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:01:32.577998: step 263140, loss = 0.30 (1533.3 examples/sec; 0.083 sec/batch)
2017-06-02 09:01:33.444315: step 263150, loss = 0.33 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:01:34.347982: step 263160, loss = 0.24 (1416.4 examples/sec; 0.090 sec/batch)
2017-06-02 09:01:35.227785: step 263170, loss = 0.26 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:01:36.111122: step 263180, loss = 0.32 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:01:37.017431: step 263190, loss = 0.37 (1412.3 examples/sec; 0.091 sec/batch)
2017-06-02 09:01:37.996018: step 263200, loss = 0.26 (1308.0 examples/sec; 0.098 sec/batch)
2017-06-02 09:01:38.796763: step 263210, loss = 0.29 (1598.5 examples/sec; 0.080 sec/batch)
2017-06-02 09:01:39.677100: step 263220, loss = 0.31 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:01:40.562807: step 263230, loss = 0.34 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:01:41.429677: step 263240, loss = 0.28 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:01:42.299592: step 263250, loss = 0.28 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:01:43.136593: step 263260, loss = 0.24 (1529.3 examples/sec; 0.084 sec/batch)
2017-06-02 09:01:43.996635: step 263270, loss = 0.19 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:01:44.849350: step 263280, loss = 0.22 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:01:45.732558: step 263290, loss = 0.30 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:01:46.794152: step 263300, loss = 0.22 (1205.7 examples/sec; 0.106 sec/batch)
2017-06-02 09:01:47.491850: step 263310, loss = 0.32 (1834.6 examples/sec; 0.070 sec/batch)
2017-06-02 09:01:48.366625: step 263320, loss = 0.45 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:01:49.275586: step 263330, loss = 0.22 (1408.2 examples/sec; 0.091 sec/batch)
2017-06-02 09:01:50.166706: step 263340, loss = 0.22 (1436.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:01:51.035613: step 263350, loss = 0.30 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:01:51.899243: step 263360, loss = 0.28 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:01:52.771440: step 263370, loss = 0.30 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:01:53.634667: step 263380, loss = 0.21 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:01:54.492981: step 263390, loss = 0.32 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:01:55.463131: step 263400, loss = 0.37 (1319.4 examples/sec; 0.097 sec/batch)
2017-06-02 09:01:56.248150: step 263410, loss = 0.23 (1630.6 examples/sec; 0.079 sec/batch)
2017-06-02 09:01:57.130615: step 263420, loss = 0.35 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:01:57.990984: step 263430, loss = 0.31 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:01:58.863546: step 263440, loss = 0.33 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:01:59.765564: step 263450, loss = 0.31 (1419.0 examples/sec; 0.090 sec/batch)
2017-06-02 09:02:00.639275: step 263460, loss = 0.53 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:02:01.484768: step 263470, loss = 0.36 (1513.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:02:02.347756: step 263480, loss = 0.37 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:02:03.218594: step 263490, loss = 0.34 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:02:04.198170: step 263500, loss = 0.31 (1306.7 examples/sec; 0.098 sec/batch)
2017-06-02 09:02:04.962548: step 263510, loss = 0.29 (1674.6 examples/sec; 0.076 sec/batch)
2017-06-02 09:02:05.860517: step 263520, loss = 0.32 (1425.4 examples/sec; 0.090 sec/batch)
2017-06-02 09:02:06.703816: step 263530, loss = 0.32 (1517.9 examples/sec; 0.084 sec/batch)
2017-06-02 09:02:07.567327: step 263540, loss = 0.28 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:02:08.432407: step 263550, loss = 0.24 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:02:09.319200: step 263560, loss = 0.35 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:02:10.206723: step 263570, loss = 0.23 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:02:11.074318: step 263580, loss = 0.39 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:02:11.960246: step 263590, loss = 0.38 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:02:12.900213: step 263600, loss = 0.27 (1361.8 examples/sec; 0.094 sec/batch)
2017-06-02 09:02:13.677317: step 263610, loss = 0.29 (1647.1 examples/sec; 0.078 sec/batch)
2017-06-02 09:02:14.520187: step 263620, loss = 0.25 (1518.6 examples/sec; 0.084 sec/batch)
2017-06-02 09:02:15.387034: step 263630, loss = 0.30 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:02:16.265128: step 263640, loss = 0.33 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:02:17.144452: step 263650, loss = 0.35 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:02:18.042298: step 263660, loss = 0.27 (1425.6 examples/sec; 0.090 sec/batch)
2017-06-02 09:02:18.938658: step 263670, loss = 0.24 (1428.0 examples/sec; 0.090 sec/batch)
2017-06-02 09:02:19.822901: step 263680, loss = 0.33 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:02:20.702686: step 263690, loss = 0.36 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:02:21.711664: step 263700, loss = 0.47 (1268.6 examples/sec; 0.101 sec/batch)
2017-06-02 09:02:22.485270: step 263710, loss = 0.26 (1654.6 examples/sec; 0.077 sec/batch)
2017-06-02 09:02:23.394171: step 263720, loss = 0.24 (1408.3 examples/sec; 0.091 sec/batch)
2017-06-02 09:02:24.297252: step 263730, loss = 0.27 (1417.4 examples/sec; 0.090 sec/batch)
2017-06-02 09:02:25.203767: step 263740, loss = 0.24 (1412.0 examples/sec; 0.091 sec/batch)
2017-06-02 09:02:26.089551: step 263750, loss = 0.33 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:02:26.966790: step 263760, loss = 0.26 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:02:27.833335: step 263770, loss = 0.24 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:02:28.691618: step 263780, loss = 0.38 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:02:29.583419: step 263790, loss = 0.29 (1435.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:02:30.568806: step 263800, loss = 0.30 (1299.0 examples/sec; 0.099 sec/batch)
2017-06-02 09:02:31.332523: step 263810, loss = 0.29 (1676.0 examples/sec; 0.076 sec/batch)
2017-06-02 09:02:32.225368: step 263820, loss = 0.22 (1433.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:02:33.097024: step 263830, loss = 0.27 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:02:33.966180: step 263840, loss = 0.30 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:02:34.852939: step 263850, loss = 0.42 (1443.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:02:35.705881: step 263860, loss = 0.36 (1500.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:02:36.590793: step 263870, loss = 0.28 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:02:37.465797: step 263880, loss = 0.26 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:02:38.349954: step 263890, loss = 0.22 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:02:39.319774: step 263900, loss = 0.34 (1319.8 examples/sec; 0.097 sec/batch)
2017-06-02 09:02:40.082074: step 263910, loss = 0.31 (1679.1 examples/sec; 0.076 sec/batch)
2017-06-02 09:02:40.954898: step 263920, loss = 0.27 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:02:41.810496: step 263930, loss = 0.38 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:02:42.673723: step 263940, loss = 0.36 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:02:43.515826: step 263950, loss = 0.31 (1520.0 examples/sec; 0.084 sec/batch)
2017-06-02 09:02:44.372662: step 263960, loss = 0.22 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:02:45.219412: step 263970, loss = 0.24 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:02:46.068533: step 263980, loss = 0.29 (1507.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:02:46.926267: step 263990, loss = 0.31 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:02:47.895938: step 264000, loss = 0.24 (1320.0 examples/sec; 0.097 sec/batch)
2017-06-02 09:02:48.652498: step 264010, loss = 0.28 (1691.9 examples/sec; 0.076 sec/batch)
2017-06-02 09:02:49.525498: step 264020, loss = 0.28 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:02:50.398903: step 264030, loss = 0.25 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:02:51.263665: step 264040, loss = 0.28 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:02:52.123248: step 264050, loss = 0.26 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:02:52.969860: step 264060, loss = 0.26 (1511.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:02:53.826085: step 264070, loss = 0.33 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:02:54.700163: step 264080, loss = 0.28 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:02:55.576496: step 264090, loss = 0.23 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:02:56.546599: step 264100, loss = 0.28 (1319.4 examples/sec; 0.097 sec/batch)
2017-06-02 09:02:57.324415: step 264110, loss = 0.33 (1645.6 examples/sec; 0.078 sec/batch)
2017-06-02 09:02:58.202426: step 264120, loss = 0.25 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:02:59.087831: step 264130, loss = 0.27 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 09:02:59.961389: step 264140, loss = 0.40 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:03:00.843550: step 264150, loss = 0.27 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:03:01.711376: step 264160, loss = 0.26 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:03:02.564598: step 264170, loss = 0.22 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:03:03.439868: step 264180, loss = 0.27 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:03:04.323905: step 264190, loss = 0.30 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:03:05.283399: step 264200, loss = 0.27 (1334.0 examples/sec; 0.096 sec/batch)
2017-06-02 09:03:06.074143: step 264210, loss = 0.29 (1618.7 examples/sec; 0.079 sec/batch)
2017-06-02 09:03:06.956929: step 264220, loss = 0.25 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:03:07.831897: step 264230, loss = 0.34 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:03:08.712208: step 264240, loss = 0.27 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:03:09.575444: step 264250, loss = 0.23 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:03:10.436621: step 264260, loss = 0.28 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:03:11.318513: step 264270, loss = 0.25 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:03:12.207785: step 264280, loss = 0.23 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:03:13.092804: step 264290, loss = 0.29 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:03:14.048221: step 264300, loss = 0.40 (1339.7 examples/sec; 0.096 sec/batch)
2017-06-02 09:03:14.822605: step 264310, loss = 0.28 (1652.9 examples/sec; 0.077 sec/batch)
2017-06-02 09:03:15.682395: step 264320, loss = 0.32 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:03:16.570995: step 264330, loss = 0.29 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:03:17.437976: step 264340, loss = 0.27 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:03:18.334309: step 264350, loss = 0.31 (1428.1 examples/sec; 0.090 sec/batch)
2017-06-02 09:03:19.165932: step 264360, loss = 0.32 (1539.2 examples/sec; 0.083 sec/batch)
2017-06-02 09:03:20.022459: step 264370, loss = 0.23 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:03:20.876414: step 264380, loss = 0.31 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:03:21.731009: step 264390, loss = 0.29 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:03:22.714020: step 264400, loss = 0.36 (1302.1 examples/sec; 0.098 sec/batch)
2017-06-02 09:03:23.515980: step 264410, loss = 0.26 (1596.1 examples/sec; 0.080 sec/batch)
2017-06-02 09:03:24.388915: step 264420, loss = 0.28 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:03:25.261327: step 264430, loss = 0.29 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:03:26.130504: step 264440, loss = 0.30 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:03:26.997099: step 264450, loss = 0.37 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:03:27.887377: step 264460, loss = 0.35 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:03:28.749081: step 264470, loss = 0.29 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:03:29.624624: step 264480, loss = 0.27 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:03:30.481555: step 264490, loss = 0.28 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:03:31.448425: step 264500, loss = 0.29 (1323.8 examples/sec; 0.097 sec/batch)
2017-06-02 09:03:32.230691: step 264510, loss = 0.28 (1636.3 examples/sec; 0.078 sec/batch)
2017-06-02 09:03:33.117228: step 264520, loss = 0.31 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:03:33.967438: step 264530, loss = 0.22 (1505.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:03:34.851435: step 264540, loss = 0.24 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:03:35.731360: step 264550, loss = 0.28 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:03:36.625785: step 264560, loss = 0.26 (1431.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:03:37.491650: step 264570, loss = 0.29 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:03:38.361634: step 264580, loss = 0.26 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:03:39.218275: step 264590, loss = 0.41 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:03:40.208115: step 264600, loss = 0.30 (1293.1 examples/sec; 0.099 sec/batch)
2017-06-02 09:03:40.947100: step 264610, loss = 0.27 (1732.1 examples/sec; 0.074 sec/batch)
2017-06-02 09:03:41.829895: step 264620, loss = 0.27 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:03:42.693692: step 264630, loss = 0.27 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:03:43.567748: step 264640, loss = 0.30 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:03:44.451349: step 264650, loss = 0.33 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:03:45.370078: step 264660, loss = 0.24 (1393.2 examples/sec; 0.092 sec/batch)
2017-06-02 09:03:46.265708: step 264670, loss = 0.30 (1429.2 examples/sec; 0.090 sec/batch)
2017-06-02 09:03:47.132820: step 264680, loss = 0.25 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:03:48.007151: step 264690, loss = 0.27 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:03:49.004118: step 264700, loss = 0.32 (1283.9 examples/sec; 0.100 sec/batch)
2017-06-02 09:03:49.798652: step 264710, loss = 0.34 (1611.0 examples/sec; 0.079 sec/batch)
2017-06-02 09:03:50.659031: step 264720, loss = 0.27 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:03:51.532085: step 264730, loss = 0.27 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:03:52.409222: step 264740, loss = 0.25 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:03:53.297468: step 264750, loss = 0.27 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:03:54.144356: step 264760, loss = 0.35 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:03:55.017593: step 264770, loss = 0.34 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:03:55.891146: step 264780, loss = 0.20 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:03:56.759633: step 264790, loss = 0.26 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:03:57.714770: step 264800, loss = 0.24 (1340.1 examples/sec; 0.096 sec/batch)
2017-06-02 09:03:58.487274: step 264810, loss = 0.30 (1657.0 examples/sec; 0.077 sec/batch)
2017-06-02 09:03:59.322849: step 264820, loss = 0.25 (1531.9 examples/sec; 0.084 sec/batch)
2017-06-02 09:04:00.198452: step 264830, loss = 0.33 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:04:01.064592: step 264840, loss = 0.26 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:04:01.967751: step 264850, loss = 0.31 (1417.2 examples/sec; 0.090 sec/batch)
2017-06-02 09:04:02.826981: step 264860, loss = 0.33 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:04:03.714637: step 264870, loss = 0.28 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:04:04.594447: step 264880, loss = 0.31 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:04:05.464818: step 264890, loss = 0.33 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:04:06.423309: step 264900, loss = 0.30 (1335.4 examples/sec; 0.096 sec/batch)
2017-06-02 09:04:07.197074: step 264910, loss = 0.29 (1654.3 examples/sec; 0.077 sec/batch)
2017-06-02 09:04:08.065453: step 264920, loss = 0.32 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:04:08.926862: step 264930, loss = 0.36 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:04:09.809711: step 264940, loss = 0.38 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:04:10.690894: step 264950, loss = 0.34 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:04:11.581126: step 264960, loss = 0.31 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:04:12.423956: step 264970, loss = 0.26 (1518.7 examples/sec; 0.084 sec/batch)
2017-06-02 09:04:13.318573: step 264980, loss = 0.36 (1430.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:04:14.193501: step 264990, loss = 0.35 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:04:15.168931: step 265000, loss = 0.29 (1312.2 examples/sec; 0.098 sec/batch)
2017-06-02 09:04:15.941954: step 265010, loss = 0.31 (1655.8 examples/sec; 0.077 sec/batch)
2017-06-02 09:04:16.809412: step 265020, loss = 0.28 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:04:17.700709: step 265030, loss = 0.35 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:04:18.529891: step 265040, loss = 0.24 (1543.7 examples/sec; 0.083 sec/batch)
2017-06-02 09:04:19.422673: step 265050, loss = 0.28 (1433.7 examples/sec; 0.089 sec/batch)
2017-06-02 09:04:20.282408: step 265060, loss = 0.24 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:04:21.136373: step 265070, loss = 0.25 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:04:22.011890: step 265080, loss = 0.31 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:04:22.898719: step 265090, loss = 0.31 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:04:23.883823: step 265100, loss = 0.34 (1299.4 examples/sec; 0.099 sec/batch)
2017-06-02 09:04:24.650533: step 265110, loss = 0.31 (1669.5 examples/sec; 0.077 sec/batch)
2017-06-02 09:04:25.530355: step 265120, loss = 0.36 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:04:26.405283: step 265130, loss = 0.24 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:04:27.263162: step 265140, loss = 0.34 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:04:28.152759: step 265150, loss = 0.23 (1438.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:04:29.001942: step 265160, loss = 0.34 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:04:29.887624: step 265170, loss = 0.38 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:04:30.772102: step 265180, loss = 0.29 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:04:31.666606: step 265190, loss = 0.31 (1431.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:04:32.632172: step 265200, loss = 0.32 (1325.6 examples/sec; 0.097 sec/batch)
2017-06-02 09:04:33.396495: step 265210, loss = 0.33 (1674.7 examples/sec; 0.076 sec/batch)
2017-06-02 09:04:34.269330: step 265220, loss = 0.25 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:04:35.154413: step 265230, loss = 0.22 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:04:36.034353: step 265240, loss = 0.23 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:04:36.911414: step 265250, loss = 0.35 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:04:37.788435: step 265260, loss = 0.27 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:04:38.652589: step 265270, loss = 0.25 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:04:39.519567: step 265280, loss = 0.33 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:04:40.384936: step 265290, loss = 0.26 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:04:41.406312: step 265300, loss = 0.42 (1253.2 examples/sec; 0.102 sec/batch)
2017-06-02 09:04:42.129474: step 265310, loss = 0.33 (1770.0 examples/sec; 0.072 sec/batch)
2017-06-02 09:04:42.990282: step 265320, loss = 0.27 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:04:43.850770: step 265330, loss = 0.26 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:04:44.727897: step 265340, loss = 0.24 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:04:45.596972: step 265350, loss = 0.21 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:04:46.477039: step 265360, loss = 0.29 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:04:47.357516: step 265370, loss = 0.31 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:04:48.205423: step 265380, loss = 0.39 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:04:49.089135: step 265390, loss = 0.32 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:04:50.093059: step 265400, loss = 0.28 (1275.0 examples/sec; 0.100 sec/batch)
2017-06-02 09:04:50.839019: step 265410, loss = 0.35 (1715.9 examples/sec; 0.075 sec/batch)
2017-06-02 09:04:51.701006: step 265420, loss = 0.26 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:04:52.582659: step 265430, loss = 0.37 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:04:53.473175: step 265440, loss = 0.24 (1437.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:04:54.355070: step 265450, loss = 0.35 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:04:55.220376: step 265460, loss = 0.29 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:04:56.098267: step 265470, loss = 0.31 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:04:56.974446: step 265480, loss = 0.34 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:04:57.845256: step 265490, loss = 0.35 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:04:58.825620: step 265500, loss = 0.25 (1305.6 examples/sec; 0.098 sec/batch)
2017-06-02 09:04:59.605255: step 265510, loss = 0.32 (1641.8 examples/sec; 0.078 sec/batch)
2017-06-02 09:05:00.465408: step 265520, loss = 0.41 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:05:01.339177: step 265530, loss = 0.28 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:05:02.198368: step 265540, loss = 0.30 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:05:03.073331: step 265550, loss = 0.33 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:05:03.954418: step 265560, loss = 0.28 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:05:04.823701: step 265570, loss = 0.28 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:05:05.694926: step 265580, loss = 0.36 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:05:06.581133: step 265590, loss = 0.37 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:05:07.573171: step 265600, loss = 0.21 (1290.3 examples/sec; 0.099 sec/batch)
2017-06-02 09:05:08.373469: step 265610, loss = 0.34 (1599.4 examples/sec; 0.080 sec/batch)
2017-06-02 09:05:09.232540: step 265620, loss = 0.26 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:05:10.099857: step 265630, loss = 0.24 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:05:10.987967: step 265640, loss = 0.24 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:05:11.846453: step 265650, loss = 0.24 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:05:12.708837: step 265660, loss = 0.31 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:05:13.588589: step 265670, loss = 0.20 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:05:14.441677: step 265680, loss = 0.34 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:05:15.292512: step 265690, loss = 0.21 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:05:16.325427: step 265700, loss = 0.21 (1239.2 examples/sec; 0.103 sec/batch)
2017-06-02 09:05:17.032391: step 265710, loss = 0.31 (1810.6 examples/sec; 0.071 sec/batch)
2017-06-02 09:05:17.865809: step 265720, loss = 0.28 (1535.8 examples/sec; 0.083 sec/batch)
2017-06-02 09:05:18.746069: step 265730, loss = 0.28 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:05:19.619447: step 265740, loss = 0.30 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:05:20.470799: step 265750, loss = 0.26 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:05:21.349728: step 265760, loss = 0.23 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:05:22.232251: step 265770, loss = 0.33 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:05:23.095184: step 265780, loss = 0.22 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:05:23.962557: step 265790, loss = 0.41 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:05:24.934538: step 265800, loss = 0.38 (1316.9 examples/sec; 0.097 sec/batch)
2017-06-02 09:05:25.673959: step 265810, loss = 0.27 (1731.1 examples/sec; 0.074 sec/batch)
2017-06-02 09:05:26.515347: step 265820, loss = 0.26 (1521.3 examples/sec; 0.084 sec/batch)
2017-06-02 09:05:27.350241: step 265830, loss = 0.24 (1533.1 examples/sec; 0.083 sec/batch)
2017-06-02 09:05:28.233615: step 265840, loss = 0.29 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:05:29.096020: step 265850, loss = 0.30 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:05:29.944194: step 265860, loss = 0.26 (1509.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:05:30.793837: step 265870, loss = 0.37 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:05:31.668464: step 265880, loss = 0.33 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:05:32.550191: step 265890, loss = 0.28 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:05:33.522092: step 265900, loss = 0.32 (1317.0 examples/sec; 0.097 sec/batch)
2017-06-02 09:05:34.297029: step 265910, loss = 0.39 (1651.7 examples/sec; 0.077 sec/batch)
2017-06-02 09:05:35.179184: step 265920, loss = 0.30 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:05:36.063056: step 265930, loss = 0.22 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:05:36.927402: step 265940, loss = 0.27 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:05:37.798433: step 265950, loss = 0.32 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:05:38.677908: step 265960, loss = 0.28 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:05:39.566802: step 265970, loss = 0.24 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:05:40.451978: step 265980, loss = 0.22 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:05:41.339320: step 265990, loss = 0.27 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:05:42.324969: step 266000, loss = 0.31 (1298.6 examples/sec; 0.099 sec/batch)
2017-06-02 09:05:43.124276: step 266010, loss = 0.25 (1601.4 examples/sec; 0.080 sec/batch)
2017-06-02 09:05:43.997503: step 266020, loss = 0.31 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:05:44.883053: step 266030, loss = 0.22 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:05:45.759481: step 266040, loss = 0.26 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:05:46.642563: step 266050, loss = 0.30 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:05:47.482375: step 266060, loss = 0.38 (1524.2 examples/sec; 0.084 sec/batch)
2017-06-02 09:05:48.337776: step 266070, loss = 0.36 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:05:49.200656: step 266080, loss = 0.33 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:05:50.074320: step 266090, loss = 0.23 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:05:51.048097: step 266100, loss = 0.28 (1314.5 examples/sec; 0.097 sec/batch)
2017-06-02 09:05:51.829738: step 266110, loss = 0.26 (1637.6 examples/sec; 0.078 sec/batch)
2017-06-02 09:05:52.696203: step 266120, loss = 0.33 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:05:53.550592: step 266130, loss = 0.25 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:05:54.456781: step 266140, loss = 0.23 (1412.5 examples/sec; 0.091 sec/batch)
2017-06-02 09:05:55.362445: step 266150, loss = 0.32 (1413.3 examples/sec; 0.091 sec/batch)
2017-06-02 09:05:56.220980: step 266160, loss = 0.27 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:05:57.090082: step 266170, loss = 0.24 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:05:57.967252: step 266180, loss = 0.30 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:05:58.836904: step 266190, loss = 0.34 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:05:59.819917: step 266200, loss = 0.29 (1302.1 examples/sec; 0.098 sec/batch)
2017-06-02 09:06:00.612917: step 266210, loss = 0.24 (1614.1 examples/sec; 0.079 sec/batch)
2017-06-02 09:06:01.486174: step 266220, loss = 0.28 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:06:02.361428: step 266230, loss = 0.32 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:06:03.248383: step 266240, loss = 0.31 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:06:04.135346: step 266250, loss = 0.33 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:06:05.012985: step 266260, loss = 0.23 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:06:05.895260: step 266270, loss = 0.36 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:06:06.773202: step 266280, loss = 0.28 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:06:07.655552: step 266290, loss = 0.34 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:06:08.632873: step 266300, loss = 0.26 (1309.7 examples/sec; 0.098 sec/batch)
2017-06-02 09:06:09.417292: step 266310, loss = 0.30 (1631.8 examples/sec; 0.078 sec/batch)
2017-06-02 09:06:10.310267: step 266320, loss = 0.25 (1433.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:06:11.192447: step 266330, loss = 0.28 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:06:12.082795: step 266340, loss = 0.26 (1437.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:06:12.959257: step 266350, loss = 0.31 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:06:13.844687: step 266360, loss = 0.27 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:06:14.724394: step 266370, loss = 0.29 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:06:15.611437: step 266380, loss = 0.26 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:06:16.476440: step 266390, loss = 0.23 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:06:17.439108: step 266400, loss = 0.23 (1329.6 examples/sec; 0.096 sec/batch)
2017-06-02 09:06:18.211949: step 266410, loss = 0.24 (1656.2 examples/sec; 0.077 sec/batch)
2017-06-02 09:06:19.074720: step 266420, loss = 0.34 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:06:19.939217: step 266430, loss = 0.29 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:06:20.779028: step 266440, loss = 0.38 (1524.2 examples/sec; 0.084 sec/batch)
2017-06-02 09:06:21.623524: step 266450, loss = 0.30 (1515.7 examples/sec; 0.084 sec/batch)
2017-06-02 09:06:22.487083: step 266460, loss = 0.19 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:06:23.355253: step 266470, loss = 0.35 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:06:24.236976: step 266480, loss = 0.30 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:06:25.107147: step 266490, loss = 0.23 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:06:26.027459: step 266500, loss = 0.28 (1390.8 examples/sec; 0.092 sec/batch)
2017-06-02 09:06:26.809450: step 266510, loss = 0.27 (1636.8 examples/sec; 0.078 sec/batch)
2017-06-02 09:06:27.683190: step 266520, loss = 0.23 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:06:28.546046: step 266530, loss = 0.25 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:06:29.438970: step 266540, loss = 0.26 (1433.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:06:30.282524: step 266550, loss = 0.27 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:06:31.148081: step 266560, loss = 0.29 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:06:32.014619: step 266570, loss = 0.28 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:06:32.877774: step 266580, loss = 0.28 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:06:33.784138: step 266590, loss = 0.27 (1412.2 examples/sec; 0.091 sec/batch)
2017-06-02 09:06:34.732853: step 266600, loss = 0.31 (1349.2 examples/sec; 0.095 sec/batch)
2017-06-02 09:06:35.515940: step 266610, loss = 0.39 (1634.6 examples/sec; 0.078 sec/batch)
2017-06-02 09:06:36.401074: step 266620, loss = 0.35 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:06:37.235853: step 266630, loss = 0.36 (1533.3 examples/sec; 0.083 sec/batch)
2017-06-02 09:06:38.103530: step 266640, loss = 0.33 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:06:38.990876: step 266650, loss = 0.32 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:06:39.866276: step 266660, loss = 0.26 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:06:40.708494: step 266670, loss = 0.29 (1519.8 examples/sec; 0.084 sec/batch)
2017-06-02 09:06:41.549286: step 266680, loss = 0.31 (1522.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:06:42.406467: step 266690, loss = 0.23 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:06:43.387510: step 266700, loss = 0.38 (1304.7 examples/sec; 0.098 sec/batch)
2017-06-02 09:06:44.144184: step 266710, loss = 0.43 (1691.6 examples/sec; 0.076 sec/batch)
2017-06-02 09:06:45.016572: step 266720, loss = 0.32 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:06:45.895335: step 266730, loss = 0.38 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:06:46.756083: step 266740, loss = 0.29 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:06:47.626634: step 266750, loss = 0.32 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:06:48.499205: step 266760, loss = 0.39 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:06:49.340449: step 266770, loss = 0.31 (1521.6 examples/sec; 0.084 sec/batch)
2017-06-02 09:06:50.200434: step 266780, loss = 0.27 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:06:51.071832: step 266790, loss = 0.28 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:06:52.029320: step 266800, loss = 0.30 (1336.8 examples/sec; 0.096 sec/batch)
2017-06-02 09:06:52.773996: step 266810, loss = 0.30 (1718.9 examples/sec; 0.074 sec/batch)
2017-06-02 09:06:53.653424: step 266820, loss = 0.27 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:06:54.519000: step 266830, loss = 0.25 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:06:55.378591: step 266840, loss = 0.29 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:06:56.226178: step 266850, loss = 0.35 (1509.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:06:57.102495: step 266860, loss = 0.24 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:06:57.933247: step 266870, loss = 0.27 (1540.8 examples/sec; 0.083 sec/batch)
2017-06-02 09:06:58.811348: step 266880, loss = 0.35 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:06:59.719071: step 266890, loss = 0.36 (1410.1 examples/sec; 0.091 sec/batch)
2017-06-02 09:07:00.722179: step 266900, loss = 0.32 (1276.0 examples/sec; 0.100 sec/batch)
2017-06-02 09:07:01.470330: step 266910, loss = 0.25 (1710.9 examples/sec; 0.075 sec/batch)
2017-06-02 09:07:02.363084: step 266920, loss = 0.28 (1433.7 examples/sec; 0.089 sec/batch)
2017-06-02 09:07:03.201797: step 266930, loss = 0.33 (1526.2 examples/sec; 0.084 sec/batch)
2017-06-02 09:07:04.100748: step 266940, loss = 0.31 (1423.9 examples/sec; 0.090 sec/batch)
2017-06-02 09:07:04.977174: step 266950, loss = 0.31 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:07:05.847807: step 266960, loss = 0.22 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:07:06.713628: step 266970, loss = 0.32 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:07:07.603848: step 266980, loss = 0.25 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:07:08.495372: step 266990, loss = 0.37 (1435.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:07:09.488614: step 267000, loss = 0.33 (1288.7 examples/sec; 0.099 sec/batch)
2017-06-02 09:07:10.254259: step 267010, loss = 0.36 (1671.8 examples/sec; 0.077 sec/batch)
2017-06-02 09:07:11.133676: step 267020, loss = 0.33 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:07:12.017213: step 267030, loss = 0.28 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:07:12.889837: step 267040, loss = 0.23 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:07:13.754203: step 267050, loss = 0.32 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:07:14.632250: step 267060, loss = 0.25 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:07:15.500052: step 267070, loss = 0.33 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:07:16.364380: step 267080, loss = 0.32 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:07:17.251945: step 267090, loss = 0.23 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:07:18.231810: step 267100, loss = 0.28 (1306.3 examples/sec; 0.098 sec/batch)
2017-06-02 09:07:18.997395: step 267110, loss = 0.27 (1671.9 examples/sec; 0.077 sec/batch)
2017-06-02 09:07:19.888472: step 267120, loss = 0.23 (1436.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:07:20.749726: step 267130, loss = 0.26 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:07:21.651555: step 267140, loss = 0.30 (1419.3 examples/sec; 0.090 sec/batch)
2017-06-02 09:07:22.536930: step 267150, loss = 0.36 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 09:07:23.406882: step 267160, loss = 0.35 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:07:24.302594: step 267170, loss = 0.26 (1429.0 examples/sec; 0.090 sec/batch)
2017-06-02 09:07:25.188343: step 267180, loss = 0.22 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:07:26.067188: step 267190, loss = 0.26 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:07:27.084494: step 267200, loss = 0.22 (1258.2 examples/sec; 0.102 sec/batch)
2017-06-02 09:07:27.816371: step 267210, loss = 0.22 (1748.9 examples/sec; 0.073 sec/batch)
2017-06-02 09:07:28.708162: step 267220, loss = 0.27 (1435.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:07:29.585721: step 267230, loss = 0.27 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:07:30.463432: step 267240, loss = 0.28 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:07:31.339525: step 267250, loss = 0.34 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:07:32.222305: step 267260, loss = 0.31 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:07:33.067810: step 267270, loss = 0.24 (1513.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:07:33.935242: step 267280, loss = 0.31 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:07:34.830259: step 267290, loss = 0.26 (1430.1 examples/sec; 0.090 sec/batch)
2017-06-02 09:07:35.823566: step 267300, loss = 0.28 (1288.6 examples/sec; 0.099 sec/batch)
2017-06-02 09:07:36.610401: step 267310, loss = 0.37 (1626.8 examples/sec; 0.079 sec/batch)
2017-06-02 09:07:37.498619: step 267320, loss = 0.29 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:07:38.389092: step 267330, loss = 0.32 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:07:39.282129: step 267340, loss = 0.29 (1433.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:07:40.168049: step 267350, loss = 0.29 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:07:41.071043: step 267360, loss = 0.26 (1417.5 examples/sec; 0.090 sec/batch)
2017-06-02 09:07:41.928972: step 267370, loss = 0.28 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:07:42.781572: step 267380, loss = 0.24 (1501.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:07:43.647662: step 267390, loss = 0.37 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:07:44.636838: step 267400, loss = 0.31 (1294.0 examples/sec; 0.099 sec/batch)
2017-06-02 09:07:45.390806: step 267410, loss = 0.29 (1697.7 examples/sec; 0.075 sec/batch)
2017-06-02 09:07:46.229966: step 267420, loss = 0.25 (1525.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:07:47.086003: step 267430, loss = 0.28 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:07:47.965160: step 267440, loss = 0.31 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:07:48.860477: step 267450, loss = 0.46 (1429.7 examples/sec; 0.090 sec/batch)
2017-06-02 09:07:49.743985: step 267460, loss = 0.27 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:07:50.619249: step 267470, loss = 0.39 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:07:51.476397: step 267480, loss = 0.24 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:07:52.363025: step 267490, loss = 0.34 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 09:07:53.330559: step 267500, loss = 0.25 (1323.0 examples/sec; 0.097 sec/batch)
2017-06-02 09:07:54.085000: step 267510, loss = 0.30 (1696.6 examples/sec; 0.075 sec/batch)
2017-06-02 09:07:54.967222: step 267520, loss = 0.23 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:07:55.843213: step 267530, loss = 0.26 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:07:56.708682: step 267540, loss = 0.19 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:07:57.577024: step 267550, loss = 0.27 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:07:58.442544: step 267560, loss = 0.20 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:07:59.333534: step 267570, loss = 0.24 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:08:00.214784: step 267580, loss = 0.27 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:08:01.092430: step 267590, loss = 0.26 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:08:02.050839: step 267600, loss = 0.32 (1335.5 examples/sec; 0.096 sec/batch)
2017-06-02 09:08:02.829076: step 267610, loss = 0.33 (1644.8 examples/sec; 0.078 sec/batch)
2017-06-02 09:08:03.706925: step 267620, loss = 0.28 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:08:04.570753: step 267630, loss = 0.34 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:08:05.431823: step 267640, loss = 0.24 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:08:06.304692: step 267650, loss = 0.27 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:08:07.179069: step 267660, loss = 0.34 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:08:08.060889: step 267670, loss = 0.25 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:08:08.940106: step 267680, loss = 0.32 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:08:09.807614: step 267690, loss = 0.20 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:08:10.786911: step 267700, loss = 0.27 (1307.1 examples/sec; 0.098 sec/batch)
2017-06-02 09:08:11.558283: step 267710, loss = 0.34 (1659.4 examples/sec; 0.077 sec/batch)
2017-06-02 09:08:12.403517: step 267720, loss = 0.32 (1514.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:08:13.271614: step 267730, loss = 0.42 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:08:14.152889: step 267740, loss = 0.27 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:08:15.016053: step 267750, loss = 0.26 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:08:15.867215: step 267760, loss = 0.25 (1503.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:08:16.738667: step 267770, loss = 0.30 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:08:17.599486: step 267780, loss = 0.27 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:08:18.453546: step 267790, loss = 0.24 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:08:19.413145: step 267800, loss = 0.25 (1333.9 examples/sec; 0.096 sec/batch)
2017-06-02 09:08:20.185104: step 267810, loss = 0.33 (1658.1 examples/sec; 0.077 sec/batch)
2017-06-02 09:08:21.035580: step 267820, loss = 0.24 (1505.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:08:21.924197: step 267830, loss = 0.26 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:08:22.770908: step 267840, loss = 0.26 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:08:23.646273: step 267850, loss = 0.30 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:08:24.502692: step 267860, loss = 0.23 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:08:25.384806: step 267870, loss = 0.34 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:08:26.284519: step 267880, loss = 0.32 (1422.7 examples/sec; 0.090 sec/batch)
2017-06-02 09:08:27.182410: step 267890, loss = 0.35 (1425.6 examples/sec; 0.090 sec/batch)
2017-06-02 09:08:28.156560: step 267900, loss = 0.27 (1314.0 examples/sec; 0.097 sec/batch)
2017-06-02 09:08:28.928312: step 267910, loss = 0.24 (1658.6 examples/sec; 0.077 sec/batch)
2017-06-02 09:08:29.805005: step 267920, loss = 0.27 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:08:30.684940: step 267930, loss = 0.41 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:08:31.569981: step 267940, loss = 0.32 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:08:32.440329: step 267950, loss = 0.23 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:08:33.312638: step 267960, loss = 0.36 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:08:34.189613: step 267970, loss = 0.35 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:08:35.068098: step 267980, loss = 0.24 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:08:35.944679: step 267990, loss = 0.28 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:08:36.921191: step 268000, loss = 0.29 (1310.8 examples/sec; 0.098 sec/batch)
2017-06-02 09:08:37.693878: step 268010, loss = 0.26 (1656.6 examples/sec; 0.077 sec/batch)
2017-06-02 09:08:38.597838: step 268020, loss = 0.23 (1416.0 examples/sec; 0.090 sec/batch)
2017-06-02 09:08:39.476386: step 268030, loss = 0.35 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:08:40.346716: step 268040, loss = 0.28 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:08:41.176147: step 268050, loss = 0.23 (1543.2 examples/sec; 0.083 sec/batch)
2017-06-02 09:08:42.025427: step 268060, loss = 0.31 (1507.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:08:42.910182: step 268070, loss = 0.31 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:08:43.781712: step 268080, loss = 0.25 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:08:44.644904: step 268090, loss = 0.31 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:08:45.615654: step 268100, loss = 0.32 (1318.6 examples/sec; 0.097 sec/batch)
2017-06-02 09:08:46.363259: step 268110, loss = 0.26 (1712.1 examples/sec; 0.075 sec/batch)
2017-06-02 09:08:47.214103: step 268120, loss = 0.46 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:08:48.076085: step 268130, loss = 0.23 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:08:48.934351: step 268140, loss = 0.34 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:08:49.807829: step 268150, loss = 0.21 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:08:50.683788: step 268160, loss = 0.27 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:08:51.525889: step 268170, loss = 0.28 (1520.0 examples/sec; 0.084 sec/batch)
2017-06-02 09:08:52.401595: step 268180, loss = 0.32 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:08:53.267288: step 268190, loss = 0.31 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:08:54.262969: step 268200, loss = 0.26 (1285.6 examples/sec; 0.100 sec/batch)
2017-06-02 09:08:55.037868: step 268210, loss = 0.30 (1651.8 examples/sec; 0.077 sec/batch)
2017-06-02 09:08:55.916621: step 268220, loss = 0.28 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:08:56.786470: step 268230, loss = 0.26 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:08:57.655019: step 268240, loss = 0.26 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:08:58.521153: step 268250, loss = 0.27 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:08:59.414026: step 268260, loss = 0.24 (1433.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:09:00.253980: step 268270, loss = 0.37 (1524.0 examples/sec; 0.084 sec/batch)
2017-06-02 09:09:01.109704: step 268280, loss = 0.28 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:09:01.961580: step 268290, loss = 0.22 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:09:02.917051: step 268300, loss = 0.27 (1339.6 examples/sec; 0.096 sec/batch)
2017-06-02 09:09:03.675861: step 268310, loss = 0.31 (1686.8 examples/sec; 0.076 sec/batch)
2017-06-02 09:09:04.540698: step 268320, loss = 0.26 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:09:05.422286: step 268330, loss = 0.26 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:09:06.263039: step 268340, loss = 0.26 (1522.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:09:07.153021: step 268350, loss = 0.27 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:09:08.040002: step 268360, loss = 0.36 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:09:08.904635: step 268370, loss = 0.25 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:09:09.791078: step 268380, loss = 0.29 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:09:10.671937: step 268390, loss = 0.26 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:09:11.652895: step 268400, loss = 0.32 (1304.8 examples/sec; 0.098 sec/batch)
2017-06-02 09:09:12.428373: step 268410, loss = 0.28 (1650.6 examples/sec; 0.078 sec/batch)
2017-06-02 09:09:13.317548: step 268420, loss = 0.29 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:09:14.149242: step 268430, loss = 0.29 (1539.0 examples/sec; 0.083 sec/batch)
2017-06-02 09:09:15.019562: step 268440, loss = 0.24 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:09:15.861322: step 268450, loss = 0.20 (1520.6 examples/sec; 0.084 sec/batch)
2017-06-02 09:09:16.748880: step 268460, loss = 0.31 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:09:17.605707: step 268470, loss = 0.26 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:09:18.456201: step 268480, loss = 0.35 (1505.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:09:19.323779: step 268490, loss = 0.27 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:09:20.310432: step 268500, loss = 0.38 (1297.3 examples/sec; 0.099 sec/batch)
2017-06-02 09:09:21.076458: step 268510, loss = 0.31 (1671.0 examples/sec; 0.077 sec/batch)
2017-06-02 09:09:21.944095: step 268520, loss = 0.25 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:09:22.799761: step 268530, loss = 0.21 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:09:23.658763: step 268540, loss = 0.27 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:09:24.515543: step 268550, loss = 0.29 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:09:25.386407: step 268560, loss = 0.23 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:09:26.229175: step 268570, loss = 0.32 (1518.8 examples/sec; 0.084 sec/batch)
2017-06-02 09:09:27.103944: step 268580, loss = 0.23 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:09:27.958583: step 268590, loss = 0.31 (1497.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:09:28.925539: step 268600, loss = 0.30 (1323.7 examples/sec; 0.097 sec/batch)
2017-06-02 09:09:29.696388: step 268610, loss = 0.33 (1660.5 examples/sec; 0.077 sec/batch)
2017-06-02 09:09:30.548771: step 268620, loss = 0.35 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:09:31.437804: step 268630, loss = 0.29 (1439.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:09:32.285509: step 268640, loss = 0.31 (1510.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:09:33.166156: step 268650, loss = 0.23 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:09:34.026551: step 268660, loss = 0.25 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:09:34.910206: step 268670, loss = 0.22 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:09:35.822541: step 268680, loss = 0.25 (1403.0 examples/sec; 0.091 sec/batch)
2017-06-02 09:09:36.709700: step 268690, loss = 0.29 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:09:37.696158: step 268700, loss = 0.22 (1297.6 examples/sec; 0.099 sec/batch)
2017-06-02 09:09:38.473608: step 268710, loss = 0.33 (1646.4 examples/sec; 0.078 sec/batch)
2017-06-02 09:09:39.337950: step 268720, loss = 0.23 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:09:40.217144: step 268730, loss = 0.34 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:09:41.100796: step 268740, loss = 0.34 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:09:41.980634: step 268750, loss = 0.27 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:09:42.890653: step 268760, loss = 0.33 (1406.6 examples/sec; 0.091 sec/batch)
2017-06-02 09:09:43.775197: step 268770, loss = 0.28 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:09:44.655719: step 268780, loss = 0.34 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:09:45.546845: step 268790, loss = 0.33 (1436.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:09:46.522609: step 268800, loss = 0.29 (1311.8 examples/sec; 0.098 sec/batch)
2017-06-02 09:09:47.287597: step 268810, loss = 0.40 (1673.2 examples/sec; 0.076 sec/batch)
2017-06-02 09:09:48.163463: step 268820, loss = 0.31 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:09:49.023490: step 268830, loss = 0.33 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:09:49.882015: step 268840, loss = 0.30 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:09:50.743262: step 268850, loss = 0.29 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:09:51.626207: step 268860, loss = 0.26 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:09:52.540071: step 268870, loss = 0.26 (1400.6 examples/sec; 0.091 sec/batch)
2017-06-02 09:09:53.456931: step 268880, loss = 0.27 (1396.1 examples/sec; 0.092 sec/batch)
2017-06-02 09:09:54.355117: step 268890, loss = 0.29 (1425.1 examples/sec; 0.090 sec/batch)
2017-06-02 09:09:55.338090: step 268900, loss = 0.28 (1302.2 examples/sec; 0.098 sec/batch)
2017-06-02 09:09:56.136462: step 268910, loss = 0.27 (1603.3 examples/sec; 0.080 sec/batch)
2017-06-02 09:09:56.987165: step 268920, loss = 0.25 (1504.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:09:57.848045: step 268930, loss = 0.35 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:09:58.718947: step 268940, loss = 0.34 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:09:59.568279: step 268950, loss = 0.26 (1507.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:10:00.443720: step 268960, loss = 0.34 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:10:01.296184: step 268970, loss = 0.33 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:10:02.144627: step 268980, loss = 0.29 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:10:03.018889: step 268990, loss = 0.20 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:10:04.017716: step 269000, loss = 0.29 (1281.5 examples/sec; 0.100 sec/batch)
2017-06-02 09:10:04.794548: step 269010, loss = 0.23 (1647.7 examples/sec; 0.078 sec/batch)
2017-06-02 09:10:05.669046: step 269020, loss = 0.29 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:10:06.569833: step 269030, loss = 0.31 (1421.0 examples/sec; 0.090 sec/batch)
2017-06-02 09:10:07.438863: step 269040, loss = 0.36 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:10:08.293493: step 269050, loss = 0.37 (1497.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:10:09.188374: step 269060, loss = 0.26 (1430.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:10:10.053760: step 269070, loss = 0.29 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:10:10.916796: step 269080, loss = 0.26 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:10:11.771483: step 269090, loss = 0.33 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:10:12.771218: step 269100, loss = 0.37 (1280.3 examples/sec; 0.100 sec/batch)
2017-06-02 09:10:13.506373: step 269110, loss = 0.27 (1741.1 examples/sec; 0.074 sec/batch)
2017-06-02 09:10:14.386272: step 269120, loss = 0.25 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:10:15.259847: step 269130, loss = 0.29 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:10:16.120979: step 269140, loss = 0.26 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:10:17.002359: step 269150, loss = 0.27 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:10:17.876299: step 269160, loss = 0.23 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:10:18.742728: step 269170, loss = 0.43 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:10:19.610357: step 269180, loss = 0.40 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:10:20.496398: step 269190, loss = 0.33 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:10:21.478640: step 269200, loss = 0.25 (1303.1 examples/sec; 0.098 sec/batch)
2017-06-02 09:10:22.260382: step 269210, loss = 0.28 (1637.4 examples/sec; 0.078 sec/batch)
2017-06-02 09:10:23.149013: step 269220, loss = 0.36 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:10:24.019088: step 269230, loss = 0.32 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:10:24.898911: step 269240, loss = 0.24 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:10:25.775604: step 269250, loss = 0.26 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:10:26.635085: step 269260, loss = 0.28 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:10:27.523578: step 269270, loss = 0.33 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:10:28.414221: step 269280, loss = 0.33 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:10:29.297582: step 269290, loss = 0.26 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:10:30.268884: step 269300, loss = 0.38 (1317.8 examples/sec; 0.097 sec/batch)
2017-06-02 09:10:31.043324: step 269310, loss = 0.26 (1652.8 examples/sec; 0.077 sec/batch)
2017-06-02 09:10:31.927978: step 269320, loss = 0.29 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:10:32.818570: step 269330, loss = 0.24 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:10:33.690810: step 269340, loss = 0.32 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:10:34.575867: step 269350, loss = 0.28 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:10:35.438090: step 269360, loss = 0.26 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:10:36.310710: step 269370, loss = 0.27 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:10:37.172513: step 269380, loss = 0.34 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:10:38.079523: step 269390, loss = 0.33 (1411.2 examples/sec; 0.091 sec/batch)
2017-06-02 09:10:39.077105: step 269400, loss = 0.32 (1283.1 examples/sec; 0.100 sec/batch)
2017-06-02 09:10:39.835133: step 269410, loss = 0.33 (1688.6 examples/sec; 0.076 sec/batch)
2017-06-02 09:10:40.698581: step 269420, loss = 0.34 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:10:41.594478: step 269430, loss = 0.31 (1428.7 examples/sec; 0.090 sec/batch)
2017-06-02 09:10:42.473021: step 269440, loss = 0.34 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:10:43.348800: step 269450, loss = 0.33 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:10:44.218005: step 269460, loss = 0.28 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:10:45.100796: step 269470, loss = 0.34 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:10:45.986858: step 269480, loss = 0.33 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:10:46.881559: step 269490, loss = 0.26 (1430.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:10:47.873628: step 269500, loss = 0.22 (1290.2 examples/sec; 0.099 sec/batch)
2017-06-02 09:10:48.634142: step 269510, loss = 0.23 (1683.1 examples/sec; 0.076 sec/batch)
2017-06-02 09:10:49.528561: step 269520, loss = 0.34 (1431.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:10:50.384990: step 269530, loss = 0.26 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:10:51.262265: step 269540, loss = 0.23 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:10:52.148624: step 269550, loss = 0.22 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:10:53.013084: step 269560, loss = 0.23 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:10:53.885984: step 269570, loss = 0.26 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:10:54.771559: step 269580, loss = 0.31 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:10:55.645287: step 269590, loss = 0.30 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:10:56.622423: step 269600, loss = 0.27 (1309.9 examples/sec; 0.098 sec/batch)
2017-06-02 09:10:57.401329: step 269610, loss = 0.30 (1643.3 examples/sec; 0.078 sec/batch)
2017-06-02 09:10:58.250868: step 269620, loss = 0.30 (1506.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:10:59.123203: step 269630, loss = 0.25 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:11:00.012673: step 269640, loss = 0.26 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:11:00.867367: step 269650, loss = 0.28 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:11:01.755239: step 269660, loss = 0.26 (1441.7 examples/sec; 0.089 sec/batch)
2017-06-02 09:11:02.633959: step 269670, loss = 0.32 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:11:03.513626: step 269680, loss = 0.28 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:11:04.393832: step 269690, loss = 0.37 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:11:05.398525: step 269700, loss = 0.26 (1274.0 examples/sec; 0.100 sec/batch)
2017-06-02 09:11:06.171546: step 269710, loss = 0.27 (1655.8 examples/sec; 0.077 sec/batch)
2017-06-02 09:11:07.076625: step 269720, loss = 0.27 (1414.2 examples/sec; 0.091 sec/batch)
2017-06-02 09:11:07.932431: step 269730, loss = 0.20 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:11:08.827249: step 269740, loss = 0.28 (1430.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:11:09.731384: step 269750, loss = 0.34 (1415.7 examples/sec; 0.090 sec/batch)
2017-06-02 09:11:10.603315: step 269760, loss = 0.31 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:11:11.495080: step 269770, loss = 0.22 (1435.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:11:12.378134: step 269780, loss = 0.24 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:11:13.232325: step 269790, loss = 0.24 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:11:14.201838: step 269800, loss = 0.36 (1320.3 examples/sec; 0.097 sec/batch)
2017-06-02 09:11:14.967836: step 269810, loss = 0.28 (1671.0 examples/sec; 0.077 sec/batch)
2017-06-02 09:11:15.838701: step 269820, loss = 0.42 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:11:16.726185: step 269830, loss = 0.25 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:11:17.610441: step 269840, loss = 0.25 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:11:18.467058: step 269850, loss = 0.32 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:11:19.339394: step 269860, loss = 0.32 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:11:20.219936: step 269870, loss = 0.33 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:11:21.078402: step 269880, loss = 0.32 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:11:21.954548: step 269890, loss = 0.32 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:11:22.979411: step 269900, loss = 0.39 (1248.9 examples/sec; 0.102 sec/batch)
2017-06-02 09:11:23.702370: step 269910, loss = 0.31 (1770.5 examples/sec; 0.072 sec/batch)
2017-06-02 09:11:24.585562: step 269920, loss = 0.28 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:11:25.447105: step 269930, loss = 0.25 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:11:26.305649: step 269940, loss = 0.35 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:11:27.205839: step 269950, loss = 0.34 (1421.9 examples/sec; 0.090 sec/batch)
2017-06-02 09:11:28.117653: step 269960, loss = 0.28 (1403.8 examples/sec; 0.091 sec/batch)
2017-06-02 09:11:28.989659: step 269970, loss = 0.22 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:11:29.875525: step 269980, loss = 0.28 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 09:11:30.713753: step 269990, loss = 0.34 (1527.0 examples/sec; 0.084 sec/batch)
2017-06-02 09:11:31.685576: step 270000, loss = 0.36 (1317.1 examples/sec; 0.097 sec/batch)
2017-06-02 09:11:32.447470: step 270010, loss = 0.34 (1680.0 examples/sec; 0.076 sec/batch)
2017-06-02 09:11:33.327813: step 270020, loss = 0.33 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:11:34.177070: step 270030, loss = 0.26 (1507.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:11:35.033203: step 270040, loss = 0.21 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:11:35.929190: step 270050, loss = 0.23 (1428.6 examples/sec; 0.090 sec/batch)
2017-06-02 09:11:36.818611: step 270060, loss = 0.30 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:11:37.701780: step 270070, loss = 0.28 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:11:38.566726: step 270080, loss = 0.30 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:11:39.445022: step 270090, loss = 0.31 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:11:40.442001: step 270100, loss = 0.27 (1283.9 examples/sec; 0.100 sec/batch)
2017-06-02 09:11:41.220743: step 270110, loss = 0.43 (1643.7 examples/sec; 0.078 sec/batch)
2017-06-02 09:11:42.100777: step 270120, loss = 0.31 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:11:42.987765: step 270130, loss = 0.28 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:11:43.875482: step 270140, loss = 0.35 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 09:11:44.769563: step 270150, loss = 0.28 (1431.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:11:45.654610: step 270160, loss = 0.34 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:11:46.555056: step 270170, loss = 0.24 (1421.5 examples/sec; 0.090 sec/batch)
2017-06-02 09:11:47.439586: step 270180, loss = 0.25 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:11:48.318704: step 270190, loss = 0.36 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:11:49.274380: step 270200, loss = 0.32 (1339.4 examples/sec; 0.096 sec/batch)
2017-06-02 09:11:50.050839: step 270210, loss = 0.22 (1648.5 examples/sec; 0.078 sec/batch)
2017-06-02 09:11:50.922137: step 270220, loss = 0.28 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:11:51.758439: step 270230, loss = 0.28 (1530.5 examples/sec; 0.084 sec/batch)
2017-06-02 09:11:52.629557: step 270240, loss = 0.26 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:11:53.507015: step 270250, loss = 0.34 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:11:54.358678: step 270260, loss = 0.26 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:11:55.207923: step 270270, loss = 0.31 (1507.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:11:56.096355: step 270280, loss = 0.22 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 09:11:56.984278: step 270290, loss = 0.36 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:11:57.931176: step 270300, loss = 0.39 (1351.8 examples/sec; 0.095 sec/batch)
2017-06-02 09:11:58.712932: step 270310, loss = 0.26 (1637.3 examples/sec; 0.078 sec/batch)
2017-06-02 09:11:59.600811: step 270320, loss = 0.31 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:12:00.448966: step 270330, loss = 0.22 (1509.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:12:01.309971: step 270340, loss = 0.34 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:12:02.179670: step 270350, loss = 0.31 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:12:03.064072: step 270360, loss = 0.33 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:12:03.944139: step 270370, loss = 0.29 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:12:04.818267: step 270380, loss = 0.27 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:12:05.714514: step 270390, loss = 0.23 (1428.3 examples/sec; 0.090 sec/batch)
2017-06-02 09:12:06.673469: step 270400, loss = 0.25 (1334.7 examples/sec; 0.096 sec/batch)
2017-06-02 09:12:07.424954: step 270410, loss = 0.32 (1703.3 examples/sec; 0.075 sec/batch)
2017-06-02 09:12:08.295185: step 270420, loss = 0.24 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:12:09.182113: step 270430, loss = 0.28 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:12:10.073235: step 270440, loss = 0.24 (1436.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:12:10.986618: step 270450, loss = 0.31 (1401.4 examples/sec; 0.091 sec/batch)
2017-06-02 09:12:11.882215: step 270460, loss = 0.22 (1429.2 examples/sec; 0.090 sec/batch)
2017-06-02 09:12:12.758769: step 270470, loss = 0.25 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:12:13.625502: step 270480, loss = 0.23 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:12:14.491307: step 270490, loss = 0.31 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:12:15.491448: step 270500, loss = 0.25 (1279.8 examples/sec; 0.100 sec/batch)
2017-06-02 09:12:16.264131: step 270510, loss = 0.43 (1656.6 examples/sec; 0.077 sec/batch)
2017-06-02 09:12:17.176576: step 270520, loss = 0.28 (1402.8 examples/sec; 0.091 sec/batch)
2017-06-02 09:12:18.057468: step 270530, loss = 0.21 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:12:18.935244: step 270540, loss = 0.24 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:12:19.797804: step 270550, loss = 0.28 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:12:20.682913: step 270560, loss = 0.30 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:12:21.534700: step 270570, loss = 0.33 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:12:22.390210: step 270580, loss = 0.31 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:12:23.269368: step 270590, loss = 0.32 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:12:24.232883: step 270600, loss = 0.29 (1328.5 examples/sec; 0.096 sec/batch)
2017-06-02 09:12:25.000209: step 270610, loss = 0.25 (1668.2 examples/sec; 0.077 sec/batch)
2017-06-02 09:12:25.866064: step 270620, loss = 0.28 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:12:26.710442: step 270630, loss = 0.30 (1515.9 examples/sec; 0.084 sec/batch)
2017-06-02 09:12:27.565513: step 270640, loss = 0.28 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:12:28.428230: step 270650, loss = 0.28 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:12:29.318637: step 270660, loss = 0.33 (1437.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:12:30.166344: step 270670, loss = 0.33 (1509.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:12:31.022510: step 270680, loss = 0.24 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:12:31.911102: step 270690, loss = 0.29 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:12:32.872085: step 270700, loss = 0.24 (1331.9 examples/sec; 0.096 sec/batch)
2017-06-02 09:12:33.636724: step 270710, loss = 0.33 (1674.0 examples/sec; 0.076 sec/batch)
2017-06-02 09:12:34.488800: step 270720, loss = 0.27 (1502.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:12:35.342612: step 270730, loss = 0.29 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:12:36.208527: step 270740, loss = 0.22 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:12:37.106767: step 270750, loss = 0.27 (1425.0 examples/sec; 0.090 sec/batch)
2017-06-02 09:12:37.972116: step 270760, loss = 0.34 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:12:38.840922: step 270770, loss = 0.38 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:12:39.730305: step 270780, loss = 0.32 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:12:40.600822: step 270790, loss = 0.24 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:12:41.559595: step 270800, loss = 0.36 (1335.0 examples/sec; 0.096 sec/batch)
2017-06-02 09:12:42.339559: step 270810, loss = 0.29 (1641.1 examples/sec; 0.078 sec/batch)
2017-06-02 09:12:43.212895: step 270820, loss = 0.33 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:12:44.075507: step 270830, loss = 0.28 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:12:44.919108: step 270840, loss = 0.30 (1517.3 examples/sec; 0.084 sec/batch)
2017-06-02 09:12:45.784127: step 270850, loss = 0.27 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:12:46.641717: step 270860, loss = 0.25 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:12:47.507232: step 270870, loss = 0.30 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:12:48.360072: step 270880, loss = 0.38 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:12:49.212020: step 270890, loss = 0.29 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:12:50.169125: step 270900, loss = 0.31 (1337.4 examples/sec; 0.096 sec/batch)
2017-06-02 09:12:50.950265: step 270910, loss = 0.31 (1638.7 examples/sec; 0.078 sec/batch)
2017-06-02 09:12:51.820502: step 270920, loss = 0.32 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:12:52.646002: step 270930, loss = 0.40 (1550.6 examples/sec; 0.083 sec/batch)
2017-06-02 09:12:53.526688: step 270940, loss = 0.36 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:12:54.400888: step 270950, loss = 0.27 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:12:55.278431: step 270960, loss = 0.32 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:12:56.169678: step 270970, loss = 0.27 (1436.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:12:57.030620: step 270980, loss = 0.30 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:12:57.871071: step 270990, loss = 0.22 (1523.0 examples/sec; 0.084 sec/batch)
2017-06-02 09:12:58.858679: step 271000, loss = 0.26 (1296.1 examples/sec; 0.099 sec/batch)
2017-06-02 09:12:59.614753: step 271010, loss = 0.47 (1693.0 examples/sec; 0.076 sec/batch)
2017-06-02 09:13:00.484619: step 271020, loss = 0.33 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:13:01.353189: step 271030, loss = 0.35 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:13:02.235693: step 271040, loss = 0.40 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:13:03.120785: step 271050, loss = 0.30 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:13:03.988005: step 271060, loss = 0.27 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:13:04.874252: step 271070, loss = 0.28 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:13:05.737321: step 271080, loss = 0.23 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:13:06.585863: step 271090, loss = 0.35 (1508.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:13:07.539566: step 271100, loss = 0.31 (1342.1 examples/sec; 0.095 sec/batch)
2017-06-02 09:13:08.295483: step 271110, loss = 0.28 (1693.3 examples/sec; 0.076 sec/batch)
2017-06-02 09:13:09.144762: step 271120, loss = 0.24 (1507.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:13:10.030496: step 271130, loss = 0.34 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:13:10.897955: step 271140, loss = 0.27 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:13:11.793436: step 271150, loss = 0.29 (1429.4 examples/sec; 0.090 sec/batch)
2017-06-02 09:13:12.661409: step 271160, loss = 0.22 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:13:13.532563: step 271170, loss = 0.29 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:13:14.404950: step 271180, loss = 0.28 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:13:15.299968: step 271190, loss = 0.33 (1430.1 examples/sec; 0.090 sec/batch)
2017-06-02 09:13:16.286024: step 271200, loss = 0.25 (1298.1 examples/sec; 0.099 sec/batch)
2017-06-02 09:13:17.076150: step 271210, loss = 0.23 (1620.0 examples/sec; 0.079 sec/batch)
2017-06-02 09:13:17.965474: step 271220, loss = 0.24 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:13:18.870303: step 271230, loss = 0.26 (1414.7 examples/sec; 0.090 sec/batch)
2017-06-02 09:13:19.781325: step 271240, loss = 0.29 (1405.0 examples/sec; 0.091 sec/batch)
2017-06-02 09:13:20.679573: step 271250, loss = 0.40 (1425.0 examples/sec; 0.090 sec/batch)
2017-06-02 09:13:21.573990: step 271260, loss = 0.24 (1431.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:13:22.440584: step 271270, loss = 0.32 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:13:23.304186: step 271280, loss = 0.29 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:13:24.188729: step 271290, loss = 0.31 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:13:25.170801: step 271300, loss = 0.26 (1303.3 examples/sec; 0.098 sec/batch)
2017-06-02 09:13:25.952365: step 271310, loss = 0.30 (1637.8 examples/sec; 0.078 sec/batch)
2017-06-02 09:13:26.840323: step 271320, loss = 0.27 (1441.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:13:27.721217: step 271330, loss = 0.32 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:13:28.636214: step 271340, loss = 0.30 (1398.9 examples/sec; 0.092 sec/batch)
2017-06-02 09:13:29.488510: step 271350, loss = 0.28 (1501.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:13:30.360564: step 271360, loss = 0.31 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:13:31.242764: step 271370, loss = 0.34 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:13:32.109973: step 271380, loss = 0.26 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:13:32.986869: step 271390, loss = 0.26 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:13:33.953652: step 271400, loss = 0.35 (1324.0 examples/sec; 0.097 sec/batch)
2017-06-02 09:13:34.734964: step 271410, loss = 0.40 (1638.3 examples/sec; 0.078 sec/batch)
2017-06-02 09:13:35.609779: step 271420, loss = 0.32 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:13:36.496817: step 271430, loss = 0.27 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:13:37.341081: step 271440, loss = 0.29 (1516.1 examples/sec; 0.084 sec/batch)
2017-06-02 09:13:38.208906: step 271450, loss = 0.30 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:13:39.032906: step 271460, loss = 0.35 (1553.4 examples/sec; 0.082 sec/batch)
2017-06-02 09:13:39.883330: step 271470, loss = 0.32 (1505.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:13:40.705348: step 271480, loss = 0.31 (1557.1 examples/sec; 0.082 sec/batch)
2017-06-02 09:13:41.579212: step 271490, loss = 0.25 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:13:42.591746: step 271500, loss = 0.25 (1264.1 examples/sec; 0.101 sec/batch)
2017-06-02 09:13:43.354859: step 271510, loss = 0.24 (1677.3 examples/sec; 0.076 sec/batch)
2017-06-02 09:13:44.239939: step 271520, loss = 0.29 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:13:45.110283: step 271530, loss = 0.35 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:13:45.972830: step 271540, loss = 0.27 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:13:46.793678: step 271550, loss = 0.26 (1559.3 examples/sec; 0.082 sec/batch)
2017-06-02 09:13:47.643371: step 271560, loss = 0.29 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:13:48.524183: step 271570, loss = 0.27 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:13:49.424719: step 271580, loss = 0.24 (1421.4 examples/sec; 0.090 sec/batch)
2017-06-02 09:13:50.270681: step 271590, loss = 0.25 (1513.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:13:51.228668: step 271600, loss = 0.28 (1336.1 examples/sec; 0.096 sec/batch)
2017-06-02 09:13:51.987113: step 271610, loss = 0.31 (1687.7 examples/sec; 0.076 sec/batch)
2017-06-02 09:13:52.861493: step 271620, loss = 0.30 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:13:53.754238: step 271630, loss = 0.32 (1433.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:13:54.617296: step 271640, loss = 0.30 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:13:55.486274: step 271650, loss = 0.31 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:13:56.367133: step 271660, loss = 0.31 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:13:57.245165: step 271670, loss = 0.35 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:13:58.127057: step 271680, loss = 0.22 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:13:58.991568: step 271690, loss = 0.30 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:13:59.943371: step 271700, loss = 0.29 (1344.8 examples/sec; 0.095 sec/batch)
2017-06-02 09:14:00.742112: step 271710, loss = 0.24 (1602.5 examples/sec; 0.080 sec/batch)
2017-06-02 09:14:01.608020: step 271720, loss = 0.25 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:14:02.495066: step 271730, loss = 0.23 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:14:03.330980: step 271740, loss = 0.30 (1531.3 examples/sec; 0.084 sec/batch)
2017-06-02 09:14:04.203817: step 271750, loss = 0.25 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:14:05.055026: step 271760, loss = 0.22 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:14:05.961183: step 271770, loss = 0.33 (1412.6 examples/sec; 0.091 sec/batch)
2017-06-02 09:14:06.823957: step 271780, loss = 0.30 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:14:07.687966: step 271790, loss = 0.30 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:14:08.670444: step 271800, loss = 0.33 (1302.8 examples/sec; 0.098 sec/batch)
2017-06-02 09:14:09.469171: step 271810, loss = 0.28 (1602.5 examples/sec; 0.080 sec/batch)
2017-06-02 09:14:10.322289: step 271820, loss = 0.24 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:14:11.194956: step 271830, loss = 0.26 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:14:12.069664: step 271840, loss = 0.33 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:14:12.924329: step 271850, loss = 0.25 (1497.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:14:13.831526: step 271860, loss = 0.32 (1410.9 examples/sec; 0.091 sec/batch)
2017-06-02 09:14:14.728271: step 271870, loss = 0.33 (1427.4 examples/sec; 0.090 sec/batch)
2017-06-02 09:14:15.617162: step 271880, loss = 0.25 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:14:16.524418: step 271890, loss = 0.30 (1410.8 examples/sec; 0.091 sec/batch)
2017-06-02 09:14:17.534292: step 271900, loss = 0.23 (1267.5 examples/sec; 0.101 sec/batch)
2017-06-02 09:14:18.326647: step 271910, loss = 0.29 (1615.4 examples/sec; 0.079 sec/batch)
2017-06-02 09:14:19.227495: step 271920, loss = 0.30 (1420.9 examples/sec; 0.090 sec/batch)
2017-06-02 09:14:20.098236: step 271930, loss = 0.30 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:14:20.957273: step 271940, loss = 0.33 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:14:21.848646: step 271950, loss = 0.39 (1436.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:14:22.730341: step 271960, loss = 0.33 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:14:23.624602: step 271970, loss = 0.27 (1431.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:14:24.511245: step 271980, loss = 0.25 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:14:25.395112: step 271990, loss = 0.25 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:14:26.352038: step 272000, loss = 0.25 (1337.6 examples/sec; 0.096 sec/batch)
2017-06-02 09:14:27.127344: step 272010, loss = 0.27 (1651.0 examples/sec; 0.078 sec/batch)
2017-06-02 09:14:28.021046: step 272020, loss = 0.27 (1432.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:14:28.896571: step 272030, loss = 0.26 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:14:29.781448: step 272040, loss = 0.21 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:14:30.656008: step 272050, loss = 0.36 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:14:31.520747: step 272060, loss = 0.26 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:14:32.365179: step 272070, loss = 0.23 (1515.8 examples/sec; 0.084 sec/batch)
2017-06-02 09:14:33.251543: step 272080, loss = 0.29 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:14:34.126131: step 272090, loss = 0.25 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:14:35.075515: step 272100, loss = 0.30 (1348.2 examples/sec; 0.095 sec/batch)
2017-06-02 09:14:35.827434: step 272110, loss = 0.28 (1702.3 examples/sec; 0.075 sec/batch)
2017-06-02 09:14:36.717879: step 272120, loss = 0.32 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:14:37.607618: step 272130, loss = 0.27 (1438.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:14:38.467981: step 272140, loss = 0.21 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:14:39.372660: step 272150, loss = 0.31 (1414.9 examples/sec; 0.090 sec/batch)
2017-06-02 09:14:40.256613: step 272160, loss = 0.21 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:14:41.134061: step 272170, loss = 0.30 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:14:42.025576: step 272180, loss = 0.31 (1435.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:14:42.903914: step 272190, loss = 0.24 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:14:43.886500: step 272200, loss = 0.24 (1302.7 examples/sec; 0.098 sec/batch)
2017-06-02 09:14:44.661126: step 272210, loss = 0.25 (1652.4 examples/sec; 0.077 sec/batch)
2017-06-02 09:14:45.548248: step 272220, loss = 0.27 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 09:14:46.450068: step 272230, loss = 0.37 (1419.3 examples/sec; 0.090 sec/batch)
2017-06-02 09:14:47.334345: step 272240, loss = 0.22 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:14:48.227881: step 272250, loss = 0.35 (1432.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:14:49.098347: step 272260, loss = 0.33 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:14:50.001018: step 272270, loss = 0.26 (1418.0 examples/sec; 0.090 sec/batch)
2017-06-02 09:14:50.881454: step 272280, loss = 0.32 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:14:51.752789: step 272290, loss = 0.29 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:14:52.723947: step 272300, loss = 0.36 (1317.9 examples/sec; 0.097 sec/batch)
2017-06-02 09:14:53.475648: step 272310, loss = 0.29 (1702.8 examples/sec; 0.075 sec/batch)
2017-06-02 09:14:54.325595: step 272320, loss = 0.23 (1506.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:14:55.206377: step 272330, loss = 0.30 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:14:56.082845: step 272340, loss = 0.28 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:14:56.947315: step 272350, loss = 0.21 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:14:57.783910: step 272360, loss = 0.22 (1530.0 examples/sec; 0.084 sec/batch)
2017-06-02 09:14:58.648326: step 272370, loss = 0.39 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:14:59.496354: step 272380, loss = 0.23 (1509.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:15:00.349016: step 272390, loss = 0.25 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:15:01.304421: step 272400, loss = 0.43 (1339.7 examples/sec; 0.096 sec/batch)
2017-06-02 09:15:02.046682: step 272410, loss = 0.31 (1724.5 examples/sec; 0.074 sec/batch)
2017-06-02 09:15:02.917984: step 272420, loss = 0.33 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:15:03.804423: step 272430, loss = 0.36 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:15:04.687620: step 272440, loss = 0.35 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:15:05.536189: step 272450, loss = 0.30 (1508.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:15:06.426283: step 272460, loss = 0.22 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:15:07.304103: step 272470, loss = 0.34 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:15:08.176885: step 272480, loss = 0.27 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:15:09.047875: step 272490, loss = 0.30 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:15:10.008123: step 272500, loss = 0.28 (1333.0 examples/sec; 0.096 sec/batch)
2017-06-02 09:15:10.779632: step 272510, loss = 0.21 (1659.1 examples/sec; 0.077 sec/batch)
2017-06-02 09:15:11.639926: step 272520, loss = 0.30 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:15:12.528241: step 272530, loss = 0.30 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 09:15:13.393740: step 272540, loss = 0.33 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:15:14.260320: step 272550, loss = 0.33 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:15:15.143038: step 272560, loss = 0.38 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:15:16.031341: step 272570, loss = 0.27 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:15:16.903716: step 272580, loss = 0.23 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:15:17.781711: step 272590, loss = 0.22 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:15:18.737940: step 272600, loss = 0.23 (1338.6 examples/sec; 0.096 sec/batch)
2017-06-02 09:15:19.505073: step 272610, loss = 0.29 (1668.6 examples/sec; 0.077 sec/batch)
2017-06-02 09:15:20.378541: step 272620, loss = 0.29 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:15:21.229676: step 272630, loss = 0.30 (1503.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:15:22.076256: step 272640, loss = 0.27 (1512.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:15:22.946159: step 272650, loss = 0.26 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:15:23.796409: step 272660, loss = 0.29 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:15:24.678423: step 272670, loss = 0.37 (1451.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:15:25.537858: step 272680, loss = 0.32 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:15:26.429261: step 272690, loss = 0.31 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 09:15:27.388403: step 272700, loss = 0.33 (1334.5 examples/sec; 0.096 sec/batch)
2017-06-02 09:15:28.145876: step 272710, loss = 0.32 (1689.8 examples/sec; 0.076 sec/batch)
2017-06-02 09:15:28.989383: step 272720, loss = 0.27 (1517.5 examples/sec; 0.084 sec/batch)
2017-06-02 09:15:29.869353: step 272730, loss = 0.31 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:15:30.718728: step 272740, loss = 0.32 (1507.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:15:31.578944: step 272750, loss = 0.38 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:15:32.463478: step 272760, loss = 0.33 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:15:33.332659: step 272770, loss = 0.31 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:15:34.191525: step 272780, loss = 0.31 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:15:35.074985: step 272790, loss = 0.40 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:15:36.025463: step 272800, loss = 0.32 (1346.7 examples/sec; 0.095 sec/batch)
2017-06-02 09:15:36.788917: step 272810, loss = 0.39 (1676.6 examples/sec; 0.076 sec/batch)
2017-06-02 09:15:37.649343: step 272820, loss = 0.34 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:15:38.540329: step 272830, loss = 0.25 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:15:39.464825: step 272840, loss = 0.38 (1384.5 examples/sec; 0.092 sec/batch)
2017-06-02 09:15:40.339681: step 272850, loss = 0.29 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:15:41.226875: step 272860, loss = 0.25 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:15:42.110001: step 272870, loss = 0.27 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:15:43.001027: step 272880, loss = 0.31 (1436.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:15:43.886529: step 272890, loss = 0.31 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:15:44.878925: step 272900, loss = 0.31 (1289.8 examples/sec; 0.099 sec/batch)
2017-06-02 09:15:45.639421: step 272910, loss = 0.28 (1683.1 examples/sec; 0.076 sec/batch)
2017-06-02 09:15:46.506003: step 272920, loss = 0.23 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:15:47.388918: step 272930, loss = 0.32 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:15:48.250400: step 272940, loss = 0.33 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:15:49.111562: step 272950, loss = 0.41 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:15:49.972288: step 272960, loss = 0.22 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:15:50.847119: step 272970, loss = 0.28 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:15:51.739021: step 272980, loss = 0.22 (1435.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:15:52.629610: step 272990, loss = 0.25 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:15:53.595893: step 273000, loss = 0.28 (1324.7 examples/sec; 0.097 sec/batch)
2017-06-02 09:15:54.345810: step 273010, loss = 0.31 (1706.9 examples/sec; 0.075 sec/batch)
2017-06-02 09:15:55.224131: step 273020, loss = 0.32 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:15:56.064301: step 273030, loss = 0.34 (1523.5 examples/sec; 0.084 sec/batch)
2017-06-02 09:15:56.918454: step 273040, loss = 0.26 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:15:57.796022: step 273050, loss = 0.38 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:15:58.650907: step 273060, loss = 0.30 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:15:59.498792: step 273070, loss = 0.37 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:16:00.405068: step 273080, loss = 0.23 (1412.4 examples/sec; 0.091 sec/batch)
2017-06-02 09:16:01.270259: step 273090, loss = 0.30 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:16:02.254062: step 273100, loss = 0.21 (1301.1 examples/sec; 0.098 sec/batch)
2017-06-02 09:16:03.016310: step 273110, loss = 0.25 (1679.2 examples/sec; 0.076 sec/batch)
2017-06-02 09:16:03.888631: step 273120, loss = 0.31 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:16:04.774628: step 273130, loss = 0.22 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:16:05.635810: step 273140, loss = 0.35 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:16:06.522011: step 273150, loss = 0.28 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:16:07.401110: step 273160, loss = 0.40 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:16:08.288973: step 273170, loss = 0.43 (1441.7 examples/sec; 0.089 sec/batch)
2017-06-02 09:16:09.165185: step 273180, loss = 0.37 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:16:10.016262: step 273190, loss = 0.24 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:16:11.014868: step 273200, loss = 0.36 (1281.8 examples/sec; 0.100 sec/batch)
2017-06-02 09:16:11.744394: step 273210, loss = 0.27 (1754.6 examples/sec; 0.073 sec/batch)
2017-06-02 09:16:12.604308: step 273220, loss = 0.38 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:16:13.496082: step 273230, loss = 0.34 (1435.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:16:14.341963: step 273240, loss = 0.34 (1513.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:16:15.229073: step 273250, loss = 0.24 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 09:16:16.088033: step 273260, loss = 0.25 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:16:16.958387: step 273270, loss = 0.27 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:16:17.806265: step 273280, loss = 0.29 (1509.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:16:18.657291: step 273290, loss = 0.34 (1504.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:16:19.632514: step 273300, loss = 0.34 (1312.5 examples/sec; 0.098 sec/batch)
2017-06-02 09:16:20.372796: step 273310, loss = 0.26 (1729.1 examples/sec; 0.074 sec/batch)
2017-06-02 09:16:21.228993: step 273320, loss = 0.33 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:16:22.095249: step 273330, loss = 0.25 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:16:22.968552: step 273340, loss = 0.27 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:16:23.818653: step 273350, loss = 0.28 (1505.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:16:24.691300: step 273360, loss = 0.23 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:16:25.572810: step 273370, loss = 0.31 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:16:26.450432: step 273380, loss = 0.30 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:16:27.334474: step 273390, loss = 0.32 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:16:28.313398: step 273400, loss = 0.27 (1307.6 examples/sec; 0.098 sec/batch)
2017-06-02 09:16:29.069929: step 273410, loss = 0.25 (1691.9 examples/sec; 0.076 sec/batch)
2017-06-02 09:16:29.947217: step 273420, loss = 0.25 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:16:30.792792: step 273430, loss = 0.36 (1513.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:16:31.673569: step 273440, loss = 0.22 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:16:32.537535: step 273450, loss = 0.37 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:16:33.405540: step 273460, loss = 0.34 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:16:34.245072: step 273470, loss = 0.31 (1524.7 examples/sec; 0.084 sec/batch)
2017-06-02 09:16:35.130675: step 273480, loss = 0.29 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:16:35.971810: step 273490, loss = 0.20 (1521.8 examples/sec; 0.084 sec/batch)
2017-06-02 09:16:37.010696: step 273500, loss = 0.23 (1232.1 examples/sec; 0.104 sec/batch)
2017-06-02 09:16:37.739638: step 273510, loss = 0.30 (1756.0 examples/sec; 0.073 sec/batch)
2017-06-02 09:16:38.609291: step 273520, loss = 0.28 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:16:39.475496: step 273530, loss = 0.33 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:16:40.340970: step 273540, loss = 0.33 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:16:41.222619: step 273550, loss = 0.30 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:16:42.061526: step 273560, loss = 0.22 (1525.8 examples/sec; 0.084 sec/batch)
2017-06-02 09:16:42.915707: step 273570, loss = 0.25 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:16:43.774516: step 273580, loss = 0.23 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:16:44.631330: step 273590, loss = 0.32 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:16:45.626771: step 273600, loss = 0.29 (1285.8 examples/sec; 0.100 sec/batch)
2017-06-02 09:16:46.388014: step 273610, loss = 0.29 (1681.5 examples/sec; 0.076 sec/batch)
2017-06-02 09:16:47.258437: step 273620, loss = 0.27 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:16:48.090965: step 273630, loss = 0.32 (1537.5 examples/sec; 0.083 sec/batch)
2017-06-02 09:16:48.986426: step 273640, loss = 0.21 (1429.4 examples/sec; 0.090 sec/batch)
2017-06-02 09:16:49.864160: step 273650, loss = 0.22 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:16:50.747494: step 273660, loss = 0.32 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:16:51.627880: step 273670, loss = 0.32 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:16:52.510408: step 273680, loss = 0.25 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:16:53.363380: step 273690, loss = 0.25 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:16:54.359235: step 273700, loss = 0.26 (1285.3 examples/sec; 0.100 sec/batch)
2017-06-02 09:16:55.116210: step 273710, loss = 0.25 (1691.0 examples/sec; 0.076 sec/batch)
2017-06-02 09:16:56.002075: step 273720, loss = 0.25 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 09:16:56.881215: step 273730, loss = 0.26 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:16:57.766072: step 273740, loss = 0.21 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:16:58.633745: step 273750, loss = 0.25 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:16:59.477991: step 273760, loss = 0.24 (1516.1 examples/sec; 0.084 sec/batch)
2017-06-02 09:17:00.307449: step 273770, loss = 0.23 (1543.2 examples/sec; 0.083 sec/batch)
2017-06-02 09:17:01.162883: step 273780, loss = 0.33 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:17:02.033505: step 273790, loss = 0.19 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:17:03.033319: step 273800, loss = 0.29 (1280.2 examples/sec; 0.100 sec/batch)
2017-06-02 09:17:03.798102: step 273810, loss = 0.28 (1673.7 examples/sec; 0.076 sec/batch)
2017-06-02 09:17:04.685407: step 273820, loss = 0.24 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:17:05.550726: step 273830, loss = 0.30 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:17:06.411084: step 273840, loss = 0.34 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:17:07.264397: step 273850, loss = 0.24 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:17:08.111314: step 273860, loss = 0.20 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:17:08.974128: step 273870, loss = 0.24 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:17:09.828712: step 273880, loss = 0.28 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:17:10.713383: step 273890, loss = 0.28 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:17:11.678888: step 273900, loss = 0.29 (1325.7 examples/sec; 0.097 sec/batch)
2017-06-02 09:17:12.443017: step 273910, loss = 0.23 (1675.1 examples/sec; 0.076 sec/batch)
2017-06-02 09:17:13.304165: step 273920, loss = 0.25 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:17:14.145423: step 273930, loss = 0.25 (1521.5 examples/sec; 0.084 sec/batch)
2017-06-02 09:17:15.019934: step 273940, loss = 0.23 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:17:15.863486: step 273950, loss = 0.30 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:17:16.736498: step 273960, loss = 0.38 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:17:17.619459: step 273970, loss = 0.28 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:17:18.503190: step 273980, loss = 0.30 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:17:19.377718: step 273990, loss = 0.32 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:17:20.371458: step 274000, loss = 0.27 (1288.1 examples/sec; 0.099 sec/batch)
2017-06-02 09:17:21.145754: step 274010, loss = 0.36 (1653.1 examples/sec; 0.077 sec/batch)
2017-06-02 09:17:21.996439: step 274020, loss = 0.23 (1504.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:17:22.893096: step 274030, loss = 0.28 (1427.5 examples/sec; 0.090 sec/batch)
2017-06-02 09:17:23.782701: step 274040, loss = 0.32 (1438.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:17:24.683436: step 274050, loss = 0.22 (1421.1 examples/sec; 0.090 sec/batch)
2017-06-02 09:17:25.578647: step 274060, loss = 0.35 (1429.8 examples/sec; 0.090 sec/batch)
2017-06-02 09:17:26.465008: step 274070, loss = 0.20 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:17:27.344330: step 274080, loss = 0.25 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:17:28.216952: step 274090, loss = 0.25 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:17:29.200455: step 274100, loss = 0.32 (1301.5 examples/sec; 0.098 sec/batch)
2017-06-02 09:17:29.954828: step 274110, loss = 0.30 (1696.8 examples/sec; 0.075 sec/batch)
2017-06-02 09:17:30.815859: step 274120, loss = 0.27 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:17:31.702720: step 274130, loss = 0.25 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:17:32.577328: step 274140, loss = 0.24 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:17:33.460952: step 274150, loss = 0.26 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:17:34.325919: step 274160, loss = 0.22 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:17:35.230924: step 274170, loss = 0.20 (1414.3 examples/sec; 0.091 sec/batch)
2017-06-02 09:17:36.130997: step 274180, loss = 0.22 (1422.1 examples/sec; 0.090 sec/batch)
2017-06-02 09:17:36.982819: step 274190, loss = 0.27 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:17:37.948168: step 274200, loss = 0.24 (1325.9 examples/sec; 0.097 sec/batch)
2017-06-02 09:17:38.732145: step 274210, loss = 0.28 (1632.7 examples/sec; 0.078 sec/batch)
2017-06-02 09:17:39.580523: step 274220, loss = 0.21 (1508.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:17:40.438195: step 274230, loss = 0.23 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:17:41.287401: step 274240, loss = 0.28 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:17:42.162507: step 274250, loss = 0.26 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:17:43.024809: step 274260, loss = 0.29 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:17:43.886848: step 274270, loss = 0.19 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:17:44.751527: step 274280, loss = 0.24 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:17:45.615932: step 274290, loss = 0.24 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:17:46.646797: step 274300, loss = 0.21 (1241.7 examples/sec; 0.103 sec/batch)
2017-06-02 09:17:47.349996: step 274310, loss = 0.25 (1820.3 examples/sec; 0.070 sec/batch)
2017-06-02 09:17:48.218663: step 274320, loss = 0.24 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:17:49.084708: step 274330, loss = 0.22 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:17:49.937538: step 274340, loss = 0.20 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:17:50.790866: step 274350, loss = 0.29 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:17:51.638603: step 274360, loss = 0.23 (1509.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:17:52.492098: step 274370, loss = 0.30 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:17:53.362035: step 274380, loss = 0.21 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:17:54.188331: step 274390, loss = 0.24 (1549.1 examples/sec; 0.083 sec/batch)
2017-06-02 09:17:55.195520: step 274400, loss = 0.23 (1270.8 examples/sec; 0.101 sec/batch)
2017-06-02 09:17:55.930563: step 274410, loss = 0.22 (1741.4 examples/sec; 0.074 sec/batch)
2017-06-02 09:17:56.827510: step 274420, loss = 0.33 (1427.1 examples/sec; 0.090 sec/batch)
2017-06-02 09:17:57.708280: step 274430, loss = 0.23 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:17:58.574542: step 274440, loss = 0.25 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:17:59.481873: step 274450, loss = 0.33 (1410.7 examples/sec; 0.091 sec/batch)
2017-06-02 09:18:00.332153: step 274460, loss = 0.30 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:18:01.191686: step 274470, loss = 0.25 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:18:02.054005: step 274480, loss = 0.26 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:18:02.918126: step 274490, loss = 0.25 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:18:03.901365: step 274500, loss = 0.24 (1301.8 examples/sec; 0.098 sec/batch)
2017-06-02 09:18:04.658404: step 274510, loss = 0.25 (1690.8 examples/sec; 0.076 sec/batch)
2017-06-02 09:18:05.514304: step 274520, loss = 0.30 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:18:06.378238: step 274530, loss = 0.23 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:18:07.244759: step 274540, loss = 0.27 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:18:08.101595: step 274550, loss = 0.28 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:18:08.949182: step 274560, loss = 0.25 (1510.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:18:09.803075: step 274570, loss = 0.19 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:18:10.679264: step 274580, loss = 0.28 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:18:11.557224: step 274590, loss = 0.23 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:18:12.527298: step 274600, loss = 0.24 (1319.5 examples/sec; 0.097 sec/batch)
2017-06-02 09:18:13.298108: step 274610, loss = 0.24 (1660.6 examples/sec; 0.077 sec/batch)
2017-06-02 09:18:14.146039: step 274620, loss = 0.23 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:18:15.019379: step 274630, loss = 0.25 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:18:15.865447: step 274640, loss = 0.22 (1512.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:18:16.740020: step 274650, loss = 0.21 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:18:17.620865: step 274660, loss = 0.19 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:18:18.483354: step 274670, loss = 0.33 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:18:19.378256: step 274680, loss = 0.35 (1430.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:18:20.266866: step 274690, loss = 0.27 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:18:21.223367: step 274700, loss = 0.24 (1338.2 examples/sec; 0.096 sec/batch)
2017-06-02 09:18:21.978192: step 274710, loss = 0.28 (1695.8 examples/sec; 0.075 sec/batch)
2017-06-02 09:18:22.844922: step 274720, loss = 0.22 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:18:23.719790: step 274730, loss = 0.21 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:18:24.606473: step 274740, loss = 0.31 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:18:25.489111: step 274750, loss = 0.21 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:18:26.365722: step 274760, loss = 0.28 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:18:27.253633: step 274770, loss = 0.24 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:18:28.130104: step 274780, loss = 0.30 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:18:29.022918: step 274790, loss = 0.31 (1433.7 examples/sec; 0.089 sec/batch)
2017-06-02 09:18:30.060551: step 274800, loss = 0.31 (1233.6 examples/sec; 0.104 sec/batch)
2017-06-02 09:18:30.767071: step 274810, loss = 0.24 (1811.7 examples/sec; 0.071 sec/batch)
2017-06-02 09:18:31.643561: step 274820, loss = 0.23 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:18:32.518979: step 274830, loss = 0.24 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:18:33.391463: step 274840, loss = 0.28 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:18:34.223444: step 274850, loss = 0.25 (1538.5 examples/sec; 0.083 sec/batch)
2017-06-02 09:18:35.049247: step 274860, loss = 0.28 (1550.0 examples/sec; 0.083 sec/batch)
2017-06-02 09:18:35.922758: step 274870, loss = 0.23 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:18:36.798667: step 274880, loss = 0.19 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:18:37.645126: step 274890, loss = 0.23 (1512.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:18:38.591600: step 274900, loss = 0.21 (1352.4 examples/sec; 0.095 sec/batch)
2017-06-02 09:18:39.340771: step 274910, loss = 0.28 (1708.5 examples/sec; 0.075 sec/batch)
2017-06-02 09:18:40.198814: step 274920, loss = 0.19 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:18:41.070807: step 274930, loss = 0.24 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:18:41.935221: step 274940, loss = 0.26 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:18:42.802753: step 274950, loss = 0.25 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:18:43.669062: step 274960, loss = 0.23 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:18:44.554872: step 274970, loss = 0.25 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:18:45.465430: step 274980, loss = 0.28 (1405.7 examples/sec; 0.091 sec/batch)
2017-06-02 09:18:46.325942: step 274990, loss = 0.20 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:18:47.290607: step 275000, loss = 0.20 (1326.9 examples/sec; 0.096 sec/batch)
2017-06-02 09:18:48.051144: step 275010, loss = 0.27 (1683.0 examples/sec; 0.076 sec/batch)
2017-06-02 09:18:48.922506: step 275020, loss = 0.33 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:18:49.773653: step 275030, loss = 0.27 (1503.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:18:50.638106: step 275040, loss = 0.29 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:18:51.509653: step 275050, loss = 0.22 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:18:52.362039: step 275060, loss = 0.29 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:18:53.232755: step 275070, loss = 0.21 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:18:54.082491: step 275080, loss = 0.35 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:18:54.955603: step 275090, loss = 0.24 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:18:55.936723: step 275100, loss = 0.24 (1304.6 examples/sec; 0.098 sec/batch)
2017-06-02 09:18:56.708961: step 275110, loss = 0.28 (1657.6 examples/sec; 0.077 sec/batch)
2017-06-02 09:18:57.618352: step 275120, loss = 0.25 (1407.5 examples/sec; 0.091 sec/batch)
2017-06-02 09:18:58.508792: step 275130, loss = 0.25 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:18:59.381933: step 275140, loss = 0.32 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:19:00.262271: step 275150, loss = 0.25 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:19:01.133786: step 275160, loss = 0.19 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:19:02.012081: step 275170, loss = 0.18 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:19:02.890739: step 275180, loss = 0.26 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:19:03.759631: step 275190, loss = 0.33 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:19:04.764677: step 275200, loss = 0.23 (1273.6 examples/sec; 0.101 sec/batch)
2017-06-02 09:19:05.499129: step 275210, loss = 0.25 (1742.8 examples/sec; 0.073 sec/batch)
2017-06-02 09:19:06.365116: step 275220, loss = 0.22 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:19:07.221582: step 275230, loss = 0.27 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:19:08.101072: step 275240, loss = 0.24 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:19:08.970152: step 275250, loss = 0.18 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:19:09.826965: step 275260, loss = 0.21 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:19:10.689592: step 275270, loss = 0.26 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:19:11.568251: step 275280, loss = 0.24 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:19:12.452418: step 275290, loss = 0.28 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:19:13.416799: step 275300, loss = 0.26 (1327.3 examples/sec; 0.096 sec/batch)
2017-06-02 09:19:14.178760: step 275310, loss = 0.22 (1679.9 examples/sec; 0.076 sec/batch)
2017-06-02 09:19:15.036309: step 275320, loss = 0.32 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:19:15.909917: step 275330, loss = 0.23 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:19:16.775082: step 275340, loss = 0.20 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:19:17.616873: step 275350, loss = 0.26 (1520.5 examples/sec; 0.084 sec/batch)
2017-06-02 09:19:18.491175: step 275360, loss = 0.27 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:19:19.391619: step 275370, loss = 0.25 (1421.5 examples/sec; 0.090 sec/batch)
2017-06-02 09:19:20.285562: step 275380, loss = 0.24 (1431.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:19:21.165722: step 275390, loss = 0.21 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:19:22.141679: step 275400, loss = 0.22 (1311.5 examples/sec; 0.098 sec/batch)
2017-06-02 09:19:22.930565: step 275410, loss = 0.25 (1622.5 examples/sec; 0.079 sec/batch)
2017-06-02 09:19:23.816849: step 275420, loss = 0.22 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:19:24.696655: step 275430, loss = 0.20 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:19:25.587360: step 275440, loss = 0.21 (1437.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:19:26.484238: step 275450, loss = 0.19 (1427.2 examples/sec; 0.090 sec/batch)
2017-06-02 09:19:27.338028: step 275460, loss = 0.25 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:19:28.216417: step 275470, loss = 0.23 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:19:29.105209: step 275480, loss = 0.25 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:19:29.988564: step 275490, loss = 0.28 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:19:30.989856: step 275500, loss = 0.20 (1278.3 examples/sec; 0.100 sec/batch)
2017-06-02 09:19:31.736361: step 275510, loss = 0.24 (1714.7 examples/sec; 0.075 sec/batch)
2017-06-02 09:19:32.621925: step 275520, loss = 0.22 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:19:33.496185: step 275530, loss = 0.28 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:19:34.371902: step 275540, loss = 0.25 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:19:35.233590: step 275550, loss = 0.25 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:19:36.113598: step 275560, loss = 0.27 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:19:36.989077: step 275570, loss = 0.20 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:19:37.863468: step 275580, loss = 0.28 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:19:38.747437: step 275590, loss = 0.26 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:19:39.734727: step 275600, loss = 0.21 (1296.5 examples/sec; 0.099 sec/batch)
2017-06-02 09:19:40.506869: step 275610, loss = 0.22 (1657.7 examples/sec; 0.077 sec/batch)
2017-06-02 09:19:41.381233: step 275620, loss = 0.23 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:19:42.244091: step 275630, loss = 0.18 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:19:43.114365: step 275640, loss = 0.21 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:19:43.990439: step 275650, loss = 0.27 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:19:44.864226: step 275660, loss = 0.19 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:19:45.705793: step 275670, loss = 0.28 (1521.0 examples/sec; 0.084 sec/batch)
2017-06-02 09:19:46.571141: step 275680, loss = 0.20 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:19:47.431311: step 275690, loss = 0.23 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:19:48.426071: step 275700, loss = 0.17 (1286.7 examples/sec; 0.099 sec/batch)
2017-06-02 09:19:49.208601: step 275710, loss = 0.29 (1635.7 examples/sec; 0.078 sec/batch)
2017-06-02 09:19:50.075027: step 275720, loss = 0.21 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:19:50.919849: step 275730, loss = 0.27 (1515.1 examples/sec; 0.084 sec/batch)
2017-06-02 09:19:51.723767: step 275740, loss = 0.24 (1592.2 examples/sec; 0.080 sec/batch)
2017-06-02 09:19:52.641751: step 275750, loss = 0.31 (1394.3 examples/sec; 0.092 sec/batch)
2017-06-02 09:19:53.467784: step 275760, loss = 0.38 (1549.6 examples/sec; 0.083 sec/batch)
2017-06-02 09:19:54.291410: step 275770, loss = 0.26 (1554.1 examples/sec; 0.082 sec/batch)
2017-06-02 09:19:55.130260: step 275780, loss = 0.23 (1525.9 examples/sec; 0.084 sec/batch)
2017-06-02 09:19:55.937483: step 275790, loss = 0.25 (1585.7 examples/sec; 0.081 sec/batch)
2017-06-02 09:19:56.932493: step 275800, loss = 0.20 (1286.4 examples/sec; 0.100 sec/batch)
2017-06-02 09:19:57.673782: step 275810, loss = 0.25 (1726.7 examples/sec; 0.074 sec/batch)
2017-06-02 09:19:58.500006: step 275820, loss = 0.33 (1549.2 examples/sec; 0.083 sec/batch)
2017-06-02 09:19:59.354257: step 275830, loss = 0.22 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:20:00.225426: step 275840, loss = 0.28 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:20:01.117817: step 275850, loss = 0.23 (1434.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:20:01.993677: step 275860, loss = 0.30 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:20:02.894053: step 275870, loss = 0.23 (1421.6 examples/sec; 0.090 sec/batch)
2017-06-02 09:20:03.770539: step 275880, loss = 0.25 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:20:04.659323: step 275890, loss = 0.25 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:20:05.634454: step 275900, loss = 0.22 (1312.6 examples/sec; 0.098 sec/batch)
2017-06-02 09:20:06.431611: step 275910, loss = 0.29 (1605.7 examples/sec; 0.080 sec/batch)
2017-06-02 09:20:07.317586: step 275920, loss = 0.25 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 09:20:08.213971: step 275930, loss = 0.25 (1427.9 examples/sec; 0.090 sec/batch)
2017-06-02 09:20:09.101298: step 275940, loss = 0.26 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:20:09.976285: step 275950, loss = 0.21 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:20:10.884697: step 275960, loss = 0.23 (1409.0 examples/sec; 0.091 sec/batch)
2017-06-02 09:20:11.770891: step 275970, loss = 0.27 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:20:12.671020: step 275980, loss = 0.18 (1422.0 examples/sec; 0.090 sec/batch)
2017-06-02 09:20:13.557319: step 275990, loss = 0.22 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:20:14.561740: step 276000, loss = 0.20 (1274.4 examples/sec; 0.100 sec/batch)
2017-06-02 09:20:15.340708: step 276010, loss = 0.17 (1643.2 examples/sec; 0.078 sec/batch)
2017-06-02 09:20:16.228561: step 276020, loss = 0.28 (1441.7 examples/sec; 0.089 sec/batch)
2017-06-02 09:20:17.103829: step 276030, loss = 0.18 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:20:17.965502: step 276040, loss = 0.23 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:20:18.855849: step 276050, loss = 0.25 (1437.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:20:19.748777: step 276060, loss = 0.25 (1433.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:20:20.643109: step 276070, loss = 0.26 (1431.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:20:21.530441: step 276080, loss = 0.22 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:20:22.402129: step 276090, loss = 0.21 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:20:23.403269: step 276100, loss = 0.22 (1278.5 examples/sec; 0.100 sec/batch)
2017-06-02 09:20:24.181216: step 276110, loss = 0.37 (1645.4 examples/sec; 0.078 sec/batch)
2017-06-02 09:20:25.065071: step 276120, loss = 0.24 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:20:25.944360: step 276130, loss = 0.23 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:20:26.826941: step 276140, loss = 0.27 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:20:27.709760: step 276150, loss = 0.26 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:20:28.603128: step 276160, loss = 0.22 (1432.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:20:29.480378: step 276170, loss = 0.24 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:20:30.356676: step 276180, loss = 0.23 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:20:31.241780: step 276190, loss = 0.26 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:20:32.226694: step 276200, loss = 0.17 (1299.6 examples/sec; 0.098 sec/batch)
2017-06-02 09:20:33.027822: step 276210, loss = 0.25 (1597.7 examples/sec; 0.080 sec/batch)
2017-06-02 09:20:33.917620: step 276220, loss = 0.27 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:20:34.794485: step 276230, loss = 0.26 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:20:35.673175: step 276240, loss = 0.26 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:20:36.558552: step 276250, loss = 0.21 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 09:20:37.457238: step 276260, loss = 0.22 (1424.3 examples/sec; 0.090 sec/batch)
2017-06-02 09:20:38.335989: step 276270, loss = 0.20 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:20:39.213491: step 276280, loss = 0.28 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:20:40.091364: step 276290, loss = 0.27 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:20:41.077673: step 276300, loss = 0.23 (1297.8 examples/sec; 0.099 sec/batch)
2017-06-02 09:20:41.865118: step 276310, loss = 0.27 (1625.5 examples/sec; 0.079 sec/batch)
2017-06-02 09:20:42.719423: step 276320, loss = 0.25 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:20:43.610488: step 276330, loss = 0.21 (1436.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:20:44.493806: step 276340, loss = 0.27 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:20:45.369204: step 276350, loss = 0.19 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:20:46.253070: step 276360, loss = 0.24 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:20:47.142383: step 276370, loss = 0.22 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:20:48.021634: step 276380, loss = 0.24 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:20:48.912033: step 276390, loss = 0.27 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:20:49.899066: step 276400, loss = 0.24 (1296.8 examples/sec; 0.099 sec/batch)
2017-06-02 09:20:50.664538: step 276410, loss = 0.29 (1672.2 examples/sec; 0.077 sec/batch)
2017-06-02 09:20:51.555545: step 276420, loss = 0.31 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:20:52.440846: step 276430, loss = 0.19 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:20:53.303509: step 276440, loss = 0.25 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:20:54.181841: step 276450, loss = 0.24 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:20:55.053107: step 276460, loss = 0.23 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:20:55.926148: step 276470, loss = 0.43 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:20:56.780490: step 276480, loss = 0.23 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:20:57.644392: step 276490, loss = 0.20 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:20:58.633341: step 276500, loss = 0.24 (1294.3 examples/sec; 0.099 sec/batch)
2017-06-02 09:20:59.425858: step 276510, loss = 0.20 (1615.1 examples/sec; 0.079 sec/batch)
2017-06-02 09:21:00.287111: step 276520, loss = 0.23 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:21:01.174813: step 276530, loss = 0.22 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 09:21:02.035928: step 276540, loss = 0.23 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:21:02.901681: step 276550, loss = 0.19 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:21:03.763041: step 276560, loss = 0.23 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:21:04.623100: step 276570, loss = 0.25 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:21:05.464998: step 276580, loss = 0.21 (1520.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:21:06.308783: step 276590, loss = 0.23 (1517.0 examples/sec; 0.084 sec/batch)
2017-06-02 09:21:07.280501: step 276600, loss = 0.26 (1317.3 examples/sec; 0.097 sec/batch)
2017-06-02 09:21:08.070651: step 276610, loss = 0.24 (1619.9 examples/sec; 0.079 sec/batch)
2017-06-02 09:21:08.942749: step 276620, loss = 0.23 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:21:09.814296: step 276630, loss = 0.22 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:21:10.651036: step 276640, loss = 0.28 (1529.7 examples/sec; 0.084 sec/batch)
2017-06-02 09:21:11.527618: step 276650, loss = 0.23 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:21:12.364899: step 276660, loss = 0.34 (1528.8 examples/sec; 0.084 sec/batch)
2017-06-02 09:21:13.225865: step 276670, loss = 0.18 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:21:14.073813: step 276680, loss = 0.23 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:21:14.910489: step 276690, loss = 0.25 (1529.9 examples/sec; 0.084 sec/batch)
2017-06-02 09:21:15.858548: step 276700, loss = 0.29 (1350.1 examples/sec; 0.095 sec/batch)
2017-06-02 09:21:16.620515: step 276710, loss = 0.20 (1679.9 examples/sec; 0.076 sec/batch)
2017-06-02 09:21:17.473697: step 276720, loss = 0.22 (1500.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:21:18.354873: step 276730, loss = 0.23 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:21:19.217057: step 276740, loss = 0.30 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:21:20.080724: step 276750, loss = 0.23 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:21:20.963521: step 276760, loss = 0.21 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:21:21.821100: step 276770, loss = 0.26 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:21:22.674350: step 276780, loss = 0.25 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:21:23.554118: step 276790, loss = 0.28 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:21:24.547273: step 276800, loss = 0.23 (1288.8 examples/sec; 0.099 sec/batch)
2017-06-02 09:21:25.297620: step 276810, loss = 0.18 (1705.9 examples/sec; 0.075 sec/batch)
2017-06-02 09:21:26.157596: step 276820, loss = 0.29 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:21:27.037981: step 276830, loss = 0.26 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:21:27.921064: step 276840, loss = 0.25 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:21:28.814683: step 276850, loss = 0.22 (1432.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:21:29.674385: step 276860, loss = 0.27 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:21:30.538358: step 276870, loss = 0.23 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:21:31.401195: step 276880, loss = 0.25 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:21:32.267284: step 276890, loss = 0.26 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:21:33.272791: step 276900, loss = 0.20 (1273.0 examples/sec; 0.101 sec/batch)
2017-06-02 09:21:34.056635: step 276910, loss = 0.18 (1633.0 examples/sec; 0.078 sec/batch)
2017-06-02 09:21:34.929932: step 276920, loss = 0.26 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:21:35.790855: step 276930, loss = 0.23 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:21:36.667051: step 276940, loss = 0.24 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:21:37.553785: step 276950, loss = 0.22 (1443.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:21:38.399546: step 276960, loss = 0.24 (1513.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:21:39.288439: step 276970, loss = 0.20 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:21:40.190094: step 276980, loss = 0.28 (1419.6 examples/sec; 0.090 sec/batch)
2017-06-02 09:21:41.061442: step 276990, loss = 0.33 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:21:42.035776: step 277000, loss = 0.21 (1313.7 examples/sec; 0.097 sec/batch)
2017-06-02 09:21:42.812032: step 277010, loss = 0.27 (1648.9 examples/sec; 0.078 sec/batch)
2017-06-02 09:21:43.695513: step 277020, loss = 0.22 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:21:44.584275: step 277030, loss = 0.29 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:21:45.442671: step 277040, loss = 0.34 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:21:46.327534: step 277050, loss = 0.37 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:21:47.215142: step 277060, loss = 0.22 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:21:48.100860: step 277070, loss = 0.20 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:21:48.983494: step 277080, loss = 0.25 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:21:49.857637: step 277090, loss = 0.25 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:21:50.840209: step 277100, loss = 0.20 (1302.7 examples/sec; 0.098 sec/batch)
2017-06-02 09:21:51.599236: step 277110, loss = 0.26 (1686.4 examples/sec; 0.076 sec/batch)
2017-06-02 09:21:52.491593: step 277120, loss = 0.24 (1434.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:21:53.368897: step 277130, loss = 0.19 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:21:54.189369: step 277140, loss = 0.29 (1560.1 examples/sec; 0.082 sec/batch)
2017-06-02 09:21:55.038328: step 277150, loss = 0.28 (1507.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:21:55.917714: step 277160, loss = 0.23 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:21:56.804140: step 277170, loss = 0.30 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:21:57.661031: step 277180, loss = 0.25 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:21:58.534765: step 277190, loss = 0.25 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:21:59.506040: step 277200, loss = 0.19 (1317.9 examples/sec; 0.097 sec/batch)
2017-06-02 09:22:00.284144: step 277210, loss = 0.24 (1645.0 examples/sec; 0.078 sec/batch)
2017-06-02 09:22:01.151879: step 277220, loss = 0.26 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:22:02.044942: step 277230, loss = 0.19 (1433.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:22:02.890961: step 277240, loss = 0.25 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:22:03.776350: step 277250, loss = 0.25 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 09:22:04.652575: step 277260, loss = 0.27 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:22:05.541665: step 277270, loss = 0.26 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 09:22:06.429165: step 277280, loss = 0.26 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:22:07.334150: step 277290, loss = 0.22 (1414.4 examples/sec; 0.090 sec/batch)
2017-06-02 09:22:08.323492: step 277300, loss = 0.23 (1293.8 examples/sec; 0.099 sec/batch)
2017-06-02 09:22:09.096024: step 277310, loss = 0.21 (1656.9 examples/sec; 0.077 sec/batch)
2017-06-02 09:22:09.952885: step 277320, loss = 0.24 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:22:10.814343: step 277330, loss = 0.19 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:22:11.663230: step 277340, loss = 0.22 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:22:12.551785: step 277350, loss = 0.26 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:22:13.431509: step 277360, loss = 0.18 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:22:14.295436: step 277370, loss = 0.21 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:22:15.156943: step 277380, loss = 0.19 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:22:16.051256: step 277390, loss = 0.20 (1431.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:22:17.036670: step 277400, loss = 0.32 (1298.9 examples/sec; 0.099 sec/batch)
2017-06-02 09:22:17.790377: step 277410, loss = 0.22 (1698.3 examples/sec; 0.075 sec/batch)
2017-06-02 09:22:18.676645: step 277420, loss = 0.22 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:22:19.562460: step 277430, loss = 0.22 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:22:20.439063: step 277440, loss = 0.21 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:22:21.318036: step 277450, loss = 0.22 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:22:22.194758: step 277460, loss = 0.20 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:22:23.054470: step 277470, loss = 0.29 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:22:23.916980: step 277480, loss = 0.24 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:22:24.789532: step 277490, loss = 0.30 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:22:25.778281: step 277500, loss = 0.25 (1294.6 examples/sec; 0.099 sec/batch)
2017-06-02 09:22:26.454871: step 277510, loss = 0.21 (1891.8 examples/sec; 0.068 sec/batch)
2017-06-02 09:22:27.313747: step 277520, loss = 0.20 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:22:28.179574: step 277530, loss = 0.25 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:22:29.023442: step 277540, loss = 0.28 (1516.8 examples/sec; 0.084 sec/batch)
2017-06-02 09:22:29.866978: step 277550, loss = 0.22 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:22:30.748064: step 277560, loss = 0.24 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:22:31.634655: step 277570, loss = 0.28 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 09:22:32.510183: step 277580, loss = 0.26 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:22:33.391566: step 277590, loss = 0.23 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:22:34.440622: step 277600, loss = 0.25 (1220.1 examples/sec; 0.105 sec/batch)
2017-06-02 09:22:35.162908: step 277610, loss = 0.20 (1772.1 examples/sec; 0.072 sec/batch)
2017-06-02 09:22:36.060929: step 277620, loss = 0.26 (1425.4 examples/sec; 0.090 sec/batch)
2017-06-02 09:22:36.923803: step 277630, loss = 0.24 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:22:37.757518: step 277640, loss = 0.23 (1535.3 examples/sec; 0.083 sec/batch)
2017-06-02 09:22:38.625541: step 277650, loss = 0.23 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:22:39.486176: step 277660, loss = 0.26 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:22:40.346351: step 277670, loss = 0.20 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:22:41.212340: step 277680, loss = 0.29 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:22:42.099189: step 277690, loss = 0.23 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:22:43.091274: step 277700, loss = 0.27 (1290.2 examples/sec; 0.099 sec/batch)
2017-06-02 09:22:43.861118: step 277710, loss = 0.27 (1662.7 examples/sec; 0.077 sec/batch)
2017-06-02 09:22:44.695931: step 277720, loss = 0.27 (1533.3 examples/sec; 0.083 sec/batch)
2017-06-02 09:22:45.573671: step 277730, loss = 0.24 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:22:46.418410: step 277740, loss = 0.19 (1515.3 examples/sec; 0.084 sec/batch)
2017-06-02 09:22:47.271854: step 277750, loss = 0.25 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:22:48.125240: step 277760, loss = 0.24 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:22:49.007586: step 277770, loss = 0.25 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:22:49.893921: step 277780, loss = 0.23 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:22:50.780024: step 277790, loss = 0.36 (1444.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:22:51.744259: step 277800, loss = 0.31 (1327.5 examples/sec; 0.096 sec/batch)
2017-06-02 09:22:52.510262: step 277810, loss = 0.21 (1671.0 examples/sec; 0.077 sec/batch)
2017-06-02 09:22:53.354897: step 277820, loss = 0.24 (1515.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:22:54.239134: step 277830, loss = 0.17 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:22:55.093973: step 277840, loss = 0.22 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:22:55.966060: step 277850, loss = 0.21 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:22:56.843597: step 277860, loss = 0.25 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:22:57.700016: step 277870, loss = 0.16 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:22:58.560677: step 277880, loss = 0.26 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:22:59.436645: step 277890, loss = 0.29 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:23:00.492000: step 277900, loss = 0.20 (1212.9 examples/sec; 0.106 sec/batch)
2017-06-02 09:23:01.185879: step 277910, loss = 0.22 (1844.7 examples/sec; 0.069 sec/batch)
2017-06-02 09:23:02.062366: step 277920, loss = 0.27 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:23:02.918866: step 277930, loss = 0.21 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:23:03.791875: step 277940, loss = 0.25 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:23:04.667804: step 277950, loss = 0.21 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:23:05.544261: step 277960, loss = 0.27 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:23:06.396088: step 277970, loss = 0.19 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:23:07.268845: step 277980, loss = 0.27 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:23:08.134934: step 277990, loss = 0.31 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:23:09.113217: step 278000, loss = 0.21 (1308.4 examples/sec; 0.098 sec/batch)
2017-06-02 09:23:09.885774: step 278010, loss = 0.23 (1656.9 examples/sec; 0.077 sec/batch)
2017-06-02 09:23:10.750814: step 278020, loss = 0.18 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:23:11.613818: step 278030, loss = 0.25 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:23:12.444360: step 278040, loss = 0.18 (1541.2 examples/sec; 0.083 sec/batch)
2017-06-02 09:23:13.280196: step 278050, loss = 0.28 (1531.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:23:14.089863: step 278060, loss = 0.22 (1580.9 examples/sec; 0.081 sec/batch)
2017-06-02 09:23:14.910582: step 278070, loss = 0.20 (1559.6 examples/sec; 0.082 sec/batch)
2017-06-02 09:23:15.790652: step 278080, loss = 0.24 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:23:16.669693: step 278090, loss = 0.25 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:23:17.639983: step 278100, loss = 0.19 (1319.2 examples/sec; 0.097 sec/batch)
2017-06-02 09:23:18.414706: step 278110, loss = 0.31 (1652.2 examples/sec; 0.077 sec/batch)
2017-06-02 09:23:19.278059: step 278120, loss = 0.26 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:23:20.182970: step 278130, loss = 0.24 (1414.4 examples/sec; 0.090 sec/batch)
2017-06-02 09:23:21.061900: step 278140, loss = 0.22 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:23:21.934185: step 278150, loss = 0.23 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:23:22.766646: step 278160, loss = 0.21 (1537.6 examples/sec; 0.083 sec/batch)
2017-06-02 09:23:23.614884: step 278170, loss = 0.19 (1509.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:23:24.484613: step 278180, loss = 0.23 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:23:25.359902: step 278190, loss = 0.22 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:23:26.328804: step 278200, loss = 0.25 (1321.1 examples/sec; 0.097 sec/batch)
2017-06-02 09:23:27.098014: step 278210, loss = 0.20 (1664.0 examples/sec; 0.077 sec/batch)
2017-06-02 09:23:27.961606: step 278220, loss = 0.18 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:23:28.844126: step 278230, loss = 0.22 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:23:29.717477: step 278240, loss = 0.24 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:23:30.574069: step 278250, loss = 0.22 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:23:31.449244: step 278260, loss = 0.17 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:23:32.318874: step 278270, loss = 0.28 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:23:33.212906: step 278280, loss = 0.19 (1431.7 examples/sec; 0.089 sec/batch)
2017-06-02 09:23:34.084075: step 278290, loss = 0.28 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:23:35.060467: step 278300, loss = 0.26 (1310.9 examples/sec; 0.098 sec/batch)
2017-06-02 09:23:35.852935: step 278310, loss = 0.25 (1615.2 examples/sec; 0.079 sec/batch)
2017-06-02 09:23:36.735519: step 278320, loss = 0.19 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:23:37.583833: step 278330, loss = 0.23 (1508.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:23:38.440824: step 278340, loss = 0.25 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:23:39.306000: step 278350, loss = 0.25 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:23:40.202780: step 278360, loss = 0.18 (1427.3 examples/sec; 0.090 sec/batch)
2017-06-02 09:23:41.070636: step 278370, loss = 0.23 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:23:41.952796: step 278380, loss = 0.20 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:23:42.803585: step 278390, loss = 0.25 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:23:43.773577: step 278400, loss = 0.25 (1319.6 examples/sec; 0.097 sec/batch)
2017-06-02 09:23:44.560152: step 278410, loss = 0.21 (1627.3 examples/sec; 0.079 sec/batch)
2017-06-02 09:23:45.450352: step 278420, loss = 0.20 (1437.9 examples/sec; 0.089 sec/batch)
2017-06-02 09:23:46.336173: step 278430, loss = 0.23 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:23:47.228258: step 278440, loss = 0.26 (1434.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:23:48.103466: step 278450, loss = 0.23 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:23:48.987392: step 278460, loss = 0.25 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:23:49.826657: step 278470, loss = 0.19 (1525.2 examples/sec; 0.084 sec/batch)
2017-06-02 09:23:50.714345: step 278480, loss = 0.22 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 09:23:51.574368: step 278490, loss = 0.24 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:23:52.548757: step 278500, loss = 0.21 (1313.6 examples/sec; 0.097 sec/batch)
2017-06-02 09:23:53.310565: step 278510, loss = 0.19 (1680.2 examples/sec; 0.076 sec/batch)
2017-06-02 09:23:54.166017: step 278520, loss = 0.22 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:23:55.027566: step 278530, loss = 0.19 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:23:55.933484: step 278540, loss = 0.26 (1412.9 examples/sec; 0.091 sec/batch)
2017-06-02 09:23:56.799033: step 278550, loss = 0.23 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:23:57.675708: step 278560, loss = 0.22 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:23:58.564205: step 278570, loss = 0.20 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:23:59.455938: step 278580, loss = 0.21 (1435.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:24:00.322116: step 278590, loss = 0.18 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:24:01.294072: step 278600, loss = 0.28 (1316.9 examples/sec; 0.097 sec/batch)
2017-06-02 09:24:02.058559: step 278610, loss = 0.22 (1674.3 examples/sec; 0.076 sec/batch)
2017-06-02 09:24:02.926252: step 278620, loss = 0.26 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:24:03.809132: step 278630, loss = 0.23 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:24:04.686197: step 278640, loss = 0.26 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:24:05.548597: step 278650, loss = 0.23 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:24:06.422301: step 278660, loss = 0.28 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:24:07.254534: step 278670, loss = 0.25 (1538.0 examples/sec; 0.083 sec/batch)
2017-06-02 09:24:08.104489: step 278680, loss = 0.26 (1506.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:24:08.982285: step 278690, loss = 0.20 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:24:09.982916: step 278700, loss = 0.21 (1279.2 examples/sec; 0.100 sec/batch)
2017-06-02 09:24:10.759390: step 278710, loss = 0.20 (1648.5 examples/sec; 0.078 sec/batch)
2017-06-02 09:24:11.607695: step 278720, loss = 0.28 (1508.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:24:12.468523: step 278730, loss = 0.26 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:24:13.326708: step 278740, loss = 0.25 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:24:14.179502: step 278750, loss = 0.19 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:24:15.041190: step 278760, loss = 0.25 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:24:15.915082: step 278770, loss = 0.20 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:24:16.760388: step 278780, loss = 0.20 (1514.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:24:17.651082: step 278790, loss = 0.25 (1437.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:24:18.599080: step 278800, loss = 0.20 (1350.2 examples/sec; 0.095 sec/batch)
2017-06-02 09:24:19.362540: step 278810, loss = 0.20 (1676.6 examples/sec; 0.076 sec/batch)
2017-06-02 09:24:20.230503: step 278820, loss = 0.21 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:24:21.095096: step 278830, loss = 0.33 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:24:21.954011: step 278840, loss = 0.24 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:24:22.810550: step 278850, loss = 0.24 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:24:23.660281: step 278860, loss = 0.25 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:24:24.549831: step 278870, loss = 0.23 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 09:24:25.433621: step 278880, loss = 0.25 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:24:26.313764: step 278890, loss = 0.22 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:24:27.354290: step 278900, loss = 0.27 (1230.1 examples/sec; 0.104 sec/batch)
2017-06-02 09:24:28.083657: step 278910, loss = 0.31 (1755.0 examples/sec; 0.073 sec/batch)
2017-06-02 09:24:28.958973: step 278920, loss = 0.25 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:24:29.831037: step 278930, loss = 0.24 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:24:30.689235: step 278940, loss = 0.20 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:24:31.563312: step 278950, loss = 0.30 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:24:32.426764: step 278960, loss = 0.20 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:24:33.289832: step 278970, loss = 0.20 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:24:34.153326: step 278980, loss = 0.19 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:24:35.011538: step 278990, loss = 0.27 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:24:36.004019: step 279000, loss = 0.23 (1289.7 examples/sec; 0.099 sec/batch)
2017-06-02 09:24:36.764436: step 279010, loss = 0.27 (1683.3 examples/sec; 0.076 sec/batch)
2017-06-02 09:24:37.628523: step 279020, loss = 0.24 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:24:38.518999: step 279030, loss = 0.18 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:24:39.376659: step 279040, loss = 0.18 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:24:40.239518: step 279050, loss = 0.19 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:24:41.104636: step 279060, loss = 0.21 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:24:41.970246: step 279070, loss = 0.25 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:24:42.841856: step 279080, loss = 0.21 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:24:43.685781: step 279090, loss = 0.22 (1516.7 examples/sec; 0.084 sec/batch)
2017-06-02 09:24:44.668400: step 279100, loss = 0.21 (1302.6 examples/sec; 0.098 sec/batch)
2017-06-02 09:24:45.401566: step 279110, loss = 0.23 (1745.9 examples/sec; 0.073 sec/batch)
2017-06-02 09:24:46.264812: step 279120, loss = 0.23 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:24:47.144724: step 279130, loss = 0.28 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:24:47.996355: step 279140, loss = 0.22 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:24:48.877668: step 279150, loss = 0.26 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:24:49.746301: step 279160, loss = 0.23 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:24:50.623137: step 279170, loss = 0.29 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:24:51.495014: step 279180, loss = 0.22 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:24:52.377413: step 279190, loss = 0.16 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:24:53.372788: step 279200, loss = 0.19 (1285.9 examples/sec; 0.100 sec/batch)
2017-06-02 09:24:54.159994: step 279210, loss = 0.22 (1626.0 examples/sec; 0.079 sec/batch)
2017-06-02 09:24:55.009494: step 279220, loss = 0.22 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:24:55.889678: step 279230, loss = 0.21 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:24:56.747223: step 279240, loss = 0.18 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:24:57.610520: step 279250, loss = 0.20 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:24:58.466973: step 279260, loss = 0.31 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:24:59.344964: step 279270, loss = 0.25 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:25:00.218578: step 279280, loss = 0.21 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:25:01.082463: step 279290, loss = 0.24 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:25:02.052022: step 279300, loss = 0.24 (1320.2 examples/sec; 0.097 sec/batch)
2017-06-02 09:25:02.818520: step 279310, loss = 0.20 (1669.9 examples/sec; 0.077 sec/batch)
2017-06-02 09:25:03.684373: step 279320, loss = 0.24 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:25:04.522745: step 279330, loss = 0.24 (1526.8 examples/sec; 0.084 sec/batch)
2017-06-02 09:25:05.395830: step 279340, loss = 0.28 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:25:06.267095: step 279350, loss = 0.22 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:25:07.130421: step 279360, loss = 0.27 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:25:08.020633: step 279370, loss = 0.22 (1437.9 examples/sec; 0.089 sec/batch)
2017-06-02 09:25:08.883952: step 279380, loss = 0.19 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:25:09.747174: step 279390, loss = 0.25 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:25:10.724789: step 279400, loss = 0.28 (1309.3 examples/sec; 0.098 sec/batch)
2017-06-02 09:25:11.494395: step 279410, loss = 0.18 (1663.2 examples/sec; 0.077 sec/batch)
2017-06-02 09:25:12.368956: step 279420, loss = 0.23 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:25:13.222780: step 279430, loss = 0.22 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:25:14.068900: step 279440, loss = 0.24 (1512.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:25:14.922412: step 279450, loss = 0.27 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:25:15.791241: step 279460, loss = 0.24 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:25:16.671674: step 279470, loss = 0.24 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:25:17.517779: step 279480, loss = 0.25 (1512.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:25:18.354897: step 279490, loss = 0.25 (1529.1 examples/sec; 0.084 sec/batch)
2017-06-02 09:25:19.339847: step 279500, loss = 0.24 (1299.5 examples/sec; 0.098 sec/batch)
2017-06-02 09:25:20.118665: step 279510, loss = 0.18 (1643.5 examples/sec; 0.078 sec/batch)
2017-06-02 09:25:20.996540: step 279520, loss = 0.29 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:25:21.862906: step 279530, loss = 0.33 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:25:22.734409: step 279540, loss = 0.19 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:25:23.629948: step 279550, loss = 0.22 (1429.3 examples/sec; 0.090 sec/batch)
2017-06-02 09:25:24.492671: step 279560, loss = 0.24 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:25:25.373786: step 279570, loss = 0.21 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:25:26.236506: step 279580, loss = 0.26 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:25:27.098870: step 279590, loss = 0.21 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:25:28.066119: step 279600, loss = 0.26 (1323.3 examples/sec; 0.097 sec/batch)
2017-06-02 09:25:28.832614: step 279610, loss = 0.29 (1669.9 examples/sec; 0.077 sec/batch)
2017-06-02 09:25:29.668323: step 279620, loss = 0.21 (1531.7 examples/sec; 0.084 sec/batch)
2017-06-02 09:25:30.517454: step 279630, loss = 0.19 (1507.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:25:31.399325: step 279640, loss = 0.25 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:25:32.266569: step 279650, loss = 0.25 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:25:33.128862: step 279660, loss = 0.22 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:25:34.000267: step 279670, loss = 0.24 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:25:34.870914: step 279680, loss = 0.21 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:25:35.716948: step 279690, loss = 0.22 (1512.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:25:36.684385: step 279700, loss = 0.22 (1323.1 examples/sec; 0.097 sec/batch)
2017-06-02 09:25:37.467415: step 279710, loss = 0.21 (1634.7 examples/sec; 0.078 sec/batch)
2017-06-02 09:25:38.319255: step 279720, loss = 0.18 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:25:39.166861: step 279730, loss = 0.22 (1510.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:25:40.006378: step 279740, loss = 0.20 (1524.7 examples/sec; 0.084 sec/batch)
2017-06-02 09:25:40.860697: step 279750, loss = 0.20 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:25:41.704796: step 279760, loss = 0.29 (1516.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:25:42.563478: step 279770, loss = 0.24 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:25:43.464572: step 279780, loss = 0.20 (1420.5 examples/sec; 0.090 sec/batch)
2017-06-02 09:25:44.325271: step 279790, loss = 0.19 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:25:45.337806: step 279800, loss = 0.18 (1264.2 examples/sec; 0.101 sec/batch)
2017-06-02 09:25:46.114434: step 279810, loss = 0.24 (1648.2 examples/sec; 0.078 sec/batch)
2017-06-02 09:25:46.981063: step 279820, loss = 0.25 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:25:47.851591: step 279830, loss = 0.29 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:25:48.716681: step 279840, loss = 0.23 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:25:49.571476: step 279850, loss = 0.23 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:25:50.441445: step 279860, loss = 0.26 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:25:51.319526: step 279870, loss = 0.24 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:25:52.196724: step 279880, loss = 0.23 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:25:53.044829: step 279890, loss = 0.21 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:25:54.009030: step 279900, loss = 0.28 (1327.5 examples/sec; 0.096 sec/batch)
2017-06-02 09:25:54.786753: step 279910, loss = 0.29 (1645.9 examples/sec; 0.078 sec/batch)
2017-06-02 09:25:55.669649: step 279920, loss = 0.27 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:25:56.543516: step 279930, loss = 0.20 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:25:57.387337: step 279940, loss = 0.21 (1516.9 examples/sec; 0.084 sec/batch)
2017-06-02 09:25:58.239009: step 279950, loss = 0.18 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:25:59.099492: step 279960, loss = 0.22 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:25:59.947134: step 279970, loss = 0.22 (1510.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:26:00.825477: step 279980, loss = 0.20 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:26:01.702109: step 279990, loss = 0.24 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:26:02.684186: step 280000, loss = 0.26 (1303.3 examples/sec; 0.098 sec/batch)
2017-06-02 09:26:03.458596: step 280010, loss = 0.21 (1652.9 examples/sec; 0.077 sec/batch)
2017-06-02 09:26:04.338630: step 280020, loss = 0.24 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:26:05.227215: step 280030, loss = 0.25 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:26:06.119123: step 280040, loss = 0.23 (1435.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:26:07.008236: step 280050, loss = 0.22 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:26:07.889800: step 280060, loss = 0.25 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:26:08.748256: step 280070, loss = 0.23 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:26:09.628932: step 280080, loss = 0.31 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:26:10.484704: step 280090, loss = 0.19 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:26:11.472492: step 280100, loss = 0.25 (1295.8 examples/sec; 0.099 sec/batch)
2017-06-02 09:26:12.226994: step 280110, loss = 0.28 (1696.5 examples/sec; 0.075 sec/batch)
2017-06-02 09:26:13.118091: step 280120, loss = 0.21 (1436.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:26:13.985930: step 280130, loss = 0.18 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:26:14.869913: step 280140, loss = 0.21 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:26:15.748923: step 280150, loss = 0.22 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:26:16.642003: step 280160, loss = 0.19 (1433.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:26:17.531053: step 280170, loss = 0.21 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 09:26:18.405351: step 280180, loss = 0.20 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:26:19.292831: step 280190, loss = 0.26 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:26:20.255169: step 280200, loss = 0.20 (1330.1 examples/sec; 0.096 sec/batch)
2017-06-02 09:26:21.045466: step 280210, loss = 0.23 (1619.6 examples/sec; 0.079 sec/batch)
2017-06-02 09:26:21.941750: step 280220, loss = 0.21 (1428.1 examples/sec; 0.090 sec/batch)
2017-06-02 09:26:22.820319: step 280230, loss = 0.24 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:26:23.667231: step 280240, loss = 0.18 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:26:24.565551: step 280250, loss = 0.22 (1424.9 examples/sec; 0.090 sec/batch)
2017-06-02 09:26:25.472192: step 280260, loss = 0.21 (1411.8 examples/sec; 0.091 sec/batch)
2017-06-02 09:26:26.361596: step 280270, loss = 0.22 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:26:27.237760: step 280280, loss = 0.17 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:26:28.124957: step 280290, loss = 0.23 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 09:26:29.097221: step 280300, loss = 0.24 (1316.5 examples/sec; 0.097 sec/batch)
2017-06-02 09:26:29.889807: step 280310, loss = 0.21 (1615.0 examples/sec; 0.079 sec/batch)
2017-06-02 09:26:30.779448: step 280320, loss = 0.29 (1438.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:26:31.663630: step 280330, loss = 0.16 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:26:32.564651: step 280340, loss = 0.21 (1420.6 examples/sec; 0.090 sec/batch)
2017-06-02 09:26:33.475941: step 280350, loss = 0.20 (1404.6 examples/sec; 0.091 sec/batch)
2017-06-02 09:26:34.352225: step 280360, loss = 0.30 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:26:35.215787: step 280370, loss = 0.22 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:26:36.079792: step 280380, loss = 0.21 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:26:36.966319: step 280390, loss = 0.24 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:26:37.968246: step 280400, loss = 0.20 (1277.5 examples/sec; 0.100 sec/batch)
2017-06-02 09:26:38.671598: step 280410, loss = 0.26 (1819.9 examples/sec; 0.070 sec/batch)
2017-06-02 09:26:39.521658: step 280420, loss = 0.22 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:26:40.391250: step 280430, loss = 0.24 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:26:41.244116: step 280440, loss = 0.20 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:26:42.116592: step 280450, loss = 0.25 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:26:42.979283: step 280460, loss = 0.21 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:26:43.856042: step 280470, loss = 0.22 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:26:44.749063: step 280480, loss = 0.22 (1433.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:26:45.634162: step 280490, loss = 0.17 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:26:46.596725: step 280500, loss = 0.25 (1329.8 examples/sec; 0.096 sec/batch)
2017-06-02 09:26:47.364261: step 280510, loss = 0.27 (1667.7 examples/sec; 0.077 sec/batch)
2017-06-02 09:26:48.267773: step 280520, loss = 0.23 (1416.7 examples/sec; 0.090 sec/batch)
2017-06-02 09:26:49.158551: step 280530, loss = 0.18 (1437.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:26:50.041772: step 280540, loss = 0.24 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:26:50.940853: step 280550, loss = 0.20 (1423.7 examples/sec; 0.090 sec/batch)
2017-06-02 09:26:51.827032: step 280560, loss = 0.22 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:26:52.682257: step 280570, loss = 0.26 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:26:53.561435: step 280580, loss = 0.26 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:26:54.456167: step 280590, loss = 0.20 (1430.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:26:55.435121: step 280600, loss = 0.31 (1307.5 examples/sec; 0.098 sec/batch)
2017-06-02 09:26:56.210751: step 280610, loss = 0.25 (1650.3 examples/sec; 0.078 sec/batch)
2017-06-02 09:26:57.086125: step 280620, loss = 0.23 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:26:57.951897: step 280630, loss = 0.17 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:26:58.846902: step 280640, loss = 0.31 (1430.1 examples/sec; 0.090 sec/batch)
2017-06-02 09:26:59.742553: step 280650, loss = 0.23 (1429.1 examples/sec; 0.090 sec/batch)
2017-06-02 09:27:00.624184: step 280660, loss = 0.27 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:27:01.484870: step 280670, loss = 0.18 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:27:02.362629: step 280680, loss = 0.19 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:27:03.238452: step 280690, loss = 0.22 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:27:04.244782: step 280700, loss = 0.27 (1271.9 examples/sec; 0.101 sec/batch)
2017-06-02 09:27:04.964120: step 280710, loss = 0.20 (1779.4 examples/sec; 0.072 sec/batch)
2017-06-02 09:27:05.832194: step 280720, loss = 0.25 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:27:06.703319: step 280730, loss = 0.19 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:27:07.571991: step 280740, loss = 0.26 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:27:08.438648: step 280750, loss = 0.20 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:27:09.301934: step 280760, loss = 0.26 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:27:10.175319: step 280770, loss = 0.24 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:27:11.075262: step 280780, loss = 0.23 (1422.3 examples/sec; 0.090 sec/batch)
2017-06-02 09:27:11.954515: step 280790, loss = 0.20 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:27:12.993280: step 280800, loss = 0.25 (1232.2 examples/sec; 0.104 sec/batch)
2017-06-02 09:27:13.710339: step 280810, loss = 0.20 (1785.1 examples/sec; 0.072 sec/batch)
2017-06-02 09:27:14.551661: step 280820, loss = 0.28 (1521.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:27:15.432169: step 280830, loss = 0.20 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:27:16.285806: step 280840, loss = 0.26 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:27:17.126574: step 280850, loss = 0.19 (1522.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:27:17.965959: step 280860, loss = 0.26 (1524.9 examples/sec; 0.084 sec/batch)
2017-06-02 09:27:18.837363: step 280870, loss = 0.27 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:27:19.712694: step 280880, loss = 0.25 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:27:20.576295: step 280890, loss = 0.30 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:27:21.598607: step 280900, loss = 0.24 (1252.0 examples/sec; 0.102 sec/batch)
2017-06-02 09:27:22.343472: step 280910, loss = 0.20 (1718.4 examples/sec; 0.074 sec/batch)
2017-06-02 09:27:23.219588: step 280920, loss = 0.34 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:27:24.071477: step 280930, loss = 0.20 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:27:24.961600: step 280940, loss = 0.19 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:27:25.806277: step 280950, loss = 0.16 (1515.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:27:26.660236: step 280960, loss = 0.21 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:27:27.551204: step 280970, loss = 0.23 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:27:28.412862: step 280980, loss = 0.21 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:27:29.294367: step 280990, loss = 0.22 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:27:30.248149: step 281000, loss = 0.21 (1342.0 examples/sec; 0.095 sec/batch)
2017-06-02 09:27:31.026486: step 281010, loss = 0.20 (1644.5 examples/sec; 0.078 sec/batch)
2017-06-02 09:27:31.910400: step 281020, loss = 0.19 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:27:32.773115: step 281030, loss = 0.27 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:27:33.644011: step 281040, loss = 0.22 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:27:34.529840: step 281050, loss = 0.21 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:27:35.397449: step 281060, loss = 0.24 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:27:36.269399: step 281070, loss = 0.30 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:27:37.146629: step 281080, loss = 0.23 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:27:38.015242: step 281090, loss = 0.21 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:27:38.993719: step 281100, loss = 0.15 (1308.1 examples/sec; 0.098 sec/batch)
2017-06-02 09:27:39.766235: step 281110, loss = 0.28 (1656.9 examples/sec; 0.077 sec/batch)
2017-06-02 09:27:40.636511: step 281120, loss = 0.25 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:27:41.523283: step 281130, loss = 0.25 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:27:42.388664: step 281140, loss = 0.18 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:27:43.246704: step 281150, loss = 0.19 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:27:44.098824: step 281160, loss = 0.18 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:27:44.985895: step 281170, loss = 0.20 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 09:27:45.859873: step 281180, loss = 0.34 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:27:46.718281: step 281190, loss = 0.22 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:27:47.680914: step 281200, loss = 0.22 (1329.7 examples/sec; 0.096 sec/batch)
2017-06-02 09:27:48.455489: step 281210, loss = 0.25 (1652.5 examples/sec; 0.077 sec/batch)
2017-06-02 09:27:49.314363: step 281220, loss = 0.21 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:27:50.177394: step 281230, loss = 0.17 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:27:51.038978: step 281240, loss = 0.32 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:27:51.920495: step 281250, loss = 0.21 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:27:52.782159: step 281260, loss = 0.21 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:27:53.655942: step 281270, loss = 0.19 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:27:54.513718: step 281280, loss = 0.27 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:27:55.413726: step 281290, loss = 0.26 (1422.2 examples/sec; 0.090 sec/batch)
2017-06-02 09:27:56.417615: step 281300, loss = 0.19 (1275.0 examples/sec; 0.100 sec/batch)
2017-06-02 09:27:57.182545: step 281310, loss = 0.21 (1673.4 examples/sec; 0.076 sec/batch)
2017-06-02 09:27:58.035765: step 281320, loss = 0.26 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:27:58.918211: step 281330, loss = 0.18 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:27:59.806669: step 281340, loss = 0.17 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 09:28:00.669194: step 281350, loss = 0.18 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:28:01.531076: step 281360, loss = 0.17 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:28:02.434499: step 281370, loss = 0.23 (1416.8 examples/sec; 0.090 sec/batch)
2017-06-02 09:28:03.305199: step 281380, loss = 0.24 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:28:04.161318: step 281390, loss = 0.23 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:28:05.138779: step 281400, loss = 0.21 (1309.5 examples/sec; 0.098 sec/batch)
2017-06-02 09:28:05.895212: step 281410, loss = 0.24 (1692.2 examples/sec; 0.076 sec/batch)
2017-06-02 09:28:06.760157: step 281420, loss = 0.20 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:28:07.632702: step 281430, loss = 0.20 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:28:08.511554: step 281440, loss = 0.26 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:28:09.374964: step 281450, loss = 0.22 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:28:10.228715: step 281460, loss = 0.17 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:28:11.080842: step 281470, loss = 0.17 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:28:11.932107: step 281480, loss = 0.17 (1503.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:28:12.803450: step 281490, loss = 0.25 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:28:13.822726: step 281500, loss = 0.22 (1255.8 examples/sec; 0.102 sec/batch)
2017-06-02 09:28:14.565481: step 281510, loss = 0.22 (1723.4 examples/sec; 0.074 sec/batch)
2017-06-02 09:28:15.415648: step 281520, loss = 0.21 (1505.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:28:16.280960: step 281530, loss = 0.32 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:28:17.144031: step 281540, loss = 0.21 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:28:18.014033: step 281550, loss = 0.24 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:28:18.895110: step 281560, loss = 0.25 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:28:19.780534: step 281570, loss = 0.23 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:28:20.645049: step 281580, loss = 0.27 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:28:21.509980: step 281590, loss = 0.22 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:28:22.491270: step 281600, loss = 0.23 (1304.4 examples/sec; 0.098 sec/batch)
2017-06-02 09:28:23.256813: step 281610, loss = 0.27 (1672.0 examples/sec; 0.077 sec/batch)
2017-06-02 09:28:24.124548: step 281620, loss = 0.26 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:28:25.006416: step 281630, loss = 0.21 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:28:25.866063: step 281640, loss = 0.19 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:28:26.713162: step 281650, loss = 0.27 (1511.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:28:27.572596: step 281660, loss = 0.23 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:28:28.437373: step 281670, loss = 0.19 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:28:29.286105: step 281680, loss = 0.26 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:28:30.146771: step 281690, loss = 0.23 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:28:31.110107: step 281700, loss = 0.19 (1328.7 examples/sec; 0.096 sec/batch)
2017-06-02 09:28:31.879992: step 281710, loss = 0.21 (1662.6 examples/sec; 0.077 sec/batch)
2017-06-02 09:28:32.751010: step 281720, loss = 0.24 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:28:33.643489: step 281730, loss = 0.21 (1434.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:28:34.506768: step 281740, loss = 0.26 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:28:35.360405: step 281750, loss = 0.22 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:28:36.226694: step 281760, loss = 0.21 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:28:37.087022: step 281770, loss = 0.24 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:28:37.976099: step 281780, loss = 0.19 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:28:38.818320: step 281790, loss = 0.26 (1519.8 examples/sec; 0.084 sec/batch)
2017-06-02 09:28:39.806330: step 281800, loss = 0.17 (1295.5 examples/sec; 0.099 sec/batch)
2017-06-02 09:28:40.576643: step 281810, loss = 0.18 (1661.7 examples/sec; 0.077 sec/batch)
2017-06-02 09:28:41.434639: step 281820, loss = 0.20 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:28:42.327605: step 281830, loss = 0.23 (1433.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:28:43.182589: step 281840, loss = 0.20 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:28:44.042251: step 281850, loss = 0.19 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:28:44.927290: step 281860, loss = 0.24 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:28:45.792507: step 281870, loss = 0.21 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:28:46.671818: step 281880, loss = 0.23 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:28:47.538579: step 281890, loss = 0.25 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:28:48.485747: step 281900, loss = 0.32 (1351.4 examples/sec; 0.095 sec/batch)
2017-06-02 09:28:49.247053: step 281910, loss = 0.28 (1681.3 examples/sec; 0.076 sec/batch)
2017-06-02 09:28:50.091061: step 281920, loss = 0.21 (1516.6 examples/sec; 0.084 sec/batch)
2017-06-02 09:28:50.950674: step 281930, loss = 0.21 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:28:51.807190: step 281940, loss = 0.17 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:28:52.678419: step 281950, loss = 0.27 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:28:53.552123: step 281960, loss = 0.21 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:28:54.424316: step 281970, loss = 0.18 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:28:55.315593: step 281980, loss = 0.18 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:28:56.173188: step 281990, loss = 0.34 (1492.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:28:57.142461: step 282000, loss = 0.21 (1320.6 examples/sec; 0.097 sec/batch)
2017-06-02 09:28:57.921218: step 282010, loss = 0.23 (1643.6 examples/sec; 0.078 sec/batch)
2017-06-02 09:28:58.788872: step 282020, loss = 0.25 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:28:59.646132: step 282030, loss = 0.18 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:29:00.536773: step 282040, loss = 0.20 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:29:01.402663: step 282050, loss = 0.17 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:29:02.254679: step 282060, loss = 0.20 (1502.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:29:03.091328: step 282070, loss = 0.27 (1529.9 examples/sec; 0.084 sec/batch)
2017-06-02 09:29:03.981621: step 282080, loss = 0.22 (1437.7 examples/sec; 0.089 sec/batch)
2017-06-02 09:29:04.852691: step 282090, loss = 0.20 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:29:05.808856: step 282100, loss = 0.19 (1338.7 examples/sec; 0.096 sec/batch)
2017-06-02 09:29:06.593462: step 282110, loss = 0.22 (1631.3 examples/sec; 0.078 sec/batch)
2017-06-02 09:29:07.442777: step 282120, loss = 0.19 (1507.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:29:08.337823: step 282130, loss = 0.22 (1430.0 examples/sec; 0.090 sec/batch)
2017-06-02 09:29:09.202279: step 282140, loss = 0.19 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:29:10.086629: step 282150, loss = 0.20 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:29:10.963368: step 282160, loss = 0.24 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:29:11.858979: step 282170, loss = 0.30 (1429.2 examples/sec; 0.090 sec/batch)
2017-06-02 09:29:12.738007: step 282180, loss = 0.25 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:29:13.628511: step 282190, loss = 0.20 (1437.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:29:14.605917: step 282200, loss = 0.30 (1309.6 examples/sec; 0.098 sec/batch)
2017-06-02 09:29:15.391485: step 282210, loss = 0.23 (1629.4 examples/sec; 0.079 sec/batch)
2017-06-02 09:29:16.293683: step 282220, loss = 0.20 (1418.7 examples/sec; 0.090 sec/batch)
2017-06-02 09:29:17.164939: step 282230, loss = 0.24 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:29:18.029848: step 282240, loss = 0.17 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:29:18.911938: step 282250, loss = 0.18 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:29:19.804486: step 282260, loss = 0.24 (1434.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:29:20.680372: step 282270, loss = 0.19 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:29:21.571902: step 282280, loss = 0.24 (1435.7 examples/sec; 0.089 sec/batch)
2017-06-02 09:29:22.442553: step 282290, loss = 0.21 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:29:23.477004: step 282300, loss = 0.38 (1237.4 examples/sec; 0.103 sec/batch)
2017-06-02 09:29:24.206805: step 282310, loss = 0.18 (1753.9 examples/sec; 0.073 sec/batch)
2017-06-02 09:29:25.080565: step 282320, loss = 0.26 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:29:25.933523: step 282330, loss = 0.22 (1500.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:29:26.812065: step 282340, loss = 0.17 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:29:27.677640: step 282350, loss = 0.21 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:29:28.558299: step 282360, loss = 0.21 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:29:29.427074: step 282370, loss = 0.20 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:29:30.314242: step 282380, loss = 0.17 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:29:31.209088: step 282390, loss = 0.21 (1430.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:29:32.171077: step 282400, loss = 0.22 (1330.6 examples/sec; 0.096 sec/batch)
2017-06-02 09:29:32.954961: step 282410, loss = 0.21 (1632.9 examples/sec; 0.078 sec/batch)
2017-06-02 09:29:33.844220: step 282420, loss = 0.24 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:29:34.731957: step 282430, loss = 0.22 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 09:29:35.607535: step 282440, loss = 0.18 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:29:36.459339: step 282450, loss = 0.18 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:29:37.332789: step 282460, loss = 0.19 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:29:38.189753: step 282470, loss = 0.22 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:29:39.051844: step 282480, loss = 0.24 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:29:39.917218: step 282490, loss = 0.21 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:29:40.905684: step 282500, loss = 0.26 (1294.9 examples/sec; 0.099 sec/batch)
2017-06-02 09:29:41.687315: step 282510, loss = 0.20 (1637.6 examples/sec; 0.078 sec/batch)
2017-06-02 09:29:42.528800: step 282520, loss = 0.22 (1521.1 examples/sec; 0.084 sec/batch)
2017-06-02 09:29:43.401185: step 282530, loss = 0.20 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:29:44.276699: step 282540, loss = 0.30 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:29:45.123714: step 282550, loss = 0.21 (1511.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:29:46.008635: step 282560, loss = 0.26 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:29:46.891539: step 282570, loss = 0.20 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:29:47.754923: step 282580, loss = 0.19 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:29:48.609988: step 282590, loss = 0.20 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:29:49.586966: step 282600, loss = 0.26 (1310.2 examples/sec; 0.098 sec/batch)
2017-06-02 09:29:50.354046: step 282610, loss = 0.23 (1668.7 examples/sec; 0.077 sec/batch)
2017-06-02 09:29:51.207029: step 282620, loss = 0.32 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:29:52.071170: step 282630, loss = 0.19 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:29:53.006361: step 282640, loss = 0.28 (1368.7 examples/sec; 0.094 sec/batch)
2017-06-02 09:29:53.867244: step 282650, loss = 0.22 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:29:54.741518: step 282660, loss = 0.21 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:29:55.613192: step 282670, loss = 0.24 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:29:56.523856: step 282680, loss = 0.20 (1405.6 examples/sec; 0.091 sec/batch)
2017-06-02 09:29:57.415182: step 282690, loss = 0.21 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:29:58.384063: step 282700, loss = 0.16 (1321.1 examples/sec; 0.097 sec/batch)
2017-06-02 09:29:59.165070: step 282710, loss = 0.20 (1638.9 examples/sec; 0.078 sec/batch)
2017-06-02 09:30:00.057631: step 282720, loss = 0.26 (1434.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:30:00.928397: step 282730, loss = 0.18 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:30:01.789645: step 282740, loss = 0.22 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:30:02.692975: step 282750, loss = 0.27 (1417.0 examples/sec; 0.090 sec/batch)
2017-06-02 09:30:03.561724: step 282760, loss = 0.25 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:30:04.441489: step 282770, loss = 0.17 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:30:05.319336: step 282780, loss = 0.20 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:30:06.171908: step 282790, loss = 0.19 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:30:07.144652: step 282800, loss = 0.23 (1315.9 examples/sec; 0.097 sec/batch)
2017-06-02 09:30:07.910350: step 282810, loss = 0.25 (1671.7 examples/sec; 0.077 sec/batch)
2017-06-02 09:30:08.774101: step 282820, loss = 0.23 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:30:09.641388: step 282830, loss = 0.23 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:30:10.514753: step 282840, loss = 0.30 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:30:11.394358: step 282850, loss = 0.23 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:30:12.285065: step 282860, loss = 0.21 (1437.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:30:13.155849: step 282870, loss = 0.27 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:30:14.034390: step 282880, loss = 0.24 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:30:14.902702: step 282890, loss = 0.20 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:30:15.886915: step 282900, loss = 0.22 (1300.5 examples/sec; 0.098 sec/batch)
2017-06-02 09:30:16.665196: step 282910, loss = 0.18 (1644.7 examples/sec; 0.078 sec/batch)
2017-06-02 09:30:17.536898: step 282920, loss = 0.22 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:30:18.403119: step 282930, loss = 0.25 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:30:19.273446: step 282940, loss = 0.20 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:30:20.142155: step 282950, loss = 0.27 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:30:21.027193: step 282960, loss = 0.22 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:30:21.893478: step 282970, loss = 0.26 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:30:22.774137: step 282980, loss = 0.22 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:30:23.652478: step 282990, loss = 0.18 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:30:24.648445: step 283000, loss = 0.17 (1285.2 examples/sec; 0.100 sec/batch)
2017-06-02 09:30:25.409307: step 283010, loss = 0.23 (1682.5 examples/sec; 0.076 sec/batch)
2017-06-02 09:30:26.281130: step 283020, loss = 0.26 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:30:27.151679: step 283030, loss = 0.18 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:30:28.016491: step 283040, loss = 0.23 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:30:28.882851: step 283050, loss = 0.24 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:30:29.751423: step 283060, loss = 0.25 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:30:30.632486: step 283070, loss = 0.23 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:30:31.508617: step 283080, loss = 0.21 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:30:32.391054: step 283090, loss = 0.21 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:30:33.386546: step 283100, loss = 0.23 (1285.8 examples/sec; 0.100 sec/batch)
2017-06-02 09:30:34.123636: step 283110, loss = 0.19 (1736.7 examples/sec; 0.074 sec/batch)
2017-06-02 09:30:34.966669: step 283120, loss = 0.19 (1518.2 examples/sec; 0.084 sec/batch)
2017-06-02 09:30:35.853079: step 283130, loss = 0.21 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:30:36.737798: step 283140, loss = 0.19 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:30:37.607831: step 283150, loss = 0.15 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:30:38.461111: step 283160, loss = 0.27 (1500.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:30:39.336759: step 283170, loss = 0.18 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:30:40.223910: step 283180, loss = 0.17 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:30:41.114327: step 283190, loss = 0.19 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:30:42.082406: step 283200, loss = 0.19 (1322.2 examples/sec; 0.097 sec/batch)
2017-06-02 09:30:42.862284: step 283210, loss = 0.20 (1641.3 examples/sec; 0.078 sec/batch)
2017-06-02 09:30:43.748598: step 283220, loss = 0.20 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:30:44.631586: step 283230, loss = 0.21 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:30:45.503634: step 283240, loss = 0.22 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:30:46.378171: step 283250, loss = 0.20 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:30:47.251756: step 283260, loss = 0.18 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:30:48.158166: step 283270, loss = 0.23 (1412.1 examples/sec; 0.091 sec/batch)
2017-06-02 09:30:49.010443: step 283280, loss = 0.26 (1501.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:30:49.910247: step 283290, loss = 0.33 (1422.5 examples/sec; 0.090 sec/batch)
2017-06-02 09:30:50.865219: step 283300, loss = 0.23 (1340.4 examples/sec; 0.095 sec/batch)
2017-06-02 09:30:51.619674: step 283310, loss = 0.22 (1696.6 examples/sec; 0.075 sec/batch)
2017-06-02 09:30:52.500882: step 283320, loss = 0.22 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:30:53.380664: step 283330, loss = 0.19 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:30:54.253715: step 283340, loss = 0.25 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:30:55.123901: step 283350, loss = 0.24 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:30:55.982711: step 283360, loss = 0.21 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:30:56.831426: step 283370, loss = 0.16 (1508.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:30:57.676796: step 283380, loss = 0.20 (1514.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:30:58.548944: step 283390, loss = 0.21 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:30:59.509737: step 283400, loss = 0.18 (1332.2 examples/sec; 0.096 sec/batch)
2017-06-02 09:31:00.265240: step 283410, loss = 0.21 (1694.2 examples/sec; 0.076 sec/batch)
2017-06-02 09:31:01.128464: step 283420, loss = 0.27 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:31:02.002208: step 283430, loss = 0.24 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:31:02.894092: step 283440, loss = 0.25 (1435.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:31:03.788034: step 283450, loss = 0.19 (1431.9 examples/sec; 0.089 sec/batch)
2017-06-02 09:31:04.648145: step 283460, loss = 0.19 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:31:05.536353: step 283470, loss = 0.26 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:31:06.427058: step 283480, loss = 0.22 (1437.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:31:07.280625: step 283490, loss = 0.26 (1499.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:31:08.314083: step 283500, loss = 0.26 (1238.6 examples/sec; 0.103 sec/batch)
2017-06-02 09:31:09.044463: step 283510, loss = 0.20 (1752.5 examples/sec; 0.073 sec/batch)
2017-06-02 09:31:09.906771: step 283520, loss = 0.17 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:31:10.745957: step 283530, loss = 0.22 (1525.3 examples/sec; 0.084 sec/batch)
2017-06-02 09:31:11.625567: step 283540, loss = 0.20 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:31:12.480882: step 283550, loss = 0.19 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:31:13.350325: step 283560, loss = 0.23 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:31:14.225849: step 283570, loss = 0.22 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:31:15.078800: step 283580, loss = 0.22 (1500.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:31:15.915830: step 283590, loss = 0.21 (1529.2 examples/sec; 0.084 sec/batch)
2017-06-02 09:31:16.916033: step 283600, loss = 0.24 (1279.7 examples/sec; 0.100 sec/batch)
2017-06-02 09:31:17.649171: step 283610, loss = 0.19 (1745.9 examples/sec; 0.073 sec/batch)
2017-06-02 09:31:18.527171: step 283620, loss = 0.16 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:31:19.413460: step 283630, loss = 0.17 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:31:20.285235: step 283640, loss = 0.23 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:31:21.140723: step 283650, loss = 0.28 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:31:21.994779: step 283660, loss = 0.23 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:31:22.846968: step 283670, loss = 0.21 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:31:23.697368: step 283680, loss = 0.19 (1505.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:31:24.543263: step 283690, loss = 0.32 (1513.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:31:25.527323: step 283700, loss = 0.23 (1300.7 examples/sec; 0.098 sec/batch)
2017-06-02 09:31:26.291934: step 283710, loss = 0.24 (1674.1 examples/sec; 0.076 sec/batch)
2017-06-02 09:31:27.189704: step 283720, loss = 0.23 (1425.8 examples/sec; 0.090 sec/batch)
2017-06-02 09:31:28.055210: step 283730, loss = 0.23 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:31:28.926693: step 283740, loss = 0.19 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:31:29.800974: step 283750, loss = 0.20 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:31:30.680009: step 283760, loss = 0.22 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:31:31.541621: step 283770, loss = 0.22 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:31:32.396440: step 283780, loss = 0.28 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:31:33.278997: step 283790, loss = 0.21 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:31:34.323307: step 283800, loss = 0.16 (1225.7 examples/sec; 0.104 sec/batch)
2017-06-02 09:31:35.029685: step 283810, loss = 0.19 (1812.1 examples/sec; 0.071 sec/batch)
2017-06-02 09:31:35.931825: step 283820, loss = 0.26 (1418.8 examples/sec; 0.090 sec/batch)
2017-06-02 09:31:36.813267: step 283830, loss = 0.24 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:31:37.668820: step 283840, loss = 0.22 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:31:38.537380: step 283850, loss = 0.20 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:31:39.421667: step 283860, loss = 0.24 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:31:40.277449: step 283870, loss = 0.18 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:31:41.155527: step 283880, loss = 0.19 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:31:42.018185: step 283890, loss = 0.21 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:31:43.017302: step 283900, loss = 0.21 (1281.1 examples/sec; 0.100 sec/batch)
2017-06-02 09:31:43.730842: step 283910, loss = 0.20 (1793.9 examples/sec; 0.071 sec/batch)
2017-06-02 09:31:44.604026: step 283920, loss = 0.21 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:31:45.476196: step 283930, loss = 0.17 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:31:46.358347: step 283940, loss = 0.27 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:31:47.201415: step 283950, loss = 0.18 (1518.3 examples/sec; 0.084 sec/batch)
2017-06-02 09:31:48.036484: step 283960, loss = 0.20 (1532.8 examples/sec; 0.084 sec/batch)
2017-06-02 09:31:48.889429: step 283970, loss = 0.18 (1500.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:31:49.749100: step 283980, loss = 0.26 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:31:50.624159: step 283990, loss = 0.21 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:31:51.592808: step 284000, loss = 0.22 (1321.4 examples/sec; 0.097 sec/batch)
2017-06-02 09:31:52.356827: step 284010, loss = 0.25 (1675.4 examples/sec; 0.076 sec/batch)
2017-06-02 09:31:53.198598: step 284020, loss = 0.26 (1520.6 examples/sec; 0.084 sec/batch)
2017-06-02 09:31:54.068510: step 284030, loss = 0.23 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:31:54.911494: step 284040, loss = 0.26 (1518.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:31:55.791344: step 284050, loss = 0.19 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:31:56.643868: step 284060, loss = 0.24 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:31:57.523863: step 284070, loss = 0.24 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:31:58.408377: step 284080, loss = 0.22 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:31:59.260833: step 284090, loss = 0.19 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:32:00.238937: step 284100, loss = 0.18 (1308.6 examples/sec; 0.098 sec/batch)
2017-06-02 09:32:01.010081: step 284110, loss = 0.25 (1659.9 examples/sec; 0.077 sec/batch)
2017-06-02 09:32:01.864128: step 284120, loss = 0.19 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:32:02.709395: step 284130, loss = 0.25 (1514.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:32:03.568094: step 284140, loss = 0.20 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:32:04.454484: step 284150, loss = 0.20 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:32:05.313962: step 284160, loss = 0.21 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:32:06.155227: step 284170, loss = 0.27 (1521.5 examples/sec; 0.084 sec/batch)
2017-06-02 09:32:07.007632: step 284180, loss = 0.21 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:32:07.860855: step 284190, loss = 0.24 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:32:08.868996: step 284200, loss = 0.17 (1269.7 examples/sec; 0.101 sec/batch)
2017-06-02 09:32:09.570837: step 284210, loss = 0.23 (1823.8 examples/sec; 0.070 sec/batch)
2017-06-02 09:32:10.416901: step 284220, loss = 0.15 (1512.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:32:11.272060: step 284230, loss = 0.21 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:32:12.129880: step 284240, loss = 0.24 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:32:12.967513: step 284250, loss = 0.20 (1528.1 examples/sec; 0.084 sec/batch)
2017-06-02 09:32:13.789450: step 284260, loss = 0.28 (1557.3 examples/sec; 0.082 sec/batch)
2017-06-02 09:32:14.653015: step 284270, loss = 0.18 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:32:15.542456: step 284280, loss = 0.23 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:32:16.386244: step 284290, loss = 0.20 (1516.9 examples/sec; 0.084 sec/batch)
2017-06-02 09:32:17.342620: step 284300, loss = 0.29 (1338.4 examples/sec; 0.096 sec/batch)
2017-06-02 09:32:18.104639: step 284310, loss = 0.17 (1679.8 examples/sec; 0.076 sec/batch)
2017-06-02 09:32:18.949342: step 284320, loss = 0.23 (1515.3 examples/sec; 0.084 sec/batch)
2017-06-02 09:32:19.818125: step 284330, loss = 0.24 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:32:20.711306: step 284340, loss = 0.21 (1433.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:32:21.566189: step 284350, loss = 0.21 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:32:22.452611: step 284360, loss = 0.23 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:32:23.339230: step 284370, loss = 0.20 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 09:32:24.199502: step 284380, loss = 0.19 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:32:25.069886: step 284390, loss = 0.26 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:32:26.035631: step 284400, loss = 0.22 (1325.4 examples/sec; 0.097 sec/batch)
2017-06-02 09:32:26.822470: step 284410, loss = 0.20 (1626.8 examples/sec; 0.079 sec/batch)
2017-06-02 09:32:27.690177: step 284420, loss = 0.22 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:32:28.564289: step 284430, loss = 0.18 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:32:29.438494: step 284440, loss = 0.29 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:32:30.305645: step 284450, loss = 0.22 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:32:31.170506: step 284460, loss = 0.24 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:32:32.041100: step 284470, loss = 0.24 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:32:32.916153: step 284480, loss = 0.26 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:32:33.791402: step 284490, loss = 0.25 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:32:34.754804: step 284500, loss = 0.26 (1328.6 examples/sec; 0.096 sec/batch)
2017-06-02 09:32:35.544636: step 284510, loss = 0.20 (1620.7 examples/sec; 0.079 sec/batch)
2017-06-02 09:32:36.408601: step 284520, loss = 0.23 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:32:37.284396: step 284530, loss = 0.23 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:32:38.151016: step 284540, loss = 0.17 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:32:39.012847: step 284550, loss = 0.23 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:32:39.875613: step 284560, loss = 0.26 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:32:40.725480: step 284570, loss = 0.23 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:32:41.582201: step 284580, loss = 0.29 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:32:42.432331: step 284590, loss = 0.17 (1505.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:32:43.384220: step 284600, loss = 0.23 (1344.7 examples/sec; 0.095 sec/batch)
2017-06-02 09:32:44.168214: step 284610, loss = 0.32 (1632.7 examples/sec; 0.078 sec/batch)
2017-06-02 09:32:45.043848: step 284620, loss = 0.19 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:32:45.933983: step 284630, loss = 0.18 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:32:46.801362: step 284640, loss = 0.27 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:32:47.646928: step 284650, loss = 0.22 (1513.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:32:48.492467: step 284660, loss = 0.25 (1513.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:32:49.398752: step 284670, loss = 0.20 (1412.4 examples/sec; 0.091 sec/batch)
2017-06-02 09:32:50.273245: step 284680, loss = 0.17 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:32:51.158634: step 284690, loss = 0.18 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 09:32:52.124283: step 284700, loss = 0.24 (1325.5 examples/sec; 0.097 sec/batch)
2017-06-02 09:32:52.896998: step 284710, loss = 0.18 (1656.5 examples/sec; 0.077 sec/batch)
2017-06-02 09:32:53.766451: step 284720, loss = 0.21 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:32:54.641796: step 284730, loss = 0.22 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:32:55.511284: step 284740, loss = 0.21 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:32:56.364750: step 284750, loss = 0.20 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:32:57.239524: step 284760, loss = 0.22 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:32:58.088562: step 284770, loss = 0.26 (1507.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:32:58.961956: step 284780, loss = 0.20 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:32:59.823035: step 284790, loss = 0.17 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:33:00.770282: step 284800, loss = 0.19 (1351.3 examples/sec; 0.095 sec/batch)
2017-06-02 09:33:01.553335: step 284810, loss = 0.21 (1634.6 examples/sec; 0.078 sec/batch)
2017-06-02 09:33:02.437607: step 284820, loss = 0.19 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:33:03.287243: step 284830, loss = 0.24 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:33:04.143847: step 284840, loss = 0.19 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:33:05.000380: step 284850, loss = 0.20 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:33:05.871930: step 284860, loss = 0.24 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:33:06.758700: step 284870, loss = 0.18 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:33:07.653708: step 284880, loss = 0.24 (1430.2 examples/sec; 0.090 sec/batch)
2017-06-02 09:33:08.557248: step 284890, loss = 0.19 (1416.6 examples/sec; 0.090 sec/batch)
2017-06-02 09:33:09.568658: step 284900, loss = 0.22 (1265.6 examples/sec; 0.101 sec/batch)
2017-06-02 09:33:10.338841: step 284910, loss = 0.21 (1662.0 examples/sec; 0.077 sec/batch)
2017-06-02 09:33:11.236669: step 284920, loss = 0.19 (1425.7 examples/sec; 0.090 sec/batch)
2017-06-02 09:33:12.140837: step 284930, loss = 0.18 (1415.7 examples/sec; 0.090 sec/batch)
2017-06-02 09:33:12.989662: step 284940, loss = 0.18 (1508.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:33:13.854829: step 284950, loss = 0.23 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:33:14.729653: step 284960, loss = 0.19 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:33:15.591555: step 284970, loss = 0.20 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:33:16.450162: step 284980, loss = 0.22 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:33:17.308991: step 284990, loss = 0.22 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:33:18.257123: step 285000, loss = 0.18 (1350.0 examples/sec; 0.095 sec/batch)
2017-06-02 09:33:18.995824: step 285010, loss = 0.21 (1732.8 examples/sec; 0.074 sec/batch)
2017-06-02 09:33:19.860568: step 285020, loss = 0.19 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:33:20.726173: step 285030, loss = 0.22 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:33:21.590047: step 285040, loss = 0.20 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:33:22.462006: step 285050, loss = 0.22 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:33:23.321747: step 285060, loss = 0.19 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:33:24.205144: step 285070, loss = 0.20 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:33:25.073453: step 285080, loss = 0.21 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:33:25.930329: step 285090, loss = 0.24 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:33:26.892292: step 285100, loss = 0.21 (1330.6 examples/sec; 0.096 sec/batch)
2017-06-02 09:33:27.641197: step 285110, loss = 0.23 (1709.2 examples/sec; 0.075 sec/batch)
2017-06-02 09:33:28.500712: step 285120, loss = 0.24 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:33:29.383282: step 285130, loss = 0.20 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:33:30.221861: step 285140, loss = 0.22 (1526.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:33:31.088597: step 285150, loss = 0.23 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:33:31.950130: step 285160, loss = 0.23 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:33:32.832593: step 285170, loss = 0.18 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:33:33.664796: step 285180, loss = 0.19 (1538.1 examples/sec; 0.083 sec/batch)
2017-06-02 09:33:34.529408: step 285190, loss = 0.20 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:33:35.519113: step 285200, loss = 0.21 (1293.3 examples/sec; 0.099 sec/batch)
2017-06-02 09:33:36.259293: step 285210, loss = 0.18 (1729.3 examples/sec; 0.074 sec/batch)
2017-06-02 09:33:37.126202: step 285220, loss = 0.22 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:33:38.005324: step 285230, loss = 0.18 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:33:38.837427: step 285240, loss = 0.18 (1538.3 examples/sec; 0.083 sec/batch)
2017-06-02 09:33:39.721162: step 285250, loss = 0.26 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:33:40.600212: step 285260, loss = 0.24 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:33:41.482302: step 285270, loss = 0.25 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:33:42.352840: step 285280, loss = 0.24 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:33:43.245295: step 285290, loss = 0.19 (1434.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:33:44.260847: step 285300, loss = 0.25 (1260.4 examples/sec; 0.102 sec/batch)
2017-06-02 09:33:44.991138: step 285310, loss = 0.24 (1752.7 examples/sec; 0.073 sec/batch)
2017-06-02 09:33:45.872153: step 285320, loss = 0.20 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:33:46.731075: step 285330, loss = 0.23 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:33:47.606271: step 285340, loss = 0.24 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:33:48.510081: step 285350, loss = 0.25 (1416.2 examples/sec; 0.090 sec/batch)
2017-06-02 09:33:49.348123: step 285360, loss = 0.22 (1527.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:33:50.232201: step 285370, loss = 0.24 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:33:51.071775: step 285380, loss = 0.22 (1524.6 examples/sec; 0.084 sec/batch)
2017-06-02 09:33:51.943380: step 285390, loss = 0.22 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:33:52.919137: step 285400, loss = 0.25 (1311.8 examples/sec; 0.098 sec/batch)
2017-06-02 09:33:53.708213: step 285410, loss = 0.25 (1622.1 examples/sec; 0.079 sec/batch)
2017-06-02 09:33:54.569454: step 285420, loss = 0.18 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:33:55.463160: step 285430, loss = 0.23 (1432.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:33:56.331168: step 285440, loss = 0.20 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:33:57.203825: step 285450, loss = 0.24 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:33:58.087414: step 285460, loss = 0.19 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:33:58.980108: step 285470, loss = 0.25 (1433.9 examples/sec; 0.089 sec/batch)
2017-06-02 09:33:59.869273: step 285480, loss = 0.26 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:34:00.725749: step 285490, loss = 0.27 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:34:01.710979: step 285500, loss = 0.27 (1299.2 examples/sec; 0.099 sec/batch)
2017-06-02 09:34:02.486861: step 285510, loss = 0.15 (1649.7 examples/sec; 0.078 sec/batch)
2017-06-02 09:34:03.373411: step 285520, loss = 0.20 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:34:04.226501: step 285530, loss = 0.25 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:34:05.068589: step 285540, loss = 0.16 (1520.0 examples/sec; 0.084 sec/batch)
2017-06-02 09:34:05.923002: step 285550, loss = 0.16 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:34:06.797739: step 285560, loss = 0.22 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:34:07.656418: step 285570, loss = 0.24 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:34:08.489058: step 285580, loss = 0.21 (1537.3 examples/sec; 0.083 sec/batch)
2017-06-02 09:34:09.348812: step 285590, loss = 0.24 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:34:10.294063: step 285600, loss = 0.26 (1354.1 examples/sec; 0.095 sec/batch)
2017-06-02 09:34:11.059762: step 285610, loss = 0.23 (1671.7 examples/sec; 0.077 sec/batch)
2017-06-02 09:34:11.934536: step 285620, loss = 0.17 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:34:12.807380: step 285630, loss = 0.24 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:34:13.673687: step 285640, loss = 0.20 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:34:14.536612: step 285650, loss = 0.21 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:34:15.428228: step 285660, loss = 0.25 (1435.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:34:16.295868: step 285670, loss = 0.20 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:34:17.167306: step 285680, loss = 0.17 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:34:18.025804: step 285690, loss = 0.21 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:34:18.969494: step 285700, loss = 0.34 (1356.4 examples/sec; 0.094 sec/batch)
2017-06-02 09:34:19.704999: step 285710, loss = 0.19 (1740.3 examples/sec; 0.074 sec/batch)
2017-06-02 09:34:20.568577: step 285720, loss = 0.23 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:34:21.439802: step 285730, loss = 0.19 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:34:22.303205: step 285740, loss = 0.20 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:34:23.178539: step 285750, loss = 0.28 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:34:24.037103: step 285760, loss = 0.21 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:34:24.879418: step 285770, loss = 0.18 (1519.6 examples/sec; 0.084 sec/batch)
2017-06-02 09:34:25.711486: step 285780, loss = 0.19 (1538.3 examples/sec; 0.083 sec/batch)
2017-06-02 09:34:26.532799: step 285790, loss = 0.21 (1558.5 examples/sec; 0.082 sec/batch)
2017-06-02 09:34:27.483061: step 285800, loss = 0.21 (1347.0 examples/sec; 0.095 sec/batch)
2017-06-02 09:34:28.234846: step 285810, loss = 0.17 (1702.6 examples/sec; 0.075 sec/batch)
2017-06-02 09:34:29.103668: step 285820, loss = 0.22 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:34:29.912927: step 285830, loss = 0.20 (1581.7 examples/sec; 0.081 sec/batch)
2017-06-02 09:34:30.774693: step 285840, loss = 0.18 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:34:31.606090: step 285850, loss = 0.18 (1539.6 examples/sec; 0.083 sec/batch)
2017-06-02 09:34:32.465345: step 285860, loss = 0.22 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:34:33.297168: step 285870, loss = 0.20 (1538.8 examples/sec; 0.083 sec/batch)
2017-06-02 09:34:34.157795: step 285880, loss = 0.17 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:34:35.012364: step 285890, loss = 0.28 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:34:35.958546: step 285900, loss = 0.19 (1352.8 examples/sec; 0.095 sec/batch)
2017-06-02 09:34:36.726585: step 285910, loss = 0.23 (1666.6 examples/sec; 0.077 sec/batch)
2017-06-02 09:34:37.596607: step 285920, loss = 0.18 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:34:38.465457: step 285930, loss = 0.24 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:34:39.331418: step 285940, loss = 0.21 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:34:40.192373: step 285950, loss = 0.23 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:34:41.064230: step 285960, loss = 0.21 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:34:41.929311: step 285970, loss = 0.19 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:34:42.789585: step 285980, loss = 0.27 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:34:43.649289: step 285990, loss = 0.18 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:34:44.603097: step 286000, loss = 0.21 (1342.0 examples/sec; 0.095 sec/batch)
2017-06-02 09:34:45.377382: step 286010, loss = 0.18 (1653.2 examples/sec; 0.077 sec/batch)
2017-06-02 09:34:46.245795: step 286020, loss = 0.25 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:34:47.133828: step 286030, loss = 0.29 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:34:48.018495: step 286040, loss = 0.20 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:34:48.879029: step 286050, loss = 0.18 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:34:49.771618: step 286060, loss = 0.20 (1434.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:34:50.630861: step 286070, loss = 0.18 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:34:51.520894: step 286080, loss = 0.22 (1438.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:34:52.382102: step 286090, loss = 0.22 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:34:53.369617: step 286100, loss = 0.23 (1296.2 examples/sec; 0.099 sec/batch)
2017-06-02 09:34:54.139605: step 286110, loss = 0.19 (1662.4 examples/sec; 0.077 sec/batch)
2017-06-02 09:34:54.998723: step 286120, loss = 0.19 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:34:55.852291: step 286130, loss = 0.21 (1499.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:34:56.693728: step 286140, loss = 0.20 (1521.2 examples/sec; 0.084 sec/batch)
2017-06-02 09:34:57.559863: step 286150, loss = 0.23 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:34:58.424585: step 286160, loss = 0.24 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:34:59.279613: step 286170, loss = 0.25 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:35:00.135984: step 286180, loss = 0.20 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:35:00.970613: step 286190, loss = 0.17 (1533.6 examples/sec; 0.083 sec/batch)
2017-06-02 09:35:01.943988: step 286200, loss = 0.24 (1315.0 examples/sec; 0.097 sec/batch)
2017-06-02 09:35:02.697262: step 286210, loss = 0.28 (1699.3 examples/sec; 0.075 sec/batch)
2017-06-02 09:35:03.561329: step 286220, loss = 0.22 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:35:04.389444: step 286230, loss = 0.21 (1545.7 examples/sec; 0.083 sec/batch)
2017-06-02 09:35:05.266286: step 286240, loss = 0.23 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:35:06.137270: step 286250, loss = 0.24 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:35:06.988414: step 286260, loss = 0.16 (1503.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:35:07.861654: step 286270, loss = 0.20 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:35:08.701870: step 286280, loss = 0.18 (1523.5 examples/sec; 0.084 sec/batch)
2017-06-02 09:35:09.573718: step 286290, loss = 0.23 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:35:10.534456: step 286300, loss = 0.19 (1332.3 examples/sec; 0.096 sec/batch)
2017-06-02 09:35:11.317158: step 286310, loss = 0.19 (1635.4 examples/sec; 0.078 sec/batch)
2017-06-02 09:35:12.198971: step 286320, loss = 0.23 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:35:13.057570: step 286330, loss = 0.20 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:35:13.908010: step 286340, loss = 0.18 (1505.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:35:14.753710: step 286350, loss = 0.22 (1513.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:35:15.617190: step 286360, loss = 0.19 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:35:16.483678: step 286370, loss = 0.21 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:35:17.333878: step 286380, loss = 0.19 (1505.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:35:18.172677: step 286390, loss = 0.22 (1526.0 examples/sec; 0.084 sec/batch)
2017-06-02 09:35:19.119296: step 286400, loss = 0.18 (1352.2 examples/sec; 0.095 sec/batch)
2017-06-02 09:35:19.889961: step 286410, loss = 0.23 (1660.9 examples/sec; 0.077 sec/batch)
2017-06-02 09:35:20.742383: step 286420, loss = 0.22 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:35:21.618114: step 286430, loss = 0.26 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:35:22.462237: step 286440, loss = 0.18 (1516.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:35:23.318929: step 286450, loss = 0.18 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:35:24.172611: step 286460, loss = 0.22 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:35:25.024789: step 286470, loss = 0.22 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:35:25.882331: step 286480, loss = 0.17 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:35:26.739483: step 286490, loss = 0.24 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:35:27.696977: step 286500, loss = 0.23 (1336.8 examples/sec; 0.096 sec/batch)
2017-06-02 09:35:28.464708: step 286510, loss = 0.18 (1667.3 examples/sec; 0.077 sec/batch)
2017-06-02 09:35:29.363770: step 286520, loss = 0.32 (1423.7 examples/sec; 0.090 sec/batch)
2017-06-02 09:35:30.229850: step 286530, loss = 0.21 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:35:31.103188: step 286540, loss = 0.20 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:35:31.946034: step 286550, loss = 0.21 (1518.7 examples/sec; 0.084 sec/batch)
2017-06-02 09:35:32.800279: step 286560, loss = 0.21 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:35:33.647713: step 286570, loss = 0.19 (1510.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:35:34.538276: step 286580, loss = 0.24 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:35:35.398647: step 286590, loss = 0.27 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:35:36.389190: step 286600, loss = 0.19 (1292.2 examples/sec; 0.099 sec/batch)
2017-06-02 09:35:37.088204: step 286610, loss = 0.25 (1831.2 examples/sec; 0.070 sec/batch)
2017-06-02 09:35:37.949757: step 286620, loss = 0.20 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:35:38.802053: step 286630, loss = 0.26 (1501.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:35:39.647583: step 286640, loss = 0.30 (1513.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:35:40.498366: step 286650, loss = 0.18 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:35:41.369228: step 286660, loss = 0.17 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:35:42.248833: step 286670, loss = 0.18 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:35:43.127789: step 286680, loss = 0.25 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:35:43.967871: step 286690, loss = 0.27 (1523.7 examples/sec; 0.084 sec/batch)
2017-06-02 09:35:44.928292: step 286700, loss = 0.23 (1332.7 examples/sec; 0.096 sec/batch)
2017-06-02 09:35:45.689036: step 286710, loss = 0.23 (1682.5 examples/sec; 0.076 sec/batch)
2017-06-02 09:35:46.505772: step 286720, loss = 0.20 (1567.2 examples/sec; 0.082 sec/batch)
2017-06-02 09:35:47.387605: step 286730, loss = 0.21 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:35:48.238468: step 286740, loss = 0.25 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:35:49.085546: step 286750, loss = 0.24 (1511.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:35:49.932077: step 286760, loss = 0.20 (1512.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:35:50.811191: step 286770, loss = 0.21 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:35:51.626503: step 286780, loss = 0.22 (1569.9 examples/sec; 0.082 sec/batch)
2017-06-02 09:35:52.481225: step 286790, loss = 0.19 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:35:53.435339: step 286800, loss = 0.21 (1341.6 examples/sec; 0.095 sec/batch)
2017-06-02 09:35:54.198532: step 286810, loss = 0.23 (1677.2 examples/sec; 0.076 sec/batch)
2017-06-02 09:35:55.044030: step 286820, loss = 0.17 (1513.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:35:55.923865: step 286830, loss = 0.25 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:35:56.767528: step 286840, loss = 0.24 (1517.2 examples/sec; 0.084 sec/batch)
2017-06-02 09:35:57.615474: step 286850, loss = 0.19 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:35:58.466158: step 286860, loss = 0.17 (1504.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:35:59.319364: step 286870, loss = 0.27 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:36:00.189910: step 286880, loss = 0.21 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:36:01.046837: step 286890, loss = 0.20 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:36:02.009788: step 286900, loss = 0.23 (1329.2 examples/sec; 0.096 sec/batch)
2017-06-02 09:36:02.765114: step 286910, loss = 0.20 (1694.7 examples/sec; 0.076 sec/batch)
2017-06-02 09:36:03.638004: step 286920, loss = 0.24 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:36:04.480289: step 286930, loss = 0.23 (1519.7 examples/sec; 0.084 sec/batch)
2017-06-02 09:36:05.317855: step 286940, loss = 0.19 (1528.2 examples/sec; 0.084 sec/batch)
2017-06-02 09:36:06.170459: step 286950, loss = 0.28 (1501.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:36:07.037008: step 286960, loss = 0.25 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:36:07.921347: step 286970, loss = 0.24 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:36:08.766058: step 286980, loss = 0.22 (1515.3 examples/sec; 0.084 sec/batch)
2017-06-02 09:36:09.615377: step 286990, loss = 0.19 (1507.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:36:10.566921: step 287000, loss = 0.17 (1345.2 examples/sec; 0.095 sec/batch)
2017-06-02 09:36:11.324064: step 287010, loss = 0.21 (1690.6 examples/sec; 0.076 sec/batch)
2017-06-02 09:36:12.194183: step 287020, loss = 0.20 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:36:13.039763: step 287030, loss = 0.22 (1513.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:36:13.909421: step 287040, loss = 0.24 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:36:14.752445: step 287050, loss = 0.26 (1518.3 examples/sec; 0.084 sec/batch)
2017-06-02 09:36:15.631819: step 287060, loss = 0.20 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:36:16.505703: step 287070, loss = 0.25 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:36:17.328168: step 287080, loss = 0.20 (1556.3 examples/sec; 0.082 sec/batch)
2017-06-02 09:36:18.199894: step 287090, loss = 0.30 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:36:19.180235: step 287100, loss = 0.21 (1305.7 examples/sec; 0.098 sec/batch)
2017-06-02 09:36:19.952131: step 287110, loss = 0.22 (1658.3 examples/sec; 0.077 sec/batch)
2017-06-02 09:36:20.839521: step 287120, loss = 0.22 (1442.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:36:21.719244: step 287130, loss = 0.23 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:36:22.589514: step 287140, loss = 0.18 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:36:23.467577: step 287150, loss = 0.19 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:36:24.328994: step 287160, loss = 0.17 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:36:25.207232: step 287170, loss = 0.17 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:36:26.096866: step 287180, loss = 0.16 (1438.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:36:26.983305: step 287190, loss = 0.21 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:36:27.968104: step 287200, loss = 0.23 (1299.8 examples/sec; 0.098 sec/batch)
2017-06-02 09:36:28.746827: step 287210, loss = 0.16 (1643.7 examples/sec; 0.078 sec/batch)
2017-06-02 09:36:29.649217: step 287220, loss = 0.18 (1418.5 examples/sec; 0.090 sec/batch)
2017-06-02 09:36:30.533333: step 287230, loss = 0.27 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:36:31.407871: step 287240, loss = 0.21 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:36:32.294108: step 287250, loss = 0.27 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:36:33.169071: step 287260, loss = 0.22 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:36:34.039862: step 287270, loss = 0.21 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:36:34.913106: step 287280, loss = 0.23 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:36:35.777479: step 287290, loss = 0.17 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:36:36.748940: step 287300, loss = 0.21 (1317.6 examples/sec; 0.097 sec/batch)
2017-06-02 09:36:37.505071: step 287310, loss = 0.25 (1692.8 examples/sec; 0.076 sec/batch)
2017-06-02 09:36:38.375574: step 287320, loss = 0.17 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:36:39.242961: step 287330, loss = 0.23 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:36:40.095431: step 287340, loss = 0.21 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:36:40.950128: step 287350, loss = 0.21 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:36:41.804695: step 287360, loss = 0.25 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:36:42.664039: step 287370, loss = 0.19 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:36:43.559004: step 287380, loss = 0.24 (1430.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:36:44.417879: step 287390, loss = 0.19 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:36:45.392432: step 287400, loss = 0.16 (1313.4 examples/sec; 0.097 sec/batch)
2017-06-02 09:36:46.152238: step 287410, loss = 0.22 (1684.6 examples/sec; 0.076 sec/batch)
2017-06-02 09:36:47.008233: step 287420, loss = 0.20 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:36:47.844857: step 287430, loss = 0.20 (1530.0 examples/sec; 0.084 sec/batch)
2017-06-02 09:36:48.713870: step 287440, loss = 0.24 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:36:49.563011: step 287450, loss = 0.26 (1507.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:36:50.419727: step 287460, loss = 0.19 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:36:51.298599: step 287470, loss = 0.19 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:36:52.146221: step 287480, loss = 0.16 (1510.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:36:53.027686: step 287490, loss = 0.22 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:36:53.990248: step 287500, loss = 0.24 (1329.8 examples/sec; 0.096 sec/batch)
2017-06-02 09:36:54.771753: step 287510, loss = 0.19 (1637.9 examples/sec; 0.078 sec/batch)
2017-06-02 09:36:55.658172: step 287520, loss = 0.20 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:36:56.513351: step 287530, loss = 0.21 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:36:57.403386: step 287540, loss = 0.23 (1438.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:36:58.263840: step 287550, loss = 0.17 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:36:59.129663: step 287560, loss = 0.26 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:36:59.997226: step 287570, loss = 0.19 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:37:00.856536: step 287580, loss = 0.22 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:37:01.723447: step 287590, loss = 0.23 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:37:02.687468: step 287600, loss = 0.17 (1327.8 examples/sec; 0.096 sec/batch)
2017-06-02 09:37:03.469782: step 287610, loss = 0.20 (1636.2 examples/sec; 0.078 sec/batch)
2017-06-02 09:37:04.356199: step 287620, loss = 0.17 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:37:05.229163: step 287630, loss = 0.19 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:37:06.116031: step 287640, loss = 0.20 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:37:06.978711: step 287650, loss = 0.21 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:37:07.869587: step 287660, loss = 0.25 (1436.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:37:08.740138: step 287670, loss = 0.16 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:37:09.620205: step 287680, loss = 0.21 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:37:10.459088: step 287690, loss = 0.22 (1525.9 examples/sec; 0.084 sec/batch)
2017-06-02 09:37:11.434658: step 287700, loss = 0.25 (1312.0 examples/sec; 0.098 sec/batch)
2017-06-02 09:37:12.216220: step 287710, loss = 0.23 (1637.8 examples/sec; 0.078 sec/batch)
2017-06-02 09:37:13.048411: step 287720, loss = 0.18 (1538.1 examples/sec; 0.083 sec/batch)
2017-06-02 09:37:13.891832: step 287730, loss = 0.20 (1517.6 examples/sec; 0.084 sec/batch)
2017-06-02 09:37:14.758017: step 287740, loss = 0.28 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:37:15.594022: step 287750, loss = 0.17 (1531.1 examples/sec; 0.084 sec/batch)
2017-06-02 09:37:16.472598: step 287760, loss = 0.25 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:37:17.347298: step 287770, loss = 0.20 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:37:18.225181: step 287780, loss = 0.21 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:37:19.106989: step 287790, loss = 0.22 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:37:20.070710: step 287800, loss = 0.21 (1328.2 examples/sec; 0.096 sec/batch)
2017-06-02 09:37:20.853924: step 287810, loss = 0.26 (1634.3 examples/sec; 0.078 sec/batch)
2017-06-02 09:37:21.718008: step 287820, loss = 0.21 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:37:22.541829: step 287830, loss = 0.21 (1553.7 examples/sec; 0.082 sec/batch)
2017-06-02 09:37:23.389029: step 287840, loss = 0.32 (1510.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:37:24.255552: step 287850, loss = 0.19 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:37:25.132633: step 287860, loss = 0.26 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:37:25.992245: step 287870, loss = 0.22 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:37:26.849210: step 287880, loss = 0.22 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:37:27.718549: step 287890, loss = 0.16 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:37:28.766157: step 287900, loss = 0.20 (1221.8 examples/sec; 0.105 sec/batch)
2017-06-02 09:37:29.424217: step 287910, loss = 0.17 (1945.1 examples/sec; 0.066 sec/batch)
2017-06-02 09:37:30.283280: step 287920, loss = 0.22 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:37:31.155467: step 287930, loss = 0.28 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:37:32.019359: step 287940, loss = 0.26 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:37:32.867840: step 287950, loss = 0.23 (1508.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:37:33.702013: step 287960, loss = 0.22 (1534.2 examples/sec; 0.083 sec/batch)
2017-06-02 09:37:34.543881: step 287970, loss = 0.21 (1520.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:37:35.400358: step 287980, loss = 0.17 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:37:36.260193: step 287990, loss = 0.26 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:37:37.220800: step 288000, loss = 0.19 (1332.5 examples/sec; 0.096 sec/batch)
2017-06-02 09:37:38.000797: step 288010, loss = 0.20 (1641.0 examples/sec; 0.078 sec/batch)
2017-06-02 09:37:38.860670: step 288020, loss = 0.19 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:37:39.729450: step 288030, loss = 0.19 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:37:40.578790: step 288040, loss = 0.22 (1507.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:37:41.447044: step 288050, loss = 0.21 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:37:42.320132: step 288060, loss = 0.20 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:37:43.170903: step 288070, loss = 0.21 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:37:44.034665: step 288080, loss = 0.19 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:37:44.902071: step 288090, loss = 0.27 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:37:45.867401: step 288100, loss = 0.25 (1326.0 examples/sec; 0.097 sec/batch)
2017-06-02 09:37:46.625108: step 288110, loss = 0.20 (1689.4 examples/sec; 0.076 sec/batch)
2017-06-02 09:37:47.486611: step 288120, loss = 0.22 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:37:48.321501: step 288130, loss = 0.24 (1533.1 examples/sec; 0.083 sec/batch)
2017-06-02 09:37:49.197587: step 288140, loss = 0.19 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:37:50.052544: step 288150, loss = 0.19 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:37:50.905107: step 288160, loss = 0.22 (1501.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:37:51.768644: step 288170, loss = 0.18 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:37:52.641695: step 288180, loss = 0.17 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:37:53.499762: step 288190, loss = 0.18 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:37:54.472726: step 288200, loss = 0.22 (1315.6 examples/sec; 0.097 sec/batch)
2017-06-02 09:37:55.227530: step 288210, loss = 0.24 (1695.8 examples/sec; 0.075 sec/batch)
2017-06-02 09:37:56.110298: step 288220, loss = 0.23 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:37:56.968044: step 288230, loss = 0.16 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:37:57.829597: step 288240, loss = 0.20 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:37:58.695032: step 288250, loss = 0.20 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:37:59.571969: step 288260, loss = 0.22 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:38:00.412083: step 288270, loss = 0.23 (1523.6 examples/sec; 0.084 sec/batch)
2017-06-02 09:38:01.268752: step 288280, loss = 0.17 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:38:02.136179: step 288290, loss = 0.19 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:38:03.099810: step 288300, loss = 0.20 (1328.3 examples/sec; 0.096 sec/batch)
2017-06-02 09:38:03.834100: step 288310, loss = 0.27 (1743.2 examples/sec; 0.073 sec/batch)
2017-06-02 09:38:04.705761: step 288320, loss = 0.20 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:38:05.557883: step 288330, loss = 0.18 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:38:06.414948: step 288340, loss = 0.19 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:38:07.287662: step 288350, loss = 0.17 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:38:08.153889: step 288360, loss = 0.16 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:38:09.026909: step 288370, loss = 0.18 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:38:09.894577: step 288380, loss = 0.19 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:38:10.756783: step 288390, loss = 0.19 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:38:11.748549: step 288400, loss = 0.19 (1290.6 examples/sec; 0.099 sec/batch)
2017-06-02 09:38:12.535560: step 288410, loss = 0.19 (1626.4 examples/sec; 0.079 sec/batch)
2017-06-02 09:38:13.396567: step 288420, loss = 0.24 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:38:14.272748: step 288430, loss = 0.16 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:38:15.131723: step 288440, loss = 0.30 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:38:16.015783: step 288450, loss = 0.21 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:38:16.863022: step 288460, loss = 0.23 (1510.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:38:17.701947: step 288470, loss = 0.20 (1525.8 examples/sec; 0.084 sec/batch)
2017-06-02 09:38:18.564272: step 288480, loss = 0.19 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:38:19.453127: step 288490, loss = 0.20 (1440.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:38:20.426029: step 288500, loss = 0.24 (1315.7 examples/sec; 0.097 sec/batch)
2017-06-02 09:38:21.180169: step 288510, loss = 0.18 (1697.3 examples/sec; 0.075 sec/batch)
2017-06-02 09:38:22.033382: step 288520, loss = 0.25 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:38:22.922622: step 288530, loss = 0.22 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:38:23.783615: step 288540, loss = 0.29 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:38:24.654458: step 288550, loss = 0.18 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:38:25.517098: step 288560, loss = 0.17 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:38:26.414112: step 288570, loss = 0.23 (1427.0 examples/sec; 0.090 sec/batch)
2017-06-02 09:38:27.287807: step 288580, loss = 0.17 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:38:28.167540: step 288590, loss = 0.19 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:38:29.215241: step 288600, loss = 0.20 (1221.7 examples/sec; 0.105 sec/batch)
2017-06-02 09:38:29.935942: step 288610, loss = 0.22 (1776.0 examples/sec; 0.072 sec/batch)
2017-06-02 09:38:30.779842: step 288620, loss = 0.18 (1516.8 examples/sec; 0.084 sec/batch)
2017-06-02 09:38:31.627751: step 288630, loss = 0.19 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:38:32.501703: step 288640, loss = 0.21 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:38:33.395034: step 288650, loss = 0.22 (1432.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:38:34.261711: step 288660, loss = 0.22 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:38:35.130825: step 288670, loss = 0.19 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:38:36.012008: step 288680, loss = 0.22 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:38:36.899517: step 288690, loss = 0.16 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:38:37.885317: step 288700, loss = 0.17 (1298.4 examples/sec; 0.099 sec/batch)
2017-06-02 09:38:38.677326: step 288710, loss = 0.29 (1616.1 examples/sec; 0.079 sec/batch)
2017-06-02 09:38:39.553767: step 288720, loss = 0.25 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:38:40.438252: step 288730, loss = 0.20 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:38:41.279435: step 288740, loss = 0.21 (1521.7 examples/sec; 0.084 sec/batch)
2017-06-02 09:38:42.148900: step 288750, loss = 0.22 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:38:43.023422: step 288760, loss = 0.21 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:38:43.870303: step 288770, loss = 0.20 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:38:44.726201: step 288780, loss = 0.18 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:38:45.604946: step 288790, loss = 0.23 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:38:46.601660: step 288800, loss = 0.18 (1284.2 examples/sec; 0.100 sec/batch)
2017-06-02 09:38:47.389681: step 288810, loss = 0.19 (1624.3 examples/sec; 0.079 sec/batch)
2017-06-02 09:38:48.245335: step 288820, loss = 0.19 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:38:49.097313: step 288830, loss = 0.20 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:38:49.974925: step 288840, loss = 0.20 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:38:50.835361: step 288850, loss = 0.19 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:38:51.708007: step 288860, loss = 0.17 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:38:52.571537: step 288870, loss = 0.17 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:38:53.414937: step 288880, loss = 0.22 (1517.7 examples/sec; 0.084 sec/batch)
2017-06-02 09:38:54.276078: step 288890, loss = 0.27 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:38:55.277425: step 288900, loss = 0.25 (1278.2 examples/sec; 0.100 sec/batch)
2017-06-02 09:38:55.988635: step 288910, loss = 0.22 (1799.8 examples/sec; 0.071 sec/batch)
2017-06-02 09:38:56.821555: step 288920, loss = 0.25 (1536.8 examples/sec; 0.083 sec/batch)
2017-06-02 09:38:57.656357: step 288930, loss = 0.16 (1533.5 examples/sec; 0.083 sec/batch)
2017-06-02 09:38:58.521162: step 288940, loss = 0.17 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:38:59.392724: step 288950, loss = 0.19 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:39:00.256404: step 288960, loss = 0.25 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:39:01.120717: step 288970, loss = 0.24 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:39:01.942453: step 288980, loss = 0.20 (1557.7 examples/sec; 0.082 sec/batch)
2017-06-02 09:39:02.801448: step 288990, loss = 0.19 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:39:03.754995: step 289000, loss = 0.18 (1342.4 examples/sec; 0.095 sec/batch)
2017-06-02 09:39:04.528846: step 289010, loss = 0.21 (1654.1 examples/sec; 0.077 sec/batch)
2017-06-02 09:39:05.392186: step 289020, loss = 0.22 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:39:06.276064: step 289030, loss = 0.21 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:39:07.142245: step 289040, loss = 0.23 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:39:08.023246: step 289050, loss = 0.17 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:39:08.884260: step 289060, loss = 0.21 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:39:09.747857: step 289070, loss = 0.19 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:39:10.617425: step 289080, loss = 0.23 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:39:11.506292: step 289090, loss = 0.26 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:39:12.467489: step 289100, loss = 0.21 (1331.7 examples/sec; 0.096 sec/batch)
2017-06-02 09:39:13.245475: step 289110, loss = 0.18 (1645.3 examples/sec; 0.078 sec/batch)
2017-06-02 09:39:14.124877: step 289120, loss = 0.21 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:39:14.979693: step 289130, loss = 0.20 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:39:15.846568: step 289140, loss = 0.20 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:39:16.725446: step 289150, loss = 0.21 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:39:17.586348: step 289160, loss = 0.20 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:39:18.449775: step 289170, loss = 0.18 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:39:19.321351: step 289180, loss = 0.21 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:39:20.183825: step 289190, loss = 0.26 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:39:21.148071: step 289200, loss = 0.24 (1327.5 examples/sec; 0.096 sec/batch)
2017-06-02 09:39:21.913122: step 289210, loss = 0.19 (1673.1 examples/sec; 0.077 sec/batch)
2017-06-02 09:39:22.780380: step 289220, loss = 0.21 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:39:23.656183: step 289230, loss = 0.17 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:39:24.536469: step 289240, loss = 0.17 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:39:25.418650: step 289250, loss = 0.24 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:39:26.261564: step 289260, loss = 0.20 (1518.5 examples/sec; 0.084 sec/batch)
2017-06-02 09:39:27.150087: step 289270, loss = 0.17 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:39:28.025044: step 289280, loss = 0.18 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:39:28.882987: step 289290, loss = 0.26 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:39:29.930594: step 289300, loss = 0.23 (1221.8 examples/sec; 0.105 sec/batch)
2017-06-02 09:39:30.655172: step 289310, loss = 0.19 (1766.5 examples/sec; 0.072 sec/batch)
2017-06-02 09:39:31.515578: step 289320, loss = 0.23 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:39:32.368598: step 289330, loss = 0.17 (1500.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:39:33.218320: step 289340, loss = 0.16 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:39:34.103915: step 289350, loss = 0.22 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:39:34.962435: step 289360, loss = 0.26 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:39:35.871510: step 289370, loss = 0.19 (1408.0 examples/sec; 0.091 sec/batch)
2017-06-02 09:39:36.703441: step 289380, loss = 0.19 (1538.6 examples/sec; 0.083 sec/batch)
2017-06-02 09:39:37.570631: step 289390, loss = 0.16 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:39:38.536932: step 289400, loss = 0.21 (1324.6 examples/sec; 0.097 sec/batch)
2017-06-02 09:39:39.303975: step 289410, loss = 0.24 (1668.8 examples/sec; 0.077 sec/batch)
2017-06-02 09:39:40.177161: step 289420, loss = 0.24 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:39:41.056229: step 289430, loss = 0.21 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:39:41.925595: step 289440, loss = 0.14 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:39:42.820334: step 289450, loss = 0.24 (1430.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:39:43.694856: step 289460, loss = 0.23 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:39:44.552377: step 289470, loss = 0.17 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:39:45.424715: step 289480, loss = 0.20 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:39:46.268915: step 289490, loss = 0.20 (1516.2 examples/sec; 0.084 sec/batch)
2017-06-02 09:39:47.271454: step 289500, loss = 0.22 (1276.8 examples/sec; 0.100 sec/batch)
2017-06-02 09:39:48.006649: step 289510, loss = 0.21 (1741.0 examples/sec; 0.074 sec/batch)
2017-06-02 09:39:48.850521: step 289520, loss = 0.17 (1516.8 examples/sec; 0.084 sec/batch)
2017-06-02 09:39:49.708369: step 289530, loss = 0.22 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:39:50.570460: step 289540, loss = 0.24 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:39:51.420759: step 289550, loss = 0.17 (1505.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:39:52.400299: step 289560, loss = 0.21 (1306.7 examples/sec; 0.098 sec/batch)
2017-06-02 09:39:53.204906: step 289570, loss = 0.22 (1590.9 examples/sec; 0.080 sec/batch)
2017-06-02 09:39:54.081770: step 289580, loss = 0.24 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:39:54.940417: step 289590, loss = 0.26 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:39:55.909687: step 289600, loss = 0.24 (1320.6 examples/sec; 0.097 sec/batch)
2017-06-02 09:39:56.643877: step 289610, loss = 0.23 (1743.4 examples/sec; 0.073 sec/batch)
2017-06-02 09:39:57.519959: step 289620, loss = 0.22 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:39:58.359598: step 289630, loss = 0.20 (1524.5 examples/sec; 0.084 sec/batch)
2017-06-02 09:39:59.232210: step 289640, loss = 0.17 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:40:00.113489: step 289650, loss = 0.22 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:40:00.980011: step 289660, loss = 0.23 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:40:01.842259: step 289670, loss = 0.23 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:40:02.720255: step 289680, loss = 0.19 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:40:03.579525: step 289690, loss = 0.21 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:40:04.569787: step 289700, loss = 0.17 (1292.6 examples/sec; 0.099 sec/batch)
2017-06-02 09:40:05.352821: step 289710, loss = 0.19 (1634.7 examples/sec; 0.078 sec/batch)
2017-06-02 09:40:06.227660: step 289720, loss = 0.18 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:40:07.098189: step 289730, loss = 0.19 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:40:07.973316: step 289740, loss = 0.17 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:40:08.847620: step 289750, loss = 0.18 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:40:09.713441: step 289760, loss = 0.26 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:40:10.598279: step 289770, loss = 0.21 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:40:11.494674: step 289780, loss = 0.20 (1427.9 examples/sec; 0.090 sec/batch)
2017-06-02 09:40:12.363826: step 289790, loss = 0.26 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:40:13.331207: step 289800, loss = 0.18 (1323.1 examples/sec; 0.097 sec/batch)
2017-06-02 09:40:14.103398: step 289810, loss = 0.24 (1657.6 examples/sec; 0.077 sec/batch)
2017-06-02 09:40:14.963298: step 289820, loss = 0.17 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:40:15.830521: step 289830, loss = 0.20 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:40:16.698632: step 289840, loss = 0.24 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:40:17.573228: step 289850, loss = 0.19 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:40:18.473405: step 289860, loss = 0.21 (1421.9 examples/sec; 0.090 sec/batch)
2017-06-02 09:40:19.341516: step 289870, loss = 0.32 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:40:20.219631: step 289880, loss = 0.18 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:40:21.076826: step 289890, loss = 0.28 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:40:22.077347: step 289900, loss = 0.23 (1279.3 examples/sec; 0.100 sec/batch)
2017-06-02 09:40:22.801589: step 289910, loss = 0.32 (1767.4 examples/sec; 0.072 sec/batch)
2017-06-02 09:40:23.627022: step 289920, loss = 0.20 (1550.7 examples/sec; 0.083 sec/batch)
2017-06-02 09:40:24.481065: step 289930, loss = 0.27 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:40:25.363814: step 289940, loss = 0.29 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:40:26.199819: step 289950, loss = 0.25 (1531.1 examples/sec; 0.084 sec/batch)
2017-06-02 09:40:27.072824: step 289960, loss = 0.20 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:40:27.940669: step 289970, loss = 0.21 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:40:28.786066: step 289980, loss = 0.23 (1514.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:40:29.648968: step 289990, loss = 0.21 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:40:30.604872: step 290000, loss = 0.21 (1339.0 examples/sec; 0.096 sec/batch)
2017-06-02 09:40:31.358981: step 290010, loss = 0.22 (1697.4 examples/sec; 0.075 sec/batch)
2017-06-02 09:40:32.196027: step 290020, loss = 0.26 (1529.2 examples/sec; 0.084 sec/batch)
2017-06-02 09:40:33.061050: step 290030, loss = 0.23 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:40:33.905525: step 290040, loss = 0.21 (1515.7 examples/sec; 0.084 sec/batch)
2017-06-02 09:40:34.750895: step 290050, loss = 0.18 (1514.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:40:35.627790: step 290060, loss = 0.22 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:40:36.501151: step 290070, loss = 0.17 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:40:37.362752: step 290080, loss = 0.18 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:40:38.234445: step 290090, loss = 0.20 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:40:39.225514: step 290100, loss = 0.16 (1291.5 examples/sec; 0.099 sec/batch)
2017-06-02 09:40:39.985329: step 290110, loss = 0.24 (1684.6 examples/sec; 0.076 sec/batch)
2017-06-02 09:40:40.843951: step 290120, loss = 0.19 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:40:41.709990: step 290130, loss = 0.21 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:40:42.592621: step 290140, loss = 0.25 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:40:43.470138: step 290150, loss = 0.18 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:40:44.313511: step 290160, loss = 0.20 (1517.7 examples/sec; 0.084 sec/batch)
2017-06-02 09:40:45.185373: step 290170, loss = 0.23 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:40:46.051315: step 290180, loss = 0.21 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:40:46.903942: step 290190, loss = 0.21 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:40:47.910332: step 290200, loss = 0.23 (1271.9 examples/sec; 0.101 sec/batch)
2017-06-02 09:40:48.619561: step 290210, loss = 0.19 (1804.8 examples/sec; 0.071 sec/batch)
2017-06-02 09:40:49.486690: step 290220, loss = 0.23 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:40:50.348240: step 290230, loss = 0.25 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:40:51.219074: step 290240, loss = 0.21 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:40:52.081143: step 290250, loss = 0.17 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:40:52.966406: step 290260, loss = 0.25 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 09:40:53.823320: step 290270, loss = 0.22 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:40:54.707217: step 290280, loss = 0.24 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:40:55.565393: step 290290, loss = 0.22 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:40:56.519895: step 290300, loss = 0.23 (1341.0 examples/sec; 0.095 sec/batch)
2017-06-02 09:40:57.297751: step 290310, loss = 0.22 (1645.6 examples/sec; 0.078 sec/batch)
2017-06-02 09:40:58.161191: step 290320, loss = 0.19 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:40:59.020319: step 290330, loss = 0.22 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:40:59.848881: step 290340, loss = 0.19 (1544.8 examples/sec; 0.083 sec/batch)
2017-06-02 09:41:00.672405: step 290350, loss = 0.22 (1554.3 examples/sec; 0.082 sec/batch)
2017-06-02 09:41:01.507309: step 290360, loss = 0.24 (1533.1 examples/sec; 0.083 sec/batch)
2017-06-02 09:41:02.377107: step 290370, loss = 0.24 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:41:03.219064: step 290380, loss = 0.20 (1520.3 examples/sec; 0.084 sec/batch)
2017-06-02 09:41:04.085596: step 290390, loss = 0.22 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:41:05.067606: step 290400, loss = 0.16 (1303.4 examples/sec; 0.098 sec/batch)
2017-06-02 09:41:05.797385: step 290410, loss = 0.19 (1753.9 examples/sec; 0.073 sec/batch)
2017-06-02 09:41:06.630041: step 290420, loss = 0.16 (1537.3 examples/sec; 0.083 sec/batch)
2017-06-02 09:41:07.493936: step 290430, loss = 0.21 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:41:08.344867: step 290440, loss = 0.24 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:41:09.182716: step 290450, loss = 0.20 (1527.7 examples/sec; 0.084 sec/batch)
2017-06-02 09:41:10.061516: step 290460, loss = 0.24 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:41:10.957868: step 290470, loss = 0.23 (1428.0 examples/sec; 0.090 sec/batch)
2017-06-02 09:41:11.822945: step 290480, loss = 0.21 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:41:12.695250: step 290490, loss = 0.21 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:41:13.676932: step 290500, loss = 0.17 (1303.9 examples/sec; 0.098 sec/batch)
2017-06-02 09:41:14.463322: step 290510, loss = 0.22 (1627.7 examples/sec; 0.079 sec/batch)
2017-06-02 09:41:15.314210: step 290520, loss = 0.20 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:41:16.163092: step 290530, loss = 0.20 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:41:17.004350: step 290540, loss = 0.27 (1521.5 examples/sec; 0.084 sec/batch)
2017-06-02 09:41:17.879807: step 290550, loss = 0.23 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:41:18.774542: step 290560, loss = 0.17 (1430.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:41:19.633921: step 290570, loss = 0.25 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:41:20.482267: step 290580, loss = 0.16 (1508.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:41:21.337368: step 290590, loss = 0.25 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:41:22.317201: step 290600, loss = 0.23 (1306.3 examples/sec; 0.098 sec/batch)
2017-06-02 09:41:23.099528: step 290610, loss = 0.18 (1636.2 examples/sec; 0.078 sec/batch)
2017-06-02 09:41:23.973637: step 290620, loss = 0.21 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:41:24.830432: step 290630, loss = 0.24 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:41:25.714213: step 290640, loss = 0.22 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:41:26.549423: step 290650, loss = 0.18 (1532.5 examples/sec; 0.084 sec/batch)
2017-06-02 09:41:27.413078: step 290660, loss = 0.21 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:41:28.269991: step 290670, loss = 0.22 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:41:29.122475: step 290680, loss = 0.21 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:41:29.991568: step 290690, loss = 0.18 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:41:30.948513: step 290700, loss = 0.22 (1337.6 examples/sec; 0.096 sec/batch)
2017-06-02 09:41:31.723979: step 290710, loss = 0.23 (1650.6 examples/sec; 0.078 sec/batch)
2017-06-02 09:41:32.560933: step 290720, loss = 0.22 (1529.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:41:33.432087: step 290730, loss = 0.17 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:41:34.286813: step 290740, loss = 0.18 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:41:35.159853: step 290750, loss = 0.17 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:41:36.030823: step 290760, loss = 0.19 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:41:36.895569: step 290770, loss = 0.22 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:41:37.743066: step 290780, loss = 0.22 (1510.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:41:38.619509: step 290790, loss = 0.19 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:41:39.571965: step 290800, loss = 0.17 (1343.9 examples/sec; 0.095 sec/batch)
2017-06-02 09:41:40.351847: step 290810, loss = 0.19 (1641.3 examples/sec; 0.078 sec/batch)
2017-06-02 09:41:41.216632: step 290820, loss = 0.22 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:41:42.091477: step 290830, loss = 0.23 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:41:42.959517: step 290840, loss = 0.17 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:41:43.814293: step 290850, loss = 0.23 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:41:44.689086: step 290860, loss = 0.24 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:41:45.522122: step 290870, loss = 0.19 (1536.5 examples/sec; 0.083 sec/batch)
2017-06-02 09:41:46.395111: step 290880, loss = 0.21 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:41:47.252630: step 290890, loss = 0.26 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:41:48.229843: step 290900, loss = 0.23 (1309.8 examples/sec; 0.098 sec/batch)
2017-06-02 09:41:48.972409: step 290910, loss = 0.18 (1723.8 examples/sec; 0.074 sec/batch)
2017-06-02 09:41:49.833245: step 290920, loss = 0.19 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:41:50.683055: step 290930, loss = 0.26 (1506.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:41:51.554168: step 290940, loss = 0.19 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:41:52.425242: step 290950, loss = 0.17 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:41:53.321215: step 290960, loss = 0.17 (1428.6 examples/sec; 0.090 sec/batch)
2017-06-02 09:41:54.187689: step 290970, loss = 0.18 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:41:55.027314: step 290980, loss = 0.18 (1524.5 examples/sec; 0.084 sec/batch)
2017-06-02 09:41:55.891126: step 290990, loss = 0.19 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:41:56.864100: step 291000, loss = 0.20 (1315.5 examples/sec; 0.097 sec/batch)
2017-06-02 09:41:57.627892: step 291010, loss = 0.22 (1675.9 examples/sec; 0.076 sec/batch)
2017-06-02 09:41:58.497781: step 291020, loss = 0.20 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:41:59.356030: step 291030, loss = 0.25 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:42:00.215313: step 291040, loss = 0.17 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:42:01.078752: step 291050, loss = 0.17 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:42:01.921351: step 291060, loss = 0.21 (1519.1 examples/sec; 0.084 sec/batch)
2017-06-02 09:42:02.778070: step 291070, loss = 0.18 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:42:03.641206: step 291080, loss = 0.25 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:42:04.531095: step 291090, loss = 0.16 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:42:05.536890: step 291100, loss = 0.26 (1272.6 examples/sec; 0.101 sec/batch)
2017-06-02 09:42:06.267503: step 291110, loss = 0.21 (1751.9 examples/sec; 0.073 sec/batch)
2017-06-02 09:42:07.113091: step 291120, loss = 0.20 (1513.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:42:08.000020: step 291130, loss = 0.19 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:42:08.824432: step 291140, loss = 0.26 (1552.6 examples/sec; 0.082 sec/batch)
2017-06-02 09:42:09.705439: step 291150, loss = 0.22 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:42:10.548318: step 291160, loss = 0.25 (1518.6 examples/sec; 0.084 sec/batch)
2017-06-02 09:42:11.400717: step 291170, loss = 0.19 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:42:12.254777: step 291180, loss = 0.19 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:42:13.095475: step 291190, loss = 0.19 (1522.5 examples/sec; 0.084 sec/batch)
2017-06-02 09:42:14.112120: step 291200, loss = 0.21 (1259.0 examples/sec; 0.102 sec/batch)
2017-06-02 09:42:14.839342: step 291210, loss = 0.23 (1760.2 examples/sec; 0.073 sec/batch)
2017-06-02 09:42:15.714333: step 291220, loss = 0.20 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:42:16.559628: step 291230, loss = 0.24 (1514.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:42:17.427889: step 291240, loss = 0.20 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:42:18.275244: step 291250, loss = 0.21 (1510.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:42:19.127589: step 291260, loss = 0.21 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:42:19.983216: step 291270, loss = 0.20 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:42:20.857591: step 291280, loss = 0.18 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:42:21.691431: step 291290, loss = 0.19 (1535.1 examples/sec; 0.083 sec/batch)
2017-06-02 09:42:22.656254: step 291300, loss = 0.17 (1326.6 examples/sec; 0.096 sec/batch)
2017-06-02 09:42:23.420810: step 291310, loss = 0.16 (1674.2 examples/sec; 0.076 sec/batch)
2017-06-02 09:42:24.309108: step 291320, loss = 0.21 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:42:25.180308: step 291330, loss = 0.21 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:42:26.034490: step 291340, loss = 0.18 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:42:26.894501: step 291350, loss = 0.18 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:42:27.750047: step 291360, loss = 0.16 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:42:28.622946: step 291370, loss = 0.19 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:42:29.509651: step 291380, loss = 0.20 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:42:30.361897: step 291390, loss = 0.19 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:42:31.343058: step 291400, loss = 0.24 (1304.6 examples/sec; 0.098 sec/batch)
2017-06-02 09:42:32.084787: step 291410, loss = 0.20 (1725.7 examples/sec; 0.074 sec/batch)
2017-06-02 09:42:32.966186: step 291420, loss = 0.23 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:42:33.827639: step 291430, loss = 0.18 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:42:34.687063: step 291440, loss = 0.25 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:42:35.558592: step 291450, loss = 0.24 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:42:36.405941: step 291460, loss = 0.18 (1510.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:42:37.253623: step 291470, loss = 0.23 (1510.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:42:38.085753: step 291480, loss = 0.18 (1538.3 examples/sec; 0.083 sec/batch)
2017-06-02 09:42:38.928335: step 291490, loss = 0.21 (1519.1 examples/sec; 0.084 sec/batch)
2017-06-02 09:42:39.909138: step 291500, loss = 0.21 (1305.1 examples/sec; 0.098 sec/batch)
2017-06-02 09:42:40.670109: step 291510, loss = 0.14 (1682.1 examples/sec; 0.076 sec/batch)
2017-06-02 09:42:41.541116: step 291520, loss = 0.25 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:42:42.411255: step 291530, loss = 0.24 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:42:43.259171: step 291540, loss = 0.23 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:42:44.118339: step 291550, loss = 0.17 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:42:44.993077: step 291560, loss = 0.24 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:42:45.826392: step 291570, loss = 0.23 (1536.0 examples/sec; 0.083 sec/batch)
2017-06-02 09:42:46.697613: step 291580, loss = 0.18 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:42:47.554579: step 291590, loss = 0.20 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:42:48.546498: step 291600, loss = 0.26 (1290.4 examples/sec; 0.099 sec/batch)
2017-06-02 09:42:49.275041: step 291610, loss = 0.24 (1756.9 examples/sec; 0.073 sec/batch)
2017-06-02 09:42:50.135841: step 291620, loss = 0.27 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:42:50.987394: step 291630, loss = 0.25 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:42:51.848997: step 291640, loss = 0.18 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:42:52.725564: step 291650, loss = 0.21 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:42:53.585663: step 291660, loss = 0.27 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:42:54.457535: step 291670, loss = 0.20 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:42:55.320489: step 291680, loss = 0.22 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:42:56.159077: step 291690, loss = 0.18 (1526.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:42:57.141394: step 291700, loss = 0.19 (1303.0 examples/sec; 0.098 sec/batch)
2017-06-02 09:42:57.924557: step 291710, loss = 0.27 (1634.4 examples/sec; 0.078 sec/batch)
2017-06-02 09:42:58.758229: step 291720, loss = 0.25 (1535.4 examples/sec; 0.083 sec/batch)
2017-06-02 09:42:59.635804: step 291730, loss = 0.16 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:43:00.511626: step 291740, loss = 0.17 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:43:01.369427: step 291750, loss = 0.24 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:43:02.218956: step 291760, loss = 0.18 (1506.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:43:03.090173: step 291770, loss = 0.22 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:43:03.940504: step 291780, loss = 0.24 (1505.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:43:04.801946: step 291790, loss = 0.20 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:43:05.770566: step 291800, loss = 0.22 (1321.5 examples/sec; 0.097 sec/batch)
2017-06-02 09:43:06.526327: step 291810, loss = 0.18 (1693.7 examples/sec; 0.076 sec/batch)
2017-06-02 09:43:07.392701: step 291820, loss = 0.15 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:43:08.269362: step 291830, loss = 0.23 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:43:09.106615: step 291840, loss = 0.24 (1528.8 examples/sec; 0.084 sec/batch)
2017-06-02 09:43:09.960352: step 291850, loss = 0.24 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:43:10.841615: step 291860, loss = 0.16 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:43:11.718680: step 291870, loss = 0.24 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:43:12.593619: step 291880, loss = 0.21 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:43:13.459298: step 291890, loss = 0.26 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:43:14.411314: step 291900, loss = 0.28 (1344.5 examples/sec; 0.095 sec/batch)
2017-06-02 09:43:15.183665: step 291910, loss = 0.24 (1657.3 examples/sec; 0.077 sec/batch)
2017-06-02 09:43:16.064336: step 291920, loss = 0.17 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:43:16.936594: step 291930, loss = 0.24 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:43:17.818986: step 291940, loss = 0.18 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:43:18.694385: step 291950, loss = 0.17 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:43:19.552545: step 291960, loss = 0.23 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:43:20.435444: step 291970, loss = 0.22 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:43:21.328345: step 291980, loss = 0.23 (1433.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:43:22.198442: step 291990, loss = 0.19 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:43:23.149897: step 292000, loss = 0.18 (1345.3 examples/sec; 0.095 sec/batch)
2017-06-02 09:43:23.911649: step 292010, loss = 0.19 (1680.4 examples/sec; 0.076 sec/batch)
2017-06-02 09:43:24.803610: step 292020, loss = 0.21 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:43:25.697496: step 292030, loss = 0.21 (1432.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:43:26.551655: step 292040, loss = 0.18 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:43:27.447006: step 292050, loss = 0.24 (1429.6 examples/sec; 0.090 sec/batch)
2017-06-02 09:43:28.286655: step 292060, loss = 0.18 (1524.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:43:29.146358: step 292070, loss = 0.18 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:43:29.992758: step 292080, loss = 0.20 (1512.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:43:30.857539: step 292090, loss = 0.18 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:43:31.848581: step 292100, loss = 0.22 (1291.6 examples/sec; 0.099 sec/batch)
2017-06-02 09:43:32.641638: step 292110, loss = 0.18 (1614.0 examples/sec; 0.079 sec/batch)
2017-06-02 09:43:33.545604: step 292120, loss = 0.23 (1416.0 examples/sec; 0.090 sec/batch)
2017-06-02 09:43:34.380104: step 292130, loss = 0.23 (1533.9 examples/sec; 0.083 sec/batch)
2017-06-02 09:43:35.251410: step 292140, loss = 0.19 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:43:36.141597: step 292150, loss = 0.20 (1437.9 examples/sec; 0.089 sec/batch)
2017-06-02 09:43:37.008968: step 292160, loss = 0.18 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:43:37.848685: step 292170, loss = 0.27 (1524.3 examples/sec; 0.084 sec/batch)
2017-06-02 09:43:38.725726: step 292180, loss = 0.19 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:43:39.587166: step 292190, loss = 0.17 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:43:40.571951: step 292200, loss = 0.20 (1299.8 examples/sec; 0.098 sec/batch)
2017-06-02 09:43:41.335460: step 292210, loss = 0.19 (1676.5 examples/sec; 0.076 sec/batch)
2017-06-02 09:43:42.181308: step 292220, loss = 0.18 (1513.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:43:43.067427: step 292230, loss = 0.22 (1444.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:43:43.951607: step 292240, loss = 0.22 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:43:44.809022: step 292250, loss = 0.20 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:43:45.656332: step 292260, loss = 0.18 (1510.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:43:46.535699: step 292270, loss = 0.23 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:43:47.408643: step 292280, loss = 0.20 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:43:48.280382: step 292290, loss = 0.20 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:43:49.244830: step 292300, loss = 0.21 (1327.2 examples/sec; 0.096 sec/batch)
2017-06-02 09:43:50.005822: step 292310, loss = 0.18 (1682.0 examples/sec; 0.076 sec/batch)
2017-06-02 09:43:50.851837: step 292320, loss = 0.21 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:43:51.709624: step 292330, loss = 0.28 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:43:52.575826: step 292340, loss = 0.19 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:43:53.427266: step 292350, loss = 0.25 (1503.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:43:54.267769: step 292360, loss = 0.24 (1522.9 examples/sec; 0.084 sec/batch)
2017-06-02 09:43:55.152917: step 292370, loss = 0.23 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:43:56.003319: step 292380, loss = 0.20 (1505.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:43:56.886580: step 292390, loss = 0.23 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:43:57.854133: step 292400, loss = 0.20 (1322.9 examples/sec; 0.097 sec/batch)
2017-06-02 09:43:58.609219: step 292410, loss = 0.17 (1695.2 examples/sec; 0.076 sec/batch)
2017-06-02 09:43:59.476458: step 292420, loss = 0.17 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:44:00.305521: step 292430, loss = 0.25 (1543.9 examples/sec; 0.083 sec/batch)
2017-06-02 09:44:01.195426: step 292440, loss = 0.19 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:44:02.063342: step 292450, loss = 0.19 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:44:02.877395: step 292460, loss = 0.19 (1572.4 examples/sec; 0.081 sec/batch)
2017-06-02 09:44:03.709873: step 292470, loss = 0.18 (1537.6 examples/sec; 0.083 sec/batch)
2017-06-02 09:44:04.580042: step 292480, loss = 0.22 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:44:05.458841: step 292490, loss = 0.22 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:44:06.397320: step 292500, loss = 0.22 (1363.9 examples/sec; 0.094 sec/batch)
2017-06-02 09:44:07.174840: step 292510, loss = 0.19 (1646.3 examples/sec; 0.078 sec/batch)
2017-06-02 09:44:08.042121: step 292520, loss = 0.20 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:44:08.922056: step 292530, loss = 0.15 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:44:09.781246: step 292540, loss = 0.22 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:44:10.619826: step 292550, loss = 0.19 (1526.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:44:11.503998: step 292560, loss = 0.18 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:44:12.362958: step 292570, loss = 0.20 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:44:13.249219: step 292580, loss = 0.18 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:44:14.119423: step 292590, loss = 0.19 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:44:15.127903: step 292600, loss = 0.19 (1269.2 examples/sec; 0.101 sec/batch)
2017-06-02 09:44:15.830997: step 292610, loss = 0.25 (1820.5 examples/sec; 0.070 sec/batch)
2017-06-02 09:44:16.687727: step 292620, loss = 0.20 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:44:17.559860: step 292630, loss = 0.23 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:44:18.424515: step 292640, loss = 0.23 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:44:19.299832: step 292650, loss = 0.22 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:44:20.167722: step 292660, loss = 0.20 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:44:21.033646: step 292670, loss = 0.24 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:44:21.883752: step 292680, loss = 0.18 (1505.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:44:22.753440: step 292690, loss = 0.28 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:44:23.708768: step 292700, loss = 0.24 (1339.8 examples/sec; 0.096 sec/batch)
2017-06-02 09:44:24.495830: step 292710, loss = 0.23 (1626.3 examples/sec; 0.079 sec/batch)
2017-06-02 09:44:25.375276: step 292720, loss = 0.24 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:44:26.229908: step 292730, loss = 0.20 (1497.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:44:27.100022: step 292740, loss = 0.19 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:44:27.958521: step 292750, loss = 0.22 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:44:28.828131: step 292760, loss = 0.20 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:44:29.691956: step 292770, loss = 0.18 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:44:30.553709: step 292780, loss = 0.22 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:44:31.419347: step 292790, loss = 0.22 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:44:32.384863: step 292800, loss = 0.21 (1325.7 examples/sec; 0.097 sec/batch)
2017-06-02 09:44:33.160578: step 292810, loss = 0.21 (1650.1 examples/sec; 0.078 sec/batch)
2017-06-02 09:44:34.008561: step 292820, loss = 0.25 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:44:34.859903: step 292830, loss = 0.20 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:44:35.730196: step 292840, loss = 0.23 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:44:36.596216: step 292850, loss = 0.18 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:44:37.458334: step 292860, loss = 0.22 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:44:38.337138: step 292870, loss = 0.16 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:44:39.205266: step 292880, loss = 0.24 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:44:40.043243: step 292890, loss = 0.18 (1527.5 examples/sec; 0.084 sec/batch)
2017-06-02 09:44:41.008408: step 292900, loss = 0.22 (1326.2 examples/sec; 0.097 sec/batch)
2017-06-02 09:44:41.733826: step 292910, loss = 0.17 (1764.5 examples/sec; 0.073 sec/batch)
2017-06-02 09:44:42.606283: step 292920, loss = 0.20 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:44:43.463468: step 292930, loss = 0.19 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:44:44.328368: step 292940, loss = 0.21 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:44:45.173558: step 292950, loss = 0.21 (1514.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:44:46.009575: step 292960, loss = 0.19 (1531.1 examples/sec; 0.084 sec/batch)
2017-06-02 09:44:46.864269: step 292970, loss = 0.28 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:44:47.738473: step 292980, loss = 0.18 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:44:48.605518: step 292990, loss = 0.22 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:44:49.630044: step 293000, loss = 0.18 (1249.3 examples/sec; 0.102 sec/batch)
2017-06-02 09:44:50.327881: step 293010, loss = 0.18 (1834.2 examples/sec; 0.070 sec/batch)
2017-06-02 09:44:51.177371: step 293020, loss = 0.22 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:44:52.023361: step 293030, loss = 0.23 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:44:52.865702: step 293040, loss = 0.18 (1519.6 examples/sec; 0.084 sec/batch)
2017-06-02 09:44:53.711672: step 293050, loss = 0.21 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:44:54.561078: step 293060, loss = 0.17 (1506.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:44:55.422918: step 293070, loss = 0.21 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:44:56.283002: step 293080, loss = 0.21 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:44:57.172438: step 293090, loss = 0.25 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:44:58.153409: step 293100, loss = 0.20 (1304.8 examples/sec; 0.098 sec/batch)
2017-06-02 09:44:58.886820: step 293110, loss = 0.21 (1745.3 examples/sec; 0.073 sec/batch)
2017-06-02 09:44:59.765420: step 293120, loss = 0.22 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:45:00.620283: step 293130, loss = 0.20 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:45:01.508916: step 293140, loss = 0.17 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:45:02.387511: step 293150, loss = 0.23 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:45:03.269741: step 293160, loss = 0.19 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:45:04.145256: step 293170, loss = 0.21 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:45:05.015797: step 293180, loss = 0.17 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:45:05.886860: step 293190, loss = 0.21 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:45:06.859023: step 293200, loss = 0.27 (1316.7 examples/sec; 0.097 sec/batch)
2017-06-02 09:45:07.631821: step 293210, loss = 0.22 (1656.3 examples/sec; 0.077 sec/batch)
2017-06-02 09:45:08.493968: step 293220, loss = 0.18 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:45:09.382057: step 293230, loss = 0.25 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:45:10.244200: step 293240, loss = 0.18 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:45:11.134103: step 293250, loss = 0.19 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:45:11.986884: step 293260, loss = 0.20 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:45:12.821997: step 293270, loss = 0.20 (1532.7 examples/sec; 0.084 sec/batch)
2017-06-02 09:45:13.683523: step 293280, loss = 0.20 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:45:14.557695: step 293290, loss = 0.22 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:45:15.539563: step 293300, loss = 0.22 (1303.6 examples/sec; 0.098 sec/batch)
2017-06-02 09:45:16.305220: step 293310, loss = 0.22 (1671.8 examples/sec; 0.077 sec/batch)
2017-06-02 09:45:17.167169: step 293320, loss = 0.20 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:45:18.016026: step 293330, loss = 0.19 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:45:18.880968: step 293340, loss = 0.17 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:45:19.717557: step 293350, loss = 0.18 (1530.0 examples/sec; 0.084 sec/batch)
2017-06-02 09:45:20.557872: step 293360, loss = 0.20 (1523.2 examples/sec; 0.084 sec/batch)
2017-06-02 09:45:21.426355: step 293370, loss = 0.23 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:45:22.316388: step 293380, loss = 0.19 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:45:23.153641: step 293390, loss = 0.26 (1528.8 examples/sec; 0.084 sec/batch)
2017-06-02 09:45:24.104352: step 293400, loss = 0.23 (1346.3 examples/sec; 0.095 sec/batch)
2017-06-02 09:45:24.875387: step 293410, loss = 0.22 (1660.1 examples/sec; 0.077 sec/batch)
2017-06-02 09:45:25.735092: step 293420, loss = 0.21 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:45:26.597166: step 293430, loss = 0.20 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:45:27.441203: step 293440, loss = 0.19 (1516.5 examples/sec; 0.084 sec/batch)
2017-06-02 09:45:28.307456: step 293450, loss = 0.18 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:45:29.167444: step 293460, loss = 0.27 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:45:30.059057: step 293470, loss = 0.24 (1435.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:45:30.928781: step 293480, loss = 0.19 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:45:31.803352: step 293490, loss = 0.18 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:45:32.811808: step 293500, loss = 0.25 (1269.3 examples/sec; 0.101 sec/batch)
2017-06-02 09:45:33.523167: step 293510, loss = 0.18 (1799.4 examples/sec; 0.071 sec/batch)
2017-06-02 09:45:34.374683: step 293520, loss = 0.17 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:45:35.262996: step 293530, loss = 0.20 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 09:45:36.149476: step 293540, loss = 0.30 (1443.9 examples/sec; 0.089 sec/batch)
2017-06-02 09:45:37.016433: step 293550, loss = 0.17 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:45:37.873974: step 293560, loss = 0.20 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:45:38.778268: step 293570, loss = 0.21 (1415.5 examples/sec; 0.090 sec/batch)
2017-06-02 09:45:39.656992: step 293580, loss = 0.18 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:45:40.541974: step 293590, loss = 0.19 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:45:41.529890: step 293600, loss = 0.22 (1295.7 examples/sec; 0.099 sec/batch)
2017-06-02 09:45:42.257230: step 293610, loss = 0.24 (1759.8 examples/sec; 0.073 sec/batch)
2017-06-02 09:45:43.114226: step 293620, loss = 0.25 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:45:43.981869: step 293630, loss = 0.22 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:45:44.854604: step 293640, loss = 0.18 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:45:45.711441: step 293650, loss = 0.18 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:45:46.590827: step 293660, loss = 0.20 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:45:47.462766: step 293670, loss = 0.22 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:45:48.353175: step 293680, loss = 0.21 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:45:49.225798: step 293690, loss = 0.20 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:45:50.184817: step 293700, loss = 0.18 (1334.7 examples/sec; 0.096 sec/batch)
2017-06-02 09:45:50.951710: step 293710, loss = 0.20 (1669.1 examples/sec; 0.077 sec/batch)
2017-06-02 09:45:51.827695: step 293720, loss = 0.20 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:45:52.709441: step 293730, loss = 0.21 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:45:53.579910: step 293740, loss = 0.24 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:45:54.409879: step 293750, loss = 0.20 (1542.2 examples/sec; 0.083 sec/batch)
2017-06-02 09:45:55.270695: step 293760, loss = 0.18 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:45:56.130712: step 293770, loss = 0.19 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:45:56.984440: step 293780, loss = 0.20 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:45:57.834325: step 293790, loss = 0.19 (1506.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:45:58.802576: step 293800, loss = 0.25 (1322.0 examples/sec; 0.097 sec/batch)
2017-06-02 09:45:59.595178: step 293810, loss = 0.20 (1614.9 examples/sec; 0.079 sec/batch)
2017-06-02 09:46:00.482917: step 293820, loss = 0.24 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 09:46:01.373770: step 293830, loss = 0.17 (1436.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:46:02.232063: step 293840, loss = 0.20 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:46:03.109001: step 293850, loss = 0.20 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:46:03.969426: step 293860, loss = 0.16 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:46:04.823516: step 293870, loss = 0.20 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:46:05.711535: step 293880, loss = 0.25 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:46:06.570514: step 293890, loss = 0.20 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:46:07.540750: step 293900, loss = 0.19 (1319.3 examples/sec; 0.097 sec/batch)
2017-06-02 09:46:08.308467: step 293910, loss = 0.23 (1667.3 examples/sec; 0.077 sec/batch)
2017-06-02 09:46:09.172699: step 293920, loss = 0.21 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:46:10.010436: step 293930, loss = 0.24 (1527.9 examples/sec; 0.084 sec/batch)
2017-06-02 09:46:10.873423: step 293940, loss = 0.25 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:46:11.731928: step 293950, loss = 0.19 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:46:12.621259: step 293960, loss = 0.17 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:46:13.482125: step 293970, loss = 0.21 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:46:14.335458: step 293980, loss = 0.21 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:46:15.178569: step 293990, loss = 0.24 (1518.2 examples/sec; 0.084 sec/batch)
2017-06-02 09:46:16.133688: step 294000, loss = 0.23 (1340.1 examples/sec; 0.096 sec/batch)
2017-06-02 09:46:16.902264: step 294010, loss = 0.20 (1665.4 examples/sec; 0.077 sec/batch)
2017-06-02 09:46:17.755506: step 294020, loss = 0.16 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:46:18.593840: step 294030, loss = 0.20 (1526.8 examples/sec; 0.084 sec/batch)
2017-06-02 09:46:19.488780: step 294040, loss = 0.16 (1430.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:46:20.347873: step 294050, loss = 0.19 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:46:21.217696: step 294060, loss = 0.21 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:46:22.087308: step 294070, loss = 0.18 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:46:22.949792: step 294080, loss = 0.26 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:46:23.810164: step 294090, loss = 0.19 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:46:24.779361: step 294100, loss = 0.21 (1320.7 examples/sec; 0.097 sec/batch)
2017-06-02 09:46:25.546248: step 294110, loss = 0.24 (1669.1 examples/sec; 0.077 sec/batch)
2017-06-02 09:46:26.406778: step 294120, loss = 0.21 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:46:27.283470: step 294130, loss = 0.20 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:46:28.152337: step 294140, loss = 0.16 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:46:29.016072: step 294150, loss = 0.17 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:46:29.868229: step 294160, loss = 0.23 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:46:30.732858: step 294170, loss = 0.19 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:46:31.588140: step 294180, loss = 0.24 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:46:32.421236: step 294190, loss = 0.20 (1536.5 examples/sec; 0.083 sec/batch)
2017-06-02 09:46:33.372156: step 294200, loss = 0.21 (1346.1 examples/sec; 0.095 sec/batch)
2017-06-02 09:46:34.134537: step 294210, loss = 0.22 (1678.9 examples/sec; 0.076 sec/batch)
2017-06-02 09:46:34.983045: step 294220, loss = 0.22 (1508.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:46:35.855864: step 294230, loss = 0.19 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:46:36.712054: step 294240, loss = 0.16 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:46:37.540218: step 294250, loss = 0.21 (1545.6 examples/sec; 0.083 sec/batch)
2017-06-02 09:46:38.400007: step 294260, loss = 0.22 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:46:39.258333: step 294270, loss = 0.24 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:46:40.089959: step 294280, loss = 0.22 (1539.2 examples/sec; 0.083 sec/batch)
2017-06-02 09:46:40.959934: step 294290, loss = 0.15 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:46:41.925225: step 294300, loss = 0.16 (1326.0 examples/sec; 0.097 sec/batch)
2017-06-02 09:46:42.671404: step 294310, loss = 0.22 (1715.4 examples/sec; 0.075 sec/batch)
2017-06-02 09:46:43.536673: step 294320, loss = 0.19 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:46:44.432194: step 294330, loss = 0.26 (1429.3 examples/sec; 0.090 sec/batch)
2017-06-02 09:46:45.305027: step 294340, loss = 0.22 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:46:46.152690: step 294350, loss = 0.19 (1510.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:46:47.032199: step 294360, loss = 0.26 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:46:47.917678: step 294370, loss = 0.25 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:46:48.771450: step 294380, loss = 0.21 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:46:49.648593: step 294390, loss = 0.17 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:46:50.623965: step 294400, loss = 0.22 (1312.3 examples/sec; 0.098 sec/batch)
2017-06-02 09:46:51.429711: step 294410, loss = 0.20 (1588.6 examples/sec; 0.081 sec/batch)
2017-06-02 09:46:52.316783: step 294420, loss = 0.22 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 09:46:53.180086: step 294430, loss = 0.19 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:46:54.021169: step 294440, loss = 0.21 (1521.8 examples/sec; 0.084 sec/batch)
2017-06-02 09:46:54.872808: step 294450, loss = 0.16 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:46:55.724214: step 294460, loss = 0.17 (1503.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:46:56.594861: step 294470, loss = 0.23 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:46:57.453794: step 294480, loss = 0.19 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:46:58.311099: step 294490, loss = 0.16 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:46:59.266311: step 294500, loss = 0.16 (1340.0 examples/sec; 0.096 sec/batch)
2017-06-02 09:47:00.036474: step 294510, loss = 0.20 (1662.0 examples/sec; 0.077 sec/batch)
2017-06-02 09:47:00.902852: step 294520, loss = 0.22 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:47:01.781338: step 294530, loss = 0.21 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:47:02.601012: step 294540, loss = 0.23 (1561.6 examples/sec; 0.082 sec/batch)
2017-06-02 09:47:03.441712: step 294550, loss = 0.22 (1522.6 examples/sec; 0.084 sec/batch)
2017-06-02 09:47:04.295538: step 294560, loss = 0.18 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:47:05.162220: step 294570, loss = 0.19 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:47:06.021205: step 294580, loss = 0.20 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:47:06.859772: step 294590, loss = 0.22 (1526.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:47:07.834077: step 294600, loss = 0.17 (1313.8 examples/sec; 0.097 sec/batch)
2017-06-02 09:47:08.571993: step 294610, loss = 0.18 (1734.6 examples/sec; 0.074 sec/batch)
2017-06-02 09:47:09.408171: step 294620, loss = 0.19 (1530.8 examples/sec; 0.084 sec/batch)
2017-06-02 09:47:10.281445: step 294630, loss = 0.25 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:47:11.164211: step 294640, loss = 0.22 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:47:12.019642: step 294650, loss = 0.18 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:47:12.873672: step 294660, loss = 0.20 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:47:13.708826: step 294670, loss = 0.18 (1532.7 examples/sec; 0.084 sec/batch)
2017-06-02 09:47:14.566897: step 294680, loss = 0.21 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:47:15.408176: step 294690, loss = 0.19 (1521.5 examples/sec; 0.084 sec/batch)
2017-06-02 09:47:16.424170: step 294700, loss = 0.17 (1259.8 examples/sec; 0.102 sec/batch)
2017-06-02 09:47:17.117768: step 294710, loss = 0.20 (1845.5 examples/sec; 0.069 sec/batch)
2017-06-02 09:47:17.991588: step 294720, loss = 0.22 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:47:18.844267: step 294730, loss = 0.22 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:47:19.700373: step 294740, loss = 0.22 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:47:20.565202: step 294750, loss = 0.20 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:47:21.418421: step 294760, loss = 0.22 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:47:22.278587: step 294770, loss = 0.16 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:47:23.156373: step 294780, loss = 0.24 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:47:24.033714: step 294790, loss = 0.20 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:47:24.986046: step 294800, loss = 0.19 (1344.1 examples/sec; 0.095 sec/batch)
2017-06-02 09:47:25.746775: step 294810, loss = 0.21 (1682.6 examples/sec; 0.076 sec/batch)
2017-06-02 09:47:26.581047: step 294820, loss = 0.19 (1534.3 examples/sec; 0.083 sec/batch)
2017-06-02 09:47:27.465084: step 294830, loss = 0.22 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:47:28.336682: step 294840, loss = 0.17 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:47:29.199925: step 294850, loss = 0.22 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:47:30.083457: step 294860, loss = 0.18 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:47:30.930875: step 294870, loss = 0.18 (1510.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:47:31.821974: step 294880, loss = 0.23 (1436.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:47:32.690919: step 294890, loss = 0.21 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:47:33.646638: step 294900, loss = 0.16 (1339.3 examples/sec; 0.096 sec/batch)
2017-06-02 09:47:34.417470: step 294910, loss = 0.20 (1660.5 examples/sec; 0.077 sec/batch)
2017-06-02 09:47:35.281694: step 294920, loss = 0.19 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:47:36.149261: step 294930, loss = 0.24 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:47:37.009424: step 294940, loss = 0.22 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:47:37.869971: step 294950, loss = 0.16 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:47:38.743154: step 294960, loss = 0.26 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:47:39.633380: step 294970, loss = 0.15 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:47:40.462719: step 294980, loss = 0.17 (1543.4 examples/sec; 0.083 sec/batch)
2017-06-02 09:47:41.362119: step 294990, loss = 0.22 (1423.2 examples/sec; 0.090 sec/batch)
2017-06-02 09:47:42.320985: step 295000, loss = 0.21 (1334.9 examples/sec; 0.096 sec/batch)
2017-06-02 09:47:43.085527: step 295010, loss = 0.24 (1674.2 examples/sec; 0.076 sec/batch)
2017-06-02 09:47:43.970633: step 295020, loss = 0.22 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:47:44.856834: step 295030, loss = 0.16 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:47:45.738197: step 295040, loss = 0.20 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:47:46.605707: step 295050, loss = 0.31 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:47:47.469319: step 295060, loss = 0.21 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:47:48.329965: step 295070, loss = 0.16 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:47:49.175285: step 295080, loss = 0.15 (1514.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:47:50.021271: step 295090, loss = 0.17 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:47:51.000646: step 295100, loss = 0.21 (1307.0 examples/sec; 0.098 sec/batch)
2017-06-02 09:47:51.772457: step 295110, loss = 0.20 (1658.4 examples/sec; 0.077 sec/batch)
2017-06-02 09:47:52.646277: step 295120, loss = 0.20 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:47:53.520779: step 295130, loss = 0.21 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:47:54.385434: step 295140, loss = 0.18 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:47:55.223537: step 295150, loss = 0.17 (1527.3 examples/sec; 0.084 sec/batch)
2017-06-02 09:47:56.073503: step 295160, loss = 0.20 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:47:56.905975: step 295170, loss = 0.24 (1537.6 examples/sec; 0.083 sec/batch)
2017-06-02 09:47:57.756697: step 295180, loss = 0.24 (1504.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:47:58.596696: step 295190, loss = 0.24 (1523.8 examples/sec; 0.084 sec/batch)
2017-06-02 09:47:59.563303: step 295200, loss = 0.25 (1324.2 examples/sec; 0.097 sec/batch)
2017-06-02 09:48:00.324201: step 295210, loss = 0.22 (1682.2 examples/sec; 0.076 sec/batch)
2017-06-02 09:48:01.180191: step 295220, loss = 0.18 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:48:02.050701: step 295230, loss = 0.18 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:48:02.946560: step 295240, loss = 0.22 (1428.8 examples/sec; 0.090 sec/batch)
2017-06-02 09:48:03.832444: step 295250, loss = 0.19 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 09:48:04.734103: step 295260, loss = 0.19 (1419.6 examples/sec; 0.090 sec/batch)
2017-06-02 09:48:05.613851: step 295270, loss = 0.18 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:48:06.477637: step 295280, loss = 0.25 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:48:07.335961: step 295290, loss = 0.17 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:48:08.306725: step 295300, loss = 0.23 (1318.5 examples/sec; 0.097 sec/batch)
2017-06-02 09:48:09.062662: step 295310, loss = 0.21 (1693.3 examples/sec; 0.076 sec/batch)
2017-06-02 09:48:09.919293: step 295320, loss = 0.22 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:48:10.763392: step 295330, loss = 0.24 (1516.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:48:11.614149: step 295340, loss = 0.18 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:48:12.462347: step 295350, loss = 0.18 (1509.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:48:13.308538: step 295360, loss = 0.19 (1512.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:48:14.164663: step 295370, loss = 0.20 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:48:14.993376: step 295380, loss = 0.18 (1544.6 examples/sec; 0.083 sec/batch)
2017-06-02 09:48:15.849607: step 295390, loss = 0.23 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:48:16.828767: step 295400, loss = 0.19 (1307.2 examples/sec; 0.098 sec/batch)
2017-06-02 09:48:17.579483: step 295410, loss = 0.16 (1705.0 examples/sec; 0.075 sec/batch)
2017-06-02 09:48:18.451510: step 295420, loss = 0.20 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:48:19.294918: step 295430, loss = 0.24 (1517.6 examples/sec; 0.084 sec/batch)
2017-06-02 09:48:20.127034: step 295440, loss = 0.18 (1538.2 examples/sec; 0.083 sec/batch)
2017-06-02 09:48:20.965087: step 295450, loss = 0.18 (1527.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:48:21.831989: step 295460, loss = 0.24 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:48:22.692273: step 295470, loss = 0.21 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:48:23.570078: step 295480, loss = 0.20 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:48:24.417441: step 295490, loss = 0.19 (1510.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:48:25.367687: step 295500, loss = 0.20 (1347.0 examples/sec; 0.095 sec/batch)
2017-06-02 09:48:26.132746: step 295510, loss = 0.18 (1673.1 examples/sec; 0.077 sec/batch)
2017-06-02 09:48:26.976374: step 295520, loss = 0.27 (1517.2 examples/sec; 0.084 sec/batch)
2017-06-02 09:48:27.837203: step 295530, loss = 0.22 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:48:28.697967: step 295540, loss = 0.21 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:48:29.565352: step 295550, loss = 0.23 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:48:30.407035: step 295560, loss = 0.15 (1520.8 examples/sec; 0.084 sec/batch)
2017-06-02 09:48:31.216697: step 295570, loss = 0.23 (1580.9 examples/sec; 0.081 sec/batch)
2017-06-02 09:48:32.069930: step 295580, loss = 0.20 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:48:32.912219: step 295590, loss = 0.18 (1519.7 examples/sec; 0.084 sec/batch)
2017-06-02 09:48:33.924816: step 295600, loss = 0.24 (1264.1 examples/sec; 0.101 sec/batch)
2017-06-02 09:48:34.605692: step 295610, loss = 0.18 (1880.0 examples/sec; 0.068 sec/batch)
2017-06-02 09:48:35.472293: step 295620, loss = 0.24 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:48:36.325766: step 295630, loss = 0.23 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:48:37.206659: step 295640, loss = 0.20 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:48:38.039400: step 295650, loss = 0.16 (1537.1 examples/sec; 0.083 sec/batch)
2017-06-02 09:48:38.904076: step 295660, loss = 0.20 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:48:39.772922: step 295670, loss = 0.20 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:48:40.603448: step 295680, loss = 0.20 (1541.2 examples/sec; 0.083 sec/batch)
2017-06-02 09:48:41.453195: step 295690, loss = 0.20 (1506.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:48:42.414357: step 295700, loss = 0.17 (1331.7 examples/sec; 0.096 sec/batch)
2017-06-02 09:48:43.168617: step 295710, loss = 0.21 (1697.1 examples/sec; 0.075 sec/batch)
2017-06-02 09:48:44.010894: step 295720, loss = 0.20 (1519.7 examples/sec; 0.084 sec/batch)
2017-06-02 09:48:44.871120: step 295730, loss = 0.18 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:48:45.716557: step 295740, loss = 0.19 (1514.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:48:46.556822: step 295750, loss = 0.15 (1523.3 examples/sec; 0.084 sec/batch)
2017-06-02 09:48:47.423624: step 295760, loss = 0.19 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:48:48.291752: step 295770, loss = 0.17 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:48:49.175233: step 295780, loss = 0.23 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:48:50.033111: step 295790, loss = 0.20 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:48:50.987475: step 295800, loss = 0.19 (1341.2 examples/sec; 0.095 sec/batch)
2017-06-02 09:48:51.761339: step 295810, loss = 0.25 (1654.0 examples/sec; 0.077 sec/batch)
2017-06-02 09:48:52.633348: step 295820, loss = 0.20 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:48:53.481042: step 295830, loss = 0.27 (1510.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:48:54.339704: step 295840, loss = 0.19 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:48:55.202048: step 295850, loss = 0.24 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:48:56.063062: step 295860, loss = 0.25 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:48:56.929722: step 295870, loss = 0.17 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:48:57.786203: step 295880, loss = 0.19 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:48:58.642850: step 295890, loss = 0.21 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:48:59.611048: step 295900, loss = 0.21 (1322.0 examples/sec; 0.097 sec/batch)
2017-06-02 09:49:00.382010: step 295910, loss = 0.20 (1660.3 examples/sec; 0.077 sec/batch)
2017-06-02 09:49:01.249492: step 295920, loss = 0.21 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:49:02.120909: step 295930, loss = 0.23 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:49:03.003412: step 295940, loss = 0.19 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:49:03.856797: step 295950, loss = 0.20 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:49:04.736363: step 295960, loss = 0.16 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:49:05.605122: step 295970, loss = 0.21 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:49:06.451291: step 295980, loss = 0.20 (1512.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:49:07.299475: step 295990, loss = 0.15 (1509.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:49:08.255269: step 296000, loss = 0.22 (1339.2 examples/sec; 0.096 sec/batch)
2017-06-02 09:49:09.041406: step 296010, loss = 0.18 (1628.2 examples/sec; 0.079 sec/batch)
2017-06-02 09:49:09.901152: step 296020, loss = 0.22 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:49:10.772930: step 296030, loss = 0.22 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:49:11.655863: step 296040, loss = 0.16 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:49:12.528952: step 296050, loss = 0.19 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:49:13.402180: step 296060, loss = 0.23 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:49:14.249994: step 296070, loss = 0.25 (1509.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:49:15.108515: step 296080, loss = 0.20 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:49:15.966147: step 296090, loss = 0.18 (1492.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:49:16.911421: step 296100, loss = 0.20 (1354.1 examples/sec; 0.095 sec/batch)
2017-06-02 09:49:17.672346: step 296110, loss = 0.21 (1682.2 examples/sec; 0.076 sec/batch)
2017-06-02 09:49:18.531990: step 296120, loss = 0.19 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:49:19.396705: step 296130, loss = 0.15 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:49:20.244332: step 296140, loss = 0.22 (1510.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:49:21.090172: step 296150, loss = 0.23 (1513.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:49:21.931587: step 296160, loss = 0.21 (1521.3 examples/sec; 0.084 sec/batch)
2017-06-02 09:49:22.815182: step 296170, loss = 0.25 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:49:23.678354: step 296180, loss = 0.19 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:49:24.518749: step 296190, loss = 0.21 (1523.1 examples/sec; 0.084 sec/batch)
2017-06-02 09:49:25.494738: step 296200, loss = 0.18 (1311.5 examples/sec; 0.098 sec/batch)
2017-06-02 09:49:26.269077: step 296210, loss = 0.21 (1653.0 examples/sec; 0.077 sec/batch)
2017-06-02 09:49:27.156238: step 296220, loss = 0.18 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:49:28.019468: step 296230, loss = 0.25 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:49:28.920256: step 296240, loss = 0.17 (1421.0 examples/sec; 0.090 sec/batch)
2017-06-02 09:49:29.791089: step 296250, loss = 0.19 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:49:30.679268: step 296260, loss = 0.19 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:49:31.546543: step 296270, loss = 0.17 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:49:32.419695: step 296280, loss = 0.20 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:49:33.285968: step 296290, loss = 0.24 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:49:34.232048: step 296300, loss = 0.20 (1352.9 examples/sec; 0.095 sec/batch)
2017-06-02 09:49:34.972303: step 296310, loss = 0.22 (1729.1 examples/sec; 0.074 sec/batch)
2017-06-02 09:49:35.826482: step 296320, loss = 0.17 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:49:36.708183: step 296330, loss = 0.20 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:49:37.583708: step 296340, loss = 0.18 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:49:38.465822: step 296350, loss = 0.23 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:49:39.327035: step 296360, loss = 0.21 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:49:40.187503: step 296370, loss = 0.20 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:49:41.066130: step 296380, loss = 0.15 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:49:41.917704: step 296390, loss = 0.24 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:49:42.859775: step 296400, loss = 0.20 (1358.7 examples/sec; 0.094 sec/batch)
2017-06-02 09:49:43.624435: step 296410, loss = 0.26 (1673.9 examples/sec; 0.076 sec/batch)
2017-06-02 09:49:44.483242: step 296420, loss = 0.24 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:49:45.345777: step 296430, loss = 0.20 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:49:46.222877: step 296440, loss = 0.18 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:49:47.089441: step 296450, loss = 0.17 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:49:47.933872: step 296460, loss = 0.17 (1515.8 examples/sec; 0.084 sec/batch)
2017-06-02 09:49:48.792129: step 296470, loss = 0.19 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:49:49.653937: step 296480, loss = 0.20 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:49:50.514724: step 296490, loss = 0.19 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:49:51.483856: step 296500, loss = 0.18 (1320.8 examples/sec; 0.097 sec/batch)
2017-06-02 09:49:52.247480: step 296510, loss = 0.18 (1676.2 examples/sec; 0.076 sec/batch)
2017-06-02 09:49:53.211215: step 296520, loss = 0.18 (1328.2 examples/sec; 0.096 sec/batch)
2017-06-02 09:49:54.057288: step 296530, loss = 0.18 (1512.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:49:54.930178: step 296540, loss = 0.17 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:49:55.781641: step 296550, loss = 0.19 (1503.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:49:56.631224: step 296560, loss = 0.22 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:49:57.486067: step 296570, loss = 0.18 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:49:58.335755: step 296580, loss = 0.21 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:49:59.168835: step 296590, loss = 0.17 (1536.4 examples/sec; 0.083 sec/batch)
2017-06-02 09:50:00.148559: step 296600, loss = 0.25 (1306.5 examples/sec; 0.098 sec/batch)
2017-06-02 09:50:00.927390: step 296610, loss = 0.25 (1643.5 examples/sec; 0.078 sec/batch)
2017-06-02 09:50:01.772029: step 296620, loss = 0.26 (1515.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:50:02.660540: step 296630, loss = 0.17 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:50:03.539633: step 296640, loss = 0.20 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:50:04.382315: step 296650, loss = 0.24 (1519.0 examples/sec; 0.084 sec/batch)
2017-06-02 09:50:05.227871: step 296660, loss = 0.21 (1513.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:50:06.071229: step 296670, loss = 0.16 (1517.7 examples/sec; 0.084 sec/batch)
2017-06-02 09:50:06.941887: step 296680, loss = 0.22 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:50:07.802053: step 296690, loss = 0.21 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:50:08.773517: step 296700, loss = 0.21 (1317.6 examples/sec; 0.097 sec/batch)
2017-06-02 09:50:09.522370: step 296710, loss = 0.24 (1709.3 examples/sec; 0.075 sec/batch)
2017-06-02 09:50:10.378759: step 296720, loss = 0.18 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:50:11.236160: step 296730, loss = 0.18 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:50:12.114565: step 296740, loss = 0.18 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:50:12.986999: step 296750, loss = 0.23 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:50:13.830337: step 296760, loss = 0.14 (1517.8 examples/sec; 0.084 sec/batch)
2017-06-02 09:50:14.724135: step 296770, loss = 0.21 (1432.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:50:15.582923: step 296780, loss = 0.17 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:50:16.438129: step 296790, loss = 0.20 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:50:17.404709: step 296800, loss = 0.16 (1324.2 examples/sec; 0.097 sec/batch)
2017-06-02 09:50:18.169853: step 296810, loss = 0.20 (1672.9 examples/sec; 0.077 sec/batch)
2017-06-02 09:50:19.032720: step 296820, loss = 0.18 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:50:19.895986: step 296830, loss = 0.27 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:50:20.729373: step 296840, loss = 0.22 (1535.9 examples/sec; 0.083 sec/batch)
2017-06-02 09:50:21.605097: step 296850, loss = 0.20 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:50:22.455099: step 296860, loss = 0.21 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:50:23.320076: step 296870, loss = 0.22 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:50:24.193373: step 296880, loss = 0.19 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:50:25.048256: step 296890, loss = 0.17 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:50:26.003302: step 296900, loss = 0.19 (1340.3 examples/sec; 0.096 sec/batch)
2017-06-02 09:50:26.755727: step 296910, loss = 0.17 (1701.2 examples/sec; 0.075 sec/batch)
2017-06-02 09:50:27.607140: step 296920, loss = 0.24 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:50:28.463032: step 296930, loss = 0.21 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:50:29.332278: step 296940, loss = 0.19 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:50:30.171442: step 296950, loss = 0.20 (1525.3 examples/sec; 0.084 sec/batch)
2017-06-02 09:50:31.055673: step 296960, loss = 0.18 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:50:31.923711: step 296970, loss = 0.18 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:50:32.796110: step 296980, loss = 0.18 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:50:33.655272: step 296990, loss = 0.17 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:50:34.649440: step 297000, loss = 0.21 (1287.5 examples/sec; 0.099 sec/batch)
2017-06-02 09:50:35.413175: step 297010, loss = 0.26 (1676.0 examples/sec; 0.076 sec/batch)
2017-06-02 09:50:36.259825: step 297020, loss = 0.18 (1511.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:50:37.137582: step 297030, loss = 0.17 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:50:38.038676: step 297040, loss = 0.19 (1420.5 examples/sec; 0.090 sec/batch)
2017-06-02 09:50:38.889777: step 297050, loss = 0.18 (1503.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:50:39.772378: step 297060, loss = 0.22 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:50:40.625897: step 297070, loss = 0.20 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:50:41.514931: step 297080, loss = 0.17 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 09:50:42.373634: step 297090, loss = 0.17 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:50:43.328414: step 297100, loss = 0.16 (1340.6 examples/sec; 0.095 sec/batch)
2017-06-02 09:50:44.095034: step 297110, loss = 0.20 (1669.7 examples/sec; 0.077 sec/batch)
2017-06-02 09:50:44.948782: step 297120, loss = 0.19 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:50:45.827965: step 297130, loss = 0.20 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:50:46.655840: step 297140, loss = 0.18 (1546.2 examples/sec; 0.083 sec/batch)
2017-06-02 09:50:47.524330: step 297150, loss = 0.25 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:50:48.377314: step 297160, loss = 0.17 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:50:49.242081: step 297170, loss = 0.29 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:50:50.120567: step 297180, loss = 0.22 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:50:51.009534: step 297190, loss = 0.21 (1439.9 examples/sec; 0.089 sec/batch)
2017-06-02 09:50:51.970122: step 297200, loss = 0.23 (1332.5 examples/sec; 0.096 sec/batch)
2017-06-02 09:50:52.736162: step 297210, loss = 0.17 (1671.0 examples/sec; 0.077 sec/batch)
2017-06-02 09:50:53.601125: step 297220, loss = 0.17 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:50:54.439117: step 297230, loss = 0.20 (1527.5 examples/sec; 0.084 sec/batch)
2017-06-02 09:50:55.324192: step 297240, loss = 0.26 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:50:56.206427: step 297250, loss = 0.19 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:50:57.058560: step 297260, loss = 0.19 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:50:57.919212: step 297270, loss = 0.18 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:50:58.779302: step 297280, loss = 0.21 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:50:59.648839: step 297290, loss = 0.19 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:51:00.599633: step 297300, loss = 0.21 (1346.2 examples/sec; 0.095 sec/batch)
2017-06-02 09:51:01.379383: step 297310, loss = 0.20 (1641.6 examples/sec; 0.078 sec/batch)
2017-06-02 09:51:02.240913: step 297320, loss = 0.22 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:51:03.109589: step 297330, loss = 0.26 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:51:03.968728: step 297340, loss = 0.18 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:51:04.840405: step 297350, loss = 0.25 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:51:05.703883: step 297360, loss = 0.16 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:51:06.564157: step 297370, loss = 0.18 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:51:07.438703: step 297380, loss = 0.19 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:51:08.293723: step 297390, loss = 0.19 (1497.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:51:09.283477: step 297400, loss = 0.16 (1293.2 examples/sec; 0.099 sec/batch)
2017-06-02 09:51:10.052333: step 297410, loss = 0.20 (1664.8 examples/sec; 0.077 sec/batch)
2017-06-02 09:51:10.922191: step 297420, loss = 0.23 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:51:11.789863: step 297430, loss = 0.16 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:51:12.679079: step 297440, loss = 0.23 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:51:13.548851: step 297450, loss = 0.16 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:51:14.430964: step 297460, loss = 0.21 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:51:15.304288: step 297470, loss = 0.17 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:51:16.182447: step 297480, loss = 0.19 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:51:17.048249: step 297490, loss = 0.19 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:51:18.027152: step 297500, loss = 0.17 (1307.6 examples/sec; 0.098 sec/batch)
2017-06-02 09:51:18.776333: step 297510, loss = 0.22 (1708.5 examples/sec; 0.075 sec/batch)
2017-06-02 09:51:19.649543: step 297520, loss = 0.31 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:51:20.518678: step 297530, loss = 0.18 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:51:21.388000: step 297540, loss = 0.18 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:51:22.246483: step 297550, loss = 0.21 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:51:23.113346: step 297560, loss = 0.24 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:51:23.974377: step 297570, loss = 0.18 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:51:24.824246: step 297580, loss = 0.20 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:51:25.688425: step 297590, loss = 0.15 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:51:26.662198: step 297600, loss = 0.25 (1314.5 examples/sec; 0.097 sec/batch)
2017-06-02 09:51:27.451865: step 297610, loss = 0.20 (1620.9 examples/sec; 0.079 sec/batch)
2017-06-02 09:51:28.340739: step 297620, loss = 0.18 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:51:29.204403: step 297630, loss = 0.21 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:51:30.071589: step 297640, loss = 0.22 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:51:30.956003: step 297650, loss = 0.19 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:51:31.817349: step 297660, loss = 0.22 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:51:32.685607: step 297670, loss = 0.24 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:51:33.529808: step 297680, loss = 0.19 (1516.2 examples/sec; 0.084 sec/batch)
2017-06-02 09:51:34.368215: step 297690, loss = 0.19 (1526.7 examples/sec; 0.084 sec/batch)
2017-06-02 09:51:35.349252: step 297700, loss = 0.16 (1304.7 examples/sec; 0.098 sec/batch)
2017-06-02 09:51:36.135018: step 297710, loss = 0.16 (1629.0 examples/sec; 0.079 sec/batch)
2017-06-02 09:51:36.997252: step 297720, loss = 0.18 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:51:37.865147: step 297730, loss = 0.16 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:51:38.730761: step 297740, loss = 0.19 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:51:39.560853: step 297750, loss = 0.15 (1542.0 examples/sec; 0.083 sec/batch)
2017-06-02 09:51:40.422436: step 297760, loss = 0.23 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:51:41.293229: step 297770, loss = 0.15 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:51:42.160509: step 297780, loss = 0.20 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:51:43.044831: step 297790, loss = 0.24 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:51:44.014974: step 297800, loss = 0.20 (1319.4 examples/sec; 0.097 sec/batch)
2017-06-02 09:51:44.788147: step 297810, loss = 0.17 (1655.5 examples/sec; 0.077 sec/batch)
2017-06-02 09:51:45.646626: step 297820, loss = 0.17 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:51:46.511904: step 297830, loss = 0.15 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:51:47.378898: step 297840, loss = 0.16 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:51:48.246439: step 297850, loss = 0.27 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:51:49.079309: step 297860, loss = 0.26 (1536.9 examples/sec; 0.083 sec/batch)
2017-06-02 09:51:49.953237: step 297870, loss = 0.18 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:51:50.795104: step 297880, loss = 0.19 (1520.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:51:51.644811: step 297890, loss = 0.21 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:51:52.596683: step 297900, loss = 0.25 (1344.7 examples/sec; 0.095 sec/batch)
2017-06-02 09:51:53.358750: step 297910, loss = 0.27 (1679.6 examples/sec; 0.076 sec/batch)
2017-06-02 09:51:54.205445: step 297920, loss = 0.15 (1511.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:51:55.061216: step 297930, loss = 0.20 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:51:55.905319: step 297940, loss = 0.20 (1516.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:51:56.739113: step 297950, loss = 0.22 (1535.1 examples/sec; 0.083 sec/batch)
2017-06-02 09:51:57.575948: step 297960, loss = 0.22 (1529.6 examples/sec; 0.084 sec/batch)
2017-06-02 09:51:58.416057: step 297970, loss = 0.24 (1523.6 examples/sec; 0.084 sec/batch)
2017-06-02 09:51:59.278110: step 297980, loss = 0.22 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:52:00.116292: step 297990, loss = 0.18 (1527.1 examples/sec; 0.084 sec/batch)
2017-06-02 09:52:01.086963: step 298000, loss = 0.16 (1318.6 examples/sec; 0.097 sec/batch)
2017-06-02 09:52:01.804404: step 298010, loss = 0.28 (1784.2 examples/sec; 0.072 sec/batch)
2017-06-02 09:52:02.654482: step 298020, loss = 0.24 (1505.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:52:03.528854: step 298030, loss = 0.17 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:52:04.391815: step 298040, loss = 0.20 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:52:05.296445: step 298050, loss = 0.17 (1415.0 examples/sec; 0.090 sec/batch)
2017-06-02 09:52:06.152516: step 298060, loss = 0.15 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:52:07.056416: step 298070, loss = 0.21 (1416.1 examples/sec; 0.090 sec/batch)
2017-06-02 09:52:07.944672: step 298080, loss = 0.24 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 09:52:08.853829: step 298090, loss = 0.21 (1407.9 examples/sec; 0.091 sec/batch)
2017-06-02 09:52:09.828882: step 298100, loss = 0.18 (1312.8 examples/sec; 0.098 sec/batch)
2017-06-02 09:52:10.597241: step 298110, loss = 0.18 (1665.9 examples/sec; 0.077 sec/batch)
2017-06-02 09:52:11.480132: step 298120, loss = 0.20 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:52:12.350791: step 298130, loss = 0.22 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:52:13.224605: step 298140, loss = 0.20 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:52:14.103866: step 298150, loss = 0.19 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:52:14.967842: step 298160, loss = 0.18 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:52:15.837281: step 298170, loss = 0.19 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:52:16.722770: step 298180, loss = 0.27 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:52:17.602750: step 298190, loss = 0.19 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:52:18.586106: step 298200, loss = 0.20 (1301.7 examples/sec; 0.098 sec/batch)
2017-06-02 09:52:19.330550: step 298210, loss = 0.24 (1719.4 examples/sec; 0.074 sec/batch)
2017-06-02 09:52:20.216043: step 298220, loss = 0.21 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:52:21.090785: step 298230, loss = 0.20 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:52:21.964806: step 298240, loss = 0.24 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:52:22.848561: step 298250, loss = 0.20 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:52:23.721278: step 298260, loss = 0.23 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:52:24.551999: step 298270, loss = 0.21 (1540.9 examples/sec; 0.083 sec/batch)
2017-06-02 09:52:25.414323: step 298280, loss = 0.19 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:52:26.287500: step 298290, loss = 0.15 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:52:27.314014: step 298300, loss = 0.20 (1246.9 examples/sec; 0.103 sec/batch)
2017-06-02 09:52:28.023957: step 298310, loss = 0.22 (1803.0 examples/sec; 0.071 sec/batch)
2017-06-02 09:52:28.850901: step 298320, loss = 0.21 (1547.9 examples/sec; 0.083 sec/batch)
2017-06-02 09:52:29.723776: step 298330, loss = 0.18 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:52:30.577093: step 298340, loss = 0.15 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:52:31.440910: step 298350, loss = 0.16 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:52:32.274086: step 298360, loss = 0.18 (1536.3 examples/sec; 0.083 sec/batch)
2017-06-02 09:52:33.155169: step 298370, loss = 0.19 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:52:34.044616: step 298380, loss = 0.15 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:52:34.883341: step 298390, loss = 0.26 (1526.1 examples/sec; 0.084 sec/batch)
2017-06-02 09:52:35.861952: step 298400, loss = 0.22 (1308.0 examples/sec; 0.098 sec/batch)
2017-06-02 09:52:36.620200: step 298410, loss = 0.16 (1688.1 examples/sec; 0.076 sec/batch)
2017-06-02 09:52:37.502034: step 298420, loss = 0.20 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:52:38.362594: step 298430, loss = 0.17 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:52:39.232289: step 298440, loss = 0.21 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:52:40.086671: step 298450, loss = 0.17 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:52:40.938202: step 298460, loss = 0.22 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:52:41.791097: step 298470, loss = 0.18 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:52:42.656797: step 298480, loss = 0.26 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:52:43.501779: step 298490, loss = 0.26 (1514.8 examples/sec; 0.084 sec/batch)
2017-06-02 09:52:44.480064: step 298500, loss = 0.27 (1308.4 examples/sec; 0.098 sec/batch)
2017-06-02 09:52:45.260908: step 298510, loss = 0.21 (1639.2 examples/sec; 0.078 sec/batch)
2017-06-02 09:52:46.147573: step 298520, loss = 0.16 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 09:52:47.000241: step 298530, loss = 0.22 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:52:47.890464: step 298540, loss = 0.21 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:52:48.764227: step 298550, loss = 0.23 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:52:49.640171: step 298560, loss = 0.20 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:52:50.515875: step 298570, loss = 0.27 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:52:51.376204: step 298580, loss = 0.19 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:52:52.219899: step 298590, loss = 0.19 (1517.1 examples/sec; 0.084 sec/batch)
2017-06-02 09:52:53.228287: step 298600, loss = 0.19 (1269.4 examples/sec; 0.101 sec/batch)
2017-06-02 09:52:53.934176: step 298610, loss = 0.19 (1813.3 examples/sec; 0.071 sec/batch)
2017-06-02 09:52:54.815804: step 298620, loss = 0.25 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:52:55.674925: step 298630, loss = 0.22 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:52:56.531987: step 298640, loss = 0.21 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:52:57.410225: step 298650, loss = 0.20 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:52:58.255658: step 298660, loss = 0.16 (1514.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:52:59.105121: step 298670, loss = 0.20 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:52:59.967370: step 298680, loss = 0.21 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:53:00.844672: step 298690, loss = 0.21 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:53:01.837157: step 298700, loss = 0.16 (1289.7 examples/sec; 0.099 sec/batch)
2017-06-02 09:53:02.602887: step 298710, loss = 0.21 (1671.6 examples/sec; 0.077 sec/batch)
2017-06-02 09:53:03.478278: step 298720, loss = 0.15 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:53:04.359023: step 298730, loss = 0.19 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:53:05.235823: step 298740, loss = 0.20 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:53:06.111144: step 298750, loss = 0.21 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:53:06.992706: step 298760, loss = 0.21 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:53:07.813759: step 298770, loss = 0.18 (1559.0 examples/sec; 0.082 sec/batch)
2017-06-02 09:53:08.655769: step 298780, loss = 0.18 (1520.2 examples/sec; 0.084 sec/batch)
2017-06-02 09:53:09.515426: step 298790, loss = 0.21 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:53:10.471773: step 298800, loss = 0.24 (1339.0 examples/sec; 0.096 sec/batch)
2017-06-02 09:53:11.240059: step 298810, loss = 0.18 (1665.1 examples/sec; 0.077 sec/batch)
2017-06-02 09:53:12.127032: step 298820, loss = 0.25 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:53:13.016387: step 298830, loss = 0.24 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:53:13.892749: step 298840, loss = 0.17 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:53:14.735515: step 298850, loss = 0.18 (1518.8 examples/sec; 0.084 sec/batch)
2017-06-02 09:53:15.581470: step 298860, loss = 0.23 (1513.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:53:16.466779: step 298870, loss = 0.23 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:53:17.346740: step 298880, loss = 0.23 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:53:18.212564: step 298890, loss = 0.18 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:53:19.252458: step 298900, loss = 0.17 (1230.9 examples/sec; 0.104 sec/batch)
2017-06-02 09:53:19.975479: step 298910, loss = 0.19 (1770.3 examples/sec; 0.072 sec/batch)
2017-06-02 09:53:20.844518: step 298920, loss = 0.17 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:53:21.708421: step 298930, loss = 0.16 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:53:22.578371: step 298940, loss = 0.20 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:53:23.428302: step 298950, loss = 0.27 (1506.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:53:24.278663: step 298960, loss = 0.18 (1505.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:53:25.107049: step 298970, loss = 0.24 (1545.2 examples/sec; 0.083 sec/batch)
2017-06-02 09:53:25.984822: step 298980, loss = 0.20 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:53:26.839935: step 298990, loss = 0.24 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:53:27.848089: step 299000, loss = 0.23 (1269.7 examples/sec; 0.101 sec/batch)
2017-06-02 09:53:28.600046: step 299010, loss = 0.16 (1702.2 examples/sec; 0.075 sec/batch)
2017-06-02 09:53:29.471557: step 299020, loss = 0.22 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:53:30.321251: step 299030, loss = 0.23 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:53:31.197836: step 299040, loss = 0.17 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:53:32.070906: step 299050, loss = 0.17 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:53:32.958234: step 299060, loss = 0.19 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:53:33.841106: step 299070, loss = 0.25 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:53:34.707614: step 299080, loss = 0.24 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:53:35.577479: step 299090, loss = 0.19 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:53:36.538316: step 299100, loss = 0.17 (1332.2 examples/sec; 0.096 sec/batch)
2017-06-02 09:53:37.322298: step 299110, loss = 0.21 (1632.7 examples/sec; 0.078 sec/batch)
2017-06-02 09:53:38.165304: step 299120, loss = 0.20 (1518.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:53:39.010015: step 299130, loss = 0.22 (1515.3 examples/sec; 0.084 sec/batch)
2017-06-02 09:53:39.865430: step 299140, loss = 0.17 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:53:40.729640: step 299150, loss = 0.19 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:53:41.582384: step 299160, loss = 0.16 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:53:42.441638: step 299170, loss = 0.21 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:53:43.306885: step 299180, loss = 0.21 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:53:44.185911: step 299190, loss = 0.19 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:53:45.151772: step 299200, loss = 0.16 (1325.2 examples/sec; 0.097 sec/batch)
2017-06-02 09:53:45.929569: step 299210, loss = 0.19 (1645.7 examples/sec; 0.078 sec/batch)
2017-06-02 09:53:46.793295: step 299220, loss = 0.23 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:53:47.659465: step 299230, loss = 0.15 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:53:48.511309: step 299240, loss = 0.19 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:53:49.378254: step 299250, loss = 0.18 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:53:50.229979: step 299260, loss = 0.19 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:53:51.108081: step 299270, loss = 0.22 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:53:51.953227: step 299280, loss = 0.25 (1514.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:53:52.806219: step 299290, loss = 0.25 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:53:53.803740: step 299300, loss = 0.20 (1283.2 examples/sec; 0.100 sec/batch)
2017-06-02 09:53:54.555614: step 299310, loss = 0.18 (1702.4 examples/sec; 0.075 sec/batch)
2017-06-02 09:53:55.415400: step 299320, loss = 0.22 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:53:56.291876: step 299330, loss = 0.17 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:53:57.166948: step 299340, loss = 0.23 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:53:58.043367: step 299350, loss = 0.22 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:53:58.908145: step 299360, loss = 0.23 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:53:59.729408: step 299370, loss = 0.19 (1558.6 examples/sec; 0.082 sec/batch)
2017-06-02 09:54:00.589518: step 299380, loss = 0.19 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:54:01.439444: step 299390, loss = 0.23 (1506.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:54:02.407988: step 299400, loss = 0.22 (1321.6 examples/sec; 0.097 sec/batch)
2017-06-02 09:54:03.192253: step 299410, loss = 0.22 (1632.1 examples/sec; 0.078 sec/batch)
2017-06-02 09:54:04.073531: step 299420, loss = 0.21 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:54:04.920898: step 299430, loss = 0.16 (1510.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:54:05.801531: step 299440, loss = 0.23 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:54:06.682549: step 299450, loss = 0.26 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:54:07.557389: step 299460, loss = 0.20 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:54:08.426193: step 299470, loss = 0.18 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:54:09.301315: step 299480, loss = 0.22 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:54:10.187692: step 299490, loss = 0.18 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:54:11.157525: step 299500, loss = 0.20 (1319.8 examples/sec; 0.097 sec/batch)
2017-06-02 09:54:11.904653: step 299510, loss = 0.19 (1713.2 examples/sec; 0.075 sec/batch)
2017-06-02 09:54:12.754894: step 299520, loss = 0.24 (1505.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:54:13.602015: step 299530, loss = 0.19 (1511.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:54:14.474256: step 299540, loss = 0.22 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:54:15.354744: step 299550, loss = 0.17 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:54:16.212745: step 299560, loss = 0.16 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:54:17.066986: step 299570, loss = 0.16 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:54:17.948600: step 299580, loss = 0.25 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:54:18.796631: step 299590, loss = 0.15 (1509.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:54:19.734843: step 299600, loss = 0.18 (1364.3 examples/sec; 0.094 sec/batch)
2017-06-02 09:54:20.497678: step 299610, loss = 0.26 (1677.9 examples/sec; 0.076 sec/batch)
2017-06-02 09:54:21.342901: step 299620, loss = 0.19 (1514.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:54:22.197375: step 299630, loss = 0.17 (1498.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:54:23.057221: step 299640, loss = 0.18 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:54:23.899077: step 299650, loss = 0.18 (1520.5 examples/sec; 0.084 sec/batch)
2017-06-02 09:54:24.768903: step 299660, loss = 0.20 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:54:25.627911: step 299670, loss = 0.22 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:54:26.462144: step 299680, loss = 0.19 (1534.4 examples/sec; 0.083 sec/batch)
2017-06-02 09:54:27.312944: step 299690, loss = 0.26 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:54:28.278572: step 299700, loss = 0.18 (1325.6 examples/sec; 0.097 sec/batch)
2017-06-02 09:54:29.021792: step 299710, loss = 0.20 (1722.2 examples/sec; 0.074 sec/batch)
2017-06-02 09:54:29.876566: step 299720, loss = 0.17 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:54:30.723903: step 299730, loss = 0.16 (1510.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:54:31.575242: step 299740, loss = 0.17 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:54:32.439883: step 299750, loss = 0.18 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:54:33.283602: step 299760, loss = 0.18 (1517.1 examples/sec; 0.084 sec/batch)
2017-06-02 09:54:34.173000: step 299770, loss = 0.21 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:54:35.029254: step 299780, loss = 0.20 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:54:35.890062: step 299790, loss = 0.15 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:54:36.831236: step 299800, loss = 0.22 (1360.0 examples/sec; 0.094 sec/batch)
2017-06-02 09:54:37.576638: step 299810, loss = 0.15 (1717.2 examples/sec; 0.075 sec/batch)
2017-06-02 09:54:38.442808: step 299820, loss = 0.21 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:54:39.300547: step 299830, loss = 0.22 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:54:40.125587: step 299840, loss = 0.18 (1551.4 examples/sec; 0.083 sec/batch)
2017-06-02 09:54:40.977214: step 299850, loss = 0.18 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:54:41.859018: step 299860, loss = 0.20 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:54:42.727000: step 299870, loss = 0.24 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:54:43.586547: step 299880, loss = 0.18 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:54:44.421625: step 299890, loss = 0.21 (1532.8 examples/sec; 0.084 sec/batch)
2017-06-02 09:54:45.370469: step 299900, loss = 0.14 (1349.0 examples/sec; 0.095 sec/batch)
2017-06-02 09:54:46.125400: step 299910, loss = 0.20 (1695.5 examples/sec; 0.075 sec/batch)
2017-06-02 09:54:46.982317: step 299920, loss = 0.14 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:54:47.845240: step 299930, loss = 0.22 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:54:48.715188: step 299940, loss = 0.16 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:54:49.565644: step 299950, loss = 0.20 (1505.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:54:50.416092: step 299960, loss = 0.17 (1505.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:54:51.299197: step 299970, loss = 0.17 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:54:52.161312: step 299980, loss = 0.19 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:54:53.048407: step 299990, loss = 0.18 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 09:54:54.056969: step 300000, loss = 0.22 (1269.1 examples/sec; 0.101 sec/batch)
2017-06-02 09:54:54.773299: step 300010, loss = 0.18 (1786.9 examples/sec; 0.072 sec/batch)
2017-06-02 09:54:55.624714: step 300020, loss = 0.17 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:54:56.490040: step 300030, loss = 0.21 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:54:57.335444: step 300040, loss = 0.19 (1514.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:54:58.196448: step 300050, loss = 0.20 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:54:59.075487: step 300060, loss = 0.25 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:54:59.913140: step 300070, loss = 0.20 (1528.1 examples/sec; 0.084 sec/batch)
2017-06-02 09:55:00.762740: step 300080, loss = 0.15 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:55:01.623531: step 300090, loss = 0.15 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:55:02.601873: step 300100, loss = 0.21 (1308.3 examples/sec; 0.098 sec/batch)
2017-06-02 09:55:03.383075: step 300110, loss = 0.18 (1639.2 examples/sec; 0.078 sec/batch)
2017-06-02 09:55:04.266656: step 300120, loss = 0.19 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:55:05.188096: step 300130, loss = 0.17 (1389.1 examples/sec; 0.092 sec/batch)
2017-06-02 09:55:06.064250: step 300140, loss = 0.15 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:55:06.958221: step 300150, loss = 0.21 (1431.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:55:07.820760: step 300160, loss = 0.21 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:55:08.691559: step 300170, loss = 0.20 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:55:09.560697: step 300180, loss = 0.19 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:55:10.449710: step 300190, loss = 0.20 (1439.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:55:11.498159: step 300200, loss = 0.21 (1220.9 examples/sec; 0.105 sec/batch)
2017-06-02 09:55:12.220279: step 300210, loss = 0.17 (1772.6 examples/sec; 0.072 sec/batch)
2017-06-02 09:55:13.071938: step 300220, loss = 0.18 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:55:13.961217: step 300230, loss = 0.20 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:55:14.841728: step 300240, loss = 0.22 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:55:15.709956: step 300250, loss = 0.22 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:55:16.590977: step 300260, loss = 0.20 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:55:17.449385: step 300270, loss = 0.22 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:55:18.336237: step 300280, loss = 0.20 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 09:55:19.210766: step 300290, loss = 0.21 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:55:20.160685: step 300300, loss = 0.20 (1347.5 examples/sec; 0.095 sec/batch)
2017-06-02 09:55:20.934315: step 300310, loss = 0.19 (1654.5 examples/sec; 0.077 sec/batch)
2017-06-02 09:55:21.805105: step 300320, loss = 0.23 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:55:22.666478: step 300330, loss = 0.17 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:55:23.519812: step 300340, loss = 0.17 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:55:24.419486: step 300350, loss = 0.22 (1422.7 examples/sec; 0.090 sec/batch)
2017-06-02 09:55:25.293266: step 300360, loss = 0.20 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:55:26.155615: step 300370, loss = 0.19 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:55:27.027589: step 300380, loss = 0.21 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:55:27.883931: step 300390, loss = 0.18 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:55:28.843784: step 300400, loss = 0.20 (1333.5 examples/sec; 0.096 sec/batch)
2017-06-02 09:55:29.619387: step 300410, loss = 0.18 (1650.3 examples/sec; 0.078 sec/batch)
2017-06-02 09:55:30.517740: step 300420, loss = 0.15 (1424.8 examples/sec; 0.090 sec/batch)
2017-06-02 09:55:31.385835: step 300430, loss = 0.23 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:55:32.269770: step 300440, loss = 0.22 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:55:33.140569: step 300450, loss = 0.18 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:55:34.008482: step 300460, loss = 0.19 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:55:34.874556: step 300470, loss = 0.26 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:55:35.756138: step 300480, loss = 0.20 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:55:36.609288: step 300490, loss = 0.21 (1500.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:55:37.584282: step 300500, loss = 0.18 (1312.8 examples/sec; 0.097 sec/batch)
2017-06-02 09:55:38.351068: step 300510, loss = 0.21 (1669.3 examples/sec; 0.077 sec/batch)
2017-06-02 09:55:39.242956: step 300520, loss = 0.18 (1435.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:55:40.124532: step 300530, loss = 0.22 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:55:41.010523: step 300540, loss = 0.20 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 09:55:41.863249: step 300550, loss = 0.24 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:55:42.713212: step 300560, loss = 0.15 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:55:43.582536: step 300570, loss = 0.16 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:55:44.474263: step 300580, loss = 0.18 (1435.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:55:45.317020: step 300590, loss = 0.26 (1518.8 examples/sec; 0.084 sec/batch)
2017-06-02 09:55:46.314153: step 300600, loss = 0.21 (1283.7 examples/sec; 0.100 sec/batch)
2017-06-02 09:55:47.047936: step 300610, loss = 0.18 (1744.4 examples/sec; 0.073 sec/batch)
2017-06-02 09:55:47.891299: step 300620, loss = 0.18 (1517.7 examples/sec; 0.084 sec/batch)
2017-06-02 09:55:48.745989: step 300630, loss = 0.21 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:55:49.615857: step 300640, loss = 0.16 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:55:50.466221: step 300650, loss = 0.19 (1505.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:55:51.337957: step 300660, loss = 0.23 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:55:52.199209: step 300670, loss = 0.20 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:55:53.060223: step 300680, loss = 0.22 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:55:53.910206: step 300690, loss = 0.20 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:55:54.867300: step 300700, loss = 0.23 (1337.4 examples/sec; 0.096 sec/batch)
2017-06-02 09:55:55.631478: step 300710, loss = 0.20 (1675.0 examples/sec; 0.076 sec/batch)
2017-06-02 09:55:56.480956: step 300720, loss = 0.25 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:55:57.329489: step 300730, loss = 0.20 (1508.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:55:58.160337: step 300740, loss = 0.21 (1540.6 examples/sec; 0.083 sec/batch)
2017-06-02 09:55:59.029765: step 300750, loss = 0.15 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:55:59.881601: step 300760, loss = 0.19 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 09:56:00.770458: step 300770, loss = 0.26 (1440.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:56:01.631982: step 300780, loss = 0.15 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:56:02.484608: step 300790, loss = 0.23 (1501.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:56:03.432186: step 300800, loss = 0.23 (1350.7 examples/sec; 0.095 sec/batch)
2017-06-02 09:56:04.176055: step 300810, loss = 0.22 (1720.7 examples/sec; 0.074 sec/batch)
2017-06-02 09:56:05.027099: step 300820, loss = 0.16 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:56:05.884524: step 300830, loss = 0.19 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:56:06.771918: step 300840, loss = 0.17 (1442.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:56:07.634905: step 300850, loss = 0.19 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:56:08.501693: step 300860, loss = 0.19 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:56:09.361702: step 300870, loss = 0.19 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:56:10.229273: step 300880, loss = 0.18 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:56:11.078522: step 300890, loss = 0.20 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:56:12.025744: step 300900, loss = 0.15 (1351.3 examples/sec; 0.095 sec/batch)
2017-06-02 09:56:12.823665: step 300910, loss = 0.19 (1604.1 examples/sec; 0.080 sec/batch)
2017-06-02 09:56:13.720941: step 300920, loss = 0.16 (1426.5 examples/sec; 0.090 sec/batch)
2017-06-02 09:56:14.579075: step 300930, loss = 0.25 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:56:15.445727: step 300940, loss = 0.21 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:56:16.303292: step 300950, loss = 0.18 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:56:17.161305: step 300960, loss = 0.18 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:56:17.999432: step 300970, loss = 0.22 (1527.2 examples/sec; 0.084 sec/batch)
2017-06-02 09:56:18.876918: step 300980, loss = 0.19 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:56:19.737219: step 300990, loss = 0.20 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:56:20.705403: step 301000, loss = 0.18 (1322.1 examples/sec; 0.097 sec/batch)
2017-06-02 09:56:21.463796: step 301010, loss = 0.22 (1687.8 examples/sec; 0.076 sec/batch)
2017-06-02 09:56:22.325348: step 301020, loss = 0.18 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:56:23.190121: step 301030, loss = 0.17 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:56:24.066882: step 301040, loss = 0.18 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:56:24.937539: step 301050, loss = 0.18 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:56:25.801821: step 301060, loss = 0.22 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:56:26.669145: step 301070, loss = 0.20 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:56:27.567599: step 301080, loss = 0.23 (1424.7 examples/sec; 0.090 sec/batch)
2017-06-02 09:56:28.467453: step 301090, loss = 0.20 (1422.4 examples/sec; 0.090 sec/batch)
2017-06-02 09:56:29.481275: step 301100, loss = 0.16 (1262.6 examples/sec; 0.101 sec/batch)
2017-06-02 09:56:30.199509: step 301110, loss = 0.18 (1782.2 examples/sec; 0.072 sec/batch)
2017-06-02 09:56:31.080105: step 301120, loss = 0.22 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:56:31.940369: step 301130, loss = 0.20 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:56:32.811883: step 301140, loss = 0.19 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:56:33.692271: step 301150, loss = 0.17 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:56:34.523103: step 301160, loss = 0.21 (1540.6 examples/sec; 0.083 sec/batch)
2017-06-02 09:56:35.373135: step 301170, loss = 0.21 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:56:36.230551: step 301180, loss = 0.15 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:56:37.067985: step 301190, loss = 0.16 (1528.5 examples/sec; 0.084 sec/batch)
2017-06-02 09:56:38.022118: step 301200, loss = 0.24 (1341.5 examples/sec; 0.095 sec/batch)
2017-06-02 09:56:38.791003: step 301210, loss = 0.23 (1664.8 examples/sec; 0.077 sec/batch)
2017-06-02 09:56:39.664227: step 301220, loss = 0.21 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:56:40.530395: step 301230, loss = 0.14 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:56:41.385356: step 301240, loss = 0.20 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:56:42.230879: step 301250, loss = 0.16 (1513.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:56:43.071108: step 301260, loss = 0.24 (1523.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:56:43.947360: step 301270, loss = 0.18 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:56:44.807682: step 301280, loss = 0.18 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:56:45.686868: step 301290, loss = 0.17 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:56:46.673771: step 301300, loss = 0.20 (1297.0 examples/sec; 0.099 sec/batch)
2017-06-02 09:56:47.472540: step 301310, loss = 0.18 (1602.5 examples/sec; 0.080 sec/batch)
2017-06-02 09:56:48.334704: step 301320, loss = 0.21 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:56:49.203718: step 301330, loss = 0.21 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:56:50.077755: step 301340, loss = 0.22 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:56:50.978500: step 301350, loss = 0.17 (1421.0 examples/sec; 0.090 sec/batch)
2017-06-02 09:56:51.853358: step 301360, loss = 0.18 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:56:52.728056: step 301370, loss = 0.22 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:56:53.595420: step 301380, loss = 0.25 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:56:54.427061: step 301390, loss = 0.23 (1539.1 examples/sec; 0.083 sec/batch)
2017-06-02 09:56:55.444531: step 301400, loss = 0.19 (1258.0 examples/sec; 0.102 sec/batch)
2017-06-02 09:56:56.168183: step 301410, loss = 0.17 (1768.8 examples/sec; 0.072 sec/batch)
2017-06-02 09:56:57.038793: step 301420, loss = 0.21 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:56:57.894251: step 301430, loss = 0.25 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:56:58.746998: step 301440, loss = 0.19 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:56:59.594640: step 301450, loss = 0.18 (1510.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:57:00.456201: step 301460, loss = 0.22 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:57:01.327934: step 301470, loss = 0.22 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:57:02.190263: step 301480, loss = 0.20 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:57:03.063507: step 301490, loss = 0.18 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:57:04.036442: step 301500, loss = 0.22 (1315.6 examples/sec; 0.097 sec/batch)
2017-06-02 09:57:04.772595: step 301510, loss = 0.22 (1738.8 examples/sec; 0.074 sec/batch)
2017-06-02 09:57:05.645825: step 301520, loss = 0.20 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:57:06.491333: step 301530, loss = 0.17 (1513.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:57:07.403688: step 301540, loss = 0.28 (1402.9 examples/sec; 0.091 sec/batch)
2017-06-02 09:57:08.257530: step 301550, loss = 0.30 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:57:09.130320: step 301560, loss = 0.24 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:57:09.999521: step 301570, loss = 0.22 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:57:10.849941: step 301580, loss = 0.22 (1505.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:57:11.699131: step 301590, loss = 0.21 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:57:12.665235: step 301600, loss = 0.21 (1324.9 examples/sec; 0.097 sec/batch)
2017-06-02 09:57:13.441248: step 301610, loss = 0.23 (1649.4 examples/sec; 0.078 sec/batch)
2017-06-02 09:57:14.289237: step 301620, loss = 0.21 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:57:15.170754: step 301630, loss = 0.22 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:57:16.028572: step 301640, loss = 0.16 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:57:16.893008: step 301650, loss = 0.21 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:57:17.769777: step 301660, loss = 0.16 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:57:18.623946: step 301670, loss = 0.18 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:57:19.473147: step 301680, loss = 0.21 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:57:20.328052: step 301690, loss = 0.21 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:57:21.279272: step 301700, loss = 0.17 (1345.6 examples/sec; 0.095 sec/batch)
2017-06-02 09:57:22.056813: step 301710, loss = 0.26 (1646.2 examples/sec; 0.078 sec/batch)
2017-06-02 09:57:22.898600: step 301720, loss = 0.21 (1520.6 examples/sec; 0.084 sec/batch)
2017-06-02 09:57:23.778675: step 301730, loss = 0.22 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:57:24.627962: step 301740, loss = 0.21 (1507.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:57:25.501578: step 301750, loss = 0.17 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:57:26.363256: step 301760, loss = 0.15 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:57:27.211626: step 301770, loss = 0.17 (1508.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:57:28.068926: step 301780, loss = 0.17 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:57:28.925354: step 301790, loss = 0.20 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:57:29.897221: step 301800, loss = 0.17 (1317.0 examples/sec; 0.097 sec/batch)
2017-06-02 09:57:30.667759: step 301810, loss = 0.16 (1661.2 examples/sec; 0.077 sec/batch)
2017-06-02 09:57:31.543930: step 301820, loss = 0.17 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 09:57:32.425364: step 301830, loss = 0.18 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:57:33.304028: step 301840, loss = 0.23 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:57:34.187035: step 301850, loss = 0.17 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:57:35.045856: step 301860, loss = 0.16 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:57:35.883285: step 301870, loss = 0.20 (1528.5 examples/sec; 0.084 sec/batch)
2017-06-02 09:57:36.751189: step 301880, loss = 0.26 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:57:37.637520: step 301890, loss = 0.17 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 09:57:38.621239: step 301900, loss = 0.20 (1301.2 examples/sec; 0.098 sec/batch)
2017-06-02 09:57:39.371208: step 301910, loss = 0.16 (1706.8 examples/sec; 0.075 sec/batch)
2017-06-02 09:57:40.232676: step 301920, loss = 0.27 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:57:41.090489: step 301930, loss = 0.17 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:57:41.940400: step 301940, loss = 0.23 (1506.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:57:42.827762: step 301950, loss = 0.22 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 09:57:43.693539: step 301960, loss = 0.15 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:57:44.563623: step 301970, loss = 0.18 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:57:45.426809: step 301980, loss = 0.19 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:57:46.287608: step 301990, loss = 0.15 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:57:47.278926: step 302000, loss = 0.22 (1291.2 examples/sec; 0.099 sec/batch)
2017-06-02 09:57:48.059840: step 302010, loss = 0.20 (1639.1 examples/sec; 0.078 sec/batch)
2017-06-02 09:57:48.931838: step 302020, loss = 0.15 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:57:49.783937: step 302030, loss = 0.20 (1502.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:57:50.634989: step 302040, loss = 0.17 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 09:57:51.498711: step 302050, loss = 0.21 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 09:57:52.374686: step 302060, loss = 0.21 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:57:53.226996: step 302070, loss = 0.20 (1501.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:57:54.068147: step 302080, loss = 0.19 (1521.7 examples/sec; 0.084 sec/batch)
2017-06-02 09:57:54.932678: step 302090, loss = 0.20 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:57:55.908342: step 302100, loss = 0.29 (1311.9 examples/sec; 0.098 sec/batch)
2017-06-02 09:57:56.674686: step 302110, loss = 0.15 (1670.3 examples/sec; 0.077 sec/batch)
2017-06-02 09:57:57.515659: step 302120, loss = 0.21 (1522.1 examples/sec; 0.084 sec/batch)
2017-06-02 09:57:58.385836: step 302130, loss = 0.15 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:57:59.246187: step 302140, loss = 0.20 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:58:00.102487: step 302150, loss = 0.14 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:58:00.963303: step 302160, loss = 0.21 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:58:01.835233: step 302170, loss = 0.31 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:58:02.716308: step 302180, loss = 0.18 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:58:03.579019: step 302190, loss = 0.16 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:58:04.566433: step 302200, loss = 0.21 (1296.3 examples/sec; 0.099 sec/batch)
2017-06-02 09:58:05.327846: step 302210, loss = 0.19 (1681.1 examples/sec; 0.076 sec/batch)
2017-06-02 09:58:06.201079: step 302220, loss = 0.23 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:58:07.062389: step 302230, loss = 0.17 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:58:07.941145: step 302240, loss = 0.20 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:58:08.804709: step 302250, loss = 0.21 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:58:09.660179: step 302260, loss = 0.17 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:58:10.517866: step 302270, loss = 0.21 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:58:11.382565: step 302280, loss = 0.18 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:58:12.259285: step 302290, loss = 0.16 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:58:13.227836: step 302300, loss = 0.19 (1321.6 examples/sec; 0.097 sec/batch)
2017-06-02 09:58:14.012004: step 302310, loss = 0.19 (1632.3 examples/sec; 0.078 sec/batch)
2017-06-02 09:58:14.864598: step 302320, loss = 0.17 (1501.3 examples/sec; 0.085 sec/batch)
2017-06-02 09:58:15.742084: step 302330, loss = 0.19 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:58:16.635371: step 302340, loss = 0.25 (1432.9 examples/sec; 0.089 sec/batch)
2017-06-02 09:58:17.503601: step 302350, loss = 0.23 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:58:18.371276: step 302360, loss = 0.19 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:58:19.256973: step 302370, loss = 0.19 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 09:58:20.132508: step 302380, loss = 0.19 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 09:58:20.997151: step 302390, loss = 0.17 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:58:21.994323: step 302400, loss = 0.25 (1283.6 examples/sec; 0.100 sec/batch)
2017-06-02 09:58:22.754439: step 302410, loss = 0.15 (1683.9 examples/sec; 0.076 sec/batch)
2017-06-02 09:58:23.627601: step 302420, loss = 0.18 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:58:24.483499: step 302430, loss = 0.31 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:58:25.319679: step 302440, loss = 0.20 (1530.8 examples/sec; 0.084 sec/batch)
2017-06-02 09:58:26.167300: step 302450, loss = 0.17 (1510.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:58:27.045400: step 302460, loss = 0.24 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 09:58:27.913635: step 302470, loss = 0.20 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:58:28.791372: step 302480, loss = 0.25 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 09:58:29.660625: step 302490, loss = 0.21 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:58:30.621385: step 302500, loss = 0.22 (1332.3 examples/sec; 0.096 sec/batch)
2017-06-02 09:58:31.399755: step 302510, loss = 0.20 (1644.5 examples/sec; 0.078 sec/batch)
2017-06-02 09:58:32.262691: step 302520, loss = 0.16 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:58:33.149792: step 302530, loss = 0.19 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 09:58:33.996684: step 302540, loss = 0.21 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:58:34.863798: step 302550, loss = 0.24 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:58:35.727617: step 302560, loss = 0.17 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:58:36.588413: step 302570, loss = 0.25 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 09:58:37.459448: step 302580, loss = 0.17 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:58:38.314222: step 302590, loss = 0.25 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 09:58:39.313165: step 302600, loss = 0.21 (1281.3 examples/sec; 0.100 sec/batch)
2017-06-02 09:58:40.064666: step 302610, loss = 0.14 (1703.3 examples/sec; 0.075 sec/batch)
2017-06-02 09:58:40.940708: step 302620, loss = 0.19 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:58:41.791109: step 302630, loss = 0.19 (1505.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:58:42.661378: step 302640, loss = 0.20 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:58:43.529513: step 302650, loss = 0.23 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:58:44.397634: step 302660, loss = 0.20 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:58:45.258569: step 302670, loss = 0.20 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:58:46.121591: step 302680, loss = 0.20 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:58:46.993474: step 302690, loss = 0.21 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:58:47.947816: step 302700, loss = 0.25 (1341.2 examples/sec; 0.095 sec/batch)
2017-06-02 09:58:48.688984: step 302710, loss = 0.16 (1727.0 examples/sec; 0.074 sec/batch)
2017-06-02 09:58:49.538860: step 302720, loss = 0.18 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:58:50.421129: step 302730, loss = 0.15 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:58:51.264338: step 302740, loss = 0.15 (1518.0 examples/sec; 0.084 sec/batch)
2017-06-02 09:58:52.139190: step 302750, loss = 0.19 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:58:53.009549: step 302760, loss = 0.21 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:58:53.853110: step 302770, loss = 0.17 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:58:54.720544: step 302780, loss = 0.19 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:58:55.587076: step 302790, loss = 0.21 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:58:56.563076: step 302800, loss = 0.17 (1311.7 examples/sec; 0.098 sec/batch)
2017-06-02 09:58:57.341383: step 302810, loss = 0.16 (1644.2 examples/sec; 0.078 sec/batch)
2017-06-02 09:58:58.215149: step 302820, loss = 0.17 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:58:59.080474: step 302830, loss = 0.23 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:58:59.932552: step 302840, loss = 0.20 (1502.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:59:00.796163: step 302850, loss = 0.19 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 09:59:01.650526: step 302860, loss = 0.15 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:59:02.494076: step 302870, loss = 0.16 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 09:59:03.374063: step 302880, loss = 0.21 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 09:59:04.266418: step 302890, loss = 0.18 (1434.4 examples/sec; 0.089 sec/batch)
2017-06-02 09:59:05.271221: step 302900, loss = 0.21 (1273.9 examples/sec; 0.100 sec/batch)
2017-06-02 09:59:06.037175: step 302910, loss = 0.19 (1671.1 examples/sec; 0.077 sec/batch)
2017-06-02 09:59:06.902579: step 302920, loss = 0.18 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:59:07.756563: step 302930, loss = 0.20 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:59:08.630529: step 302940, loss = 0.25 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:59:09.512799: step 302950, loss = 0.21 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 09:59:10.379297: step 302960, loss = 0.23 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:59:11.253199: step 302970, loss = 0.20 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 09:59:12.119457: step 302980, loss = 0.23 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:59:12.983651: step 302990, loss = 0.16 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 09:59:13.967963: step 303000, loss = 0.16 (1300.4 examples/sec; 0.098 sec/batch)
2017-06-02 09:59:14.733414: step 303010, loss = 0.16 (1672.2 examples/sec; 0.077 sec/batch)
2017-06-02 09:59:15.582085: step 303020, loss = 0.17 (1508.2 examples/sec; 0.085 sec/batch)
2017-06-02 09:59:16.450275: step 303030, loss = 0.21 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:59:17.292131: step 303040, loss = 0.21 (1520.5 examples/sec; 0.084 sec/batch)
2017-06-02 09:59:18.163517: step 303050, loss = 0.21 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 09:59:19.027326: step 303060, loss = 0.16 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:59:19.881898: step 303070, loss = 0.17 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 09:59:20.748625: step 303080, loss = 0.18 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:59:21.606156: step 303090, loss = 0.25 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 09:59:22.585904: step 303100, loss = 0.23 (1306.5 examples/sec; 0.098 sec/batch)
2017-06-02 09:59:23.371537: step 303110, loss = 0.15 (1629.3 examples/sec; 0.079 sec/batch)
2017-06-02 09:59:24.232563: step 303120, loss = 0.17 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 09:59:25.100589: step 303130, loss = 0.20 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:59:25.948462: step 303140, loss = 0.18 (1509.7 examples/sec; 0.085 sec/batch)
2017-06-02 09:59:26.814340: step 303150, loss = 0.19 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 09:59:27.669803: step 303160, loss = 0.17 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 09:59:28.544676: step 303170, loss = 0.18 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:59:29.393846: step 303180, loss = 0.15 (1507.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:59:30.273832: step 303190, loss = 0.21 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 09:59:31.228225: step 303200, loss = 0.16 (1341.2 examples/sec; 0.095 sec/batch)
2017-06-02 09:59:32.024813: step 303210, loss = 0.18 (1606.9 examples/sec; 0.080 sec/batch)
2017-06-02 09:59:32.865790: step 303220, loss = 0.17 (1522.0 examples/sec; 0.084 sec/batch)
2017-06-02 09:59:33.751710: step 303230, loss = 0.21 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 09:59:34.624928: step 303240, loss = 0.21 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 09:59:35.460660: step 303250, loss = 0.21 (1531.6 examples/sec; 0.084 sec/batch)
2017-06-02 09:59:36.307248: step 303260, loss = 0.20 (1511.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:59:37.173217: step 303270, loss = 0.23 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 09:59:38.048518: step 303280, loss = 0.17 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 09:59:38.883549: step 303290, loss = 0.22 (1532.9 examples/sec; 0.084 sec/batch)
2017-06-02 09:59:39.843969: step 303300, loss = 0.18 (1332.7 examples/sec; 0.096 sec/batch)
2017-06-02 09:59:40.610775: step 303310, loss = 0.25 (1669.3 examples/sec; 0.077 sec/batch)
2017-06-02 09:59:41.477027: step 303320, loss = 0.15 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 09:59:42.329017: step 303330, loss = 0.18 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 09:59:43.197751: step 303340, loss = 0.19 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:59:44.070547: step 303350, loss = 0.16 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 09:59:44.923940: step 303360, loss = 0.19 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:59:45.781975: step 303370, loss = 0.20 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 09:59:46.647439: step 303380, loss = 0.13 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:59:47.528295: step 303390, loss = 0.19 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 09:59:48.492249: step 303400, loss = 0.19 (1327.9 examples/sec; 0.096 sec/batch)
2017-06-02 09:59:49.221326: step 303410, loss = 0.19 (1755.7 examples/sec; 0.073 sec/batch)
2017-06-02 09:59:50.083023: step 303420, loss = 0.20 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 09:59:50.948217: step 303430, loss = 0.18 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 09:59:51.804098: step 303440, loss = 0.17 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 09:59:52.746695: step 303450, loss = 0.18 (1357.9 examples/sec; 0.094 sec/batch)
2017-06-02 09:59:53.623306: step 303460, loss = 0.15 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 09:59:54.498069: step 303470, loss = 0.17 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 09:59:55.368210: step 303480, loss = 0.17 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 09:59:56.218746: step 303490, loss = 0.21 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 09:59:57.188694: step 303500, loss = 0.18 (1319.6 examples/sec; 0.097 sec/batch)
2017-06-02 09:59:57.960920: step 303510, loss = 0.21 (1657.5 examples/sec; 0.077 sec/batch)
2017-06-02 09:59:58.814217: step 303520, loss = 0.19 (1500.1 examples/sec; 0.085 sec/batch)
2017-06-02 09:59:59.699848: step 303530, loss = 0.16 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:00:00.564659: step 303540, loss = 0.20 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:00:01.425315: step 303550, loss = 0.16 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:00:02.302175: step 303560, loss = 0.18 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:00:03.171532: step 303570, loss = 0.14 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:00:04.055917: step 303580, loss = 0.28 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:00:04.909546: step 303590, loss = 0.17 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:00:05.889418: step 303600, loss = 0.19 (1306.3 examples/sec; 0.098 sec/batch)
2017-06-02 10:00:06.667132: step 303610, loss = 0.15 (1645.8 examples/sec; 0.078 sec/batch)
2017-06-02 10:00:07.524958: step 303620, loss = 0.19 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:00:08.380346: step 303630, loss = 0.17 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:00:09.262488: step 303640, loss = 0.17 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:00:10.101378: step 303650, loss = 0.20 (1525.8 examples/sec; 0.084 sec/batch)
2017-06-02 10:00:10.968190: step 303660, loss = 0.21 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:00:11.822095: step 303670, loss = 0.16 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:00:12.699998: step 303680, loss = 0.21 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:00:13.556081: step 303690, loss = 0.25 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:00:14.532382: step 303700, loss = 0.20 (1311.1 examples/sec; 0.098 sec/batch)
2017-06-02 10:00:15.303416: step 303710, loss = 0.13 (1660.1 examples/sec; 0.077 sec/batch)
2017-06-02 10:00:16.176646: step 303720, loss = 0.17 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:00:17.056184: step 303730, loss = 0.20 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:00:17.913174: step 303740, loss = 0.15 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:00:18.778825: step 303750, loss = 0.19 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:00:19.664556: step 303760, loss = 0.18 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 10:00:20.536270: step 303770, loss = 0.21 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:00:21.404656: step 303780, loss = 0.17 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:00:22.272425: step 303790, loss = 0.25 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:00:23.232191: step 303800, loss = 0.19 (1333.6 examples/sec; 0.096 sec/batch)
2017-06-02 10:00:23.972338: step 303810, loss = 0.17 (1729.4 examples/sec; 0.074 sec/batch)
2017-06-02 10:00:24.851995: step 303820, loss = 0.20 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:00:25.729780: step 303830, loss = 0.20 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:00:26.594413: step 303840, loss = 0.22 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:00:27.464226: step 303850, loss = 0.18 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:00:28.338020: step 303860, loss = 0.20 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:00:29.199336: step 303870, loss = 0.17 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:00:30.070896: step 303880, loss = 0.21 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:00:30.925202: step 303890, loss = 0.16 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:00:31.909070: step 303900, loss = 0.19 (1301.0 examples/sec; 0.098 sec/batch)
2017-06-02 10:00:32.679704: step 303910, loss = 0.24 (1661.0 examples/sec; 0.077 sec/batch)
2017-06-02 10:00:33.545198: step 303920, loss = 0.24 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:00:34.394357: step 303930, loss = 0.18 (1507.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:00:35.287472: step 303940, loss = 0.21 (1433.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:00:36.169997: step 303950, loss = 0.13 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:00:37.062926: step 303960, loss = 0.20 (1433.5 examples/sec; 0.089 sec/batch)
2017-06-02 10:00:37.934008: step 303970, loss = 0.17 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:00:38.807409: step 303980, loss = 0.16 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:00:39.678540: step 303990, loss = 0.17 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:00:40.622144: step 304000, loss = 0.20 (1356.5 examples/sec; 0.094 sec/batch)
2017-06-02 10:00:41.393271: step 304010, loss = 0.17 (1659.9 examples/sec; 0.077 sec/batch)
2017-06-02 10:00:42.260909: step 304020, loss = 0.22 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:00:43.135841: step 304030, loss = 0.26 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:00:44.020370: step 304040, loss = 0.19 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:00:44.891153: step 304050, loss = 0.17 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:00:45.771453: step 304060, loss = 0.19 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:00:46.636610: step 304070, loss = 0.15 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:00:47.496334: step 304080, loss = 0.20 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:00:48.361769: step 304090, loss = 0.19 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:00:49.354217: step 304100, loss = 0.19 (1289.7 examples/sec; 0.099 sec/batch)
2017-06-02 10:00:50.130507: step 304110, loss = 0.15 (1648.9 examples/sec; 0.078 sec/batch)
2017-06-02 10:00:51.003413: step 304120, loss = 0.16 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:00:51.864434: step 304130, loss = 0.15 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:00:52.759156: step 304140, loss = 0.18 (1430.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:00:53.640676: step 304150, loss = 0.18 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:00:54.500929: step 304160, loss = 0.20 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:00:55.389276: step 304170, loss = 0.21 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:00:56.279014: step 304180, loss = 0.24 (1438.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:00:57.171662: step 304190, loss = 0.20 (1434.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:00:58.150721: step 304200, loss = 0.18 (1307.4 examples/sec; 0.098 sec/batch)
2017-06-02 10:00:58.923094: step 304210, loss = 0.16 (1657.2 examples/sec; 0.077 sec/batch)
2017-06-02 10:00:59.792482: step 304220, loss = 0.15 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:01:00.685605: step 304230, loss = 0.22 (1433.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:01:01.544602: step 304240, loss = 0.18 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:01:02.393033: step 304250, loss = 0.25 (1508.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:01:03.256286: step 304260, loss = 0.15 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:01:04.131774: step 304270, loss = 0.17 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:01:05.023191: step 304280, loss = 0.17 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:01:05.864525: step 304290, loss = 0.19 (1521.4 examples/sec; 0.084 sec/batch)
2017-06-02 10:01:06.839972: step 304300, loss = 0.23 (1312.2 examples/sec; 0.098 sec/batch)
2017-06-02 10:01:07.608400: step 304310, loss = 0.17 (1665.7 examples/sec; 0.077 sec/batch)
2017-06-02 10:01:08.497507: step 304320, loss = 0.23 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 10:01:09.352851: step 304330, loss = 0.23 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:01:10.196284: step 304340, loss = 0.21 (1517.6 examples/sec; 0.084 sec/batch)
2017-06-02 10:01:11.071648: step 304350, loss = 0.20 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:01:11.936597: step 304360, loss = 0.17 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:01:12.802918: step 304370, loss = 0.19 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:01:13.658378: step 304380, loss = 0.24 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:01:14.520003: step 304390, loss = 0.20 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:01:15.496366: step 304400, loss = 0.18 (1311.0 examples/sec; 0.098 sec/batch)
2017-06-02 10:01:16.277006: step 304410, loss = 0.18 (1639.7 examples/sec; 0.078 sec/batch)
2017-06-02 10:01:17.152193: step 304420, loss = 0.16 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:01:18.008393: step 304430, loss = 0.21 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:01:18.894720: step 304440, loss = 0.19 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:01:19.766308: step 304450, loss = 0.19 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:01:20.654202: step 304460, loss = 0.21 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:01:21.526348: step 304470, loss = 0.18 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:01:22.399836: step 304480, loss = 0.21 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:01:23.282875: step 304490, loss = 0.22 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:01:24.255353: step 304500, loss = 0.28 (1316.2 examples/sec; 0.097 sec/batch)
2017-06-02 10:01:25.026931: step 304510, loss = 0.18 (1658.9 examples/sec; 0.077 sec/batch)
2017-06-02 10:01:25.867995: step 304520, loss = 0.17 (1521.9 examples/sec; 0.084 sec/batch)
2017-06-02 10:01:26.735515: step 304530, loss = 0.21 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:01:27.618785: step 304540, loss = 0.17 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:01:28.476948: step 304550, loss = 0.18 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:01:29.344910: step 304560, loss = 0.20 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:01:30.204916: step 304570, loss = 0.18 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:01:31.057992: step 304580, loss = 0.22 (1500.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:01:31.930528: step 304590, loss = 0.18 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:01:32.933701: step 304600, loss = 0.16 (1276.0 examples/sec; 0.100 sec/batch)
2017-06-02 10:01:33.664134: step 304610, loss = 0.19 (1752.4 examples/sec; 0.073 sec/batch)
2017-06-02 10:01:34.525688: step 304620, loss = 0.23 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:01:35.384089: step 304630, loss = 0.21 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:01:36.253069: step 304640, loss = 0.23 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:01:37.117823: step 304650, loss = 0.19 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:01:37.954877: step 304660, loss = 0.16 (1529.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:01:38.815243: step 304670, loss = 0.25 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:01:39.672645: step 304680, loss = 0.18 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:01:40.538382: step 304690, loss = 0.25 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:01:41.492146: step 304700, loss = 0.18 (1342.1 examples/sec; 0.095 sec/batch)
2017-06-02 10:01:42.241521: step 304710, loss = 0.20 (1708.1 examples/sec; 0.075 sec/batch)
2017-06-02 10:01:43.105936: step 304720, loss = 0.23 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:01:43.981241: step 304730, loss = 0.19 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:01:44.841762: step 304740, loss = 0.18 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:01:45.696703: step 304750, loss = 0.19 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:01:46.560352: step 304760, loss = 0.19 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:01:47.430988: step 304770, loss = 0.25 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:01:48.289346: step 304780, loss = 0.18 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:01:49.140822: step 304790, loss = 0.18 (1503.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:01:50.110071: step 304800, loss = 0.18 (1320.6 examples/sec; 0.097 sec/batch)
2017-06-02 10:01:50.847943: step 304810, loss = 0.27 (1734.7 examples/sec; 0.074 sec/batch)
2017-06-02 10:01:51.715971: step 304820, loss = 0.21 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:01:52.573286: step 304830, loss = 0.20 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:01:53.429211: step 304840, loss = 0.18 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:01:54.316565: step 304850, loss = 0.17 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 10:01:55.198792: step 304860, loss = 0.24 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:01:56.067857: step 304870, loss = 0.18 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:01:56.937199: step 304880, loss = 0.17 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:01:57.809642: step 304890, loss = 0.20 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:01:58.806722: step 304900, loss = 0.14 (1283.7 examples/sec; 0.100 sec/batch)
2017-06-02 10:01:59.553423: step 304910, loss = 0.23 (1714.2 examples/sec; 0.075 sec/batch)
2017-06-02 10:02:00.407376: step 304920, loss = 0.21 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:02:01.267492: step 304930, loss = 0.15 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:02:02.135962: step 304940, loss = 0.18 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:02:02.997205: step 304950, loss = 0.24 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:02:03.869871: step 304960, loss = 0.20 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:02:04.721489: step 304970, loss = 0.19 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:02:05.599087: step 304980, loss = 0.18 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:02:06.453175: step 304990, loss = 0.24 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:02:07.433420: step 305000, loss = 0.21 (1305.8 examples/sec; 0.098 sec/batch)
2017-06-02 10:02:08.203470: step 305010, loss = 0.20 (1662.2 examples/sec; 0.077 sec/batch)
2017-06-02 10:02:09.068257: step 305020, loss = 0.22 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:02:09.906434: step 305030, loss = 0.19 (1527.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:02:10.776427: step 305040, loss = 0.15 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:02:11.661538: step 305050, loss = 0.25 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:02:12.512690: step 305060, loss = 0.17 (1503.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:02:13.395985: step 305070, loss = 0.19 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:02:14.280209: step 305080, loss = 0.15 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:02:15.148805: step 305090, loss = 0.21 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:02:16.108405: step 305100, loss = 0.24 (1333.9 examples/sec; 0.096 sec/batch)
2017-06-02 10:02:16.897489: step 305110, loss = 0.18 (1622.2 examples/sec; 0.079 sec/batch)
2017-06-02 10:02:17.769081: step 305120, loss = 0.16 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:02:18.635068: step 305130, loss = 0.26 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:02:19.499394: step 305140, loss = 0.24 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:02:20.345785: step 305150, loss = 0.22 (1512.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:02:21.204953: step 305160, loss = 0.16 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:02:22.075414: step 305170, loss = 0.23 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:02:22.963785: step 305180, loss = 0.18 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 10:02:23.836186: step 305190, loss = 0.17 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:02:24.815422: step 305200, loss = 0.20 (1307.1 examples/sec; 0.098 sec/batch)
2017-06-02 10:02:25.590350: step 305210, loss = 0.20 (1651.8 examples/sec; 0.077 sec/batch)
2017-06-02 10:02:26.466277: step 305220, loss = 0.20 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:02:27.326507: step 305230, loss = 0.14 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:02:28.166630: step 305240, loss = 0.15 (1523.6 examples/sec; 0.084 sec/batch)
2017-06-02 10:02:29.030414: step 305250, loss = 0.17 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:02:29.912786: step 305260, loss = 0.24 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:02:30.759535: step 305270, loss = 0.18 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:02:31.613344: step 305280, loss = 0.19 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:02:32.481801: step 305290, loss = 0.17 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:02:33.450121: step 305300, loss = 0.18 (1321.9 examples/sec; 0.097 sec/batch)
2017-06-02 10:02:34.214084: step 305310, loss = 0.21 (1675.5 examples/sec; 0.076 sec/batch)
2017-06-02 10:02:35.087632: step 305320, loss = 0.16 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:02:35.961077: step 305330, loss = 0.17 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:02:36.817296: step 305340, loss = 0.18 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:02:37.675205: step 305350, loss = 0.17 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:02:38.543866: step 305360, loss = 0.15 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:02:39.406245: step 305370, loss = 0.17 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:02:40.271356: step 305380, loss = 0.18 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:02:41.142749: step 305390, loss = 0.15 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:02:42.090091: step 305400, loss = 0.20 (1351.1 examples/sec; 0.095 sec/batch)
2017-06-02 10:02:42.852594: step 305410, loss = 0.15 (1678.7 examples/sec; 0.076 sec/batch)
2017-06-02 10:02:43.727963: step 305420, loss = 0.18 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:02:44.592090: step 305430, loss = 0.24 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:02:45.440506: step 305440, loss = 0.21 (1508.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:02:46.285673: step 305450, loss = 0.17 (1514.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:02:47.148973: step 305460, loss = 0.18 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:02:48.010547: step 305470, loss = 0.18 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:02:48.891482: step 305480, loss = 0.16 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:02:49.773539: step 305490, loss = 0.21 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:02:50.754246: step 305500, loss = 0.18 (1305.2 examples/sec; 0.098 sec/batch)
2017-06-02 10:02:51.511696: step 305510, loss = 0.18 (1689.9 examples/sec; 0.076 sec/batch)
2017-06-02 10:02:52.357701: step 305520, loss = 0.20 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:02:53.213358: step 305530, loss = 0.19 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:02:54.103002: step 305540, loss = 0.21 (1438.8 examples/sec; 0.089 sec/batch)
2017-06-02 10:02:54.964721: step 305550, loss = 0.22 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:02:55.844834: step 305560, loss = 0.20 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:02:56.721994: step 305570, loss = 0.16 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:02:57.611074: step 305580, loss = 0.18 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 10:02:58.459682: step 305590, loss = 0.24 (1508.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:02:59.393768: step 305600, loss = 0.20 (1370.3 examples/sec; 0.093 sec/batch)
2017-06-02 10:03:00.162131: step 305610, loss = 0.23 (1665.9 examples/sec; 0.077 sec/batch)
2017-06-02 10:03:01.044544: step 305620, loss = 0.18 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:03:01.895377: step 305630, loss = 0.17 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:03:02.758565: step 305640, loss = 0.19 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:03:03.608042: step 305650, loss = 0.17 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:03:04.477534: step 305660, loss = 0.22 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:03:05.374223: step 305670, loss = 0.18 (1427.5 examples/sec; 0.090 sec/batch)
2017-06-02 10:03:06.259640: step 305680, loss = 0.16 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:03:07.121366: step 305690, loss = 0.22 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:03:08.102479: step 305700, loss = 0.19 (1304.6 examples/sec; 0.098 sec/batch)
2017-06-02 10:03:08.863588: step 305710, loss = 0.20 (1681.8 examples/sec; 0.076 sec/batch)
2017-06-02 10:03:09.726842: step 305720, loss = 0.16 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:03:10.625553: step 305730, loss = 0.25 (1424.3 examples/sec; 0.090 sec/batch)
2017-06-02 10:03:11.499049: step 305740, loss = 0.18 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:03:12.339748: step 305750, loss = 0.18 (1522.6 examples/sec; 0.084 sec/batch)
2017-06-02 10:03:13.187676: step 305760, loss = 0.22 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:03:14.066113: step 305770, loss = 0.18 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:03:14.927020: step 305780, loss = 0.16 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:03:15.781323: step 305790, loss = 0.15 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:03:16.767760: step 305800, loss = 0.18 (1297.6 examples/sec; 0.099 sec/batch)
2017-06-02 10:03:17.517951: step 305810, loss = 0.24 (1706.2 examples/sec; 0.075 sec/batch)
2017-06-02 10:03:18.365429: step 305820, loss = 0.16 (1510.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:03:19.221165: step 305830, loss = 0.18 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:03:20.071202: step 305840, loss = 0.18 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:03:20.919493: step 305850, loss = 0.23 (1508.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:03:21.766444: step 305860, loss = 0.17 (1511.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:03:22.631497: step 305870, loss = 0.17 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:03:23.502499: step 305880, loss = 0.16 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:03:24.383554: step 305890, loss = 0.18 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:03:25.366726: step 305900, loss = 0.15 (1301.9 examples/sec; 0.098 sec/batch)
2017-06-02 10:03:26.145173: step 305910, loss = 0.21 (1644.3 examples/sec; 0.078 sec/batch)
2017-06-02 10:03:27.010708: step 305920, loss = 0.21 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:03:27.906555: step 305930, loss = 0.17 (1428.8 examples/sec; 0.090 sec/batch)
2017-06-02 10:03:28.770520: step 305940, loss = 0.21 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:03:29.623373: step 305950, loss = 0.19 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:03:30.495094: step 305960, loss = 0.17 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:03:31.370534: step 305970, loss = 0.16 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:03:32.226624: step 305980, loss = 0.20 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:03:33.069776: step 305990, loss = 0.19 (1518.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:03:33.993365: step 306000, loss = 0.19 (1385.9 examples/sec; 0.092 sec/batch)
2017-06-02 10:03:34.766464: step 306010, loss = 0.23 (1655.7 examples/sec; 0.077 sec/batch)
2017-06-02 10:03:35.614387: step 306020, loss = 0.15 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:03:36.467208: step 306030, loss = 0.20 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:03:37.349156: step 306040, loss = 0.23 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:03:38.234466: step 306050, loss = 0.17 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 10:03:39.122641: step 306060, loss = 0.18 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:03:40.011207: step 306070, loss = 0.21 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 10:03:40.891511: step 306080, loss = 0.23 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:03:41.766456: step 306090, loss = 0.18 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:03:42.731068: step 306100, loss = 0.20 (1327.0 examples/sec; 0.096 sec/batch)
2017-06-02 10:03:43.526764: step 306110, loss = 0.19 (1608.6 examples/sec; 0.080 sec/batch)
2017-06-02 10:03:44.391587: step 306120, loss = 0.23 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:03:45.279618: step 306130, loss = 0.17 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:03:46.139818: step 306140, loss = 0.18 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:03:47.002354: step 306150, loss = 0.22 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:03:47.872596: step 306160, loss = 0.20 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:03:48.700943: step 306170, loss = 0.22 (1545.3 examples/sec; 0.083 sec/batch)
2017-06-02 10:03:49.587133: step 306180, loss = 0.20 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:03:50.473956: step 306190, loss = 0.21 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:03:51.432576: step 306200, loss = 0.17 (1336.7 examples/sec; 0.096 sec/batch)
2017-06-02 10:03:52.207361: step 306210, loss = 0.20 (1649.9 examples/sec; 0.078 sec/batch)
2017-06-02 10:03:53.056838: step 306220, loss = 0.17 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:03:53.938590: step 306230, loss = 0.24 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:03:54.825417: step 306240, loss = 0.22 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:03:55.690182: step 306250, loss = 0.16 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:03:56.550120: step 306260, loss = 0.18 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:03:57.433942: step 306270, loss = 0.23 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:03:58.303905: step 306280, loss = 0.18 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:03:59.181893: step 306290, loss = 0.15 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:04:00.149072: step 306300, loss = 0.20 (1323.4 examples/sec; 0.097 sec/batch)
2017-06-02 10:04:00.915069: step 306310, loss = 0.19 (1671.0 examples/sec; 0.077 sec/batch)
2017-06-02 10:04:01.801867: step 306320, loss = 0.14 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:04:02.657015: step 306330, loss = 0.17 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:04:03.536378: step 306340, loss = 0.20 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:04:04.405413: step 306350, loss = 0.20 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:04:05.248570: step 306360, loss = 0.15 (1518.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:04:06.126022: step 306370, loss = 0.18 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:04:06.959722: step 306380, loss = 0.26 (1535.3 examples/sec; 0.083 sec/batch)
2017-06-02 10:04:07.804701: step 306390, loss = 0.18 (1514.9 examples/sec; 0.084 sec/batch)
2017-06-02 10:04:08.775561: step 306400, loss = 0.16 (1318.4 examples/sec; 0.097 sec/batch)
2017-06-02 10:04:09.567457: step 306410, loss = 0.18 (1616.4 examples/sec; 0.079 sec/batch)
2017-06-02 10:04:10.443111: step 306420, loss = 0.22 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:04:11.333811: step 306430, loss = 0.18 (1437.1 examples/sec; 0.089 sec/batch)
2017-06-02 10:04:12.185204: step 306440, loss = 0.18 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:04:13.055125: step 306450, loss = 0.19 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:04:13.899076: step 306460, loss = 0.24 (1516.7 examples/sec; 0.084 sec/batch)
2017-06-02 10:04:14.770461: step 306470, loss = 0.24 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:04:15.606921: step 306480, loss = 0.22 (1530.3 examples/sec; 0.084 sec/batch)
2017-06-02 10:04:16.475797: step 306490, loss = 0.24 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:04:17.448124: step 306500, loss = 0.22 (1316.4 examples/sec; 0.097 sec/batch)
2017-06-02 10:04:18.194400: step 306510, loss = 0.21 (1715.2 examples/sec; 0.075 sec/batch)
2017-06-02 10:04:19.029195: step 306520, loss = 0.22 (1533.3 examples/sec; 0.083 sec/batch)
2017-06-02 10:04:19.891639: step 306530, loss = 0.30 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:04:20.751426: step 306540, loss = 0.17 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:04:21.595248: step 306550, loss = 0.15 (1516.9 examples/sec; 0.084 sec/batch)
2017-06-02 10:04:22.433202: step 306560, loss = 0.15 (1527.5 examples/sec; 0.084 sec/batch)
2017-06-02 10:04:23.301392: step 306570, loss = 0.16 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:04:24.166083: step 306580, loss = 0.23 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:04:25.011111: step 306590, loss = 0.26 (1514.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:04:26.027102: step 306600, loss = 0.20 (1259.8 examples/sec; 0.102 sec/batch)
2017-06-02 10:04:26.726396: step 306610, loss = 0.16 (1830.4 examples/sec; 0.070 sec/batch)
2017-06-02 10:04:27.572465: step 306620, loss = 0.20 (1512.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:04:28.439302: step 306630, loss = 0.19 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:04:29.297066: step 306640, loss = 0.15 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:04:30.153206: step 306650, loss = 0.20 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:04:31.009293: step 306660, loss = 0.21 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:04:31.870927: step 306670, loss = 0.16 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:04:32.716837: step 306680, loss = 0.20 (1513.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:04:33.586439: step 306690, loss = 0.19 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:04:34.532590: step 306700, loss = 0.16 (1352.8 examples/sec; 0.095 sec/batch)
2017-06-02 10:04:35.292469: step 306710, loss = 0.15 (1684.5 examples/sec; 0.076 sec/batch)
2017-06-02 10:04:36.163273: step 306720, loss = 0.17 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:04:37.005533: step 306730, loss = 0.17 (1519.7 examples/sec; 0.084 sec/batch)
2017-06-02 10:04:37.845448: step 306740, loss = 0.23 (1523.9 examples/sec; 0.084 sec/batch)
2017-06-02 10:04:38.681563: step 306750, loss = 0.20 (1530.9 examples/sec; 0.084 sec/batch)
2017-06-02 10:04:39.506974: step 306760, loss = 0.18 (1550.7 examples/sec; 0.083 sec/batch)
2017-06-02 10:04:40.330520: step 306770, loss = 0.17 (1554.3 examples/sec; 0.082 sec/batch)
2017-06-02 10:04:41.181996: step 306780, loss = 0.22 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:04:42.053397: step 306790, loss = 0.21 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:04:43.008395: step 306800, loss = 0.19 (1340.3 examples/sec; 0.095 sec/batch)
2017-06-02 10:04:43.763872: step 306810, loss = 0.20 (1694.3 examples/sec; 0.076 sec/batch)
2017-06-02 10:04:44.626687: step 306820, loss = 0.17 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:04:45.479129: step 306830, loss = 0.18 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:04:46.317707: step 306840, loss = 0.22 (1526.4 examples/sec; 0.084 sec/batch)
2017-06-02 10:04:47.166083: step 306850, loss = 0.19 (1508.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:04:48.012039: step 306860, loss = 0.20 (1513.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:04:48.865661: step 306870, loss = 0.18 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:04:49.701979: step 306880, loss = 0.20 (1530.5 examples/sec; 0.084 sec/batch)
2017-06-02 10:04:50.566438: step 306890, loss = 0.19 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:04:51.525221: step 306900, loss = 0.17 (1335.0 examples/sec; 0.096 sec/batch)
2017-06-02 10:04:52.275274: step 306910, loss = 0.16 (1706.6 examples/sec; 0.075 sec/batch)
2017-06-02 10:04:53.139237: step 306920, loss = 0.18 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:04:54.008495: step 306930, loss = 0.17 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:04:54.860511: step 306940, loss = 0.17 (1502.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:04:55.734414: step 306950, loss = 0.22 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:04:56.612477: step 306960, loss = 0.23 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:04:57.475880: step 306970, loss = 0.16 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:04:58.340237: step 306980, loss = 0.26 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:04:59.204574: step 306990, loss = 0.18 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:05:00.161902: step 307000, loss = 0.19 (1337.0 examples/sec; 0.096 sec/batch)
2017-06-02 10:05:00.924042: step 307010, loss = 0.18 (1679.5 examples/sec; 0.076 sec/batch)
2017-06-02 10:05:01.754453: step 307020, loss = 0.21 (1541.4 examples/sec; 0.083 sec/batch)
2017-06-02 10:05:02.608720: step 307030, loss = 0.17 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:05:03.454157: step 307040, loss = 0.19 (1514.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:05:04.327294: step 307050, loss = 0.18 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:05:05.195365: step 307060, loss = 0.18 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:05:06.041702: step 307070, loss = 0.19 (1512.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:05:06.877711: step 307080, loss = 0.20 (1531.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:05:07.720419: step 307090, loss = 0.17 (1518.9 examples/sec; 0.084 sec/batch)
2017-06-02 10:05:08.672653: step 307100, loss = 0.19 (1344.2 examples/sec; 0.095 sec/batch)
2017-06-02 10:05:09.440167: step 307110, loss = 0.15 (1667.7 examples/sec; 0.077 sec/batch)
2017-06-02 10:05:10.290432: step 307120, loss = 0.20 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:05:11.130741: step 307130, loss = 0.20 (1523.3 examples/sec; 0.084 sec/batch)
2017-06-02 10:05:12.026516: step 307140, loss = 0.17 (1428.9 examples/sec; 0.090 sec/batch)
2017-06-02 10:05:12.892297: step 307150, loss = 0.20 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:05:13.773504: step 307160, loss = 0.26 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:05:14.610404: step 307170, loss = 0.16 (1529.5 examples/sec; 0.084 sec/batch)
2017-06-02 10:05:15.488026: step 307180, loss = 0.20 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:05:16.337236: step 307190, loss = 0.26 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:05:17.296010: step 307200, loss = 0.15 (1335.0 examples/sec; 0.096 sec/batch)
2017-06-02 10:05:18.076727: step 307210, loss = 0.14 (1639.5 examples/sec; 0.078 sec/batch)
2017-06-02 10:05:18.941962: step 307220, loss = 0.17 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:05:19.809831: step 307230, loss = 0.22 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:05:20.654327: step 307240, loss = 0.18 (1515.7 examples/sec; 0.084 sec/batch)
2017-06-02 10:05:21.528302: step 307250, loss = 0.18 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:05:22.378130: step 307260, loss = 0.19 (1506.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:05:23.272252: step 307270, loss = 0.19 (1431.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:05:24.138411: step 307280, loss = 0.18 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:05:24.996431: step 307290, loss = 0.20 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:05:25.979676: step 307300, loss = 0.17 (1301.8 examples/sec; 0.098 sec/batch)
2017-06-02 10:05:26.727369: step 307310, loss = 0.19 (1711.9 examples/sec; 0.075 sec/batch)
2017-06-02 10:05:27.568035: step 307320, loss = 0.19 (1522.6 examples/sec; 0.084 sec/batch)
2017-06-02 10:05:28.441908: step 307330, loss = 0.20 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:05:29.291954: step 307340, loss = 0.15 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:05:30.147271: step 307350, loss = 0.19 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:05:31.002505: step 307360, loss = 0.22 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:05:31.854650: step 307370, loss = 0.19 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:05:32.713111: step 307380, loss = 0.19 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:05:33.601305: step 307390, loss = 0.16 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 10:05:34.587361: step 307400, loss = 0.16 (1298.1 examples/sec; 0.099 sec/batch)
2017-06-02 10:05:35.352748: step 307410, loss = 0.16 (1672.4 examples/sec; 0.077 sec/batch)
2017-06-02 10:05:36.210160: step 307420, loss = 0.15 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:05:37.069742: step 307430, loss = 0.16 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:05:37.913825: step 307440, loss = 0.19 (1516.4 examples/sec; 0.084 sec/batch)
2017-06-02 10:05:38.755582: step 307450, loss = 0.19 (1520.6 examples/sec; 0.084 sec/batch)
2017-06-02 10:05:39.590907: step 307460, loss = 0.17 (1532.3 examples/sec; 0.084 sec/batch)
2017-06-02 10:05:40.443415: step 307470, loss = 0.25 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:05:41.283130: step 307480, loss = 0.23 (1524.3 examples/sec; 0.084 sec/batch)
2017-06-02 10:05:42.164207: step 307490, loss = 0.15 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:05:43.113228: step 307500, loss = 0.18 (1348.8 examples/sec; 0.095 sec/batch)
2017-06-02 10:05:43.860865: step 307510, loss = 0.17 (1712.1 examples/sec; 0.075 sec/batch)
2017-06-02 10:05:44.710415: step 307520, loss = 0.20 (1506.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:05:45.583028: step 307530, loss = 0.17 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:05:46.408845: step 307540, loss = 0.19 (1550.0 examples/sec; 0.083 sec/batch)
2017-06-02 10:05:47.250016: step 307550, loss = 0.24 (1521.7 examples/sec; 0.084 sec/batch)
2017-06-02 10:05:48.107796: step 307560, loss = 0.19 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:05:48.962374: step 307570, loss = 0.19 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:05:49.829817: step 307580, loss = 0.16 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:05:50.686339: step 307590, loss = 0.18 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:05:51.644704: step 307600, loss = 0.18 (1335.6 examples/sec; 0.096 sec/batch)
2017-06-02 10:05:52.406009: step 307610, loss = 0.23 (1681.3 examples/sec; 0.076 sec/batch)
2017-06-02 10:05:53.252146: step 307620, loss = 0.24 (1512.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:05:54.136136: step 307630, loss = 0.16 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:05:55.010379: step 307640, loss = 0.16 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:05:55.891713: step 307650, loss = 0.18 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:05:56.744282: step 307660, loss = 0.19 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:05:57.608191: step 307670, loss = 0.19 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:05:58.470761: step 307680, loss = 0.23 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:05:59.339743: step 307690, loss = 0.19 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:06:00.306660: step 307700, loss = 0.16 (1323.8 examples/sec; 0.097 sec/batch)
2017-06-02 10:06:01.069034: step 307710, loss = 0.16 (1679.0 examples/sec; 0.076 sec/batch)
2017-06-02 10:06:01.914420: step 307720, loss = 0.17 (1514.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:06:02.773437: step 307730, loss = 0.18 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:06:03.663737: step 307740, loss = 0.22 (1437.7 examples/sec; 0.089 sec/batch)
2017-06-02 10:06:04.519864: step 307750, loss = 0.18 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:06:05.394227: step 307760, loss = 0.15 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:06:06.258415: step 307770, loss = 0.17 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:06:07.118574: step 307780, loss = 0.16 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:06:07.998632: step 307790, loss = 0.20 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:06:08.958621: step 307800, loss = 0.21 (1333.3 examples/sec; 0.096 sec/batch)
2017-06-02 10:06:09.735720: step 307810, loss = 0.16 (1647.2 examples/sec; 0.078 sec/batch)
2017-06-02 10:06:10.605067: step 307820, loss = 0.25 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:06:11.462152: step 307830, loss = 0.18 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:06:12.322041: step 307840, loss = 0.26 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:06:13.165917: step 307850, loss = 0.24 (1516.8 examples/sec; 0.084 sec/batch)
2017-06-02 10:06:14.031174: step 307860, loss = 0.15 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:06:14.895833: step 307870, loss = 0.21 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:06:15.744498: step 307880, loss = 0.17 (1508.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:06:16.626610: step 307890, loss = 0.18 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:06:17.575218: step 307900, loss = 0.19 (1349.4 examples/sec; 0.095 sec/batch)
2017-06-02 10:06:18.363118: step 307910, loss = 0.21 (1624.5 examples/sec; 0.079 sec/batch)
2017-06-02 10:06:19.223467: step 307920, loss = 0.21 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:06:20.072459: step 307930, loss = 0.19 (1507.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:06:20.912368: step 307940, loss = 0.21 (1524.0 examples/sec; 0.084 sec/batch)
2017-06-02 10:06:21.773982: step 307950, loss = 0.17 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:06:22.629742: step 307960, loss = 0.17 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:06:23.493990: step 307970, loss = 0.18 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:06:24.380338: step 307980, loss = 0.20 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 10:06:25.250896: step 307990, loss = 0.20 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:06:26.214671: step 308000, loss = 0.17 (1328.1 examples/sec; 0.096 sec/batch)
2017-06-02 10:06:26.966484: step 308010, loss = 0.19 (1702.6 examples/sec; 0.075 sec/batch)
2017-06-02 10:06:27.843558: step 308020, loss = 0.24 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:06:28.735039: step 308030, loss = 0.19 (1435.8 examples/sec; 0.089 sec/batch)
2017-06-02 10:06:29.610831: step 308040, loss = 0.17 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:06:30.478718: step 308050, loss = 0.19 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:06:31.334913: step 308060, loss = 0.24 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:06:32.207511: step 308070, loss = 0.19 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:06:33.044039: step 308080, loss = 0.17 (1530.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:06:33.881021: step 308090, loss = 0.18 (1529.3 examples/sec; 0.084 sec/batch)
2017-06-02 10:06:34.840314: step 308100, loss = 0.22 (1334.3 examples/sec; 0.096 sec/batch)
2017-06-02 10:06:35.606501: step 308110, loss = 0.21 (1670.6 examples/sec; 0.077 sec/batch)
2017-06-02 10:06:36.474337: step 308120, loss = 0.20 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:06:37.339686: step 308130, loss = 0.21 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:06:38.186978: step 308140, loss = 0.18 (1510.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:06:39.057137: step 308150, loss = 0.18 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:06:39.908342: step 308160, loss = 0.17 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:06:40.775533: step 308170, loss = 0.20 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:06:41.646459: step 308180, loss = 0.15 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:06:42.511503: step 308190, loss = 0.18 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:06:43.474145: step 308200, loss = 0.20 (1329.7 examples/sec; 0.096 sec/batch)
2017-06-02 10:06:44.248586: step 308210, loss = 0.25 (1652.8 examples/sec; 0.077 sec/batch)
2017-06-02 10:06:45.123066: step 308220, loss = 0.16 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:06:45.989019: step 308230, loss = 0.15 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:06:46.859398: step 308240, loss = 0.24 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:06:47.721067: step 308250, loss = 0.15 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:06:48.603413: step 308260, loss = 0.16 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:06:49.514624: step 308270, loss = 0.20 (1404.7 examples/sec; 0.091 sec/batch)
2017-06-02 10:06:50.374363: step 308280, loss = 0.17 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:06:51.261637: step 308290, loss = 0.17 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:06:52.249264: step 308300, loss = 0.15 (1296.0 examples/sec; 0.099 sec/batch)
2017-06-02 10:06:53.006693: step 308310, loss = 0.21 (1689.9 examples/sec; 0.076 sec/batch)
2017-06-02 10:06:53.863140: step 308320, loss = 0.14 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:06:54.720139: step 308330, loss = 0.16 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:06:55.597401: step 308340, loss = 0.17 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:06:56.454517: step 308350, loss = 0.16 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:06:57.326949: step 308360, loss = 0.18 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:06:58.165153: step 308370, loss = 0.24 (1527.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:06:58.998631: step 308380, loss = 0.22 (1535.7 examples/sec; 0.083 sec/batch)
2017-06-02 10:06:59.857219: step 308390, loss = 0.21 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:07:00.833530: step 308400, loss = 0.18 (1311.0 examples/sec; 0.098 sec/batch)
2017-06-02 10:07:01.586895: step 308410, loss = 0.18 (1699.1 examples/sec; 0.075 sec/batch)
2017-06-02 10:07:02.473714: step 308420, loss = 0.20 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:07:03.346454: step 308430, loss = 0.23 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:07:04.211374: step 308440, loss = 0.14 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:07:05.081258: step 308450, loss = 0.21 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:07:05.947267: step 308460, loss = 0.22 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:07:06.825784: step 308470, loss = 0.17 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:07:07.694613: step 308480, loss = 0.24 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:07:08.540569: step 308490, loss = 0.20 (1513.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:07:09.525823: step 308500, loss = 0.27 (1299.2 examples/sec; 0.099 sec/batch)
2017-06-02 10:07:10.294128: step 308510, loss = 0.22 (1666.0 examples/sec; 0.077 sec/batch)
2017-06-02 10:07:11.125588: step 308520, loss = 0.22 (1539.4 examples/sec; 0.083 sec/batch)
2017-06-02 10:07:11.975854: step 308530, loss = 0.20 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:07:12.851982: step 308540, loss = 0.15 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:07:13.723028: step 308550, loss = 0.15 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:07:14.591889: step 308560, loss = 0.21 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:07:15.428670: step 308570, loss = 0.19 (1529.7 examples/sec; 0.084 sec/batch)
2017-06-02 10:07:16.287179: step 308580, loss = 0.21 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:07:17.133391: step 308590, loss = 0.15 (1512.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:07:18.116463: step 308600, loss = 0.19 (1302.0 examples/sec; 0.098 sec/batch)
2017-06-02 10:07:18.878848: step 308610, loss = 0.19 (1678.9 examples/sec; 0.076 sec/batch)
2017-06-02 10:07:19.738244: step 308620, loss = 0.16 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:07:20.610918: step 308630, loss = 0.16 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:07:21.476842: step 308640, loss = 0.20 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:07:22.329273: step 308650, loss = 0.19 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:07:23.203194: step 308660, loss = 0.18 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:07:24.071390: step 308670, loss = 0.15 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:07:24.923230: step 308680, loss = 0.15 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:07:25.794181: step 308690, loss = 0.18 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:07:26.782428: step 308700, loss = 0.20 (1295.2 examples/sec; 0.099 sec/batch)
2017-06-02 10:07:27.546495: step 308710, loss = 0.20 (1675.3 examples/sec; 0.076 sec/batch)
2017-06-02 10:07:28.395200: step 308720, loss = 0.22 (1508.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:07:29.279160: step 308730, loss = 0.19 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:07:30.130861: step 308740, loss = 0.18 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:07:31.022389: step 308750, loss = 0.16 (1435.7 examples/sec; 0.089 sec/batch)
2017-06-02 10:07:31.886300: step 308760, loss = 0.22 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:07:32.725347: step 308770, loss = 0.17 (1525.5 examples/sec; 0.084 sec/batch)
2017-06-02 10:07:33.577604: step 308780, loss = 0.15 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:07:34.428165: step 308790, loss = 0.17 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:07:35.391478: step 308800, loss = 0.18 (1328.7 examples/sec; 0.096 sec/batch)
2017-06-02 10:07:36.138328: step 308810, loss = 0.23 (1713.9 examples/sec; 0.075 sec/batch)
2017-06-02 10:07:36.986266: step 308820, loss = 0.20 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:07:37.836948: step 308830, loss = 0.17 (1504.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:07:38.694238: step 308840, loss = 0.16 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:07:39.538461: step 308850, loss = 0.20 (1516.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:07:40.394315: step 308860, loss = 0.21 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:07:41.247600: step 308870, loss = 0.15 (1500.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:07:42.120741: step 308880, loss = 0.13 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:07:42.967626: step 308890, loss = 0.19 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:07:43.950518: step 308900, loss = 0.20 (1302.3 examples/sec; 0.098 sec/batch)
2017-06-02 10:07:44.730226: step 308910, loss = 0.20 (1641.6 examples/sec; 0.078 sec/batch)
2017-06-02 10:07:45.603307: step 308920, loss = 0.19 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:07:46.463284: step 308930, loss = 0.16 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:07:47.324087: step 308940, loss = 0.15 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:07:48.195583: step 308950, loss = 0.18 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:07:49.057373: step 308960, loss = 0.14 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:07:49.904556: step 308970, loss = 0.18 (1510.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:07:50.761309: step 308980, loss = 0.16 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:07:51.620668: step 308990, loss = 0.17 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:07:52.563554: step 309000, loss = 0.25 (1357.5 examples/sec; 0.094 sec/batch)
2017-06-02 10:07:53.341601: step 309010, loss = 0.22 (1645.1 examples/sec; 0.078 sec/batch)
2017-06-02 10:07:54.207629: step 309020, loss = 0.18 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:07:55.085928: step 309030, loss = 0.17 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:07:55.952296: step 309040, loss = 0.26 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:07:56.823668: step 309050, loss = 0.15 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:07:57.697562: step 309060, loss = 0.19 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:07:58.571908: step 309070, loss = 0.14 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:07:59.437500: step 309080, loss = 0.18 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:08:00.304074: step 309090, loss = 0.19 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:08:01.274920: step 309100, loss = 0.16 (1318.4 examples/sec; 0.097 sec/batch)
2017-06-02 10:08:02.012270: step 309110, loss = 0.19 (1736.0 examples/sec; 0.074 sec/batch)
2017-06-02 10:08:02.882080: step 309120, loss = 0.24 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:08:03.766345: step 309130, loss = 0.16 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:08:04.613369: step 309140, loss = 0.15 (1511.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:08:05.473588: step 309150, loss = 0.17 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:08:06.332478: step 309160, loss = 0.15 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:08:07.184234: step 309170, loss = 0.19 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:08:08.066684: step 309180, loss = 0.17 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:08:08.931491: step 309190, loss = 0.17 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:08:09.894938: step 309200, loss = 0.20 (1328.6 examples/sec; 0.096 sec/batch)
2017-06-02 10:08:10.644966: step 309210, loss = 0.18 (1706.6 examples/sec; 0.075 sec/batch)
2017-06-02 10:08:11.524822: step 309220, loss = 0.15 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:08:12.384968: step 309230, loss = 0.20 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:08:13.265766: step 309240, loss = 0.15 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:08:14.107759: step 309250, loss = 0.25 (1520.2 examples/sec; 0.084 sec/batch)
2017-06-02 10:08:14.964780: step 309260, loss = 0.15 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:08:15.806771: step 309270, loss = 0.15 (1520.2 examples/sec; 0.084 sec/batch)
2017-06-02 10:08:16.677623: step 309280, loss = 0.23 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:08:17.529754: step 309290, loss = 0.17 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:08:18.479659: step 309300, loss = 0.21 (1347.5 examples/sec; 0.095 sec/batch)
2017-06-02 10:08:19.232376: step 309310, loss = 0.17 (1700.5 examples/sec; 0.075 sec/batch)
2017-06-02 10:08:20.124712: step 309320, loss = 0.18 (1434.5 examples/sec; 0.089 sec/batch)
2017-06-02 10:08:20.965252: step 309330, loss = 0.18 (1522.8 examples/sec; 0.084 sec/batch)
2017-06-02 10:08:21.820716: step 309340, loss = 0.23 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:08:22.676632: step 309350, loss = 0.17 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:08:23.549551: step 309360, loss = 0.17 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:08:24.399182: step 309370, loss = 0.25 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:08:25.279930: step 309380, loss = 0.18 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:08:26.150594: step 309390, loss = 0.24 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:08:27.102722: step 309400, loss = 0.18 (1344.3 examples/sec; 0.095 sec/batch)
2017-06-02 10:08:27.876301: step 309410, loss = 0.18 (1654.6 examples/sec; 0.077 sec/batch)
2017-06-02 10:08:28.745580: step 309420, loss = 0.19 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:08:29.589971: step 309430, loss = 0.19 (1515.9 examples/sec; 0.084 sec/batch)
2017-06-02 10:08:30.456223: step 309440, loss = 0.22 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:08:31.293889: step 309450, loss = 0.15 (1528.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:08:32.161716: step 309460, loss = 0.16 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:08:33.005395: step 309470, loss = 0.18 (1517.2 examples/sec; 0.084 sec/batch)
2017-06-02 10:08:33.852948: step 309480, loss = 0.19 (1510.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:08:34.708308: step 309490, loss = 0.22 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:08:35.682230: step 309500, loss = 0.21 (1314.3 examples/sec; 0.097 sec/batch)
2017-06-02 10:08:36.447471: step 309510, loss = 0.18 (1672.7 examples/sec; 0.077 sec/batch)
2017-06-02 10:08:37.312260: step 309520, loss = 0.19 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:08:38.166169: step 309530, loss = 0.27 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:08:39.022656: step 309540, loss = 0.22 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:08:39.872141: step 309550, loss = 0.18 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:08:40.733168: step 309560, loss = 0.20 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:08:41.593015: step 309570, loss = 0.18 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:08:42.454499: step 309580, loss = 0.18 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:08:43.307868: step 309590, loss = 0.16 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:08:44.260123: step 309600, loss = 0.22 (1344.2 examples/sec; 0.095 sec/batch)
2017-06-02 10:08:45.016215: step 309610, loss = 0.16 (1692.9 examples/sec; 0.076 sec/batch)
2017-06-02 10:08:45.862299: step 309620, loss = 0.17 (1512.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:08:46.707046: step 309630, loss = 0.20 (1515.3 examples/sec; 0.084 sec/batch)
2017-06-02 10:08:47.576083: step 309640, loss = 0.19 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:08:48.440148: step 309650, loss = 0.24 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:08:49.273835: step 309660, loss = 0.20 (1535.4 examples/sec; 0.083 sec/batch)
2017-06-02 10:08:50.140079: step 309670, loss = 0.26 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:08:50.995405: step 309680, loss = 0.17 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:08:51.839899: step 309690, loss = 0.18 (1515.7 examples/sec; 0.084 sec/batch)
2017-06-02 10:08:52.808437: step 309700, loss = 0.19 (1321.6 examples/sec; 0.097 sec/batch)
2017-06-02 10:08:53.592409: step 309710, loss = 0.15 (1632.7 examples/sec; 0.078 sec/batch)
2017-06-02 10:08:54.469273: step 309720, loss = 0.16 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:08:55.336029: step 309730, loss = 0.18 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:08:56.227123: step 309740, loss = 0.16 (1436.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:08:57.077563: step 309750, loss = 0.20 (1505.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:08:57.912546: step 309760, loss = 0.19 (1533.0 examples/sec; 0.083 sec/batch)
2017-06-02 10:08:58.759933: step 309770, loss = 0.19 (1510.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:08:59.634402: step 309780, loss = 0.17 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:09:00.499402: step 309790, loss = 0.14 (1479.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:09:01.468073: step 309800, loss = 0.19 (1321.4 examples/sec; 0.097 sec/batch)
2017-06-02 10:09:02.222775: step 309810, loss = 0.22 (1696.0 examples/sec; 0.075 sec/batch)
2017-06-02 10:09:03.062687: step 309820, loss = 0.17 (1524.0 examples/sec; 0.084 sec/batch)
2017-06-02 10:09:03.919837: step 309830, loss = 0.21 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:09:04.777907: step 309840, loss = 0.17 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:09:05.602452: step 309850, loss = 0.17 (1552.4 examples/sec; 0.082 sec/batch)
2017-06-02 10:09:06.463950: step 309860, loss = 0.19 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:09:07.315392: step 309870, loss = 0.17 (1503.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:09:08.179815: step 309880, loss = 0.18 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:09:09.035933: step 309890, loss = 0.20 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:09:09.994513: step 309900, loss = 0.18 (1335.3 examples/sec; 0.096 sec/batch)
2017-06-02 10:09:10.761537: step 309910, loss = 0.19 (1668.8 examples/sec; 0.077 sec/batch)
2017-06-02 10:09:11.637438: step 309920, loss = 0.22 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:09:12.500846: step 309930, loss = 0.17 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:09:13.336204: step 309940, loss = 0.15 (1532.3 examples/sec; 0.084 sec/batch)
2017-06-02 10:09:14.167876: step 309950, loss = 0.14 (1539.1 examples/sec; 0.083 sec/batch)
2017-06-02 10:09:14.993504: step 309960, loss = 0.19 (1550.3 examples/sec; 0.083 sec/batch)
2017-06-02 10:09:15.845537: step 309970, loss = 0.18 (1502.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:09:16.712309: step 309980, loss = 0.18 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:09:17.580706: step 309990, loss = 0.21 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:09:18.560659: step 310000, loss = 0.15 (1306.2 examples/sec; 0.098 sec/batch)
2017-06-02 10:09:19.332079: step 310010, loss = 0.18 (1659.3 examples/sec; 0.077 sec/batch)
2017-06-02 10:09:20.177726: step 310020, loss = 0.17 (1513.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:09:21.018106: step 310030, loss = 0.16 (1523.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:09:21.868287: step 310040, loss = 0.19 (1505.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:09:22.747856: step 310050, loss = 0.19 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:09:23.611667: step 310060, loss = 0.18 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:09:24.464510: step 310070, loss = 0.17 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:09:25.333448: step 310080, loss = 0.17 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:09:26.209366: step 310090, loss = 0.16 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:09:27.197680: step 310100, loss = 0.16 (1295.1 examples/sec; 0.099 sec/batch)
2017-06-02 10:09:27.985666: step 310110, loss = 0.20 (1624.4 examples/sec; 0.079 sec/batch)
2017-06-02 10:09:28.836639: step 310120, loss = 0.19 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:09:29.721444: step 310130, loss = 0.18 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:09:30.557678: step 310140, loss = 0.21 (1530.7 examples/sec; 0.084 sec/batch)
2017-06-02 10:09:31.401845: step 310150, loss = 0.15 (1516.3 examples/sec; 0.084 sec/batch)
2017-06-02 10:09:32.273615: step 310160, loss = 0.16 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:09:33.148303: step 310170, loss = 0.19 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:09:34.019144: step 310180, loss = 0.17 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:09:34.868223: step 310190, loss = 0.22 (1507.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:09:35.862601: step 310200, loss = 0.19 (1287.2 examples/sec; 0.099 sec/batch)
2017-06-02 10:09:36.626589: step 310210, loss = 0.17 (1675.4 examples/sec; 0.076 sec/batch)
2017-06-02 10:09:37.497543: step 310220, loss = 0.16 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:09:38.365408: step 310230, loss = 0.19 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:09:39.224553: step 310240, loss = 0.19 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:09:40.100531: step 310250, loss = 0.17 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:09:40.972099: step 310260, loss = 0.19 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:09:41.844038: step 310270, loss = 0.16 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:09:42.728363: step 310280, loss = 0.19 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:09:43.604854: step 310290, loss = 0.21 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:09:44.600348: step 310300, loss = 0.20 (1285.8 examples/sec; 0.100 sec/batch)
2017-06-02 10:09:45.359857: step 310310, loss = 0.17 (1685.3 examples/sec; 0.076 sec/batch)
2017-06-02 10:09:46.234378: step 310320, loss = 0.19 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:09:47.121218: step 310330, loss = 0.18 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:09:47.981871: step 310340, loss = 0.20 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:09:48.863607: step 310350, loss = 0.15 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:09:49.721173: step 310360, loss = 0.16 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:09:50.587360: step 310370, loss = 0.14 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:09:51.453520: step 310380, loss = 0.17 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:09:52.318787: step 310390, loss = 0.23 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:09:53.378640: step 310400, loss = 0.18 (1207.7 examples/sec; 0.106 sec/batch)
2017-06-02 10:09:54.147955: step 310410, loss = 0.20 (1663.8 examples/sec; 0.077 sec/batch)
2017-06-02 10:09:55.021080: step 310420, loss = 0.20 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:09:55.895236: step 310430, loss = 0.17 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:09:56.753836: step 310440, loss = 0.18 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:09:57.632853: step 310450, loss = 0.16 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:09:58.499789: step 310460, loss = 0.16 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:09:59.359848: step 310470, loss = 0.17 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:10:00.185002: step 310480, loss = 0.20 (1551.2 examples/sec; 0.083 sec/batch)
2017-06-02 10:10:01.014517: step 310490, loss = 0.20 (1543.1 examples/sec; 0.083 sec/batch)
2017-06-02 10:10:01.998855: step 310500, loss = 0.18 (1300.4 examples/sec; 0.098 sec/batch)
2017-06-02 10:10:02.756295: step 310510, loss = 0.19 (1689.9 examples/sec; 0.076 sec/batch)
2017-06-02 10:10:03.613786: step 310520, loss = 0.22 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:10:04.478484: step 310530, loss = 0.16 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:10:05.337964: step 310540, loss = 0.18 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:10:06.203308: step 310550, loss = 0.24 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:10:07.053370: step 310560, loss = 0.14 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:10:07.941964: step 310570, loss = 0.16 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 10:10:08.812162: step 310580, loss = 0.19 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:10:09.670810: step 310590, loss = 0.19 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:10:10.634505: step 310600, loss = 0.16 (1328.2 examples/sec; 0.096 sec/batch)
2017-06-02 10:10:11.401162: step 310610, loss = 0.17 (1669.6 examples/sec; 0.077 sec/batch)
2017-06-02 10:10:12.274547: step 310620, loss = 0.18 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:10:13.152373: step 310630, loss = 0.17 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:10:14.012647: step 310640, loss = 0.18 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:10:14.861521: step 310650, loss = 0.22 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:10:15.741307: step 310660, loss = 0.23 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:10:16.610298: step 310670, loss = 0.22 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:10:17.443592: step 310680, loss = 0.19 (1536.1 examples/sec; 0.083 sec/batch)
2017-06-02 10:10:18.302024: step 310690, loss = 0.21 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:10:19.280167: step 310700, loss = 0.19 (1308.6 examples/sec; 0.098 sec/batch)
2017-06-02 10:10:20.057789: step 310710, loss = 0.24 (1646.1 examples/sec; 0.078 sec/batch)
2017-06-02 10:10:20.953053: step 310720, loss = 0.23 (1429.7 examples/sec; 0.090 sec/batch)
2017-06-02 10:10:21.816961: step 310730, loss = 0.18 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:10:22.707408: step 310740, loss = 0.19 (1437.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:10:23.556209: step 310750, loss = 0.16 (1508.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:10:24.420523: step 310760, loss = 0.20 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:10:25.268043: step 310770, loss = 0.19 (1510.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:10:26.147465: step 310780, loss = 0.17 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:10:27.007476: step 310790, loss = 0.17 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:10:27.990906: step 310800, loss = 0.19 (1301.6 examples/sec; 0.098 sec/batch)
2017-06-02 10:10:28.737562: step 310810, loss = 0.17 (1714.3 examples/sec; 0.075 sec/batch)
2017-06-02 10:10:29.611907: step 310820, loss = 0.20 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:10:30.476054: step 310830, loss = 0.28 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:10:31.359279: step 310840, loss = 0.27 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:10:32.235398: step 310850, loss = 0.22 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:10:33.119900: step 310860, loss = 0.16 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:10:33.995285: step 310870, loss = 0.19 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:10:34.870251: step 310880, loss = 0.19 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:10:35.737825: step 310890, loss = 0.20 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:10:36.709917: step 310900, loss = 0.24 (1316.7 examples/sec; 0.097 sec/batch)
2017-06-02 10:10:37.494167: step 310910, loss = 0.20 (1632.2 examples/sec; 0.078 sec/batch)
2017-06-02 10:10:38.352416: step 310920, loss = 0.18 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:10:39.200206: step 310930, loss = 0.16 (1509.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:10:40.076428: step 310940, loss = 0.20 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:10:40.927413: step 310950, loss = 0.17 (1504.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:10:41.777452: step 310960, loss = 0.14 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:10:42.645200: step 310970, loss = 0.20 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:10:43.515850: step 310980, loss = 0.18 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:10:44.372099: step 310990, loss = 0.24 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:10:45.319346: step 311000, loss = 0.14 (1351.3 examples/sec; 0.095 sec/batch)
2017-06-02 10:10:46.110384: step 311010, loss = 0.17 (1618.2 examples/sec; 0.079 sec/batch)
2017-06-02 10:10:46.998625: step 311020, loss = 0.19 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:10:47.836282: step 311030, loss = 0.20 (1528.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:10:48.700297: step 311040, loss = 0.17 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:10:49.578575: step 311050, loss = 0.20 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:10:50.441400: step 311060, loss = 0.20 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:10:51.353891: step 311070, loss = 0.20 (1402.8 examples/sec; 0.091 sec/batch)
2017-06-02 10:10:52.236022: step 311080, loss = 0.16 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:10:53.118011: step 311090, loss = 0.18 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:10:54.109875: step 311100, loss = 0.18 (1290.5 examples/sec; 0.099 sec/batch)
2017-06-02 10:10:54.875846: step 311110, loss = 0.16 (1671.1 examples/sec; 0.077 sec/batch)
2017-06-02 10:10:55.751644: step 311120, loss = 0.17 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:10:56.652504: step 311130, loss = 0.16 (1420.9 examples/sec; 0.090 sec/batch)
2017-06-02 10:10:57.514744: step 311140, loss = 0.18 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:10:58.344934: step 311150, loss = 0.17 (1541.8 examples/sec; 0.083 sec/batch)
2017-06-02 10:10:59.207341: step 311160, loss = 0.18 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:11:00.069820: step 311170, loss = 0.18 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:11:00.949006: step 311180, loss = 0.21 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:11:01.809317: step 311190, loss = 0.16 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:11:02.755409: step 311200, loss = 0.18 (1352.9 examples/sec; 0.095 sec/batch)
2017-06-02 10:11:03.525387: step 311210, loss = 0.19 (1662.4 examples/sec; 0.077 sec/batch)
2017-06-02 10:11:04.371181: step 311220, loss = 0.20 (1513.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:11:05.265150: step 311230, loss = 0.21 (1431.8 examples/sec; 0.089 sec/batch)
2017-06-02 10:11:06.138672: step 311240, loss = 0.24 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:11:07.024208: step 311250, loss = 0.19 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:11:07.886613: step 311260, loss = 0.18 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:11:08.753476: step 311270, loss = 0.17 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:11:09.614499: step 311280, loss = 0.16 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:11:10.505589: step 311290, loss = 0.17 (1436.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:11:11.477422: step 311300, loss = 0.20 (1317.1 examples/sec; 0.097 sec/batch)
2017-06-02 10:11:12.241027: step 311310, loss = 0.21 (1676.2 examples/sec; 0.076 sec/batch)
2017-06-02 10:11:13.115851: step 311320, loss = 0.16 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:11:13.980255: step 311330, loss = 0.16 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:11:14.843919: step 311340, loss = 0.15 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:11:15.719200: step 311350, loss = 0.18 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:11:16.579834: step 311360, loss = 0.17 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:11:17.434563: step 311370, loss = 0.24 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:11:18.301054: step 311380, loss = 0.19 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:11:19.148354: step 311390, loss = 0.18 (1510.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:11:20.126963: step 311400, loss = 0.20 (1308.0 examples/sec; 0.098 sec/batch)
2017-06-02 10:11:20.884919: step 311410, loss = 0.24 (1688.7 examples/sec; 0.076 sec/batch)
2017-06-02 10:11:21.736320: step 311420, loss = 0.14 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:11:22.596359: step 311430, loss = 0.22 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:11:23.483006: step 311440, loss = 0.23 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:11:24.350187: step 311450, loss = 0.15 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:11:25.248397: step 311460, loss = 0.24 (1425.0 examples/sec; 0.090 sec/batch)
2017-06-02 10:11:26.109091: step 311470, loss = 0.15 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:11:26.986947: step 311480, loss = 0.19 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:11:27.846019: step 311490, loss = 0.18 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:11:28.810827: step 311500, loss = 0.18 (1326.7 examples/sec; 0.096 sec/batch)
2017-06-02 10:11:29.579548: step 311510, loss = 0.16 (1665.1 examples/sec; 0.077 sec/batch)
2017-06-02 10:11:30.438584: step 311520, loss = 0.16 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:11:31.307984: step 311530, loss = 0.17 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:11:32.186175: step 311540, loss = 0.15 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:11:33.062215: step 311550, loss = 0.21 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:11:33.921682: step 311560, loss = 0.18 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:11:34.786952: step 311570, loss = 0.19 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:11:35.653663: step 311580, loss = 0.16 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:11:36.519572: step 311590, loss = 0.24 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:11:37.500825: step 311600, loss = 0.17 (1304.5 examples/sec; 0.098 sec/batch)
2017-06-02 10:11:38.285780: step 311610, loss = 0.20 (1630.7 examples/sec; 0.078 sec/batch)
2017-06-02 10:11:39.164144: step 311620, loss = 0.18 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:11:40.043073: step 311630, loss = 0.17 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:11:40.901045: step 311640, loss = 0.16 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:11:41.789369: step 311650, loss = 0.18 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:11:42.648195: step 311660, loss = 0.18 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:11:43.528518: step 311670, loss = 0.17 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:11:44.408862: step 311680, loss = 0.18 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:11:45.273918: step 311690, loss = 0.16 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:11:46.237776: step 311700, loss = 0.15 (1328.0 examples/sec; 0.096 sec/batch)
2017-06-02 10:11:47.014610: step 311710, loss = 0.19 (1647.7 examples/sec; 0.078 sec/batch)
2017-06-02 10:11:47.891967: step 311720, loss = 0.19 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:11:48.752802: step 311730, loss = 0.20 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:11:49.632555: step 311740, loss = 0.15 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:11:50.481611: step 311750, loss = 0.20 (1507.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:11:51.340246: step 311760, loss = 0.15 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:11:52.223706: step 311770, loss = 0.25 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:11:53.099355: step 311780, loss = 0.19 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:11:53.964562: step 311790, loss = 0.16 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:11:54.974876: step 311800, loss = 0.19 (1266.9 examples/sec; 0.101 sec/batch)
2017-06-02 10:11:55.680526: step 311810, loss = 0.18 (1813.9 examples/sec; 0.071 sec/batch)
2017-06-02 10:11:56.537159: step 311820, loss = 0.20 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:11:57.388868: step 311830, loss = 0.18 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:11:58.229398: step 311840, loss = 0.15 (1522.8 examples/sec; 0.084 sec/batch)
2017-06-02 10:11:59.111049: step 311850, loss = 0.22 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:11:59.990615: step 311860, loss = 0.19 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:12:00.833379: step 311870, loss = 0.15 (1518.8 examples/sec; 0.084 sec/batch)
2017-06-02 10:12:01.705612: step 311880, loss = 0.18 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:12:02.555257: step 311890, loss = 0.16 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:12:03.522033: step 311900, loss = 0.18 (1324.0 examples/sec; 0.097 sec/batch)
2017-06-02 10:12:04.283956: step 311910, loss = 0.19 (1680.0 examples/sec; 0.076 sec/batch)
2017-06-02 10:12:05.173708: step 311920, loss = 0.18 (1438.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:12:06.025481: step 311930, loss = 0.16 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:12:06.874853: step 311940, loss = 0.16 (1507.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:12:07.736172: step 311950, loss = 0.17 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:12:08.613359: step 311960, loss = 0.17 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:12:09.499839: step 311970, loss = 0.18 (1443.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:12:10.355512: step 311980, loss = 0.19 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:12:11.231354: step 311990, loss = 0.30 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:12:12.225309: step 312000, loss = 0.17 (1287.8 examples/sec; 0.099 sec/batch)
2017-06-02 10:12:13.005740: step 312010, loss = 0.20 (1640.1 examples/sec; 0.078 sec/batch)
2017-06-02 10:12:13.889370: step 312020, loss = 0.17 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:12:14.759494: step 312030, loss = 0.18 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:12:15.626041: step 312040, loss = 0.21 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:12:16.495067: step 312050, loss = 0.20 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:12:17.376411: step 312060, loss = 0.20 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:12:18.247337: step 312070, loss = 0.21 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:12:19.097134: step 312080, loss = 0.22 (1506.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:12:19.975210: step 312090, loss = 0.19 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:12:20.935594: step 312100, loss = 0.23 (1332.8 examples/sec; 0.096 sec/batch)
2017-06-02 10:12:21.691698: step 312110, loss = 0.15 (1692.9 examples/sec; 0.076 sec/batch)
2017-06-02 10:12:22.574609: step 312120, loss = 0.15 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:12:23.418434: step 312130, loss = 0.22 (1516.9 examples/sec; 0.084 sec/batch)
2017-06-02 10:12:24.290520: step 312140, loss = 0.19 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:12:25.149594: step 312150, loss = 0.23 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:12:25.996315: step 312160, loss = 0.20 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:12:26.853097: step 312170, loss = 0.15 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:12:27.694841: step 312180, loss = 0.17 (1520.7 examples/sec; 0.084 sec/batch)
2017-06-02 10:12:28.542989: step 312190, loss = 0.20 (1509.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:12:29.502423: step 312200, loss = 0.16 (1334.1 examples/sec; 0.096 sec/batch)
2017-06-02 10:12:30.257640: step 312210, loss = 0.18 (1694.9 examples/sec; 0.076 sec/batch)
2017-06-02 10:12:31.100526: step 312220, loss = 0.20 (1518.6 examples/sec; 0.084 sec/batch)
2017-06-02 10:12:31.939500: step 312230, loss = 0.19 (1525.7 examples/sec; 0.084 sec/batch)
2017-06-02 10:12:32.835747: step 312240, loss = 0.16 (1428.2 examples/sec; 0.090 sec/batch)
2017-06-02 10:12:33.707438: step 312250, loss = 0.21 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:12:34.583618: step 312260, loss = 0.16 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:12:35.473985: step 312270, loss = 0.23 (1437.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:12:36.344007: step 312280, loss = 0.16 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:12:37.211454: step 312290, loss = 0.15 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:12:38.182029: step 312300, loss = 0.15 (1318.8 examples/sec; 0.097 sec/batch)
2017-06-02 10:12:38.955114: step 312310, loss = 0.15 (1655.7 examples/sec; 0.077 sec/batch)
2017-06-02 10:12:39.843279: step 312320, loss = 0.24 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:12:40.719309: step 312330, loss = 0.16 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:12:41.594962: step 312340, loss = 0.23 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:12:42.464164: step 312350, loss = 0.15 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:12:43.315678: step 312360, loss = 0.17 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:12:44.180779: step 312370, loss = 0.23 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:12:45.043647: step 312380, loss = 0.21 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:12:45.893838: step 312390, loss = 0.19 (1505.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:12:46.852197: step 312400, loss = 0.20 (1335.6 examples/sec; 0.096 sec/batch)
2017-06-02 10:12:47.602566: step 312410, loss = 0.25 (1705.8 examples/sec; 0.075 sec/batch)
2017-06-02 10:12:48.481917: step 312420, loss = 0.20 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:12:49.352782: step 312430, loss = 0.22 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:12:50.218144: step 312440, loss = 0.20 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:12:51.078877: step 312450, loss = 0.20 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:12:51.946463: step 312460, loss = 0.17 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:12:52.812519: step 312470, loss = 0.18 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:12:53.669193: step 312480, loss = 0.16 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:12:54.562072: step 312490, loss = 0.19 (1433.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:12:55.525385: step 312500, loss = 0.17 (1328.7 examples/sec; 0.096 sec/batch)
2017-06-02 10:12:56.296255: step 312510, loss = 0.22 (1660.5 examples/sec; 0.077 sec/batch)
2017-06-02 10:12:57.186890: step 312520, loss = 0.14 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:12:58.050720: step 312530, loss = 0.18 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:12:58.926516: step 312540, loss = 0.21 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:12:59.814024: step 312550, loss = 0.14 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:13:00.661261: step 312560, loss = 0.23 (1510.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:13:01.523011: step 312570, loss = 0.15 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:13:02.366839: step 312580, loss = 0.21 (1516.9 examples/sec; 0.084 sec/batch)
2017-06-02 10:13:03.223992: step 312590, loss = 0.15 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:13:04.191013: step 312600, loss = 0.20 (1323.7 examples/sec; 0.097 sec/batch)
2017-06-02 10:13:04.966216: step 312610, loss = 0.19 (1651.2 examples/sec; 0.078 sec/batch)
2017-06-02 10:13:05.823909: step 312620, loss = 0.15 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:13:06.672155: step 312630, loss = 0.21 (1509.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:13:07.547724: step 312640, loss = 0.17 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:13:08.395362: step 312650, loss = 0.16 (1510.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:13:09.285236: step 312660, loss = 0.18 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:13:10.142524: step 312670, loss = 0.17 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:13:10.993634: step 312680, loss = 0.19 (1503.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:13:11.836981: step 312690, loss = 0.24 (1517.8 examples/sec; 0.084 sec/batch)
2017-06-02 10:13:12.797837: step 312700, loss = 0.19 (1332.1 examples/sec; 0.096 sec/batch)
2017-06-02 10:13:13.572804: step 312710, loss = 0.19 (1651.7 examples/sec; 0.077 sec/batch)
2017-06-02 10:13:14.464602: step 312720, loss = 0.17 (1435.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:13:15.313755: step 312730, loss = 0.21 (1507.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:13:16.162898: step 312740, loss = 0.13 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:13:17.048043: step 312750, loss = 0.15 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 10:13:17.887853: step 312760, loss = 0.22 (1524.2 examples/sec; 0.084 sec/batch)
2017-06-02 10:13:18.726546: step 312770, loss = 0.17 (1526.2 examples/sec; 0.084 sec/batch)
2017-06-02 10:13:19.582910: step 312780, loss = 0.15 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:13:20.473522: step 312790, loss = 0.20 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:13:21.435952: step 312800, loss = 0.19 (1330.0 examples/sec; 0.096 sec/batch)
2017-06-02 10:13:22.204367: step 312810, loss = 0.17 (1665.8 examples/sec; 0.077 sec/batch)
2017-06-02 10:13:23.095172: step 312820, loss = 0.22 (1436.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:13:23.947115: step 312830, loss = 0.22 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:13:24.798414: step 312840, loss = 0.18 (1503.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:13:25.689721: step 312850, loss = 0.22 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 10:13:26.559885: step 312860, loss = 0.16 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:13:27.450481: step 312870, loss = 0.14 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:13:28.338559: step 312880, loss = 0.17 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:13:29.218432: step 312890, loss = 0.18 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:13:30.189584: step 312900, loss = 0.18 (1318.0 examples/sec; 0.097 sec/batch)
2017-06-02 10:13:30.962993: step 312910, loss = 0.18 (1655.0 examples/sec; 0.077 sec/batch)
2017-06-02 10:13:31.810054: step 312920, loss = 0.17 (1511.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:13:32.652994: step 312930, loss = 0.19 (1518.5 examples/sec; 0.084 sec/batch)
2017-06-02 10:13:33.534583: step 312940, loss = 0.28 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:13:34.396891: step 312950, loss = 0.15 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:13:35.274397: step 312960, loss = 0.16 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:13:36.117935: step 312970, loss = 0.16 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 10:13:36.992065: step 312980, loss = 0.15 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:13:37.866891: step 312990, loss = 0.22 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:13:38.841409: step 313000, loss = 0.15 (1313.5 examples/sec; 0.097 sec/batch)
2017-06-02 10:13:39.610137: step 313010, loss = 0.21 (1665.1 examples/sec; 0.077 sec/batch)
2017-06-02 10:13:40.488191: step 313020, loss = 0.18 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:13:41.344571: step 313030, loss = 0.21 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:13:42.207891: step 313040, loss = 0.17 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:13:43.083978: step 313050, loss = 0.24 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:13:43.971125: step 313060, loss = 0.15 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 10:13:44.861844: step 313070, loss = 0.22 (1437.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:13:45.742164: step 313080, loss = 0.18 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:13:46.607818: step 313090, loss = 0.14 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:13:47.570313: step 313100, loss = 0.22 (1329.9 examples/sec; 0.096 sec/batch)
2017-06-02 10:13:48.343110: step 313110, loss = 0.20 (1656.3 examples/sec; 0.077 sec/batch)
2017-06-02 10:13:49.216601: step 313120, loss = 0.17 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:13:50.105752: step 313130, loss = 0.20 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:13:50.976251: step 313140, loss = 0.18 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:13:51.856446: step 313150, loss = 0.15 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:13:52.716746: step 313160, loss = 0.20 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:13:53.605211: step 313170, loss = 0.17 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 10:13:54.482129: step 313180, loss = 0.17 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:13:55.349057: step 313190, loss = 0.18 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:13:56.358717: step 313200, loss = 0.21 (1267.8 examples/sec; 0.101 sec/batch)
2017-06-02 10:13:57.101550: step 313210, loss = 0.18 (1723.2 examples/sec; 0.074 sec/batch)
2017-06-02 10:13:57.990798: step 313220, loss = 0.17 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:13:58.881321: step 313230, loss = 0.18 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:13:59.766095: step 313240, loss = 0.15 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:14:00.647184: step 313250, loss = 0.22 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:14:01.528457: step 313260, loss = 0.16 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:14:02.376787: step 313270, loss = 0.15 (1508.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:14:03.263217: step 313280, loss = 0.14 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:14:04.159341: step 313290, loss = 0.19 (1428.4 examples/sec; 0.090 sec/batch)
2017-06-02 10:14:05.147671: step 313300, loss = 0.18 (1295.1 examples/sec; 0.099 sec/batch)
2017-06-02 10:14:05.928597: step 313310, loss = 0.22 (1639.1 examples/sec; 0.078 sec/batch)
2017-06-02 10:14:06.781958: step 313320, loss = 0.16 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:14:07.644782: step 313330, loss = 0.20 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:14:08.524569: step 313340, loss = 0.18 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:14:09.393238: step 313350, loss = 0.13 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:14:10.249119: step 313360, loss = 0.19 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:14:11.108189: step 313370, loss = 0.15 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:14:11.979320: step 313380, loss = 0.17 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:14:12.832826: step 313390, loss = 0.26 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:14:13.795318: step 313400, loss = 0.22 (1329.9 examples/sec; 0.096 sec/batch)
2017-06-02 10:14:14.567945: step 313410, loss = 0.18 (1656.7 examples/sec; 0.077 sec/batch)
2017-06-02 10:14:15.455624: step 313420, loss = 0.22 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:14:16.315989: step 313430, loss = 0.17 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:14:17.189274: step 313440, loss = 0.20 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:14:18.056655: step 313450, loss = 0.24 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:14:18.920675: step 313460, loss = 0.18 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:14:19.776481: step 313470, loss = 0.19 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:14:20.637661: step 313480, loss = 0.20 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:14:21.493928: step 313490, loss = 0.21 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:14:22.479297: step 313500, loss = 0.19 (1299.0 examples/sec; 0.099 sec/batch)
2017-06-02 10:14:23.265148: step 313510, loss = 0.18 (1628.8 examples/sec; 0.079 sec/batch)
2017-06-02 10:14:24.130246: step 313520, loss = 0.20 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:14:24.996711: step 313530, loss = 0.16 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:14:25.855050: step 313540, loss = 0.15 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:14:26.723106: step 313550, loss = 0.18 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:14:27.570627: step 313560, loss = 0.18 (1510.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:14:28.440447: step 313570, loss = 0.14 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:14:29.315963: step 313580, loss = 0.18 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:14:30.164519: step 313590, loss = 0.18 (1508.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:14:31.121433: step 313600, loss = 0.19 (1337.6 examples/sec; 0.096 sec/batch)
2017-06-02 10:14:31.900839: step 313610, loss = 0.20 (1642.3 examples/sec; 0.078 sec/batch)
2017-06-02 10:14:32.786031: step 313620, loss = 0.16 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:14:33.656316: step 313630, loss = 0.17 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:14:34.528076: step 313640, loss = 0.17 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:14:35.396661: step 313650, loss = 0.27 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:14:36.276361: step 313660, loss = 0.20 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:14:37.130245: step 313670, loss = 0.20 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:14:38.011788: step 313680, loss = 0.15 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:14:38.848230: step 313690, loss = 0.18 (1530.3 examples/sec; 0.084 sec/batch)
2017-06-02 10:14:39.799940: step 313700, loss = 0.18 (1344.9 examples/sec; 0.095 sec/batch)
2017-06-02 10:14:40.544687: step 313710, loss = 0.20 (1718.7 examples/sec; 0.074 sec/batch)
2017-06-02 10:14:41.425818: step 313720, loss = 0.20 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:14:42.300810: step 313730, loss = 0.26 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:14:43.173872: step 313740, loss = 0.17 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:14:44.054379: step 313750, loss = 0.20 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:14:44.933512: step 313760, loss = 0.24 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:14:45.787620: step 313770, loss = 0.17 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:14:46.660812: step 313780, loss = 0.18 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:14:47.535503: step 313790, loss = 0.16 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:14:48.502276: step 313800, loss = 0.16 (1324.0 examples/sec; 0.097 sec/batch)
2017-06-02 10:14:49.279523: step 313810, loss = 0.22 (1646.8 examples/sec; 0.078 sec/batch)
2017-06-02 10:14:50.144628: step 313820, loss = 0.23 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:14:51.031055: step 313830, loss = 0.13 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:14:51.914080: step 313840, loss = 0.16 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:14:52.758510: step 313850, loss = 0.19 (1515.8 examples/sec; 0.084 sec/batch)
2017-06-02 10:14:53.618552: step 313860, loss = 0.17 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:14:54.470631: step 313870, loss = 0.15 (1502.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:14:55.347206: step 313880, loss = 0.28 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:14:56.209463: step 313890, loss = 0.16 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:14:57.192972: step 313900, loss = 0.22 (1301.5 examples/sec; 0.098 sec/batch)
2017-06-02 10:14:57.960804: step 313910, loss = 0.19 (1667.1 examples/sec; 0.077 sec/batch)
2017-06-02 10:14:58.808311: step 313920, loss = 0.17 (1510.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:14:59.681660: step 313930, loss = 0.22 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:15:00.537260: step 313940, loss = 0.20 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:15:01.402337: step 313950, loss = 0.18 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:15:02.259833: step 313960, loss = 0.28 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:15:03.140134: step 313970, loss = 0.22 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:15:03.996228: step 313980, loss = 0.17 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:15:04.842381: step 313990, loss = 0.16 (1512.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:15:05.811698: step 314000, loss = 0.17 (1320.5 examples/sec; 0.097 sec/batch)
2017-06-02 10:15:06.557905: step 314010, loss = 0.14 (1715.4 examples/sec; 0.075 sec/batch)
2017-06-02 10:15:07.412310: step 314020, loss = 0.16 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:15:08.262514: step 314030, loss = 0.15 (1505.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:15:09.118499: step 314040, loss = 0.16 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:15:09.955596: step 314050, loss = 0.18 (1529.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:15:10.869593: step 314060, loss = 0.19 (1400.4 examples/sec; 0.091 sec/batch)
2017-06-02 10:15:11.737037: step 314070, loss = 0.16 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:15:12.598411: step 314080, loss = 0.16 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:15:13.458800: step 314090, loss = 0.15 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:15:14.435750: step 314100, loss = 0.17 (1309.7 examples/sec; 0.098 sec/batch)
2017-06-02 10:15:15.196352: step 314110, loss = 0.15 (1682.9 examples/sec; 0.076 sec/batch)
2017-06-02 10:15:16.059404: step 314120, loss = 0.17 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:15:16.928010: step 314130, loss = 0.14 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:15:17.814316: step 314140, loss = 0.16 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:15:18.665266: step 314150, loss = 0.23 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:15:19.516107: step 314160, loss = 0.20 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:15:20.379343: step 314170, loss = 0.14 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:15:21.217251: step 314180, loss = 0.22 (1527.6 examples/sec; 0.084 sec/batch)
2017-06-02 10:15:22.059791: step 314190, loss = 0.25 (1519.2 examples/sec; 0.084 sec/batch)
2017-06-02 10:15:23.027660: step 314200, loss = 0.22 (1322.5 examples/sec; 0.097 sec/batch)
2017-06-02 10:15:23.796245: step 314210, loss = 0.18 (1665.4 examples/sec; 0.077 sec/batch)
2017-06-02 10:15:24.667832: step 314220, loss = 0.21 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:15:25.524909: step 314230, loss = 0.20 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:15:26.397483: step 314240, loss = 0.19 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:15:27.270851: step 314250, loss = 0.18 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:15:28.122491: step 314260, loss = 0.17 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:15:28.967321: step 314270, loss = 0.17 (1515.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:15:29.805193: step 314280, loss = 0.22 (1527.7 examples/sec; 0.084 sec/batch)
2017-06-02 10:15:30.664016: step 314290, loss = 0.18 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:15:31.637376: step 314300, loss = 0.18 (1315.0 examples/sec; 0.097 sec/batch)
2017-06-02 10:15:32.415468: step 314310, loss = 0.16 (1645.1 examples/sec; 0.078 sec/batch)
2017-06-02 10:15:33.296963: step 314320, loss = 0.19 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:15:34.157580: step 314330, loss = 0.23 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:15:35.003396: step 314340, loss = 0.16 (1513.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:15:35.869180: step 314350, loss = 0.21 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:15:36.754489: step 314360, loss = 0.17 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 10:15:37.599600: step 314370, loss = 0.22 (1514.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:15:38.471438: step 314380, loss = 0.18 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:15:39.352265: step 314390, loss = 0.15 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:15:40.326003: step 314400, loss = 0.21 (1314.5 examples/sec; 0.097 sec/batch)
2017-06-02 10:15:41.081188: step 314410, loss = 0.18 (1695.0 examples/sec; 0.076 sec/batch)
2017-06-02 10:15:41.928226: step 314420, loss = 0.13 (1511.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:15:42.797808: step 314430, loss = 0.19 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:15:43.650091: step 314440, loss = 0.22 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:15:44.517649: step 314450, loss = 0.19 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:15:45.410219: step 314460, loss = 0.18 (1434.1 examples/sec; 0.089 sec/batch)
2017-06-02 10:15:46.286930: step 314470, loss = 0.20 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:15:47.195038: step 314480, loss = 0.23 (1409.5 examples/sec; 0.091 sec/batch)
2017-06-02 10:15:48.104243: step 314490, loss = 0.16 (1407.8 examples/sec; 0.091 sec/batch)
2017-06-02 10:15:49.090015: step 314500, loss = 0.18 (1298.5 examples/sec; 0.099 sec/batch)
2017-06-02 10:15:49.860725: step 314510, loss = 0.15 (1660.8 examples/sec; 0.077 sec/batch)
2017-06-02 10:15:50.733711: step 314520, loss = 0.15 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:15:51.627457: step 314530, loss = 0.18 (1432.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:15:52.475798: step 314540, loss = 0.19 (1508.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:15:53.358662: step 314550, loss = 0.24 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:15:54.239596: step 314560, loss = 0.16 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:15:55.108380: step 314570, loss = 0.16 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:15:55.964917: step 314580, loss = 0.24 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:15:56.842650: step 314590, loss = 0.15 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:15:57.822881: step 314600, loss = 0.17 (1305.8 examples/sec; 0.098 sec/batch)
2017-06-02 10:15:58.571126: step 314610, loss = 0.30 (1710.7 examples/sec; 0.075 sec/batch)
2017-06-02 10:15:59.445962: step 314620, loss = 0.19 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:16:00.299571: step 314630, loss = 0.27 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:16:01.172021: step 314640, loss = 0.20 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:16:02.048966: step 314650, loss = 0.17 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:16:02.893051: step 314660, loss = 0.20 (1516.4 examples/sec; 0.084 sec/batch)
2017-06-02 10:16:03.773022: step 314670, loss = 0.18 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:16:04.636425: step 314680, loss = 0.19 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:16:05.477993: step 314690, loss = 0.18 (1520.9 examples/sec; 0.084 sec/batch)
2017-06-02 10:16:06.448531: step 314700, loss = 0.16 (1318.8 examples/sec; 0.097 sec/batch)
2017-06-02 10:16:07.237634: step 314710, loss = 0.20 (1622.1 examples/sec; 0.079 sec/batch)
2017-06-02 10:16:08.131059: step 314720, loss = 0.13 (1432.7 examples/sec; 0.089 sec/batch)
2017-06-02 10:16:08.979645: step 314730, loss = 0.18 (1508.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:16:09.833684: step 314740, loss = 0.21 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:16:10.706361: step 314750, loss = 0.17 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:16:11.577277: step 314760, loss = 0.16 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:16:12.416844: step 314770, loss = 0.17 (1524.6 examples/sec; 0.084 sec/batch)
2017-06-02 10:16:13.246162: step 314780, loss = 0.19 (1543.5 examples/sec; 0.083 sec/batch)
2017-06-02 10:16:14.113141: step 314790, loss = 0.18 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:16:15.119234: step 314800, loss = 0.18 (1272.2 examples/sec; 0.101 sec/batch)
2017-06-02 10:16:15.861424: step 314810, loss = 0.16 (1724.6 examples/sec; 0.074 sec/batch)
2017-06-02 10:16:16.703864: step 314820, loss = 0.20 (1519.4 examples/sec; 0.084 sec/batch)
2017-06-02 10:16:17.583103: step 314830, loss = 0.15 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:16:18.445165: step 314840, loss = 0.20 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:16:19.327920: step 314850, loss = 0.27 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:16:20.198752: step 314860, loss = 0.21 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:16:21.039615: step 314870, loss = 0.16 (1522.2 examples/sec; 0.084 sec/batch)
2017-06-02 10:16:21.899262: step 314880, loss = 0.15 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:16:22.725335: step 314890, loss = 0.22 (1549.5 examples/sec; 0.083 sec/batch)
2017-06-02 10:16:23.692917: step 314900, loss = 0.24 (1322.9 examples/sec; 0.097 sec/batch)
2017-06-02 10:16:24.429982: step 314910, loss = 0.14 (1736.6 examples/sec; 0.074 sec/batch)
2017-06-02 10:16:25.327802: step 314920, loss = 0.14 (1425.7 examples/sec; 0.090 sec/batch)
2017-06-02 10:16:26.180593: step 314930, loss = 0.16 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:16:27.038124: step 314940, loss = 0.13 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:16:27.882147: step 314950, loss = 0.21 (1516.5 examples/sec; 0.084 sec/batch)
2017-06-02 10:16:28.749166: step 314960, loss = 0.20 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:16:29.607387: step 314970, loss = 0.15 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:16:30.459863: step 314980, loss = 0.19 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:16:31.314792: step 314990, loss = 0.13 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:16:32.310675: step 315000, loss = 0.18 (1285.3 examples/sec; 0.100 sec/batch)
2017-06-02 10:16:33.049792: step 315010, loss = 0.28 (1731.8 examples/sec; 0.074 sec/batch)
2017-06-02 10:16:33.896210: step 315020, loss = 0.14 (1512.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:16:34.745755: step 315030, loss = 0.18 (1506.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:16:35.607614: step 315040, loss = 0.17 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:16:36.494029: step 315050, loss = 0.19 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:16:37.314238: step 315060, loss = 0.17 (1560.6 examples/sec; 0.082 sec/batch)
2017-06-02 10:16:38.160454: step 315070, loss = 0.19 (1512.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:16:39.034167: step 315080, loss = 0.15 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:16:39.895140: step 315090, loss = 0.22 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:16:40.878345: step 315100, loss = 0.14 (1301.9 examples/sec; 0.098 sec/batch)
2017-06-02 10:16:41.673924: step 315110, loss = 0.21 (1608.9 examples/sec; 0.080 sec/batch)
2017-06-02 10:16:42.515043: step 315120, loss = 0.19 (1521.8 examples/sec; 0.084 sec/batch)
2017-06-02 10:16:43.378058: step 315130, loss = 0.17 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:16:44.268267: step 315140, loss = 0.18 (1437.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:16:45.128566: step 315150, loss = 0.21 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:16:45.965894: step 315160, loss = 0.16 (1528.7 examples/sec; 0.084 sec/batch)
2017-06-02 10:16:46.786200: step 315170, loss = 0.18 (1560.4 examples/sec; 0.082 sec/batch)
2017-06-02 10:16:47.626871: step 315180, loss = 0.15 (1522.6 examples/sec; 0.084 sec/batch)
2017-06-02 10:16:48.501018: step 315190, loss = 0.16 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:16:49.451541: step 315200, loss = 0.20 (1346.6 examples/sec; 0.095 sec/batch)
2017-06-02 10:16:50.211226: step 315210, loss = 0.20 (1684.9 examples/sec; 0.076 sec/batch)
2017-06-02 10:16:51.087320: step 315220, loss = 0.23 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:16:51.965259: step 315230, loss = 0.20 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:16:52.825017: step 315240, loss = 0.17 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:16:53.694427: step 315250, loss = 0.21 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:16:54.572359: step 315260, loss = 0.22 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:16:55.436806: step 315270, loss = 0.20 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:16:56.286856: step 315280, loss = 0.16 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:16:57.158944: step 315290, loss = 0.26 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:16:58.126896: step 315300, loss = 0.25 (1322.4 examples/sec; 0.097 sec/batch)
2017-06-02 10:16:58.906727: step 315310, loss = 0.22 (1641.4 examples/sec; 0.078 sec/batch)
2017-06-02 10:16:59.778393: step 315320, loss = 0.17 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:17:00.618708: step 315330, loss = 0.23 (1523.2 examples/sec; 0.084 sec/batch)
2017-06-02 10:17:01.493158: step 315340, loss = 0.15 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:17:02.347218: step 315350, loss = 0.15 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:17:03.223863: step 315360, loss = 0.26 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:17:04.101764: step 315370, loss = 0.17 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:17:04.964589: step 315380, loss = 0.20 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:17:05.835587: step 315390, loss = 0.20 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:17:06.852030: step 315400, loss = 0.17 (1259.3 examples/sec; 0.102 sec/batch)
2017-06-02 10:17:07.579076: step 315410, loss = 0.18 (1760.6 examples/sec; 0.073 sec/batch)
2017-06-02 10:17:08.468999: step 315420, loss = 0.16 (1438.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:17:09.318079: step 315430, loss = 0.17 (1507.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:17:10.183014: step 315440, loss = 0.20 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:17:11.098644: step 315450, loss = 0.22 (1398.0 examples/sec; 0.092 sec/batch)
2017-06-02 10:17:11.962255: step 315460, loss = 0.17 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:17:12.841505: step 315470, loss = 0.17 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:17:13.724307: step 315480, loss = 0.15 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:17:14.604306: step 315490, loss = 0.18 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:17:15.566096: step 315500, loss = 0.14 (1330.8 examples/sec; 0.096 sec/batch)
2017-06-02 10:17:16.326432: step 315510, loss = 0.17 (1683.5 examples/sec; 0.076 sec/batch)
2017-06-02 10:17:17.188039: step 315520, loss = 0.17 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:17:18.057351: step 315530, loss = 0.20 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:17:18.924103: step 315540, loss = 0.14 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:17:19.783255: step 315550, loss = 0.21 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:17:20.641315: step 315560, loss = 0.18 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:17:21.483438: step 315570, loss = 0.19 (1520.0 examples/sec; 0.084 sec/batch)
2017-06-02 10:17:22.341368: step 315580, loss = 0.24 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:17:23.221013: step 315590, loss = 0.18 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:17:24.178578: step 315600, loss = 0.15 (1336.7 examples/sec; 0.096 sec/batch)
2017-06-02 10:17:24.942200: step 315610, loss = 0.24 (1676.2 examples/sec; 0.076 sec/batch)
2017-06-02 10:17:25.802190: step 315620, loss = 0.17 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:17:26.664990: step 315630, loss = 0.20 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:17:27.528955: step 315640, loss = 0.20 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:17:28.383061: step 315650, loss = 0.23 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:17:29.279811: step 315660, loss = 0.18 (1427.3 examples/sec; 0.090 sec/batch)
2017-06-02 10:17:30.135270: step 315670, loss = 0.18 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:17:30.990226: step 315680, loss = 0.18 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:17:31.826928: step 315690, loss = 0.19 (1529.8 examples/sec; 0.084 sec/batch)
2017-06-02 10:17:32.830073: step 315700, loss = 0.18 (1276.0 examples/sec; 0.100 sec/batch)
2017-06-02 10:17:33.540391: step 315710, loss = 0.15 (1802.0 examples/sec; 0.071 sec/batch)
2017-06-02 10:17:34.407870: step 315720, loss = 0.23 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:17:35.256632: step 315730, loss = 0.18 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:17:36.135450: step 315740, loss = 0.20 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:17:37.017844: step 315750, loss = 0.18 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:17:37.878843: step 315760, loss = 0.23 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:17:38.757934: step 315770, loss = 0.18 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:17:39.629114: step 315780, loss = 0.19 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:17:40.485439: step 315790, loss = 0.22 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:17:41.467309: step 315800, loss = 0.22 (1303.6 examples/sec; 0.098 sec/batch)
2017-06-02 10:17:42.235605: step 315810, loss = 0.20 (1666.0 examples/sec; 0.077 sec/batch)
2017-06-02 10:17:43.101031: step 315820, loss = 0.17 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:17:43.950933: step 315830, loss = 0.15 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:17:44.817475: step 315840, loss = 0.19 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:17:45.680176: step 315850, loss = 0.21 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:17:46.553287: step 315860, loss = 0.19 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:17:47.413386: step 315870, loss = 0.21 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:17:48.266362: step 315880, loss = 0.22 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:17:49.126484: step 315890, loss = 0.18 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:17:50.109807: step 315900, loss = 0.24 (1301.7 examples/sec; 0.098 sec/batch)
2017-06-02 10:17:50.874702: step 315910, loss = 0.22 (1673.4 examples/sec; 0.076 sec/batch)
2017-06-02 10:17:51.748053: step 315920, loss = 0.20 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:17:52.616819: step 315930, loss = 0.14 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:17:53.448316: step 315940, loss = 0.20 (1539.4 examples/sec; 0.083 sec/batch)
2017-06-02 10:17:54.335640: step 315950, loss = 0.23 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 10:17:55.185418: step 315960, loss = 0.17 (1506.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:17:56.059182: step 315970, loss = 0.18 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:17:56.931669: step 315980, loss = 0.21 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:17:57.778624: step 315990, loss = 0.18 (1511.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:17:58.782260: step 316000, loss = 0.14 (1275.4 examples/sec; 0.100 sec/batch)
2017-06-02 10:17:59.525606: step 316010, loss = 0.18 (1722.0 examples/sec; 0.074 sec/batch)
2017-06-02 10:18:00.403897: step 316020, loss = 0.15 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:18:01.300429: step 316030, loss = 0.17 (1427.7 examples/sec; 0.090 sec/batch)
2017-06-02 10:18:02.189108: step 316040, loss = 0.15 (1440.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:18:03.042699: step 316050, loss = 0.18 (1499.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:18:03.916554: step 316060, loss = 0.18 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:18:04.763857: step 316070, loss = 0.22 (1510.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:18:05.638420: step 316080, loss = 0.17 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:18:06.474486: step 316090, loss = 0.20 (1531.0 examples/sec; 0.084 sec/batch)
2017-06-02 10:18:07.462911: step 316100, loss = 0.19 (1295.0 examples/sec; 0.099 sec/batch)
2017-06-02 10:18:08.199654: step 316110, loss = 0.19 (1737.4 examples/sec; 0.074 sec/batch)
2017-06-02 10:18:09.078895: step 316120, loss = 0.22 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:18:09.941387: step 316130, loss = 0.20 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:18:10.792952: step 316140, loss = 0.19 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:18:11.657653: step 316150, loss = 0.20 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:18:12.537533: step 316160, loss = 0.21 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:18:13.396288: step 316170, loss = 0.20 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:18:14.249346: step 316180, loss = 0.26 (1500.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:18:15.099706: step 316190, loss = 0.17 (1505.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:18:16.067615: step 316200, loss = 0.18 (1322.4 examples/sec; 0.097 sec/batch)
2017-06-02 10:18:16.828157: step 316210, loss = 0.19 (1683.1 examples/sec; 0.076 sec/batch)
2017-06-02 10:18:17.674315: step 316220, loss = 0.17 (1512.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:18:18.551062: step 316230, loss = 0.18 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:18:19.438411: step 316240, loss = 0.19 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 10:18:20.281858: step 316250, loss = 0.23 (1517.6 examples/sec; 0.084 sec/batch)
2017-06-02 10:18:21.147847: step 316260, loss = 0.16 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:18:22.019439: step 316270, loss = 0.18 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:18:22.876260: step 316280, loss = 0.19 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:18:23.750001: step 316290, loss = 0.18 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:18:24.725510: step 316300, loss = 0.25 (1312.1 examples/sec; 0.098 sec/batch)
2017-06-02 10:18:25.516582: step 316310, loss = 0.17 (1618.1 examples/sec; 0.079 sec/batch)
2017-06-02 10:18:26.397961: step 316320, loss = 0.17 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:18:27.264447: step 316330, loss = 0.22 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:18:28.139090: step 316340, loss = 0.14 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:18:29.015798: step 316350, loss = 0.15 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:18:29.882555: step 316360, loss = 0.20 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:18:30.760594: step 316370, loss = 0.18 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:18:31.622369: step 316380, loss = 0.16 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:18:32.513129: step 316390, loss = 0.21 (1437.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:18:33.475360: step 316400, loss = 0.15 (1330.2 examples/sec; 0.096 sec/batch)
2017-06-02 10:18:34.260849: step 316410, loss = 0.24 (1629.6 examples/sec; 0.079 sec/batch)
2017-06-02 10:18:35.152980: step 316420, loss = 0.20 (1434.8 examples/sec; 0.089 sec/batch)
2017-06-02 10:18:36.014858: step 316430, loss = 0.15 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:18:36.879754: step 316440, loss = 0.15 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:18:37.735720: step 316450, loss = 0.18 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:18:38.584662: step 316460, loss = 0.16 (1507.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:18:39.448869: step 316470, loss = 0.21 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:18:40.284017: step 316480, loss = 0.17 (1532.7 examples/sec; 0.084 sec/batch)
2017-06-02 10:18:41.165297: step 316490, loss = 0.19 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:18:42.129541: step 316500, loss = 0.17 (1327.5 examples/sec; 0.096 sec/batch)
2017-06-02 10:18:42.903663: step 316510, loss = 0.18 (1653.5 examples/sec; 0.077 sec/batch)
2017-06-02 10:18:43.752640: step 316520, loss = 0.21 (1507.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:18:44.613475: step 316530, loss = 0.18 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:18:45.473484: step 316540, loss = 0.22 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:18:46.352936: step 316550, loss = 0.19 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:18:47.240813: step 316560, loss = 0.17 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:18:48.095468: step 316570, loss = 0.18 (1497.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:18:48.951747: step 316580, loss = 0.16 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:18:49.826039: step 316590, loss = 0.24 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:18:50.801697: step 316600, loss = 0.18 (1311.9 examples/sec; 0.098 sec/batch)
2017-06-02 10:18:51.559940: step 316610, loss = 0.26 (1688.1 examples/sec; 0.076 sec/batch)
2017-06-02 10:18:52.418777: step 316620, loss = 0.22 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:18:53.321784: step 316630, loss = 0.20 (1417.5 examples/sec; 0.090 sec/batch)
2017-06-02 10:18:54.190886: step 316640, loss = 0.28 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:18:55.069561: step 316650, loss = 0.22 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:18:55.945324: step 316660, loss = 0.14 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:18:56.803883: step 316670, loss = 0.14 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:18:57.678151: step 316680, loss = 0.16 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:18:58.525466: step 316690, loss = 0.17 (1510.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:18:59.560857: step 316700, loss = 0.15 (1236.3 examples/sec; 0.104 sec/batch)
2017-06-02 10:19:00.316033: step 316710, loss = 0.17 (1694.9 examples/sec; 0.076 sec/batch)
2017-06-02 10:19:01.189766: step 316720, loss = 0.18 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:19:02.044755: step 316730, loss = 0.24 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:19:02.901379: step 316740, loss = 0.18 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:19:03.761107: step 316750, loss = 0.24 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:19:04.637031: step 316760, loss = 0.17 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:19:05.502989: step 316770, loss = 0.23 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:19:06.368974: step 316780, loss = 0.20 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:19:07.249149: step 316790, loss = 0.18 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:19:08.209890: step 316800, loss = 0.22 (1332.3 examples/sec; 0.096 sec/batch)
2017-06-02 10:19:08.983960: step 316810, loss = 0.19 (1653.6 examples/sec; 0.077 sec/batch)
2017-06-02 10:19:09.879862: step 316820, loss = 0.19 (1428.7 examples/sec; 0.090 sec/batch)
2017-06-02 10:19:10.733309: step 316830, loss = 0.23 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:19:11.617509: step 316840, loss = 0.23 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:19:12.474839: step 316850, loss = 0.17 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:19:13.347691: step 316860, loss = 0.17 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:19:14.230728: step 316870, loss = 0.16 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:19:15.121554: step 316880, loss = 0.18 (1436.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:19:15.989000: step 316890, loss = 0.18 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:19:16.949642: step 316900, loss = 0.22 (1332.4 examples/sec; 0.096 sec/batch)
2017-06-02 10:19:17.715265: step 316910, loss = 0.19 (1671.8 examples/sec; 0.077 sec/batch)
2017-06-02 10:19:18.555718: step 316920, loss = 0.20 (1523.0 examples/sec; 0.084 sec/batch)
2017-06-02 10:19:19.422169: step 316930, loss = 0.20 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:19:20.284462: step 316940, loss = 0.17 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:19:21.144905: step 316950, loss = 0.19 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:19:22.016672: step 316960, loss = 0.15 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:19:22.865978: step 316970, loss = 0.17 (1507.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:19:23.697725: step 316980, loss = 0.17 (1538.9 examples/sec; 0.083 sec/batch)
2017-06-02 10:19:24.538532: step 316990, loss = 0.17 (1522.4 examples/sec; 0.084 sec/batch)
2017-06-02 10:19:25.507406: step 317000, loss = 0.19 (1321.1 examples/sec; 0.097 sec/batch)
2017-06-02 10:19:26.259178: step 317010, loss = 0.15 (1702.6 examples/sec; 0.075 sec/batch)
2017-06-02 10:19:27.118864: step 317020, loss = 0.23 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:19:27.972260: step 317030, loss = 0.23 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:19:28.824555: step 317040, loss = 0.18 (1501.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:19:29.695120: step 317050, loss = 0.15 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:19:30.513518: step 317060, loss = 0.16 (1564.0 examples/sec; 0.082 sec/batch)
2017-06-02 10:19:31.384860: step 317070, loss = 0.14 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:19:32.235766: step 317080, loss = 0.15 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:19:33.123000: step 317090, loss = 0.19 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 10:19:34.097494: step 317100, loss = 0.14 (1313.5 examples/sec; 0.097 sec/batch)
2017-06-02 10:19:34.873684: step 317110, loss = 0.20 (1649.1 examples/sec; 0.078 sec/batch)
2017-06-02 10:19:35.741179: step 317120, loss = 0.24 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:19:36.602874: step 317130, loss = 0.19 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:19:37.466696: step 317140, loss = 0.18 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:19:38.325138: step 317150, loss = 0.16 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:19:39.187284: step 317160, loss = 0.16 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:19:40.030308: step 317170, loss = 0.24 (1518.3 examples/sec; 0.084 sec/batch)
2017-06-02 10:19:40.891611: step 317180, loss = 0.21 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:19:41.751137: step 317190, loss = 0.19 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:19:42.707504: step 317200, loss = 0.24 (1338.4 examples/sec; 0.096 sec/batch)
2017-06-02 10:19:43.473932: step 317210, loss = 0.15 (1670.1 examples/sec; 0.077 sec/batch)
2017-06-02 10:19:44.334535: step 317220, loss = 0.22 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:19:45.208863: step 317230, loss = 0.19 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:19:46.103242: step 317240, loss = 0.23 (1431.1 examples/sec; 0.089 sec/batch)
2017-06-02 10:19:46.976756: step 317250, loss = 0.23 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:19:47.806443: step 317260, loss = 0.15 (1542.7 examples/sec; 0.083 sec/batch)
2017-06-02 10:19:48.674787: step 317270, loss = 0.17 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:19:49.545081: step 317280, loss = 0.15 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:19:50.403638: step 317290, loss = 0.20 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:19:51.421667: step 317300, loss = 0.17 (1257.3 examples/sec; 0.102 sec/batch)
2017-06-02 10:19:52.105689: step 317310, loss = 0.17 (1871.3 examples/sec; 0.068 sec/batch)
2017-06-02 10:19:53.039060: step 317320, loss = 0.19 (1371.4 examples/sec; 0.093 sec/batch)
2017-06-02 10:19:53.916085: step 317330, loss = 0.18 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:19:54.783551: step 317340, loss = 0.14 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:19:55.646959: step 317350, loss = 0.18 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:19:56.498698: step 317360, loss = 0.18 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:19:57.367577: step 317370, loss = 0.18 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:19:58.218422: step 317380, loss = 0.21 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:19:59.079306: step 317390, loss = 0.17 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:20:00.035163: step 317400, loss = 0.15 (1339.1 examples/sec; 0.096 sec/batch)
2017-06-02 10:20:00.783024: step 317410, loss = 0.15 (1711.6 examples/sec; 0.075 sec/batch)
2017-06-02 10:20:01.673641: step 317420, loss = 0.20 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:20:02.548981: step 317430, loss = 0.17 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:20:03.399010: step 317440, loss = 0.14 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:20:04.273797: step 317450, loss = 0.20 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:20:05.115428: step 317460, loss = 0.19 (1520.9 examples/sec; 0.084 sec/batch)
2017-06-02 10:20:05.975579: step 317470, loss = 0.17 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:20:06.836327: step 317480, loss = 0.17 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:20:07.719574: step 317490, loss = 0.14 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:20:08.660563: step 317500, loss = 0.24 (1360.3 examples/sec; 0.094 sec/batch)
2017-06-02 10:20:09.433357: step 317510, loss = 0.18 (1656.3 examples/sec; 0.077 sec/batch)
2017-06-02 10:20:10.315280: step 317520, loss = 0.18 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:20:11.195967: step 317530, loss = 0.17 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:20:12.049930: step 317540, loss = 0.21 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:20:12.920994: step 317550, loss = 0.16 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:20:13.795835: step 317560, loss = 0.18 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:20:14.672644: step 317570, loss = 0.15 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:20:15.544386: step 317580, loss = 0.20 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:20:16.415411: step 317590, loss = 0.16 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:20:17.404536: step 317600, loss = 0.14 (1294.1 examples/sec; 0.099 sec/batch)
2017-06-02 10:20:18.180671: step 317610, loss = 0.16 (1649.2 examples/sec; 0.078 sec/batch)
2017-06-02 10:20:19.040014: step 317620, loss = 0.18 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:20:19.898090: step 317630, loss = 0.14 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:20:20.789328: step 317640, loss = 0.17 (1436.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:20:21.662690: step 317650, loss = 0.15 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:20:22.524761: step 317660, loss = 0.22 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:20:23.382036: step 317670, loss = 0.19 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:20:24.245016: step 317680, loss = 0.17 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:20:25.112640: step 317690, loss = 0.18 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:20:26.079235: step 317700, loss = 0.16 (1324.2 examples/sec; 0.097 sec/batch)
2017-06-02 10:20:26.856918: step 317710, loss = 0.17 (1645.9 examples/sec; 0.078 sec/batch)
2017-06-02 10:20:27.722343: step 317720, loss = 0.18 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:20:28.590739: step 317730, loss = 0.18 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:20:29.455943: step 317740, loss = 0.17 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:20:30.322301: step 317750, loss = 0.20 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:20:31.193154: step 317760, loss = 0.18 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:20:32.053402: step 317770, loss = 0.14 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:20:32.937483: step 317780, loss = 0.18 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:20:33.810564: step 317790, loss = 0.22 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:20:34.761116: step 317800, loss = 0.21 (1346.6 examples/sec; 0.095 sec/batch)
2017-06-02 10:20:35.516492: step 317810, loss = 0.21 (1694.5 examples/sec; 0.076 sec/batch)
2017-06-02 10:20:36.371539: step 317820, loss = 0.22 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:20:37.241684: step 317830, loss = 0.19 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:20:38.122509: step 317840, loss = 0.19 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:20:39.016386: step 317850, loss = 0.19 (1432.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:20:39.881243: step 317860, loss = 0.22 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:20:40.741758: step 317870, loss = 0.19 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:20:41.620862: step 317880, loss = 0.19 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:20:42.505552: step 317890, loss = 0.17 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:20:43.480062: step 317900, loss = 0.20 (1313.5 examples/sec; 0.097 sec/batch)
2017-06-02 10:20:44.229261: step 317910, loss = 0.15 (1708.5 examples/sec; 0.075 sec/batch)
2017-06-02 10:20:45.093384: step 317920, loss = 0.14 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:20:45.945579: step 317930, loss = 0.21 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:20:46.812392: step 317940, loss = 0.18 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:20:47.677867: step 317950, loss = 0.16 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:20:48.558847: step 317960, loss = 0.19 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:20:49.414209: step 317970, loss = 0.21 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:20:50.277236: step 317980, loss = 0.17 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:20:51.165080: step 317990, loss = 0.18 (1441.7 examples/sec; 0.089 sec/batch)
2017-06-02 10:20:52.136384: step 318000, loss = 0.14 (1317.8 examples/sec; 0.097 sec/batch)
2017-06-02 10:20:52.902015: step 318010, loss = 0.19 (1671.8 examples/sec; 0.077 sec/batch)
2017-06-02 10:20:53.787645: step 318020, loss = 0.14 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:20:54.659061: step 318030, loss = 0.20 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:20:55.521944: step 318040, loss = 0.17 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:20:56.389635: step 318050, loss = 0.20 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:20:57.247330: step 318060, loss = 0.14 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:20:58.118149: step 318070, loss = 0.17 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:20:58.971873: step 318080, loss = 0.19 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:20:59.831242: step 318090, loss = 0.15 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:21:00.823818: step 318100, loss = 0.15 (1289.6 examples/sec; 0.099 sec/batch)
2017-06-02 10:21:01.579369: step 318110, loss = 0.16 (1694.1 examples/sec; 0.076 sec/batch)
2017-06-02 10:21:02.455899: step 318120, loss = 0.20 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:21:03.342712: step 318130, loss = 0.18 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:21:04.197571: step 318140, loss = 0.21 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:21:05.065196: step 318150, loss = 0.16 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:21:05.916556: step 318160, loss = 0.18 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:21:06.754340: step 318170, loss = 0.16 (1527.8 examples/sec; 0.084 sec/batch)
2017-06-02 10:21:07.596091: step 318180, loss = 0.19 (1520.6 examples/sec; 0.084 sec/batch)
2017-06-02 10:21:08.468484: step 318190, loss = 0.14 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:21:09.427206: step 318200, loss = 0.17 (1335.1 examples/sec; 0.096 sec/batch)
2017-06-02 10:21:10.198172: step 318210, loss = 0.16 (1660.3 examples/sec; 0.077 sec/batch)
2017-06-02 10:21:11.067240: step 318220, loss = 0.18 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:21:11.915043: step 318230, loss = 0.25 (1509.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:21:12.765224: step 318240, loss = 0.16 (1505.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:21:13.637688: step 318250, loss = 0.21 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:21:14.500746: step 318260, loss = 0.19 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:21:15.377060: step 318270, loss = 0.17 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:21:16.232309: step 318280, loss = 0.22 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:21:17.091897: step 318290, loss = 0.19 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:21:18.080468: step 318300, loss = 0.17 (1294.8 examples/sec; 0.099 sec/batch)
2017-06-02 10:21:18.857425: step 318310, loss = 0.18 (1647.5 examples/sec; 0.078 sec/batch)
2017-06-02 10:21:19.718419: step 318320, loss = 0.23 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:21:20.558462: step 318330, loss = 0.25 (1523.7 examples/sec; 0.084 sec/batch)
2017-06-02 10:21:21.408529: step 318340, loss = 0.16 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:21:22.276345: step 318350, loss = 0.19 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:21:23.158143: step 318360, loss = 0.17 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:21:24.052664: step 318370, loss = 0.16 (1430.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:21:24.925680: step 318380, loss = 0.15 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:21:25.832503: step 318390, loss = 0.19 (1411.5 examples/sec; 0.091 sec/batch)
2017-06-02 10:21:26.813913: step 318400, loss = 0.15 (1304.3 examples/sec; 0.098 sec/batch)
2017-06-02 10:21:27.574798: step 318410, loss = 0.15 (1682.3 examples/sec; 0.076 sec/batch)
2017-06-02 10:21:28.449239: step 318420, loss = 0.18 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:21:29.301687: step 318430, loss = 0.22 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:21:30.157667: step 318440, loss = 0.15 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:21:31.035609: step 318450, loss = 0.16 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:21:31.918303: step 318460, loss = 0.18 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:21:32.794598: step 318470, loss = 0.18 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:21:33.662584: step 318480, loss = 0.17 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:21:34.524270: step 318490, loss = 0.25 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:21:35.545703: step 318500, loss = 0.18 (1253.1 examples/sec; 0.102 sec/batch)
2017-06-02 10:21:36.261523: step 318510, loss = 0.18 (1788.2 examples/sec; 0.072 sec/batch)
2017-06-02 10:21:37.163405: step 318520, loss = 0.15 (1419.2 examples/sec; 0.090 sec/batch)
2017-06-02 10:21:38.036436: step 318530, loss = 0.15 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:21:38.880940: step 318540, loss = 0.16 (1515.8 examples/sec; 0.084 sec/batch)
2017-06-02 10:21:39.768035: step 318550, loss = 0.17 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 10:21:40.644047: step 318560, loss = 0.18 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:21:41.491146: step 318570, loss = 0.17 (1511.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:21:42.355091: step 318580, loss = 0.23 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:21:43.225507: step 318590, loss = 0.15 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:21:44.203550: step 318600, loss = 0.21 (1308.7 examples/sec; 0.098 sec/batch)
2017-06-02 10:21:44.978580: step 318610, loss = 0.18 (1651.6 examples/sec; 0.078 sec/batch)
2017-06-02 10:21:45.842658: step 318620, loss = 0.16 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:21:46.710706: step 318630, loss = 0.17 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:21:47.553722: step 318640, loss = 0.20 (1518.4 examples/sec; 0.084 sec/batch)
2017-06-02 10:21:48.428087: step 318650, loss = 0.22 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:21:49.287820: step 318660, loss = 0.16 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:21:50.151599: step 318670, loss = 0.21 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:21:51.026623: step 318680, loss = 0.14 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:21:51.889804: step 318690, loss = 0.15 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:21:52.882512: step 318700, loss = 0.17 (1290.4 examples/sec; 0.099 sec/batch)
2017-06-02 10:21:53.599480: step 318710, loss = 0.16 (1783.4 examples/sec; 0.072 sec/batch)
2017-06-02 10:21:54.489354: step 318720, loss = 0.15 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:21:55.365665: step 318730, loss = 0.19 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:21:56.215680: step 318740, loss = 0.14 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:21:57.062371: step 318750, loss = 0.20 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:21:57.934133: step 318760, loss = 0.14 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:21:58.802413: step 318770, loss = 0.22 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:21:59.650085: step 318780, loss = 0.17 (1510.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:22:00.530938: step 318790, loss = 0.18 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:22:01.485170: step 318800, loss = 0.22 (1341.4 examples/sec; 0.095 sec/batch)
2017-06-02 10:22:02.227753: step 318810, loss = 0.17 (1723.7 examples/sec; 0.074 sec/batch)
2017-06-02 10:22:03.084376: step 318820, loss = 0.17 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:22:03.928190: step 318830, loss = 0.22 (1517.0 examples/sec; 0.084 sec/batch)
2017-06-02 10:22:04.793786: step 318840, loss = 0.20 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:22:05.653642: step 318850, loss = 0.20 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:22:06.511709: step 318860, loss = 0.18 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:22:07.393048: step 318870, loss = 0.17 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:22:08.283225: step 318880, loss = 0.23 (1437.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:22:09.133698: step 318890, loss = 0.18 (1505.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:22:10.094296: step 318900, loss = 0.21 (1332.5 examples/sec; 0.096 sec/batch)
2017-06-02 10:22:10.861030: step 318910, loss = 0.14 (1669.4 examples/sec; 0.077 sec/batch)
2017-06-02 10:22:11.719162: step 318920, loss = 0.18 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:22:12.585087: step 318930, loss = 0.24 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:22:13.460720: step 318940, loss = 0.17 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:22:14.308099: step 318950, loss = 0.19 (1510.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:22:15.177549: step 318960, loss = 0.14 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:22:16.049804: step 318970, loss = 0.18 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:22:16.928511: step 318980, loss = 0.17 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:22:17.775553: step 318990, loss = 0.18 (1511.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:22:18.751416: step 319000, loss = 0.19 (1311.7 examples/sec; 0.098 sec/batch)
2017-06-02 10:22:19.519565: step 319010, loss = 0.18 (1666.3 examples/sec; 0.077 sec/batch)
2017-06-02 10:22:20.369859: step 319020, loss = 0.14 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:22:21.243977: step 319030, loss = 0.15 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:22:22.106945: step 319040, loss = 0.18 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:22:22.927039: step 319050, loss = 0.16 (1560.8 examples/sec; 0.082 sec/batch)
2017-06-02 10:22:23.780917: step 319060, loss = 0.21 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:22:24.624252: step 319070, loss = 0.13 (1517.8 examples/sec; 0.084 sec/batch)
2017-06-02 10:22:25.466248: step 319080, loss = 0.23 (1520.2 examples/sec; 0.084 sec/batch)
2017-06-02 10:22:26.324595: step 319090, loss = 0.16 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:22:27.297399: step 319100, loss = 0.16 (1315.8 examples/sec; 0.097 sec/batch)
2017-06-02 10:22:28.054503: step 319110, loss = 0.22 (1690.7 examples/sec; 0.076 sec/batch)
2017-06-02 10:22:28.916525: step 319120, loss = 0.15 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:22:29.763205: step 319130, loss = 0.18 (1511.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:22:30.657311: step 319140, loss = 0.15 (1431.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:22:31.540350: step 319150, loss = 0.17 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:22:32.423568: step 319160, loss = 0.17 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:22:33.310774: step 319170, loss = 0.15 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 10:22:34.194468: step 319180, loss = 0.21 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:22:35.065228: step 319190, loss = 0.17 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:22:36.030918: step 319200, loss = 0.20 (1325.5 examples/sec; 0.097 sec/batch)
2017-06-02 10:22:36.811573: step 319210, loss = 0.24 (1639.7 examples/sec; 0.078 sec/batch)
2017-06-02 10:22:37.690292: step 319220, loss = 0.17 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:22:38.581666: step 319230, loss = 0.18 (1436.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:22:39.467423: step 319240, loss = 0.16 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:22:40.333202: step 319250, loss = 0.22 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:22:41.196383: step 319260, loss = 0.16 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:22:42.065338: step 319270, loss = 0.19 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:22:42.925571: step 319280, loss = 0.16 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:22:43.803661: step 319290, loss = 0.15 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:22:44.772077: step 319300, loss = 0.19 (1321.7 examples/sec; 0.097 sec/batch)
2017-06-02 10:22:45.553181: step 319310, loss = 0.14 (1638.7 examples/sec; 0.078 sec/batch)
2017-06-02 10:22:46.427110: step 319320, loss = 0.16 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:22:47.281085: step 319330, loss = 0.18 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:22:48.164697: step 319340, loss = 0.20 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:22:49.046436: step 319350, loss = 0.19 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:22:49.921598: step 319360, loss = 0.30 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:22:50.800987: step 319370, loss = 0.17 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:22:51.672079: step 319380, loss = 0.22 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:22:52.496615: step 319390, loss = 0.19 (1552.4 examples/sec; 0.082 sec/batch)
2017-06-02 10:22:53.501354: step 319400, loss = 0.25 (1273.9 examples/sec; 0.100 sec/batch)
2017-06-02 10:22:54.235024: step 319410, loss = 0.20 (1744.7 examples/sec; 0.073 sec/batch)
2017-06-02 10:22:55.101606: step 319420, loss = 0.21 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:22:55.997054: step 319430, loss = 0.25 (1429.5 examples/sec; 0.090 sec/batch)
2017-06-02 10:22:56.867962: step 319440, loss = 0.17 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:22:57.742343: step 319450, loss = 0.17 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:22:58.590657: step 319460, loss = 0.20 (1508.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:22:59.436351: step 319470, loss = 0.22 (1513.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:23:00.301320: step 319480, loss = 0.17 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:23:01.174826: step 319490, loss = 0.25 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:23:02.130304: step 319500, loss = 0.24 (1339.6 examples/sec; 0.096 sec/batch)
2017-06-02 10:23:02.886044: step 319510, loss = 0.16 (1693.7 examples/sec; 0.076 sec/batch)
2017-06-02 10:23:03.756956: step 319520, loss = 0.18 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:23:04.654767: step 319530, loss = 0.20 (1425.7 examples/sec; 0.090 sec/batch)
2017-06-02 10:23:05.532261: step 319540, loss = 0.16 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:23:06.391494: step 319550, loss = 0.17 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:23:07.238010: step 319560, loss = 0.18 (1512.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:23:08.111676: step 319570, loss = 0.16 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:23:08.985236: step 319580, loss = 0.14 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:23:09.853785: step 319590, loss = 0.20 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:23:10.814213: step 319600, loss = 0.18 (1332.7 examples/sec; 0.096 sec/batch)
2017-06-02 10:23:11.571879: step 319610, loss = 0.18 (1689.4 examples/sec; 0.076 sec/batch)
2017-06-02 10:23:12.427913: step 319620, loss = 0.19 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:23:13.290374: step 319630, loss = 0.18 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:23:14.159489: step 319640, loss = 0.17 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:23:15.057230: step 319650, loss = 0.18 (1425.8 examples/sec; 0.090 sec/batch)
2017-06-02 10:23:15.932416: step 319660, loss = 0.15 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:23:16.781453: step 319670, loss = 0.18 (1507.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:23:17.630893: step 319680, loss = 0.17 (1506.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:23:18.488928: step 319690, loss = 0.18 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:23:19.487584: step 319700, loss = 0.18 (1281.7 examples/sec; 0.100 sec/batch)
2017-06-02 10:23:20.255211: step 319710, loss = 0.18 (1667.5 examples/sec; 0.077 sec/batch)
2017-06-02 10:23:21.106689: step 319720, loss = 0.18 (1503.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:23:21.981861: step 319730, loss = 0.17 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:23:22.846500: step 319740, loss = 0.20 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:23:23.731664: step 319750, loss = 0.16 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:23:24.620166: step 319760, loss = 0.19 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:23:25.488489: step 319770, loss = 0.17 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:23:26.357705: step 319780, loss = 0.15 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:23:27.217094: step 319790, loss = 0.15 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:23:28.182096: step 319800, loss = 0.21 (1326.4 examples/sec; 0.097 sec/batch)
2017-06-02 10:23:28.950635: step 319810, loss = 0.18 (1665.5 examples/sec; 0.077 sec/batch)
2017-06-02 10:23:29.828451: step 319820, loss = 0.16 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:23:30.689464: step 319830, loss = 0.16 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:23:31.559535: step 319840, loss = 0.15 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:23:32.412903: step 319850, loss = 0.23 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:23:33.280355: step 319860, loss = 0.20 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:23:34.165890: step 319870, loss = 0.17 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 10:23:35.043630: step 319880, loss = 0.16 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:23:35.940144: step 319890, loss = 0.18 (1427.8 examples/sec; 0.090 sec/batch)
2017-06-02 10:23:36.910282: step 319900, loss = 0.20 (1319.4 examples/sec; 0.097 sec/batch)
2017-06-02 10:23:37.676919: step 319910, loss = 0.16 (1669.6 examples/sec; 0.077 sec/batch)
2017-06-02 10:23:38.519064: step 319920, loss = 0.15 (1519.9 examples/sec; 0.084 sec/batch)
2017-06-02 10:23:39.373371: step 319930, loss = 0.19 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:23:40.222722: step 319940, loss = 0.23 (1507.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:23:41.091434: step 319950, loss = 0.17 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:23:41.941107: step 319960, loss = 0.16 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:23:42.784144: step 319970, loss = 0.15 (1518.3 examples/sec; 0.084 sec/batch)
2017-06-02 10:23:43.648500: step 319980, loss = 0.16 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:23:44.497345: step 319990, loss = 0.18 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:23:45.459899: step 320000, loss = 0.16 (1329.8 examples/sec; 0.096 sec/batch)
2017-06-02 10:23:46.214043: step 320010, loss = 0.15 (1697.3 examples/sec; 0.075 sec/batch)
2017-06-02 10:23:47.089471: step 320020, loss = 0.22 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:23:47.938536: step 320030, loss = 0.17 (1507.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:23:48.800710: step 320040, loss = 0.15 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:23:49.673955: step 320050, loss = 0.18 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:23:50.545329: step 320060, loss = 0.15 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:23:51.420102: step 320070, loss = 0.21 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:23:52.296426: step 320080, loss = 0.18 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:23:53.153402: step 320090, loss = 0.16 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:23:54.117602: step 320100, loss = 0.19 (1327.5 examples/sec; 0.096 sec/batch)
2017-06-02 10:23:54.897487: step 320110, loss = 0.17 (1641.3 examples/sec; 0.078 sec/batch)
2017-06-02 10:23:55.786494: step 320120, loss = 0.19 (1439.8 examples/sec; 0.089 sec/batch)
2017-06-02 10:23:56.663583: step 320130, loss = 0.17 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:23:57.532080: step 320140, loss = 0.16 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:23:58.404629: step 320150, loss = 0.17 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:23:59.289351: step 320160, loss = 0.19 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:24:00.112821: step 320170, loss = 0.22 (1554.4 examples/sec; 0.082 sec/batch)
2017-06-02 10:24:00.997766: step 320180, loss = 0.21 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:24:01.888594: step 320190, loss = 0.13 (1436.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:24:02.892753: step 320200, loss = 0.17 (1274.7 examples/sec; 0.100 sec/batch)
2017-06-02 10:24:03.609185: step 320210, loss = 0.15 (1786.6 examples/sec; 0.072 sec/batch)
2017-06-02 10:24:04.470350: step 320220, loss = 0.16 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:24:05.344635: step 320230, loss = 0.15 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:24:06.204577: step 320240, loss = 0.19 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:24:07.045518: step 320250, loss = 0.14 (1522.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:24:07.935254: step 320260, loss = 0.20 (1438.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:24:08.827764: step 320270, loss = 0.19 (1434.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:24:09.698628: step 320280, loss = 0.18 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:24:10.573554: step 320290, loss = 0.18 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:24:11.557782: step 320300, loss = 0.19 (1300.5 examples/sec; 0.098 sec/batch)
2017-06-02 10:24:12.326862: step 320310, loss = 0.20 (1664.3 examples/sec; 0.077 sec/batch)
2017-06-02 10:24:13.202195: step 320320, loss = 0.16 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:24:14.091268: step 320330, loss = 0.13 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 10:24:14.966316: step 320340, loss = 0.15 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:24:15.839631: step 320350, loss = 0.15 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:24:16.696250: step 320360, loss = 0.16 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:24:17.574390: step 320370, loss = 0.15 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:24:18.433123: step 320380, loss = 0.26 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:24:19.317887: step 320390, loss = 0.18 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:24:20.309787: step 320400, loss = 0.14 (1290.4 examples/sec; 0.099 sec/batch)
2017-06-02 10:24:21.053144: step 320410, loss = 0.18 (1721.9 examples/sec; 0.074 sec/batch)
2017-06-02 10:24:21.892999: step 320420, loss = 0.18 (1524.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:24:22.732003: step 320430, loss = 0.17 (1525.6 examples/sec; 0.084 sec/batch)
2017-06-02 10:24:23.609911: step 320440, loss = 0.15 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:24:24.461915: step 320450, loss = 0.15 (1502.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:24:25.332256: step 320460, loss = 0.17 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:24:26.190146: step 320470, loss = 0.16 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:24:27.037285: step 320480, loss = 0.17 (1511.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:24:27.924076: step 320490, loss = 0.17 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:24:28.911043: step 320500, loss = 0.14 (1296.9 examples/sec; 0.099 sec/batch)
2017-06-02 10:24:29.668440: step 320510, loss = 0.15 (1690.0 examples/sec; 0.076 sec/batch)
2017-06-02 10:24:30.532865: step 320520, loss = 0.17 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:24:31.411232: step 320530, loss = 0.16 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:24:32.286577: step 320540, loss = 0.20 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:24:33.156232: step 320550, loss = 0.17 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:24:34.029600: step 320560, loss = 0.22 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:24:34.914171: step 320570, loss = 0.15 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:24:35.780501: step 320580, loss = 0.21 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:24:36.640246: step 320590, loss = 0.18 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:24:37.633565: step 320600, loss = 0.17 (1288.6 examples/sec; 0.099 sec/batch)
2017-06-02 10:24:38.387228: step 320610, loss = 0.18 (1698.4 examples/sec; 0.075 sec/batch)
2017-06-02 10:24:39.236034: step 320620, loss = 0.18 (1508.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:24:40.101760: step 320630, loss = 0.24 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:24:40.979162: step 320640, loss = 0.17 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:24:41.847074: step 320650, loss = 0.17 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:24:42.702226: step 320660, loss = 0.17 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:24:43.542362: step 320670, loss = 0.17 (1523.5 examples/sec; 0.084 sec/batch)
2017-06-02 10:24:44.404601: step 320680, loss = 0.14 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:24:45.265332: step 320690, loss = 0.16 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:24:46.283550: step 320700, loss = 0.15 (1257.1 examples/sec; 0.102 sec/batch)
2017-06-02 10:24:46.970253: step 320710, loss = 0.21 (1864.0 examples/sec; 0.069 sec/batch)
2017-06-02 10:24:47.795088: step 320720, loss = 0.18 (1551.8 examples/sec; 0.082 sec/batch)
2017-06-02 10:24:48.653085: step 320730, loss = 0.21 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:24:49.525294: step 320740, loss = 0.19 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:24:50.389217: step 320750, loss = 0.22 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:24:51.258562: step 320760, loss = 0.20 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:24:52.148502: step 320770, loss = 0.20 (1438.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:24:53.029183: step 320780, loss = 0.19 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:24:53.895824: step 320790, loss = 0.20 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:24:54.904713: step 320800, loss = 0.21 (1268.7 examples/sec; 0.101 sec/batch)
2017-06-02 10:24:55.629307: step 320810, loss = 0.17 (1766.5 examples/sec; 0.072 sec/batch)
2017-06-02 10:24:56.518544: step 320820, loss = 0.19 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:24:57.352385: step 320830, loss = 0.17 (1535.1 examples/sec; 0.083 sec/batch)
2017-06-02 10:24:58.237522: step 320840, loss = 0.23 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 10:24:59.100962: step 320850, loss = 0.21 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:24:59.967821: step 320860, loss = 0.16 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:25:00.862870: step 320870, loss = 0.17 (1430.1 examples/sec; 0.090 sec/batch)
2017-06-02 10:25:01.754872: step 320880, loss = 0.18 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:25:02.634645: step 320890, loss = 0.19 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:25:03.596479: step 320900, loss = 0.20 (1330.8 examples/sec; 0.096 sec/batch)
2017-06-02 10:25:04.382343: step 320910, loss = 0.15 (1628.8 examples/sec; 0.079 sec/batch)
2017-06-02 10:25:05.268109: step 320920, loss = 0.18 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:25:06.146086: step 320930, loss = 0.20 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:25:07.013628: step 320940, loss = 0.15 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:25:07.904760: step 320950, loss = 0.18 (1436.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:25:08.768172: step 320960, loss = 0.15 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:25:09.645167: step 320970, loss = 0.22 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:25:10.517769: step 320980, loss = 0.18 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:25:11.365584: step 320990, loss = 0.17 (1509.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:25:12.348762: step 321000, loss = 0.20 (1301.9 examples/sec; 0.098 sec/batch)
2017-06-02 10:25:13.103420: step 321010, loss = 0.18 (1696.1 examples/sec; 0.075 sec/batch)
2017-06-02 10:25:13.972009: step 321020, loss = 0.15 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:25:14.818014: step 321030, loss = 0.17 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:25:15.678286: step 321040, loss = 0.17 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:25:16.562512: step 321050, loss = 0.18 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:25:17.439108: step 321060, loss = 0.13 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:25:18.295808: step 321070, loss = 0.14 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:25:19.183229: step 321080, loss = 0.19 (1442.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:25:20.043324: step 321090, loss = 0.17 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:25:21.033125: step 321100, loss = 0.18 (1293.2 examples/sec; 0.099 sec/batch)
2017-06-02 10:25:21.789896: step 321110, loss = 0.19 (1691.4 examples/sec; 0.076 sec/batch)
2017-06-02 10:25:22.644232: step 321120, loss = 0.18 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:25:23.517015: step 321130, loss = 0.14 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:25:24.379954: step 321140, loss = 0.17 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:25:25.223760: step 321150, loss = 0.18 (1516.9 examples/sec; 0.084 sec/batch)
2017-06-02 10:25:26.102747: step 321160, loss = 0.16 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:25:26.963150: step 321170, loss = 0.15 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:25:27.821125: step 321180, loss = 0.15 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:25:28.676874: step 321190, loss = 0.24 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:25:29.633516: step 321200, loss = 0.19 (1338.0 examples/sec; 0.096 sec/batch)
2017-06-02 10:25:30.394287: step 321210, loss = 0.17 (1682.5 examples/sec; 0.076 sec/batch)
2017-06-02 10:25:31.269035: step 321220, loss = 0.17 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:25:32.126449: step 321230, loss = 0.17 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:25:33.001000: step 321240, loss = 0.21 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:25:33.873779: step 321250, loss = 0.21 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:25:34.712478: step 321260, loss = 0.18 (1526.2 examples/sec; 0.084 sec/batch)
2017-06-02 10:25:35.611764: step 321270, loss = 0.15 (1423.3 examples/sec; 0.090 sec/batch)
2017-06-02 10:25:36.493827: step 321280, loss = 0.15 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:25:37.357721: step 321290, loss = 0.16 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:25:38.343455: step 321300, loss = 0.18 (1298.5 examples/sec; 0.099 sec/batch)
2017-06-02 10:25:39.086920: step 321310, loss = 0.21 (1721.7 examples/sec; 0.074 sec/batch)
2017-06-02 10:25:39.989041: step 321320, loss = 0.15 (1418.9 examples/sec; 0.090 sec/batch)
2017-06-02 10:25:40.853883: step 321330, loss = 0.17 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:25:41.719360: step 321340, loss = 0.15 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:25:42.597852: step 321350, loss = 0.17 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:25:43.470164: step 321360, loss = 0.16 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:25:44.328060: step 321370, loss = 0.18 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:25:45.184988: step 321380, loss = 0.17 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:25:46.033377: step 321390, loss = 0.16 (1508.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:25:47.003494: step 321400, loss = 0.18 (1319.4 examples/sec; 0.097 sec/batch)
2017-06-02 10:25:47.757365: step 321410, loss = 0.17 (1697.9 examples/sec; 0.075 sec/batch)
2017-06-02 10:25:48.616431: step 321420, loss = 0.19 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:25:49.477849: step 321430, loss = 0.16 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:25:50.348529: step 321440, loss = 0.14 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:25:51.229256: step 321450, loss = 0.21 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:25:52.096024: step 321460, loss = 0.19 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:25:52.978647: step 321470, loss = 0.16 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:25:53.813961: step 321480, loss = 0.13 (1532.4 examples/sec; 0.084 sec/batch)
2017-06-02 10:25:54.665086: step 321490, loss = 0.18 (1503.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:25:55.678392: step 321500, loss = 0.18 (1263.2 examples/sec; 0.101 sec/batch)
2017-06-02 10:25:56.409096: step 321510, loss = 0.16 (1751.7 examples/sec; 0.073 sec/batch)
2017-06-02 10:25:57.261235: step 321520, loss = 0.17 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:25:58.134631: step 321530, loss = 0.19 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:25:58.961345: step 321540, loss = 0.18 (1548.3 examples/sec; 0.083 sec/batch)
2017-06-02 10:25:59.806889: step 321550, loss = 0.18 (1513.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:26:00.648689: step 321560, loss = 0.16 (1520.6 examples/sec; 0.084 sec/batch)
2017-06-02 10:26:01.503123: step 321570, loss = 0.15 (1498.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:26:02.349818: step 321580, loss = 0.15 (1511.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:26:03.176492: step 321590, loss = 0.19 (1548.4 examples/sec; 0.083 sec/batch)
2017-06-02 10:26:04.162046: step 321600, loss = 0.17 (1298.8 examples/sec; 0.099 sec/batch)
2017-06-02 10:26:04.928189: step 321610, loss = 0.17 (1670.7 examples/sec; 0.077 sec/batch)
2017-06-02 10:26:05.808228: step 321620, loss = 0.22 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:26:06.669638: step 321630, loss = 0.23 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:26:07.509470: step 321640, loss = 0.16 (1524.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:26:08.369138: step 321650, loss = 0.20 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:26:09.233487: step 321660, loss = 0.16 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:26:10.094503: step 321670, loss = 0.22 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:26:10.938276: step 321680, loss = 0.17 (1517.0 examples/sec; 0.084 sec/batch)
2017-06-02 10:26:11.807903: step 321690, loss = 0.16 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:26:12.781667: step 321700, loss = 0.15 (1314.5 examples/sec; 0.097 sec/batch)
2017-06-02 10:26:13.516596: step 321710, loss = 0.16 (1741.7 examples/sec; 0.073 sec/batch)
2017-06-02 10:26:14.394795: step 321720, loss = 0.16 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:26:15.241241: step 321730, loss = 0.15 (1512.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:26:16.074082: step 321740, loss = 0.17 (1536.9 examples/sec; 0.083 sec/batch)
2017-06-02 10:26:16.926542: step 321750, loss = 0.15 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:26:17.790554: step 321760, loss = 0.20 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:26:18.668539: step 321770, loss = 0.17 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:26:19.542345: step 321780, loss = 0.19 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:26:20.417623: step 321790, loss = 0.17 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:26:21.393408: step 321800, loss = 0.20 (1311.8 examples/sec; 0.098 sec/batch)
2017-06-02 10:26:22.172527: step 321810, loss = 0.16 (1642.9 examples/sec; 0.078 sec/batch)
2017-06-02 10:26:23.055084: step 321820, loss = 0.20 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:26:23.922952: step 321830, loss = 0.20 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:26:24.809556: step 321840, loss = 0.15 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 10:26:25.702563: step 321850, loss = 0.21 (1433.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:26:26.565885: step 321860, loss = 0.17 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:26:27.440863: step 321870, loss = 0.21 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:26:28.295794: step 321880, loss = 0.19 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:26:29.145026: step 321890, loss = 0.16 (1507.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:26:30.075790: step 321900, loss = 0.22 (1375.2 examples/sec; 0.093 sec/batch)
2017-06-02 10:26:30.841674: step 321910, loss = 0.15 (1671.3 examples/sec; 0.077 sec/batch)
2017-06-02 10:26:31.704263: step 321920, loss = 0.15 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:26:32.579358: step 321930, loss = 0.16 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:26:33.465116: step 321940, loss = 0.18 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 10:26:34.319913: step 321950, loss = 0.18 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:26:35.190721: step 321960, loss = 0.21 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:26:36.079045: step 321970, loss = 0.20 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:26:36.962811: step 321980, loss = 0.19 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:26:37.854299: step 321990, loss = 0.19 (1435.8 examples/sec; 0.089 sec/batch)
2017-06-02 10:26:38.840676: step 322000, loss = 0.26 (1297.7 examples/sec; 0.099 sec/batch)
2017-06-02 10:26:39.613353: step 322010, loss = 0.16 (1656.6 examples/sec; 0.077 sec/batch)
2017-06-02 10:26:40.500188: step 322020, loss = 0.20 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:26:41.386785: step 322030, loss = 0.24 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 10:26:42.257268: step 322040, loss = 0.19 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:26:43.132944: step 322050, loss = 0.17 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:26:43.983789: step 322060, loss = 0.21 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:26:44.832620: step 322070, loss = 0.23 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:26:45.711524: step 322080, loss = 0.16 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:26:46.578830: step 322090, loss = 0.14 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:26:47.552743: step 322100, loss = 0.21 (1314.3 examples/sec; 0.097 sec/batch)
2017-06-02 10:26:48.305232: step 322110, loss = 0.17 (1701.0 examples/sec; 0.075 sec/batch)
2017-06-02 10:26:49.163805: step 322120, loss = 0.24 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:26:50.024867: step 322130, loss = 0.17 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:26:50.867878: step 322140, loss = 0.18 (1518.3 examples/sec; 0.084 sec/batch)
2017-06-02 10:26:51.719635: step 322150, loss = 0.14 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:26:52.567604: step 322160, loss = 0.16 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:26:53.418752: step 322170, loss = 0.21 (1503.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:26:54.284767: step 322180, loss = 0.17 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:26:55.143580: step 322190, loss = 0.19 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:26:56.087968: step 322200, loss = 0.18 (1355.4 examples/sec; 0.094 sec/batch)
2017-06-02 10:26:56.853945: step 322210, loss = 0.18 (1671.1 examples/sec; 0.077 sec/batch)
2017-06-02 10:26:57.718577: step 322220, loss = 0.14 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:26:58.576981: step 322230, loss = 0.19 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:26:59.447460: step 322240, loss = 0.15 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:27:00.313639: step 322250, loss = 0.15 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:27:01.178371: step 322260, loss = 0.18 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:27:02.065186: step 322270, loss = 0.17 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:27:02.927442: step 322280, loss = 0.15 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:27:03.786092: step 322290, loss = 0.15 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:27:04.764566: step 322300, loss = 0.16 (1308.2 examples/sec; 0.098 sec/batch)
2017-06-02 10:27:05.542825: step 322310, loss = 0.20 (1644.7 examples/sec; 0.078 sec/batch)
2017-06-02 10:27:06.406404: step 322320, loss = 0.16 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:27:07.290297: step 322330, loss = 0.16 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:27:08.147724: step 322340, loss = 0.16 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:27:09.030241: step 322350, loss = 0.23 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:27:09.870198: step 322360, loss = 0.16 (1523.9 examples/sec; 0.084 sec/batch)
2017-06-02 10:27:10.749225: step 322370, loss = 0.13 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:27:11.632532: step 322380, loss = 0.20 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:27:12.480088: step 322390, loss = 0.20 (1510.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:27:13.475832: step 322400, loss = 0.18 (1285.5 examples/sec; 0.100 sec/batch)
2017-06-02 10:27:14.201441: step 322410, loss = 0.25 (1764.0 examples/sec; 0.073 sec/batch)
2017-06-02 10:27:15.079171: step 322420, loss = 0.16 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:27:15.961298: step 322430, loss = 0.20 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:27:16.824710: step 322440, loss = 0.19 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:27:17.703533: step 322450, loss = 0.17 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:27:18.542141: step 322460, loss = 0.17 (1526.3 examples/sec; 0.084 sec/batch)
2017-06-02 10:27:19.394914: step 322470, loss = 0.20 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:27:20.268781: step 322480, loss = 0.18 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:27:21.147279: step 322490, loss = 0.15 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:27:22.140724: step 322500, loss = 0.17 (1288.4 examples/sec; 0.099 sec/batch)
2017-06-02 10:27:22.898009: step 322510, loss = 0.22 (1690.3 examples/sec; 0.076 sec/batch)
2017-06-02 10:27:23.763864: step 322520, loss = 0.19 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:27:24.619521: step 322530, loss = 0.21 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:27:25.482781: step 322540, loss = 0.19 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:27:26.349467: step 322550, loss = 0.21 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:27:27.230249: step 322560, loss = 0.17 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:27:28.084698: step 322570, loss = 0.20 (1498.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:27:28.954241: step 322580, loss = 0.18 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:27:29.821255: step 322590, loss = 0.16 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:27:30.773283: step 322600, loss = 0.17 (1344.5 examples/sec; 0.095 sec/batch)
2017-06-02 10:27:31.535369: step 322610, loss = 0.16 (1679.6 examples/sec; 0.076 sec/batch)
2017-06-02 10:27:32.376591: step 322620, loss = 0.25 (1521.6 examples/sec; 0.084 sec/batch)
2017-06-02 10:27:33.233237: step 322630, loss = 0.17 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:27:34.093810: step 322640, loss = 0.20 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:27:34.959747: step 322650, loss = 0.14 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:27:35.820724: step 322660, loss = 0.13 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:27:36.713770: step 322670, loss = 0.16 (1433.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:27:37.591717: step 322680, loss = 0.19 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:27:38.439726: step 322690, loss = 0.15 (1509.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:27:39.415646: step 322700, loss = 0.19 (1311.6 examples/sec; 0.098 sec/batch)
2017-06-02 10:27:40.209006: step 322710, loss = 0.16 (1613.4 examples/sec; 0.079 sec/batch)
2017-06-02 10:27:41.106272: step 322720, loss = 0.19 (1426.5 examples/sec; 0.090 sec/batch)
2017-06-02 10:27:41.982255: step 322730, loss = 0.16 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:27:42.852867: step 322740, loss = 0.15 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:27:43.740907: step 322750, loss = 0.19 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:27:44.645363: step 322760, loss = 0.16 (1415.2 examples/sec; 0.090 sec/batch)
2017-06-02 10:27:45.523745: step 322770, loss = 0.21 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:27:46.389151: step 322780, loss = 0.15 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:27:47.299382: step 322790, loss = 0.15 (1406.2 examples/sec; 0.091 sec/batch)
2017-06-02 10:27:48.271744: step 322800, loss = 0.20 (1316.4 examples/sec; 0.097 sec/batch)
2017-06-02 10:27:49.037634: step 322810, loss = 0.14 (1671.3 examples/sec; 0.077 sec/batch)
2017-06-02 10:27:49.883776: step 322820, loss = 0.13 (1512.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:27:50.724107: step 322830, loss = 0.17 (1523.2 examples/sec; 0.084 sec/batch)
2017-06-02 10:27:51.572778: step 322840, loss = 0.13 (1508.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:27:52.448913: step 322850, loss = 0.20 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:27:53.321120: step 322860, loss = 0.17 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:27:54.195980: step 322870, loss = 0.21 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:27:55.047671: step 322880, loss = 0.19 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:27:55.913992: step 322890, loss = 0.15 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:27:56.904853: step 322900, loss = 0.24 (1291.8 examples/sec; 0.099 sec/batch)
2017-06-02 10:27:57.678904: step 322910, loss = 0.21 (1653.6 examples/sec; 0.077 sec/batch)
2017-06-02 10:27:58.544660: step 322920, loss = 0.15 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:27:59.441878: step 322930, loss = 0.16 (1426.6 examples/sec; 0.090 sec/batch)
2017-06-02 10:28:00.330164: step 322940, loss = 0.20 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:28:01.174326: step 322950, loss = 0.20 (1516.3 examples/sec; 0.084 sec/batch)
2017-06-02 10:28:02.045601: step 322960, loss = 0.16 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:28:02.924486: step 322970, loss = 0.17 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:28:03.794087: step 322980, loss = 0.19 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:28:04.652323: step 322990, loss = 0.23 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:28:05.604577: step 323000, loss = 0.18 (1344.2 examples/sec; 0.095 sec/batch)
2017-06-02 10:28:06.374615: step 323010, loss = 0.15 (1662.3 examples/sec; 0.077 sec/batch)
2017-06-02 10:28:07.235044: step 323020, loss = 0.18 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:28:08.087605: step 323030, loss = 0.15 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:28:08.933572: step 323040, loss = 0.12 (1513.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:28:09.784311: step 323050, loss = 0.14 (1504.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:28:10.666923: step 323060, loss = 0.19 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:28:11.533367: step 323070, loss = 0.25 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:28:12.388447: step 323080, loss = 0.17 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:28:13.250111: step 323090, loss = 0.18 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:28:14.223101: step 323100, loss = 0.16 (1315.5 examples/sec; 0.097 sec/batch)
2017-06-02 10:28:14.988637: step 323110, loss = 0.16 (1672.0 examples/sec; 0.077 sec/batch)
2017-06-02 10:28:15.867585: step 323120, loss = 0.15 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:28:16.751312: step 323130, loss = 0.18 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:28:17.646123: step 323140, loss = 0.16 (1430.5 examples/sec; 0.089 sec/batch)
2017-06-02 10:28:18.537933: step 323150, loss = 0.17 (1435.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:28:19.410343: step 323160, loss = 0.19 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:28:20.287288: step 323170, loss = 0.20 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:28:21.176954: step 323180, loss = 0.16 (1438.7 examples/sec; 0.089 sec/batch)
2017-06-02 10:28:22.056471: step 323190, loss = 0.16 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:28:23.026992: step 323200, loss = 0.17 (1318.9 examples/sec; 0.097 sec/batch)
2017-06-02 10:28:23.792855: step 323210, loss = 0.21 (1671.3 examples/sec; 0.077 sec/batch)
2017-06-02 10:28:24.646845: step 323220, loss = 0.14 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:28:25.511311: step 323230, loss = 0.18 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:28:26.380320: step 323240, loss = 0.13 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:28:27.258889: step 323250, loss = 0.17 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:28:28.145954: step 323260, loss = 0.13 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:28:29.036094: step 323270, loss = 0.23 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:28:29.916962: step 323280, loss = 0.17 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:28:30.783588: step 323290, loss = 0.18 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:28:31.761274: step 323300, loss = 0.19 (1309.2 examples/sec; 0.098 sec/batch)
2017-06-02 10:28:32.535812: step 323310, loss = 0.19 (1652.6 examples/sec; 0.077 sec/batch)
2017-06-02 10:28:33.414896: step 323320, loss = 0.16 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:28:34.295533: step 323330, loss = 0.15 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:28:35.154844: step 323340, loss = 0.24 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:28:36.033482: step 323350, loss = 0.18 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:28:36.877261: step 323360, loss = 0.23 (1517.0 examples/sec; 0.084 sec/batch)
2017-06-02 10:28:37.729865: step 323370, loss = 0.12 (1501.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:28:38.608324: step 323380, loss = 0.23 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:28:39.494787: step 323390, loss = 0.16 (1443.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:28:40.469673: step 323400, loss = 0.19 (1313.0 examples/sec; 0.097 sec/batch)
2017-06-02 10:28:41.217270: step 323410, loss = 0.18 (1712.2 examples/sec; 0.075 sec/batch)
2017-06-02 10:28:42.094512: step 323420, loss = 0.16 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:28:42.969923: step 323430, loss = 0.19 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:28:43.863572: step 323440, loss = 0.23 (1432.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:28:44.747606: step 323450, loss = 0.19 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:28:45.620232: step 323460, loss = 0.15 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:28:46.461539: step 323470, loss = 0.15 (1521.4 examples/sec; 0.084 sec/batch)
2017-06-02 10:28:47.303179: step 323480, loss = 0.15 (1520.8 examples/sec; 0.084 sec/batch)
2017-06-02 10:28:48.160428: step 323490, loss = 0.19 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:28:49.124069: step 323500, loss = 0.13 (1328.3 examples/sec; 0.096 sec/batch)
2017-06-02 10:28:49.904511: step 323510, loss = 0.17 (1640.1 examples/sec; 0.078 sec/batch)
2017-06-02 10:28:50.767988: step 323520, loss = 0.16 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:28:51.638257: step 323530, loss = 0.13 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:28:52.489905: step 323540, loss = 0.19 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:28:53.330268: step 323550, loss = 0.15 (1523.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:28:54.224768: step 323560, loss = 0.13 (1431.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:28:55.074880: step 323570, loss = 0.22 (1505.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:28:55.969205: step 323580, loss = 0.18 (1431.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:28:56.857789: step 323590, loss = 0.21 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 10:28:57.837692: step 323600, loss = 0.18 (1306.3 examples/sec; 0.098 sec/batch)
2017-06-02 10:28:58.610620: step 323610, loss = 0.16 (1656.0 examples/sec; 0.077 sec/batch)
2017-06-02 10:28:59.487679: step 323620, loss = 0.16 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:29:00.366017: step 323630, loss = 0.21 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:29:01.233579: step 323640, loss = 0.23 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:29:02.104318: step 323650, loss = 0.21 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:29:02.980006: step 323660, loss = 0.15 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:29:03.832107: step 323670, loss = 0.22 (1502.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:29:04.702826: step 323680, loss = 0.14 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:29:05.567222: step 323690, loss = 0.18 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:29:06.548512: step 323700, loss = 0.19 (1304.4 examples/sec; 0.098 sec/batch)
2017-06-02 10:29:07.300716: step 323710, loss = 0.17 (1701.7 examples/sec; 0.075 sec/batch)
2017-06-02 10:29:08.193567: step 323720, loss = 0.17 (1433.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:29:09.053428: step 323730, loss = 0.18 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:29:09.896833: step 323740, loss = 0.22 (1517.7 examples/sec; 0.084 sec/batch)
2017-06-02 10:29:10.773538: step 323750, loss = 0.19 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:29:11.667176: step 323760, loss = 0.20 (1432.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:29:12.548363: step 323770, loss = 0.16 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:29:13.426065: step 323780, loss = 0.19 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:29:14.286000: step 323790, loss = 0.20 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:29:15.238341: step 323800, loss = 0.15 (1344.1 examples/sec; 0.095 sec/batch)
2017-06-02 10:29:16.019658: step 323810, loss = 0.18 (1638.3 examples/sec; 0.078 sec/batch)
2017-06-02 10:29:16.878046: step 323820, loss = 0.16 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:29:17.759830: step 323830, loss = 0.16 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:29:18.629964: step 323840, loss = 0.21 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:29:19.473199: step 323850, loss = 0.20 (1518.0 examples/sec; 0.084 sec/batch)
2017-06-02 10:29:20.328361: step 323860, loss = 0.20 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:29:21.179689: step 323870, loss = 0.16 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:29:22.053905: step 323880, loss = 0.21 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:29:22.934806: step 323890, loss = 0.16 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:29:23.931807: step 323900, loss = 0.17 (1283.8 examples/sec; 0.100 sec/batch)
2017-06-02 10:29:24.701099: step 323910, loss = 0.18 (1663.9 examples/sec; 0.077 sec/batch)
2017-06-02 10:29:25.573860: step 323920, loss = 0.18 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:29:26.447636: step 323930, loss = 0.18 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:29:27.309093: step 323940, loss = 0.17 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:29:28.194128: step 323950, loss = 0.14 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:29:29.049971: step 323960, loss = 0.16 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:29:29.897594: step 323970, loss = 0.16 (1510.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:29:30.784802: step 323980, loss = 0.15 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 10:29:31.690591: step 323990, loss = 0.16 (1413.1 examples/sec; 0.091 sec/batch)
2017-06-02 10:29:32.641536: step 324000, loss = 0.15 (1346.0 examples/sec; 0.095 sec/batch)
2017-06-02 10:29:33.404540: step 324010, loss = 0.13 (1677.6 examples/sec; 0.076 sec/batch)
2017-06-02 10:29:34.286852: step 324020, loss = 0.18 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:29:35.158662: step 324030, loss = 0.19 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:29:36.021556: step 324040, loss = 0.14 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:29:36.854703: step 324050, loss = 0.22 (1536.4 examples/sec; 0.083 sec/batch)
2017-06-02 10:29:37.693296: step 324060, loss = 0.18 (1526.4 examples/sec; 0.084 sec/batch)
2017-06-02 10:29:38.552909: step 324070, loss = 0.17 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:29:39.430881: step 324080, loss = 0.14 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:29:40.303987: step 324090, loss = 0.16 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:29:41.279960: step 324100, loss = 0.13 (1311.5 examples/sec; 0.098 sec/batch)
2017-06-02 10:29:42.055610: step 324110, loss = 0.21 (1650.2 examples/sec; 0.078 sec/batch)
2017-06-02 10:29:42.927108: step 324120, loss = 0.17 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:29:43.794135: step 324130, loss = 0.17 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:29:44.674888: step 324140, loss = 0.18 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:29:45.548358: step 324150, loss = 0.16 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:29:46.414062: step 324160, loss = 0.19 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:29:47.295684: step 324170, loss = 0.21 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:29:48.147597: step 324180, loss = 0.17 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:29:48.996725: step 324190, loss = 0.15 (1507.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:29:49.966378: step 324200, loss = 0.18 (1320.1 examples/sec; 0.097 sec/batch)
2017-06-02 10:29:50.734425: step 324210, loss = 0.15 (1666.6 examples/sec; 0.077 sec/batch)
2017-06-02 10:29:51.612555: step 324220, loss = 0.21 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:29:52.659541: step 324230, loss = 0.16 (1222.6 examples/sec; 0.105 sec/batch)
2017-06-02 10:29:53.486922: step 324240, loss = 0.18 (1547.1 examples/sec; 0.083 sec/batch)
2017-06-02 10:29:54.334130: step 324250, loss = 0.17 (1510.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:29:55.173690: step 324260, loss = 0.16 (1524.6 examples/sec; 0.084 sec/batch)
2017-06-02 10:29:56.030728: step 324270, loss = 0.22 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:29:56.882976: step 324280, loss = 0.25 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:29:57.746860: step 324290, loss = 0.15 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:29:58.704903: step 324300, loss = 0.23 (1336.1 examples/sec; 0.096 sec/batch)
2017-06-02 10:29:59.458256: step 324310, loss = 0.26 (1699.1 examples/sec; 0.075 sec/batch)
2017-06-02 10:30:00.323297: step 324320, loss = 0.20 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:30:01.193688: step 324330, loss = 0.15 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:30:02.058093: step 324340, loss = 0.14 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:30:02.933186: step 324350, loss = 0.19 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:30:03.785836: step 324360, loss = 0.16 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:30:04.641845: step 324370, loss = 0.22 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:30:05.530067: step 324380, loss = 0.25 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 10:30:06.375122: step 324390, loss = 0.18 (1514.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:30:07.348043: step 324400, loss = 0.21 (1315.6 examples/sec; 0.097 sec/batch)
2017-06-02 10:30:08.117843: step 324410, loss = 0.18 (1662.8 examples/sec; 0.077 sec/batch)
2017-06-02 10:30:08.977247: step 324420, loss = 0.19 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:30:09.849986: step 324430, loss = 0.14 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:30:10.711022: step 324440, loss = 0.21 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:30:11.591071: step 324450, loss = 0.17 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:30:12.462448: step 324460, loss = 0.24 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:30:13.324943: step 324470, loss = 0.23 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:30:14.202288: step 324480, loss = 0.14 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:30:15.039254: step 324490, loss = 0.16 (1529.3 examples/sec; 0.084 sec/batch)
2017-06-02 10:30:15.995876: step 324500, loss = 0.16 (1338.0 examples/sec; 0.096 sec/batch)
2017-06-02 10:30:16.748107: step 324510, loss = 0.18 (1701.6 examples/sec; 0.075 sec/batch)
2017-06-02 10:30:17.638906: step 324520, loss = 0.18 (1436.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:30:18.542554: step 324530, loss = 0.15 (1416.5 examples/sec; 0.090 sec/batch)
2017-06-02 10:30:19.395600: step 324540, loss = 0.19 (1500.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:30:20.270973: step 324550, loss = 0.19 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:30:21.119345: step 324560, loss = 0.20 (1508.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:30:21.997265: step 324570, loss = 0.16 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:30:22.847303: step 324580, loss = 0.18 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:30:23.703841: step 324590, loss = 0.22 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:30:24.685736: step 324600, loss = 0.15 (1303.6 examples/sec; 0.098 sec/batch)
2017-06-02 10:30:25.449168: step 324610, loss = 0.21 (1676.6 examples/sec; 0.076 sec/batch)
2017-06-02 10:30:26.308494: step 324620, loss = 0.16 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:30:27.211335: step 324630, loss = 0.16 (1417.7 examples/sec; 0.090 sec/batch)
2017-06-02 10:30:28.057595: step 324640, loss = 0.14 (1512.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:30:28.915800: step 324650, loss = 0.19 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:30:29.807765: step 324660, loss = 0.17 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:30:30.643971: step 324670, loss = 0.19 (1530.7 examples/sec; 0.084 sec/batch)
2017-06-02 10:30:31.496471: step 324680, loss = 0.17 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:30:32.359794: step 324690, loss = 0.16 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:30:33.338788: step 324700, loss = 0.16 (1307.5 examples/sec; 0.098 sec/batch)
2017-06-02 10:30:34.086940: step 324710, loss = 0.19 (1710.9 examples/sec; 0.075 sec/batch)
2017-06-02 10:30:34.964094: step 324720, loss = 0.18 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:30:35.805748: step 324730, loss = 0.15 (1520.8 examples/sec; 0.084 sec/batch)
2017-06-02 10:30:36.669162: step 324740, loss = 0.17 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:30:37.509489: step 324750, loss = 0.20 (1523.2 examples/sec; 0.084 sec/batch)
2017-06-02 10:30:38.357350: step 324760, loss = 0.17 (1509.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:30:39.219369: step 324770, loss = 0.21 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:30:40.096311: step 324780, loss = 0.22 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:30:40.945287: step 324790, loss = 0.21 (1507.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:30:41.942713: step 324800, loss = 0.13 (1283.3 examples/sec; 0.100 sec/batch)
2017-06-02 10:30:42.707436: step 324810, loss = 0.17 (1673.8 examples/sec; 0.076 sec/batch)
2017-06-02 10:30:43.561626: step 324820, loss = 0.19 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:30:44.424131: step 324830, loss = 0.17 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:30:45.274200: step 324840, loss = 0.14 (1505.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:30:46.142104: step 324850, loss = 0.18 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:30:47.011399: step 324860, loss = 0.15 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:30:47.857655: step 324870, loss = 0.17 (1512.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:30:48.701498: step 324880, loss = 0.20 (1516.9 examples/sec; 0.084 sec/batch)
2017-06-02 10:30:49.557659: step 324890, loss = 0.15 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:30:50.507523: step 324900, loss = 0.26 (1347.6 examples/sec; 0.095 sec/batch)
2017-06-02 10:30:51.270260: step 324910, loss = 0.22 (1678.2 examples/sec; 0.076 sec/batch)
2017-06-02 10:30:52.140024: step 324920, loss = 0.16 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:30:52.987532: step 324930, loss = 0.17 (1510.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:30:53.851485: step 324940, loss = 0.20 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:30:54.708837: step 324950, loss = 0.18 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:30:55.580334: step 324960, loss = 0.20 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:30:56.428772: step 324970, loss = 0.13 (1508.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:30:57.276001: step 324980, loss = 0.17 (1510.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:30:58.135309: step 324990, loss = 0.15 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:30:59.100341: step 325000, loss = 0.16 (1326.4 examples/sec; 0.097 sec/batch)
2017-06-02 10:30:59.848197: step 325010, loss = 0.19 (1711.6 examples/sec; 0.075 sec/batch)
2017-06-02 10:31:00.722579: step 325020, loss = 0.21 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:31:01.572358: step 325030, loss = 0.14 (1506.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:31:02.430098: step 325040, loss = 0.17 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:31:03.279292: step 325050, loss = 0.15 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:31:04.175310: step 325060, loss = 0.16 (1428.6 examples/sec; 0.090 sec/batch)
2017-06-02 10:31:05.035265: step 325070, loss = 0.17 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:31:05.914407: step 325080, loss = 0.16 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:31:06.756995: step 325090, loss = 0.19 (1519.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:31:07.740163: step 325100, loss = 0.23 (1301.9 examples/sec; 0.098 sec/batch)
2017-06-02 10:31:08.520799: step 325110, loss = 0.17 (1639.7 examples/sec; 0.078 sec/batch)
2017-06-02 10:31:09.394663: step 325120, loss = 0.15 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:31:10.258356: step 325130, loss = 0.19 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:31:11.131517: step 325140, loss = 0.19 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:31:11.955153: step 325150, loss = 0.17 (1554.1 examples/sec; 0.082 sec/batch)
2017-06-02 10:31:12.807843: step 325160, loss = 0.16 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:31:13.660196: step 325170, loss = 0.13 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:31:14.534622: step 325180, loss = 0.14 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:31:15.391261: step 325190, loss = 0.18 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:31:16.369306: step 325200, loss = 0.15 (1308.7 examples/sec; 0.098 sec/batch)
2017-06-02 10:31:17.136609: step 325210, loss = 0.13 (1668.2 examples/sec; 0.077 sec/batch)
2017-06-02 10:31:17.990251: step 325220, loss = 0.17 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:31:18.857406: step 325230, loss = 0.22 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:31:19.710284: step 325240, loss = 0.12 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:31:20.580803: step 325250, loss = 0.17 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:31:21.441849: step 325260, loss = 0.16 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:31:22.312388: step 325270, loss = 0.17 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:31:23.181353: step 325280, loss = 0.16 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:31:24.048936: step 325290, loss = 0.14 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:31:25.033215: step 325300, loss = 0.21 (1300.4 examples/sec; 0.098 sec/batch)
2017-06-02 10:31:25.804065: step 325310, loss = 0.17 (1660.5 examples/sec; 0.077 sec/batch)
2017-06-02 10:31:26.662963: step 325320, loss = 0.19 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:31:27.537510: step 325330, loss = 0.17 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:31:28.407515: step 325340, loss = 0.20 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:31:29.273782: step 325350, loss = 0.18 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:31:30.134973: step 325360, loss = 0.19 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:31:30.995749: step 325370, loss = 0.15 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:31:31.879754: step 325380, loss = 0.16 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:31:32.771144: step 325390, loss = 0.16 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:31:33.771041: step 325400, loss = 0.22 (1280.1 examples/sec; 0.100 sec/batch)
2017-06-02 10:31:34.564763: step 325410, loss = 0.15 (1612.7 examples/sec; 0.079 sec/batch)
2017-06-02 10:31:35.421607: step 325420, loss = 0.17 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:31:36.282538: step 325430, loss = 0.19 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:31:37.142130: step 325440, loss = 0.20 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:31:38.013060: step 325450, loss = 0.23 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:31:38.882376: step 325460, loss = 0.25 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:31:39.760389: step 325470, loss = 0.19 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:31:40.613080: step 325480, loss = 0.18 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:31:41.505769: step 325490, loss = 0.13 (1433.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:31:42.463408: step 325500, loss = 0.16 (1336.6 examples/sec; 0.096 sec/batch)
2017-06-02 10:31:43.228963: step 325510, loss = 0.18 (1672.0 examples/sec; 0.077 sec/batch)
2017-06-02 10:31:44.117886: step 325520, loss = 0.18 (1439.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:31:44.987780: step 325530, loss = 0.18 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:31:45.826247: step 325540, loss = 0.22 (1526.6 examples/sec; 0.084 sec/batch)
2017-06-02 10:31:46.701281: step 325550, loss = 0.21 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:31:47.582008: step 325560, loss = 0.16 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:31:48.433895: step 325570, loss = 0.14 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:31:49.299886: step 325580, loss = 0.17 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:31:50.177806: step 325590, loss = 0.24 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:31:51.157073: step 325600, loss = 0.21 (1307.1 examples/sec; 0.098 sec/batch)
2017-06-02 10:31:51.918401: step 325610, loss = 0.28 (1681.3 examples/sec; 0.076 sec/batch)
2017-06-02 10:31:52.779034: step 325620, loss = 0.14 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:31:53.633319: step 325630, loss = 0.13 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:31:54.504045: step 325640, loss = 0.16 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:31:55.367023: step 325650, loss = 0.16 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:31:56.242837: step 325660, loss = 0.18 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:31:57.144826: step 325670, loss = 0.15 (1419.1 examples/sec; 0.090 sec/batch)
2017-06-02 10:31:58.000871: step 325680, loss = 0.18 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:31:58.852531: step 325690, loss = 0.20 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:31:59.836013: step 325700, loss = 0.16 (1301.5 examples/sec; 0.098 sec/batch)
2017-06-02 10:32:00.618727: step 325710, loss = 0.14 (1635.3 examples/sec; 0.078 sec/batch)
2017-06-02 10:32:01.499182: step 325720, loss = 0.16 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:32:02.354840: step 325730, loss = 0.19 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:32:03.212303: step 325740, loss = 0.17 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:32:04.054279: step 325750, loss = 0.14 (1520.2 examples/sec; 0.084 sec/batch)
2017-06-02 10:32:04.942415: step 325760, loss = 0.16 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:32:05.800244: step 325770, loss = 0.14 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:32:06.633452: step 325780, loss = 0.15 (1536.2 examples/sec; 0.083 sec/batch)
2017-06-02 10:32:07.471927: step 325790, loss = 0.22 (1526.6 examples/sec; 0.084 sec/batch)
2017-06-02 10:32:08.443084: step 325800, loss = 0.17 (1318.0 examples/sec; 0.097 sec/batch)
2017-06-02 10:32:09.210507: step 325810, loss = 0.19 (1667.9 examples/sec; 0.077 sec/batch)
2017-06-02 10:32:10.081478: step 325820, loss = 0.15 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:32:10.950022: step 325830, loss = 0.20 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:32:11.820557: step 325840, loss = 0.14 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:32:12.682564: step 325850, loss = 0.21 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:32:13.557115: step 325860, loss = 0.16 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:32:14.413326: step 325870, loss = 0.18 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:32:15.289782: step 325880, loss = 0.18 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:32:16.154438: step 325890, loss = 0.17 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:32:17.114685: step 325900, loss = 0.15 (1333.0 examples/sec; 0.096 sec/batch)
2017-06-02 10:32:17.886271: step 325910, loss = 0.18 (1658.9 examples/sec; 0.077 sec/batch)
2017-06-02 10:32:18.767671: step 325920, loss = 0.18 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:32:19.630555: step 325930, loss = 0.18 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:32:20.484664: step 325940, loss = 0.13 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:32:21.364363: step 325950, loss = 0.14 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:32:22.224436: step 325960, loss = 0.18 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:32:23.098313: step 325970, loss = 0.16 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:32:23.957137: step 325980, loss = 0.22 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:32:24.847277: step 325990, loss = 0.16 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:32:25.820951: step 326000, loss = 0.17 (1314.6 examples/sec; 0.097 sec/batch)
2017-06-02 10:32:26.620742: step 326010, loss = 0.18 (1600.4 examples/sec; 0.080 sec/batch)
2017-06-02 10:32:27.475951: step 326020, loss = 0.22 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:32:28.310312: step 326030, loss = 0.15 (1534.1 examples/sec; 0.083 sec/batch)
2017-06-02 10:32:29.160202: step 326040, loss = 0.15 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:32:30.029979: step 326050, loss = 0.18 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:32:30.881874: step 326060, loss = 0.31 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:32:31.732457: step 326070, loss = 0.22 (1504.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:32:32.600077: step 326080, loss = 0.19 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:32:33.464703: step 326090, loss = 0.15 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:32:34.456163: step 326100, loss = 0.23 (1291.0 examples/sec; 0.099 sec/batch)
2017-06-02 10:32:35.222591: step 326110, loss = 0.16 (1670.1 examples/sec; 0.077 sec/batch)
2017-06-02 10:32:36.082813: step 326120, loss = 0.17 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:32:36.954775: step 326130, loss = 0.18 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:32:37.823594: step 326140, loss = 0.17 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:32:38.711870: step 326150, loss = 0.20 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:32:39.588844: step 326160, loss = 0.18 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:32:40.463977: step 326170, loss = 0.16 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:32:41.332252: step 326180, loss = 0.18 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:32:42.179197: step 326190, loss = 0.17 (1511.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:32:43.184948: step 326200, loss = 0.20 (1272.7 examples/sec; 0.101 sec/batch)
2017-06-02 10:32:43.946373: step 326210, loss = 0.19 (1681.0 examples/sec; 0.076 sec/batch)
2017-06-02 10:32:44.818389: step 326220, loss = 0.18 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:32:45.668997: step 326230, loss = 0.17 (1504.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:32:46.533706: step 326240, loss = 0.17 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:32:47.373131: step 326250, loss = 0.18 (1524.8 examples/sec; 0.084 sec/batch)
2017-06-02 10:32:48.221325: step 326260, loss = 0.15 (1509.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:32:49.065251: step 326270, loss = 0.18 (1516.7 examples/sec; 0.084 sec/batch)
2017-06-02 10:32:49.941655: step 326280, loss = 0.16 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:32:50.822176: step 326290, loss = 0.19 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:32:51.776466: step 326300, loss = 0.19 (1341.3 examples/sec; 0.095 sec/batch)
2017-06-02 10:32:52.538121: step 326310, loss = 0.22 (1680.6 examples/sec; 0.076 sec/batch)
2017-06-02 10:32:53.400414: step 326320, loss = 0.15 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:32:54.267727: step 326330, loss = 0.13 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:32:55.125482: step 326340, loss = 0.15 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:32:55.959543: step 326350, loss = 0.17 (1534.7 examples/sec; 0.083 sec/batch)
2017-06-02 10:32:56.821530: step 326360, loss = 0.16 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:32:57.669052: step 326370, loss = 0.18 (1510.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:32:58.537925: step 326380, loss = 0.14 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:32:59.415213: step 326390, loss = 0.18 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:33:00.373347: step 326400, loss = 0.17 (1335.9 examples/sec; 0.096 sec/batch)
2017-06-02 10:33:01.126246: step 326410, loss = 0.18 (1700.1 examples/sec; 0.075 sec/batch)
2017-06-02 10:33:02.007113: step 326420, loss = 0.18 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:33:02.870925: step 326430, loss = 0.17 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:33:03.721312: step 326440, loss = 0.15 (1505.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:33:04.589562: step 326450, loss = 0.19 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:33:05.436467: step 326460, loss = 0.14 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:33:06.282557: step 326470, loss = 0.15 (1512.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:33:07.124574: step 326480, loss = 0.18 (1520.2 examples/sec; 0.084 sec/batch)
2017-06-02 10:33:07.966154: step 326490, loss = 0.17 (1521.0 examples/sec; 0.084 sec/batch)
2017-06-02 10:33:08.919942: step 326500, loss = 0.16 (1342.0 examples/sec; 0.095 sec/batch)
2017-06-02 10:33:09.694652: step 326510, loss = 0.14 (1652.2 examples/sec; 0.077 sec/batch)
2017-06-02 10:33:10.532442: step 326520, loss = 0.15 (1527.8 examples/sec; 0.084 sec/batch)
2017-06-02 10:33:11.390945: step 326530, loss = 0.16 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:33:12.239412: step 326540, loss = 0.15 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:33:13.100089: step 326550, loss = 0.17 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:33:13.981699: step 326560, loss = 0.16 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:33:14.852353: step 326570, loss = 0.17 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:33:15.726436: step 326580, loss = 0.19 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:33:16.568701: step 326590, loss = 0.14 (1519.7 examples/sec; 0.084 sec/batch)
2017-06-02 10:33:17.520591: step 326600, loss = 0.15 (1344.7 examples/sec; 0.095 sec/batch)
2017-06-02 10:33:18.267036: step 326610, loss = 0.18 (1714.8 examples/sec; 0.075 sec/batch)
2017-06-02 10:33:19.128394: step 326620, loss = 0.16 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:33:20.000752: step 326630, loss = 0.17 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:33:20.870347: step 326640, loss = 0.15 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:33:21.756027: step 326650, loss = 0.17 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:33:22.626204: step 326660, loss = 0.16 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:33:23.476432: step 326670, loss = 0.17 (1505.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:33:24.338573: step 326680, loss = 0.14 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:33:25.216762: step 326690, loss = 0.22 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:33:26.218009: step 326700, loss = 0.20 (1278.4 examples/sec; 0.100 sec/batch)
2017-06-02 10:33:26.963359: step 326710, loss = 0.15 (1717.3 examples/sec; 0.075 sec/batch)
2017-06-02 10:33:27.828894: step 326720, loss = 0.21 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:33:28.724069: step 326730, loss = 0.19 (1429.9 examples/sec; 0.090 sec/batch)
2017-06-02 10:33:29.574935: step 326740, loss = 0.20 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:33:30.471269: step 326750, loss = 0.15 (1428.0 examples/sec; 0.090 sec/batch)
2017-06-02 10:33:31.327934: step 326760, loss = 0.23 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:33:32.150573: step 326770, loss = 0.19 (1556.0 examples/sec; 0.082 sec/batch)
2017-06-02 10:33:32.999219: step 326780, loss = 0.18 (1508.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:33:33.833644: step 326790, loss = 0.17 (1534.0 examples/sec; 0.083 sec/batch)
2017-06-02 10:33:34.799746: step 326800, loss = 0.17 (1324.9 examples/sec; 0.097 sec/batch)
2017-06-02 10:33:35.592996: step 326810, loss = 0.16 (1613.6 examples/sec; 0.079 sec/batch)
2017-06-02 10:33:36.453375: step 326820, loss = 0.18 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:33:37.329267: step 326830, loss = 0.14 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:33:38.176432: step 326840, loss = 0.16 (1510.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:33:39.036397: step 326850, loss = 0.18 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:33:39.890070: step 326860, loss = 0.16 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:33:40.754997: step 326870, loss = 0.17 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:33:41.620682: step 326880, loss = 0.18 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:33:42.491336: step 326890, loss = 0.17 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:33:43.437503: step 326900, loss = 0.16 (1352.8 examples/sec; 0.095 sec/batch)
2017-06-02 10:33:44.216121: step 326910, loss = 0.22 (1644.0 examples/sec; 0.078 sec/batch)
2017-06-02 10:33:45.087881: step 326920, loss = 0.14 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:33:45.967805: step 326930, loss = 0.21 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:33:46.817025: step 326940, loss = 0.15 (1507.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:33:47.663521: step 326950, loss = 0.18 (1512.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:33:48.540765: step 326960, loss = 0.16 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:33:49.395020: step 326970, loss = 0.22 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:33:50.238753: step 326980, loss = 0.18 (1517.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:33:51.107473: step 326990, loss = 0.18 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:33:52.138387: step 327000, loss = 0.19 (1241.6 examples/sec; 0.103 sec/batch)
2017-06-02 10:33:52.854054: step 327010, loss = 0.14 (1788.6 examples/sec; 0.072 sec/batch)
2017-06-02 10:33:53.700830: step 327020, loss = 0.18 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:33:54.528384: step 327030, loss = 0.17 (1546.7 examples/sec; 0.083 sec/batch)
2017-06-02 10:33:55.406359: step 327040, loss = 0.19 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:33:56.268068: step 327050, loss = 0.20 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:33:57.113930: step 327060, loss = 0.20 (1513.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:33:57.958237: step 327070, loss = 0.13 (1516.0 examples/sec; 0.084 sec/batch)
2017-06-02 10:33:58.860673: step 327080, loss = 0.24 (1418.4 examples/sec; 0.090 sec/batch)
2017-06-02 10:33:59.738628: step 327090, loss = 0.16 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:34:00.739465: step 327100, loss = 0.23 (1278.9 examples/sec; 0.100 sec/batch)
2017-06-02 10:34:01.448666: step 327110, loss = 0.16 (1804.9 examples/sec; 0.071 sec/batch)
2017-06-02 10:34:02.287878: step 327120, loss = 0.15 (1525.2 examples/sec; 0.084 sec/batch)
2017-06-02 10:34:03.176838: step 327130, loss = 0.19 (1439.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:34:04.034898: step 327140, loss = 0.15 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:34:04.902640: step 327150, loss = 0.20 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:34:05.782925: step 327160, loss = 0.12 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:34:06.666314: step 327170, loss = 0.15 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:34:07.530210: step 327180, loss = 0.21 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:34:08.417784: step 327190, loss = 0.30 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 10:34:09.399290: step 327200, loss = 0.17 (1304.1 examples/sec; 0.098 sec/batch)
2017-06-02 10:34:10.177795: step 327210, loss = 0.19 (1644.2 examples/sec; 0.078 sec/batch)
2017-06-02 10:34:11.080912: step 327220, loss = 0.17 (1417.3 examples/sec; 0.090 sec/batch)
2017-06-02 10:34:11.958771: step 327230, loss = 0.16 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:34:12.840095: step 327240, loss = 0.17 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:34:13.698923: step 327250, loss = 0.18 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:34:14.580923: step 327260, loss = 0.18 (1451.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:34:15.435008: step 327270, loss = 0.18 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:34:16.329340: step 327280, loss = 0.16 (1431.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:34:17.181057: step 327290, loss = 0.13 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:34:18.136419: step 327300, loss = 0.18 (1339.8 examples/sec; 0.096 sec/batch)
2017-06-02 10:34:18.888269: step 327310, loss = 0.21 (1702.5 examples/sec; 0.075 sec/batch)
2017-06-02 10:34:19.767464: step 327320, loss = 0.14 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:34:20.658752: step 327330, loss = 0.17 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 10:34:21.552414: step 327340, loss = 0.17 (1432.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:34:22.410107: step 327350, loss = 0.14 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:34:23.290304: step 327360, loss = 0.14 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:34:24.145230: step 327370, loss = 0.14 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:34:25.030943: step 327380, loss = 0.14 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:34:25.875051: step 327390, loss = 0.18 (1516.4 examples/sec; 0.084 sec/batch)
2017-06-02 10:34:26.848982: step 327400, loss = 0.18 (1314.2 examples/sec; 0.097 sec/batch)
2017-06-02 10:34:27.626105: step 327410, loss = 0.20 (1647.1 examples/sec; 0.078 sec/batch)
2017-06-02 10:34:28.507979: step 327420, loss = 0.19 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:34:29.377239: step 327430, loss = 0.16 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:34:30.246193: step 327440, loss = 0.16 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:34:31.122124: step 327450, loss = 0.17 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:34:31.986933: step 327460, loss = 0.21 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:34:32.846360: step 327470, loss = 0.18 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:34:33.726481: step 327480, loss = 0.19 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:34:34.598807: step 327490, loss = 0.21 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:34:35.582435: step 327500, loss = 0.15 (1301.3 examples/sec; 0.098 sec/batch)
2017-06-02 10:34:36.348006: step 327510, loss = 0.17 (1672.0 examples/sec; 0.077 sec/batch)
2017-06-02 10:34:37.229219: step 327520, loss = 0.18 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:34:38.076126: step 327530, loss = 0.19 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:34:38.934470: step 327540, loss = 0.22 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:34:39.806130: step 327550, loss = 0.14 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:34:40.675160: step 327560, loss = 0.16 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:34:41.540079: step 327570, loss = 0.16 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:34:42.397610: step 327580, loss = 0.20 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:34:43.285889: step 327590, loss = 0.17 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:34:44.246429: step 327600, loss = 0.21 (1332.6 examples/sec; 0.096 sec/batch)
2017-06-02 10:34:45.008522: step 327610, loss = 0.21 (1679.6 examples/sec; 0.076 sec/batch)
2017-06-02 10:34:45.884340: step 327620, loss = 0.16 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:34:46.742537: step 327630, loss = 0.15 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:34:47.594789: step 327640, loss = 0.14 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:34:48.481252: step 327650, loss = 0.18 (1443.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:34:49.369130: step 327660, loss = 0.17 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:34:50.211034: step 327670, loss = 0.18 (1520.4 examples/sec; 0.084 sec/batch)
2017-06-02 10:34:51.072754: step 327680, loss = 0.17 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:34:51.961815: step 327690, loss = 0.18 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 10:34:52.972086: step 327700, loss = 0.16 (1267.0 examples/sec; 0.101 sec/batch)
2017-06-02 10:34:53.690552: step 327710, loss = 0.16 (1781.6 examples/sec; 0.072 sec/batch)
2017-06-02 10:34:54.556046: step 327720, loss = 0.16 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:34:55.411299: step 327730, loss = 0.13 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:34:56.271744: step 327740, loss = 0.17 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:34:57.151737: step 327750, loss = 0.16 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:34:58.018724: step 327760, loss = 0.17 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:34:58.883531: step 327770, loss = 0.17 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:34:59.757584: step 327780, loss = 0.18 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:35:00.624401: step 327790, loss = 0.20 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:35:01.567882: step 327800, loss = 0.18 (1356.7 examples/sec; 0.094 sec/batch)
2017-06-02 10:35:02.328587: step 327810, loss = 0.18 (1682.7 examples/sec; 0.076 sec/batch)
2017-06-02 10:35:03.161682: step 327820, loss = 0.19 (1536.4 examples/sec; 0.083 sec/batch)
2017-06-02 10:35:04.014673: step 327830, loss = 0.17 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:35:04.878927: step 327840, loss = 0.20 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:35:05.763382: step 327850, loss = 0.16 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:35:06.628860: step 327860, loss = 0.20 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:35:07.487631: step 327870, loss = 0.16 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:35:08.334383: step 327880, loss = 0.18 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:35:09.191962: step 327890, loss = 0.17 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:35:10.200481: step 327900, loss = 0.17 (1269.2 examples/sec; 0.101 sec/batch)
2017-06-02 10:35:10.898138: step 327910, loss = 0.13 (1834.7 examples/sec; 0.070 sec/batch)
2017-06-02 10:35:11.776460: step 327920, loss = 0.20 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:35:12.671464: step 327930, loss = 0.23 (1430.2 examples/sec; 0.090 sec/batch)
2017-06-02 10:35:13.534131: step 327940, loss = 0.18 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:35:14.409687: step 327950, loss = 0.19 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:35:15.267259: step 327960, loss = 0.17 (1492.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:35:16.110900: step 327970, loss = 0.22 (1517.2 examples/sec; 0.084 sec/batch)
2017-06-02 10:35:16.958442: step 327980, loss = 0.15 (1510.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:35:17.840853: step 327990, loss = 0.19 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:35:18.815623: step 328000, loss = 0.13 (1313.1 examples/sec; 0.097 sec/batch)
2017-06-02 10:35:19.594520: step 328010, loss = 0.17 (1643.4 examples/sec; 0.078 sec/batch)
2017-06-02 10:35:20.472304: step 328020, loss = 0.21 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:35:21.331683: step 328030, loss = 0.20 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:35:22.196901: step 328040, loss = 0.15 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:35:23.065839: step 328050, loss = 0.18 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:35:23.940842: step 328060, loss = 0.14 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:35:24.818808: step 328070, loss = 0.17 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:35:25.690515: step 328080, loss = 0.19 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:35:26.555869: step 328090, loss = 0.15 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:35:27.526812: step 328100, loss = 0.20 (1318.3 examples/sec; 0.097 sec/batch)
2017-06-02 10:35:28.297933: step 328110, loss = 0.17 (1659.9 examples/sec; 0.077 sec/batch)
2017-06-02 10:35:29.164030: step 328120, loss = 0.23 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:35:30.020216: step 328130, loss = 0.15 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:35:30.889859: step 328140, loss = 0.23 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:35:31.755781: step 328150, loss = 0.18 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:35:32.629682: step 328160, loss = 0.16 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:35:33.491657: step 328170, loss = 0.19 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:35:34.357118: step 328180, loss = 0.15 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:35:35.235073: step 328190, loss = 0.19 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:35:36.224502: step 328200, loss = 0.18 (1293.7 examples/sec; 0.099 sec/batch)
2017-06-02 10:35:36.966872: step 328210, loss = 0.23 (1724.2 examples/sec; 0.074 sec/batch)
2017-06-02 10:35:37.834255: step 328220, loss = 0.17 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:35:38.692409: step 328230, loss = 0.19 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:35:39.557389: step 328240, loss = 0.19 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:35:40.429656: step 328250, loss = 0.17 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:35:41.279558: step 328260, loss = 0.16 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:35:42.149575: step 328270, loss = 0.20 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:35:43.015986: step 328280, loss = 0.18 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:35:43.875033: step 328290, loss = 0.13 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:35:44.837201: step 328300, loss = 0.16 (1330.3 examples/sec; 0.096 sec/batch)
2017-06-02 10:35:45.608632: step 328310, loss = 0.14 (1659.3 examples/sec; 0.077 sec/batch)
2017-06-02 10:35:46.505404: step 328320, loss = 0.18 (1427.3 examples/sec; 0.090 sec/batch)
2017-06-02 10:35:47.346390: step 328330, loss = 0.16 (1522.0 examples/sec; 0.084 sec/batch)
2017-06-02 10:35:48.239400: step 328340, loss = 0.19 (1433.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:35:49.102038: step 328350, loss = 0.15 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:35:49.963372: step 328360, loss = 0.14 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:35:50.804689: step 328370, loss = 0.15 (1521.4 examples/sec; 0.084 sec/batch)
2017-06-02 10:35:51.654492: step 328380, loss = 0.20 (1506.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:35:52.521392: step 328390, loss = 0.20 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:35:53.513018: step 328400, loss = 0.17 (1290.8 examples/sec; 0.099 sec/batch)
2017-06-02 10:35:54.268455: step 328410, loss = 0.19 (1694.4 examples/sec; 0.076 sec/batch)
2017-06-02 10:35:55.133915: step 328420, loss = 0.17 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:35:55.983941: step 328430, loss = 0.16 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:35:56.851567: step 328440, loss = 0.18 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:35:57.746526: step 328450, loss = 0.18 (1430.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:35:58.577566: step 328460, loss = 0.14 (1540.2 examples/sec; 0.083 sec/batch)
2017-06-02 10:35:59.448264: step 328470, loss = 0.24 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:36:00.321773: step 328480, loss = 0.17 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:36:01.182585: step 328490, loss = 0.15 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:36:02.131386: step 328500, loss = 0.18 (1349.0 examples/sec; 0.095 sec/batch)
2017-06-02 10:36:02.901957: step 328510, loss = 0.16 (1661.1 examples/sec; 0.077 sec/batch)
2017-06-02 10:36:03.769478: step 328520, loss = 0.16 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:36:04.630689: step 328530, loss = 0.15 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:36:05.488203: step 328540, loss = 0.17 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:36:06.340124: step 328550, loss = 0.34 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:36:07.227805: step 328560, loss = 0.14 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:36:08.097244: step 328570, loss = 0.15 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:36:08.943075: step 328580, loss = 0.16 (1513.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:36:09.809720: step 328590, loss = 0.17 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:36:10.775562: step 328600, loss = 0.15 (1325.3 examples/sec; 0.097 sec/batch)
2017-06-02 10:36:11.521631: step 328610, loss = 0.18 (1715.7 examples/sec; 0.075 sec/batch)
2017-06-02 10:36:12.397562: step 328620, loss = 0.20 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:36:13.266762: step 328630, loss = 0.15 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:36:14.127409: step 328640, loss = 0.19 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:36:15.007091: step 328650, loss = 0.13 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:36:15.881786: step 328660, loss = 0.15 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:36:16.772512: step 328670, loss = 0.19 (1437.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:36:17.651512: step 328680, loss = 0.18 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:36:18.503930: step 328690, loss = 0.18 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:36:19.455430: step 328700, loss = 0.15 (1345.2 examples/sec; 0.095 sec/batch)
2017-06-02 10:36:20.225514: step 328710, loss = 0.17 (1662.2 examples/sec; 0.077 sec/batch)
2017-06-02 10:36:21.128666: step 328720, loss = 0.16 (1417.3 examples/sec; 0.090 sec/batch)
2017-06-02 10:36:22.008008: step 328730, loss = 0.18 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:36:22.878064: step 328740, loss = 0.20 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:36:23.745183: step 328750, loss = 0.17 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:36:24.650529: step 328760, loss = 0.14 (1413.8 examples/sec; 0.091 sec/batch)
2017-06-02 10:36:25.497165: step 328770, loss = 0.17 (1511.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:36:26.330051: step 328780, loss = 0.24 (1536.8 examples/sec; 0.083 sec/batch)
2017-06-02 10:36:27.211747: step 328790, loss = 0.16 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:36:28.197492: step 328800, loss = 0.16 (1298.5 examples/sec; 0.099 sec/batch)
2017-06-02 10:36:28.945710: step 328810, loss = 0.16 (1710.7 examples/sec; 0.075 sec/batch)
2017-06-02 10:36:29.808241: step 328820, loss = 0.15 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:36:30.692507: step 328830, loss = 0.15 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:36:31.572314: step 328840, loss = 0.18 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:36:32.455975: step 328850, loss = 0.16 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:36:33.335334: step 328860, loss = 0.19 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:36:34.209933: step 328870, loss = 0.14 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:36:35.082817: step 328880, loss = 0.16 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:36:35.942487: step 328890, loss = 0.17 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:36:36.875578: step 328900, loss = 0.18 (1371.8 examples/sec; 0.093 sec/batch)
2017-06-02 10:36:37.663189: step 328910, loss = 0.16 (1625.2 examples/sec; 0.079 sec/batch)
2017-06-02 10:36:38.549876: step 328920, loss = 0.16 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:36:39.429252: step 328930, loss = 0.16 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:36:40.304843: step 328940, loss = 0.18 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:36:41.180894: step 328950, loss = 0.18 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:36:42.046532: step 328960, loss = 0.17 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:36:42.910561: step 328970, loss = 0.20 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:36:43.766055: step 328980, loss = 0.16 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:36:44.620350: step 328990, loss = 0.20 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:36:45.575809: step 329000, loss = 0.23 (1339.7 examples/sec; 0.096 sec/batch)
2017-06-02 10:36:46.350578: step 329010, loss = 0.19 (1652.1 examples/sec; 0.077 sec/batch)
2017-06-02 10:36:47.253757: step 329020, loss = 0.17 (1417.2 examples/sec; 0.090 sec/batch)
2017-06-02 10:36:48.142335: step 329030, loss = 0.22 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 10:36:48.986616: step 329040, loss = 0.23 (1516.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:36:49.840077: step 329050, loss = 0.16 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:36:50.742599: step 329060, loss = 0.20 (1418.2 examples/sec; 0.090 sec/batch)
2017-06-02 10:36:51.603546: step 329070, loss = 0.18 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:36:52.463975: step 329080, loss = 0.16 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:36:53.327436: step 329090, loss = 0.16 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:36:54.307190: step 329100, loss = 0.15 (1306.5 examples/sec; 0.098 sec/batch)
2017-06-02 10:36:55.085701: step 329110, loss = 0.24 (1644.2 examples/sec; 0.078 sec/batch)
2017-06-02 10:36:55.957125: step 329120, loss = 0.21 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:36:56.830567: step 329130, loss = 0.18 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:36:57.702836: step 329140, loss = 0.16 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:36:58.545695: step 329150, loss = 0.22 (1518.7 examples/sec; 0.084 sec/batch)
2017-06-02 10:36:59.406933: step 329160, loss = 0.16 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:37:00.291677: step 329170, loss = 0.18 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:37:01.124692: step 329180, loss = 0.17 (1536.6 examples/sec; 0.083 sec/batch)
2017-06-02 10:37:01.978304: step 329190, loss = 0.15 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:37:02.949887: step 329200, loss = 0.18 (1317.4 examples/sec; 0.097 sec/batch)
2017-06-02 10:37:03.743185: step 329210, loss = 0.21 (1613.5 examples/sec; 0.079 sec/batch)
2017-06-02 10:37:04.632452: step 329220, loss = 0.17 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:37:05.523421: step 329230, loss = 0.15 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:37:06.385802: step 329240, loss = 0.18 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:37:07.225215: step 329250, loss = 0.15 (1524.9 examples/sec; 0.084 sec/batch)
2017-06-02 10:37:08.090321: step 329260, loss = 0.15 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:37:08.966438: step 329270, loss = 0.19 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:37:09.822819: step 329280, loss = 0.15 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:37:10.696721: step 329290, loss = 0.19 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:37:11.631655: step 329300, loss = 0.15 (1369.1 examples/sec; 0.093 sec/batch)
2017-06-02 10:37:12.398160: step 329310, loss = 0.14 (1669.9 examples/sec; 0.077 sec/batch)
2017-06-02 10:37:13.269172: step 329320, loss = 0.20 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:37:14.130767: step 329330, loss = 0.15 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:37:14.997275: step 329340, loss = 0.20 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:37:15.869389: step 329350, loss = 0.16 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:37:16.727692: step 329360, loss = 0.21 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:37:17.613743: step 329370, loss = 0.20 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:37:18.476407: step 329380, loss = 0.12 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:37:19.367604: step 329390, loss = 0.13 (1436.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:37:20.363242: step 329400, loss = 0.20 (1285.6 examples/sec; 0.100 sec/batch)
2017-06-02 10:37:21.128154: step 329410, loss = 0.15 (1673.4 examples/sec; 0.076 sec/batch)
2017-06-02 10:37:21.995232: step 329420, loss = 0.20 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:37:22.869713: step 329430, loss = 0.16 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:37:23.731206: step 329440, loss = 0.17 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:37:24.571894: step 329450, loss = 0.14 (1522.6 examples/sec; 0.084 sec/batch)
2017-06-02 10:37:25.428973: step 329460, loss = 0.18 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:37:26.292477: step 329470, loss = 0.18 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:37:27.121097: step 329480, loss = 0.16 (1544.7 examples/sec; 0.083 sec/batch)
2017-06-02 10:37:28.004126: step 329490, loss = 0.16 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:37:29.001907: step 329500, loss = 0.14 (1282.8 examples/sec; 0.100 sec/batch)
2017-06-02 10:37:29.750969: step 329510, loss = 0.23 (1708.8 examples/sec; 0.075 sec/batch)
2017-06-02 10:37:30.617580: step 329520, loss = 0.19 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:37:31.469569: step 329530, loss = 0.19 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:37:32.316621: step 329540, loss = 0.16 (1511.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:37:33.199197: step 329550, loss = 0.15 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:37:34.055694: step 329560, loss = 0.20 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:37:34.913661: step 329570, loss = 0.15 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:37:35.785147: step 329580, loss = 0.21 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:37:36.643789: step 329590, loss = 0.18 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:37:37.614527: step 329600, loss = 0.15 (1318.6 examples/sec; 0.097 sec/batch)
2017-06-02 10:37:38.369761: step 329610, loss = 0.14 (1694.8 examples/sec; 0.076 sec/batch)
2017-06-02 10:37:39.237132: step 329620, loss = 0.15 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:37:40.109764: step 329630, loss = 0.18 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:37:40.961933: step 329640, loss = 0.16 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:37:41.816142: step 329650, loss = 0.21 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:37:42.680373: step 329660, loss = 0.15 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:37:43.507665: step 329670, loss = 0.17 (1547.2 examples/sec; 0.083 sec/batch)
2017-06-02 10:37:44.383246: step 329680, loss = 0.14 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:37:45.246794: step 329690, loss = 0.14 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:37:46.186508: step 329700, loss = 0.18 (1362.1 examples/sec; 0.094 sec/batch)
2017-06-02 10:37:46.940065: step 329710, loss = 0.20 (1698.6 examples/sec; 0.075 sec/batch)
2017-06-02 10:37:47.816202: step 329720, loss = 0.16 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:37:48.688248: step 329730, loss = 0.14 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:37:49.594097: step 329740, loss = 0.13 (1413.0 examples/sec; 0.091 sec/batch)
2017-06-02 10:37:50.479951: step 329750, loss = 0.15 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:37:51.357879: step 329760, loss = 0.15 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:37:52.245621: step 329770, loss = 0.16 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:37:53.127591: step 329780, loss = 0.21 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:37:53.982811: step 329790, loss = 0.16 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:37:54.973772: step 329800, loss = 0.18 (1291.7 examples/sec; 0.099 sec/batch)
2017-06-02 10:37:55.738563: step 329810, loss = 0.13 (1673.7 examples/sec; 0.076 sec/batch)
2017-06-02 10:37:56.620535: step 329820, loss = 0.17 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:37:57.510219: step 329830, loss = 0.13 (1438.7 examples/sec; 0.089 sec/batch)
2017-06-02 10:37:58.365543: step 329840, loss = 0.19 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:37:59.256379: step 329850, loss = 0.19 (1436.8 examples/sec; 0.089 sec/batch)
2017-06-02 10:38:00.134178: step 329860, loss = 0.15 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:38:01.018287: step 329870, loss = 0.20 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:38:01.898782: step 329880, loss = 0.20 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:38:02.779636: step 329890, loss = 0.16 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:38:03.783131: step 329900, loss = 0.18 (1275.5 examples/sec; 0.100 sec/batch)
2017-06-02 10:38:04.585916: step 329910, loss = 0.16 (1594.5 examples/sec; 0.080 sec/batch)
2017-06-02 10:38:05.461629: step 329920, loss = 0.17 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:38:06.324174: step 329930, loss = 0.18 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:38:07.197092: step 329940, loss = 0.17 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:38:08.062856: step 329950, loss = 0.17 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:38:08.950543: step 329960, loss = 0.18 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:38:09.835551: step 329970, loss = 0.15 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:38:10.723248: step 329980, loss = 0.18 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:38:11.599515: step 329990, loss = 0.14 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:38:12.594885: step 330000, loss = 0.17 (1285.9 examples/sec; 0.100 sec/batch)
2017-06-02 10:38:13.379319: step 330010, loss = 0.19 (1631.7 examples/sec; 0.078 sec/batch)
2017-06-02 10:38:14.241938: step 330020, loss = 0.20 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:38:15.133538: step 330030, loss = 0.14 (1435.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:38:16.015838: step 330040, loss = 0.18 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:38:16.891648: step 330050, loss = 0.19 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:38:17.751103: step 330060, loss = 0.16 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:38:18.595628: step 330070, loss = 0.20 (1515.6 examples/sec; 0.084 sec/batch)
2017-06-02 10:38:19.441408: step 330080, loss = 0.19 (1513.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:38:20.318445: step 330090, loss = 0.15 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:38:21.297937: step 330100, loss = 0.16 (1306.8 examples/sec; 0.098 sec/batch)
2017-06-02 10:38:22.063568: step 330110, loss = 0.19 (1671.8 examples/sec; 0.077 sec/batch)
2017-06-02 10:38:22.931299: step 330120, loss = 0.16 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:38:23.800325: step 330130, loss = 0.17 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:38:24.664511: step 330140, loss = 0.18 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:38:25.535560: step 330150, loss = 0.16 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:38:26.413802: step 330160, loss = 0.18 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:38:27.294437: step 330170, loss = 0.16 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:38:28.179916: step 330180, loss = 0.14 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 10:38:29.067320: step 330190, loss = 0.23 (1442.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:38:30.060892: step 330200, loss = 0.17 (1288.3 examples/sec; 0.099 sec/batch)
2017-06-02 10:38:30.822002: step 330210, loss = 0.16 (1681.7 examples/sec; 0.076 sec/batch)
2017-06-02 10:38:31.714853: step 330220, loss = 0.17 (1433.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:38:32.606142: step 330230, loss = 0.22 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 10:38:33.502497: step 330240, loss = 0.16 (1428.0 examples/sec; 0.090 sec/batch)
2017-06-02 10:38:34.382061: step 330250, loss = 0.15 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:38:35.231920: step 330260, loss = 0.20 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:38:36.129645: step 330270, loss = 0.18 (1425.8 examples/sec; 0.090 sec/batch)
2017-06-02 10:38:37.001440: step 330280, loss = 0.13 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:38:37.878393: step 330290, loss = 0.16 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:38:38.829854: step 330300, loss = 0.18 (1345.3 examples/sec; 0.095 sec/batch)
2017-06-02 10:38:39.581278: step 330310, loss = 0.15 (1703.4 examples/sec; 0.075 sec/batch)
2017-06-02 10:38:40.444650: step 330320, loss = 0.17 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:38:41.333196: step 330330, loss = 0.16 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:38:42.191790: step 330340, loss = 0.16 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:38:43.074455: step 330350, loss = 0.15 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:38:43.947653: step 330360, loss = 0.21 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:38:44.792933: step 330370, loss = 0.15 (1514.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:38:45.630239: step 330380, loss = 0.17 (1528.8 examples/sec; 0.084 sec/batch)
2017-06-02 10:38:46.491487: step 330390, loss = 0.19 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:38:47.466767: step 330400, loss = 0.16 (1312.4 examples/sec; 0.098 sec/batch)
2017-06-02 10:38:48.238157: step 330410, loss = 0.16 (1659.3 examples/sec; 0.077 sec/batch)
2017-06-02 10:38:49.126454: step 330420, loss = 0.18 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:38:50.003008: step 330430, loss = 0.14 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:38:50.837669: step 330440, loss = 0.17 (1533.6 examples/sec; 0.083 sec/batch)
2017-06-02 10:38:51.687403: step 330450, loss = 0.23 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:38:52.548897: step 330460, loss = 0.21 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:38:53.403928: step 330470, loss = 0.22 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:38:54.257348: step 330480, loss = 0.13 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:38:55.090838: step 330490, loss = 0.18 (1535.7 examples/sec; 0.083 sec/batch)
2017-06-02 10:38:56.063324: step 330500, loss = 0.18 (1316.2 examples/sec; 0.097 sec/batch)
2017-06-02 10:38:56.819263: step 330510, loss = 0.16 (1693.3 examples/sec; 0.076 sec/batch)
2017-06-02 10:38:57.690059: step 330520, loss = 0.17 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:38:58.564548: step 330530, loss = 0.18 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:38:59.420492: step 330540, loss = 0.16 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:39:00.290491: step 330550, loss = 0.18 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:39:01.159621: step 330560, loss = 0.20 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:39:02.008616: step 330570, loss = 0.13 (1507.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:39:02.866443: step 330580, loss = 0.15 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:39:03.733154: step 330590, loss = 0.15 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:39:04.716393: step 330600, loss = 0.19 (1301.8 examples/sec; 0.098 sec/batch)
2017-06-02 10:39:05.458133: step 330610, loss = 0.20 (1725.7 examples/sec; 0.074 sec/batch)
2017-06-02 10:39:06.325847: step 330620, loss = 0.14 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:39:07.185806: step 330630, loss = 0.20 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:39:08.067079: step 330640, loss = 0.16 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:39:08.956885: step 330650, loss = 0.17 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 10:39:09.830917: step 330660, loss = 0.16 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:39:10.715896: step 330670, loss = 0.16 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:39:11.598201: step 330680, loss = 0.16 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:39:12.467921: step 330690, loss = 0.20 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:39:13.453497: step 330700, loss = 0.15 (1298.7 examples/sec; 0.099 sec/batch)
2017-06-02 10:39:14.231202: step 330710, loss = 0.17 (1645.9 examples/sec; 0.078 sec/batch)
2017-06-02 10:39:15.111767: step 330720, loss = 0.21 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:39:16.000132: step 330730, loss = 0.15 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:39:16.878544: step 330740, loss = 0.20 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:39:17.741523: step 330750, loss = 0.18 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:39:18.601698: step 330760, loss = 0.17 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:39:19.474970: step 330770, loss = 0.21 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:39:20.363905: step 330780, loss = 0.14 (1439.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:39:21.219237: step 330790, loss = 0.17 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:39:22.209487: step 330800, loss = 0.25 (1292.6 examples/sec; 0.099 sec/batch)
2017-06-02 10:39:22.984486: step 330810, loss = 0.17 (1651.6 examples/sec; 0.077 sec/batch)
2017-06-02 10:39:23.874935: step 330820, loss = 0.12 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 10:39:24.754115: step 330830, loss = 0.15 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:39:25.658351: step 330840, loss = 0.17 (1415.6 examples/sec; 0.090 sec/batch)
2017-06-02 10:39:26.552641: step 330850, loss = 0.23 (1431.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:39:27.443651: step 330860, loss = 0.14 (1436.5 examples/sec; 0.089 sec/batch)
2017-06-02 10:39:28.315743: step 330870, loss = 0.18 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:39:29.203502: step 330880, loss = 0.20 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 10:39:30.106107: step 330890, loss = 0.17 (1418.1 examples/sec; 0.090 sec/batch)
2017-06-02 10:39:31.159261: step 330900, loss = 0.16 (1215.4 examples/sec; 0.105 sec/batch)
2017-06-02 10:39:31.889926: step 330910, loss = 0.18 (1751.8 examples/sec; 0.073 sec/batch)
2017-06-02 10:39:32.784518: step 330920, loss = 0.20 (1430.8 examples/sec; 0.089 sec/batch)
2017-06-02 10:39:33.659862: step 330930, loss = 0.19 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:39:34.508160: step 330940, loss = 0.17 (1508.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:39:35.360972: step 330950, loss = 0.16 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:39:36.215367: step 330960, loss = 0.21 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:39:37.099238: step 330970, loss = 0.19 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:39:37.975074: step 330980, loss = 0.16 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:39:38.855448: step 330990, loss = 0.14 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:39:39.837854: step 331000, loss = 0.19 (1302.9 examples/sec; 0.098 sec/batch)
2017-06-02 10:39:40.608603: step 331010, loss = 0.20 (1660.7 examples/sec; 0.077 sec/batch)
2017-06-02 10:39:41.487684: step 331020, loss = 0.18 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:39:42.332410: step 331030, loss = 0.17 (1515.3 examples/sec; 0.084 sec/batch)
2017-06-02 10:39:43.231976: step 331040, loss = 0.16 (1422.9 examples/sec; 0.090 sec/batch)
2017-06-02 10:39:44.137022: step 331050, loss = 0.18 (1414.3 examples/sec; 0.091 sec/batch)
2017-06-02 10:39:45.011237: step 331060, loss = 0.15 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:39:45.896507: step 331070, loss = 0.16 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:39:46.763138: step 331080, loss = 0.15 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:39:47.618437: step 331090, loss = 0.18 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:39:48.592809: step 331100, loss = 0.19 (1313.7 examples/sec; 0.097 sec/batch)
2017-06-02 10:39:49.360453: step 331110, loss = 0.19 (1667.4 examples/sec; 0.077 sec/batch)
2017-06-02 10:39:50.233388: step 331120, loss = 0.15 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:39:51.116258: step 331130, loss = 0.18 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:39:51.985225: step 331140, loss = 0.17 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:39:52.924817: step 331150, loss = 0.15 (1362.3 examples/sec; 0.094 sec/batch)
2017-06-02 10:39:53.798340: step 331160, loss = 0.15 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:39:54.676995: step 331170, loss = 0.18 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:39:55.553463: step 331180, loss = 0.17 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:39:56.389906: step 331190, loss = 0.14 (1530.3 examples/sec; 0.084 sec/batch)
2017-06-02 10:39:57.342742: step 331200, loss = 0.18 (1343.4 examples/sec; 0.095 sec/batch)
2017-06-02 10:39:58.111994: step 331210, loss = 0.21 (1663.9 examples/sec; 0.077 sec/batch)
2017-06-02 10:39:59.003238: step 331220, loss = 0.22 (1436.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:39:59.879048: step 331230, loss = 0.16 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:40:00.757240: step 331240, loss = 0.18 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:40:01.641345: step 331250, loss = 0.17 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:40:02.504043: step 331260, loss = 0.20 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:40:03.359103: step 331270, loss = 0.18 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:40:04.253932: step 331280, loss = 0.15 (1430.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:40:05.113219: step 331290, loss = 0.15 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:40:06.051206: step 331300, loss = 0.21 (1364.6 examples/sec; 0.094 sec/batch)
2017-06-02 10:40:06.809975: step 331310, loss = 0.16 (1686.9 examples/sec; 0.076 sec/batch)
2017-06-02 10:40:07.673652: step 331320, loss = 0.22 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:40:08.514212: step 331330, loss = 0.19 (1522.8 examples/sec; 0.084 sec/batch)
2017-06-02 10:40:09.379077: step 331340, loss = 0.16 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:40:10.218304: step 331350, loss = 0.15 (1525.2 examples/sec; 0.084 sec/batch)
2017-06-02 10:40:11.054022: step 331360, loss = 0.19 (1531.6 examples/sec; 0.084 sec/batch)
2017-06-02 10:40:11.913757: step 331370, loss = 0.22 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:40:12.797467: step 331380, loss = 0.19 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:40:13.650572: step 331390, loss = 0.24 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:40:14.611658: step 331400, loss = 0.21 (1331.8 examples/sec; 0.096 sec/batch)
2017-06-02 10:40:15.416195: step 331410, loss = 0.14 (1591.0 examples/sec; 0.080 sec/batch)
2017-06-02 10:40:16.299432: step 331420, loss = 0.20 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:40:17.174680: step 331430, loss = 0.21 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:40:18.060652: step 331440, loss = 0.16 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 10:40:18.957683: step 331450, loss = 0.18 (1426.9 examples/sec; 0.090 sec/batch)
2017-06-02 10:40:19.822183: step 331460, loss = 0.17 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:40:20.689098: step 331470, loss = 0.14 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:40:21.579700: step 331480, loss = 0.20 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:40:22.438292: step 331490, loss = 0.15 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:40:23.422389: step 331500, loss = 0.24 (1300.7 examples/sec; 0.098 sec/batch)
2017-06-02 10:40:24.184537: step 331510, loss = 0.15 (1679.5 examples/sec; 0.076 sec/batch)
2017-06-02 10:40:25.066826: step 331520, loss = 0.16 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:40:25.928665: step 331530, loss = 0.16 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:40:26.787891: step 331540, loss = 0.17 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:40:27.669728: step 331550, loss = 0.17 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:40:28.542053: step 331560, loss = 0.17 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:40:29.400319: step 331570, loss = 0.15 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:40:30.279799: step 331580, loss = 0.14 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:40:31.155281: step 331590, loss = 0.18 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:40:32.134999: step 331600, loss = 0.18 (1306.5 examples/sec; 0.098 sec/batch)
2017-06-02 10:40:32.903251: step 331610, loss = 0.21 (1666.1 examples/sec; 0.077 sec/batch)
2017-06-02 10:40:33.795318: step 331620, loss = 0.14 (1434.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:40:34.668857: step 331630, loss = 0.19 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:40:35.546125: step 331640, loss = 0.13 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:40:36.400670: step 331650, loss = 0.12 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:40:37.291757: step 331660, loss = 0.16 (1436.5 examples/sec; 0.089 sec/batch)
2017-06-02 10:40:38.149478: step 331670, loss = 0.18 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:40:39.037554: step 331680, loss = 0.16 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:40:39.907126: step 331690, loss = 0.17 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:40:40.885764: step 331700, loss = 0.16 (1308.0 examples/sec; 0.098 sec/batch)
2017-06-02 10:40:41.650797: step 331710, loss = 0.17 (1673.1 examples/sec; 0.077 sec/batch)
2017-06-02 10:40:42.503145: step 331720, loss = 0.13 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:40:43.352680: step 331730, loss = 0.14 (1506.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:40:44.235547: step 331740, loss = 0.24 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:40:45.099801: step 331750, loss = 0.13 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:40:45.978909: step 331760, loss = 0.14 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:40:46.849436: step 331770, loss = 0.15 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:40:47.719214: step 331780, loss = 0.13 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:40:48.569121: step 331790, loss = 0.24 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:40:49.531217: step 331800, loss = 0.15 (1330.4 examples/sec; 0.096 sec/batch)
2017-06-02 10:40:50.298034: step 331810, loss = 0.19 (1669.2 examples/sec; 0.077 sec/batch)
2017-06-02 10:40:51.129251: step 331820, loss = 0.16 (1539.9 examples/sec; 0.083 sec/batch)
2017-06-02 10:40:51.977977: step 331830, loss = 0.18 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:40:52.873061: step 331840, loss = 0.16 (1430.0 examples/sec; 0.090 sec/batch)
2017-06-02 10:40:53.731140: step 331850, loss = 0.15 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:40:54.588329: step 331860, loss = 0.13 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:40:55.499104: step 331870, loss = 0.16 (1405.4 examples/sec; 0.091 sec/batch)
2017-06-02 10:40:56.386106: step 331880, loss = 0.20 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 10:40:57.258024: step 331890, loss = 0.17 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:40:58.228991: step 331900, loss = 0.16 (1318.2 examples/sec; 0.097 sec/batch)
2017-06-02 10:40:58.975539: step 331910, loss = 0.14 (1714.6 examples/sec; 0.075 sec/batch)
2017-06-02 10:40:59.847990: step 331920, loss = 0.13 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:41:00.702059: step 331930, loss = 0.17 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:41:01.588269: step 331940, loss = 0.18 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:41:02.443623: step 331950, loss = 0.19 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:41:03.319484: step 331960, loss = 0.14 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:41:04.186851: step 331970, loss = 0.16 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:41:05.050194: step 331980, loss = 0.17 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:41:05.904125: step 331990, loss = 0.17 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:41:06.875420: step 332000, loss = 0.13 (1317.8 examples/sec; 0.097 sec/batch)
2017-06-02 10:41:07.650872: step 332010, loss = 0.17 (1650.6 examples/sec; 0.078 sec/batch)
2017-06-02 10:41:08.500696: step 332020, loss = 0.18 (1506.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:41:09.363413: step 332030, loss = 0.21 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:41:10.199579: step 332040, loss = 0.21 (1530.8 examples/sec; 0.084 sec/batch)
2017-06-02 10:41:11.058102: step 332050, loss = 0.21 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:41:11.907838: step 332060, loss = 0.13 (1506.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:41:12.768444: step 332070, loss = 0.17 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:41:13.654353: step 332080, loss = 0.20 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 10:41:14.517979: step 332090, loss = 0.20 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:41:15.493629: step 332100, loss = 0.18 (1311.9 examples/sec; 0.098 sec/batch)
2017-06-02 10:41:16.252360: step 332110, loss = 0.15 (1687.0 examples/sec; 0.076 sec/batch)
2017-06-02 10:41:17.135151: step 332120, loss = 0.19 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:41:18.006302: step 332130, loss = 0.15 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:41:18.869172: step 332140, loss = 0.17 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:41:19.773795: step 332150, loss = 0.15 (1415.0 examples/sec; 0.090 sec/batch)
2017-06-02 10:41:20.642723: step 332160, loss = 0.16 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:41:21.517693: step 332170, loss = 0.15 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:41:22.365387: step 332180, loss = 0.21 (1510.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:41:23.240517: step 332190, loss = 0.22 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:41:24.232863: step 332200, loss = 0.17 (1289.9 examples/sec; 0.099 sec/batch)
2017-06-02 10:41:25.005291: step 332210, loss = 0.17 (1657.1 examples/sec; 0.077 sec/batch)
2017-06-02 10:41:25.874241: step 332220, loss = 0.15 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:41:26.735519: step 332230, loss = 0.15 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:41:27.597239: step 332240, loss = 0.14 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:41:28.461836: step 332250, loss = 0.16 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:41:29.311479: step 332260, loss = 0.14 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:41:30.178034: step 332270, loss = 0.21 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:41:31.034027: step 332280, loss = 0.16 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:41:31.901413: step 332290, loss = 0.15 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:41:32.865231: step 332300, loss = 0.19 (1328.1 examples/sec; 0.096 sec/batch)
2017-06-02 10:41:33.632153: step 332310, loss = 0.17 (1669.0 examples/sec; 0.077 sec/batch)
2017-06-02 10:41:34.498100: step 332320, loss = 0.13 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:41:35.382460: step 332330, loss = 0.15 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:41:36.258376: step 332340, loss = 0.15 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:41:37.130772: step 332350, loss = 0.19 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:41:37.998358: step 332360, loss = 0.21 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:41:38.883988: step 332370, loss = 0.15 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:41:39.740243: step 332380, loss = 0.16 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:41:40.602687: step 332390, loss = 0.14 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:41:41.575145: step 332400, loss = 0.19 (1316.2 examples/sec; 0.097 sec/batch)
2017-06-02 10:41:42.353296: step 332410, loss = 0.17 (1644.9 examples/sec; 0.078 sec/batch)
2017-06-02 10:41:43.257412: step 332420, loss = 0.13 (1415.8 examples/sec; 0.090 sec/batch)
2017-06-02 10:41:44.133563: step 332430, loss = 0.16 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:41:45.014614: step 332440, loss = 0.15 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:41:45.864052: step 332450, loss = 0.18 (1506.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:41:46.705377: step 332460, loss = 0.16 (1521.4 examples/sec; 0.084 sec/batch)
2017-06-02 10:41:47.579405: step 332470, loss = 0.23 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:41:48.461607: step 332480, loss = 0.15 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:41:49.338557: step 332490, loss = 0.21 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:41:50.316209: step 332500, loss = 0.17 (1309.3 examples/sec; 0.098 sec/batch)
2017-06-02 10:41:51.094247: step 332510, loss = 0.17 (1645.2 examples/sec; 0.078 sec/batch)
2017-06-02 10:41:51.968479: step 332520, loss = 0.19 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:41:52.824218: step 332530, loss = 0.15 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:41:53.680291: step 332540, loss = 0.16 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:41:54.557258: step 332550, loss = 0.16 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:41:55.424443: step 332560, loss = 0.15 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:41:56.297199: step 332570, loss = 0.16 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:41:57.183650: step 332580, loss = 0.17 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:41:58.052548: step 332590, loss = 0.19 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:41:59.029980: step 332600, loss = 0.15 (1309.6 examples/sec; 0.098 sec/batch)
2017-06-02 10:41:59.806424: step 332610, loss = 0.18 (1648.6 examples/sec; 0.078 sec/batch)
2017-06-02 10:42:00.670613: step 332620, loss = 0.16 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:42:01.534670: step 332630, loss = 0.20 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:42:02.400837: step 332640, loss = 0.18 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:42:03.262677: step 332650, loss = 0.16 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:42:04.134216: step 332660, loss = 0.16 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:42:04.978904: step 332670, loss = 0.19 (1515.4 examples/sec; 0.084 sec/batch)
2017-06-02 10:42:05.839618: step 332680, loss = 0.15 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:42:06.709921: step 332690, loss = 0.22 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:42:07.720222: step 332700, loss = 0.15 (1267.0 examples/sec; 0.101 sec/batch)
2017-06-02 10:42:08.449945: step 332710, loss = 0.16 (1754.1 examples/sec; 0.073 sec/batch)
2017-06-02 10:42:09.290922: step 332720, loss = 0.18 (1522.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:42:10.148523: step 332730, loss = 0.15 (1492.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:42:11.014822: step 332740, loss = 0.16 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:42:11.915652: step 332750, loss = 0.17 (1420.9 examples/sec; 0.090 sec/batch)
2017-06-02 10:42:12.798694: step 332760, loss = 0.16 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:42:13.652098: step 332770, loss = 0.15 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:42:14.516558: step 332780, loss = 0.19 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:42:15.378208: step 332790, loss = 0.24 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:42:16.358588: step 332800, loss = 0.17 (1305.6 examples/sec; 0.098 sec/batch)
2017-06-02 10:42:17.080808: step 332810, loss = 0.20 (1772.3 examples/sec; 0.072 sec/batch)
2017-06-02 10:42:17.939040: step 332820, loss = 0.24 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:42:18.805888: step 332830, loss = 0.24 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:42:19.667344: step 332840, loss = 0.16 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:42:20.531400: step 332850, loss = 0.18 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:42:21.396255: step 332860, loss = 0.22 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:42:22.230614: step 332870, loss = 0.13 (1534.1 examples/sec; 0.083 sec/batch)
2017-06-02 10:42:23.122372: step 332880, loss = 0.17 (1435.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:42:24.013003: step 332890, loss = 0.17 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:42:24.980423: step 332900, loss = 0.18 (1323.1 examples/sec; 0.097 sec/batch)
2017-06-02 10:42:25.755103: step 332910, loss = 0.15 (1652.3 examples/sec; 0.077 sec/batch)
2017-06-02 10:42:26.628857: step 332920, loss = 0.18 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:42:27.480660: step 332930, loss = 0.17 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:42:28.347762: step 332940, loss = 0.14 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:42:29.214063: step 332950, loss = 0.20 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:42:30.082652: step 332960, loss = 0.13 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:42:30.936489: step 332970, loss = 0.19 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:42:31.792297: step 332980, loss = 0.15 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:42:32.664225: step 332990, loss = 0.15 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:42:33.679995: step 333000, loss = 0.19 (1260.1 examples/sec; 0.102 sec/batch)
2017-06-02 10:42:34.447238: step 333010, loss = 0.17 (1668.3 examples/sec; 0.077 sec/batch)
2017-06-02 10:42:35.308177: step 333020, loss = 0.15 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:42:36.167962: step 333030, loss = 0.20 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:42:37.006436: step 333040, loss = 0.15 (1526.6 examples/sec; 0.084 sec/batch)
2017-06-02 10:42:37.855729: step 333050, loss = 0.17 (1507.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:42:38.739913: step 333060, loss = 0.21 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:42:39.604517: step 333070, loss = 0.17 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:42:40.501653: step 333080, loss = 0.19 (1426.8 examples/sec; 0.090 sec/batch)
2017-06-02 10:42:41.406272: step 333090, loss = 0.19 (1415.0 examples/sec; 0.090 sec/batch)
2017-06-02 10:42:42.377611: step 333100, loss = 0.17 (1317.8 examples/sec; 0.097 sec/batch)
2017-06-02 10:42:43.174301: step 333110, loss = 0.18 (1606.6 examples/sec; 0.080 sec/batch)
2017-06-02 10:42:44.061915: step 333120, loss = 0.15 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 10:42:44.956452: step 333130, loss = 0.20 (1430.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:42:45.846627: step 333140, loss = 0.18 (1437.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:42:46.728952: step 333150, loss = 0.25 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:42:47.587683: step 333160, loss = 0.16 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:42:48.477164: step 333170, loss = 0.16 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 10:42:49.367724: step 333180, loss = 0.20 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:42:50.225129: step 333190, loss = 0.15 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:42:51.206618: step 333200, loss = 0.16 (1304.1 examples/sec; 0.098 sec/batch)
2017-06-02 10:42:51.981066: step 333210, loss = 0.17 (1652.8 examples/sec; 0.077 sec/batch)
2017-06-02 10:42:52.853350: step 333220, loss = 0.13 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:42:53.697237: step 333230, loss = 0.19 (1516.8 examples/sec; 0.084 sec/batch)
2017-06-02 10:42:54.561749: step 333240, loss = 0.15 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:42:55.404831: step 333250, loss = 0.14 (1518.2 examples/sec; 0.084 sec/batch)
2017-06-02 10:42:56.256587: step 333260, loss = 0.17 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:42:57.107156: step 333270, loss = 0.18 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:42:57.989360: step 333280, loss = 0.23 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:42:58.861497: step 333290, loss = 0.11 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:42:59.833955: step 333300, loss = 0.18 (1316.2 examples/sec; 0.097 sec/batch)
2017-06-02 10:43:00.611163: step 333310, loss = 0.22 (1646.9 examples/sec; 0.078 sec/batch)
2017-06-02 10:43:01.472702: step 333320, loss = 0.19 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:43:02.346840: step 333330, loss = 0.16 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:43:03.219470: step 333340, loss = 0.18 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:43:04.084037: step 333350, loss = 0.17 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:43:04.953307: step 333360, loss = 0.18 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:43:05.822428: step 333370, loss = 0.16 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:43:06.685702: step 333380, loss = 0.17 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:43:07.546352: step 333390, loss = 0.14 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:43:08.484793: step 333400, loss = 0.18 (1364.0 examples/sec; 0.094 sec/batch)
2017-06-02 10:43:09.246828: step 333410, loss = 0.21 (1679.7 examples/sec; 0.076 sec/batch)
2017-06-02 10:43:10.101925: step 333420, loss = 0.18 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:43:10.961552: step 333430, loss = 0.18 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:43:11.832544: step 333440, loss = 0.19 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:43:12.699918: step 333450, loss = 0.15 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:43:13.550659: step 333460, loss = 0.14 (1504.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:43:14.434507: step 333470, loss = 0.18 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:43:15.307353: step 333480, loss = 0.14 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:43:16.167969: step 333490, loss = 0.17 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:43:17.157912: step 333500, loss = 0.14 (1293.0 examples/sec; 0.099 sec/batch)
2017-06-02 10:43:17.910007: step 333510, loss = 0.18 (1701.9 examples/sec; 0.075 sec/batch)
2017-06-02 10:43:18.766466: step 333520, loss = 0.19 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:43:19.644634: step 333530, loss = 0.17 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:43:20.507836: step 333540, loss = 0.18 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:43:21.382194: step 333550, loss = 0.16 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:43:22.243659: step 333560, loss = 0.17 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:43:23.100209: step 333570, loss = 0.20 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:43:23.957467: step 333580, loss = 0.15 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:43:24.809010: step 333590, loss = 0.16 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:43:25.783411: step 333600, loss = 0.19 (1313.6 examples/sec; 0.097 sec/batch)
2017-06-02 10:43:26.547066: step 333610, loss = 0.19 (1676.1 examples/sec; 0.076 sec/batch)
2017-06-02 10:43:27.442838: step 333620, loss = 0.19 (1428.9 examples/sec; 0.090 sec/batch)
2017-06-02 10:43:28.308196: step 333630, loss = 0.18 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:43:29.190551: step 333640, loss = 0.20 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:43:30.047160: step 333650, loss = 0.20 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:43:30.883137: step 333660, loss = 0.15 (1531.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:43:31.772812: step 333670, loss = 0.19 (1438.8 examples/sec; 0.089 sec/batch)
2017-06-02 10:43:32.635099: step 333680, loss = 0.21 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:43:33.527348: step 333690, loss = 0.17 (1434.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:43:34.492316: step 333700, loss = 0.22 (1326.5 examples/sec; 0.096 sec/batch)
2017-06-02 10:43:35.265559: step 333710, loss = 0.18 (1655.4 examples/sec; 0.077 sec/batch)
2017-06-02 10:43:36.091345: step 333720, loss = 0.19 (1550.0 examples/sec; 0.083 sec/batch)
2017-06-02 10:43:36.965257: step 333730, loss = 0.17 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:43:37.830633: step 333740, loss = 0.18 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:43:38.706998: step 333750, loss = 0.22 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:43:39.546713: step 333760, loss = 0.19 (1524.3 examples/sec; 0.084 sec/batch)
2017-06-02 10:43:40.421120: step 333770, loss = 0.17 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:43:41.295546: step 333780, loss = 0.16 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:43:42.138801: step 333790, loss = 0.19 (1517.9 examples/sec; 0.084 sec/batch)
2017-06-02 10:43:43.133926: step 333800, loss = 0.19 (1286.3 examples/sec; 0.100 sec/batch)
2017-06-02 10:43:43.867803: step 333810, loss = 0.14 (1744.2 examples/sec; 0.073 sec/batch)
2017-06-02 10:43:44.742779: step 333820, loss = 0.18 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:43:45.600857: step 333830, loss = 0.16 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:43:46.467291: step 333840, loss = 0.15 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:43:47.348959: step 333850, loss = 0.19 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:43:48.238066: step 333860, loss = 0.14 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 10:43:49.093286: step 333870, loss = 0.14 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:43:49.968992: step 333880, loss = 0.16 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:43:50.836135: step 333890, loss = 0.17 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:43:51.812210: step 333900, loss = 0.20 (1311.4 examples/sec; 0.098 sec/batch)
2017-06-02 10:43:52.590235: step 333910, loss = 0.14 (1645.2 examples/sec; 0.078 sec/batch)
2017-06-02 10:43:53.451116: step 333920, loss = 0.15 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:43:54.320395: step 333930, loss = 0.19 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:43:55.200118: step 333940, loss = 0.19 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:43:56.064263: step 333950, loss = 0.15 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:43:56.934082: step 333960, loss = 0.19 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:43:57.778027: step 333970, loss = 0.20 (1516.7 examples/sec; 0.084 sec/batch)
2017-06-02 10:43:58.619871: step 333980, loss = 0.13 (1520.5 examples/sec; 0.084 sec/batch)
2017-06-02 10:43:59.502476: step 333990, loss = 0.15 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:44:00.459261: step 334000, loss = 0.17 (1337.8 examples/sec; 0.096 sec/batch)
2017-06-02 10:44:01.227486: step 334010, loss = 0.16 (1666.2 examples/sec; 0.077 sec/batch)
2017-06-02 10:44:02.097888: step 334020, loss = 0.16 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:44:02.964536: step 334030, loss = 0.16 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:44:03.832747: step 334040, loss = 0.15 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:44:04.721169: step 334050, loss = 0.16 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 10:44:05.581938: step 334060, loss = 0.15 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:44:06.445111: step 334070, loss = 0.16 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:44:07.321125: step 334080, loss = 0.25 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:44:08.195573: step 334090, loss = 0.18 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:44:09.171580: step 334100, loss = 0.17 (1311.5 examples/sec; 0.098 sec/batch)
2017-06-02 10:44:09.956130: step 334110, loss = 0.19 (1631.5 examples/sec; 0.078 sec/batch)
2017-06-02 10:44:10.823232: step 334120, loss = 0.15 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:44:11.677013: step 334130, loss = 0.15 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:44:12.555352: step 334140, loss = 0.20 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:44:13.399246: step 334150, loss = 0.16 (1516.8 examples/sec; 0.084 sec/batch)
2017-06-02 10:44:14.265662: step 334160, loss = 0.20 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:44:15.147402: step 334170, loss = 0.14 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:44:16.008458: step 334180, loss = 0.15 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:44:16.867383: step 334190, loss = 0.17 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:44:17.847412: step 334200, loss = 0.17 (1306.1 examples/sec; 0.098 sec/batch)
2017-06-02 10:44:18.615085: step 334210, loss = 0.20 (1667.4 examples/sec; 0.077 sec/batch)
2017-06-02 10:44:19.471299: step 334220, loss = 0.29 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:44:20.361574: step 334230, loss = 0.19 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 10:44:21.231757: step 334240, loss = 0.15 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:44:22.087074: step 334250, loss = 0.18 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:44:22.942413: step 334260, loss = 0.15 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:44:23.800999: step 334270, loss = 0.16 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:44:24.687988: step 334280, loss = 0.19 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 10:44:25.550309: step 334290, loss = 0.17 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:44:26.534093: step 334300, loss = 0.12 (1301.1 examples/sec; 0.098 sec/batch)
2017-06-02 10:44:27.276244: step 334310, loss = 0.13 (1724.7 examples/sec; 0.074 sec/batch)
2017-06-02 10:44:28.138893: step 334320, loss = 0.13 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:44:28.993136: step 334330, loss = 0.17 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:44:29.869094: step 334340, loss = 0.14 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:44:30.737083: step 334350, loss = 0.17 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:44:31.604384: step 334360, loss = 0.13 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:44:32.451931: step 334370, loss = 0.18 (1510.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:44:33.310720: step 334380, loss = 0.17 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:44:34.192116: step 334390, loss = 0.15 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:44:35.156461: step 334400, loss = 0.15 (1327.3 examples/sec; 0.096 sec/batch)
2017-06-02 10:44:35.911514: step 334410, loss = 0.18 (1695.2 examples/sec; 0.076 sec/batch)
2017-06-02 10:44:36.784027: step 334420, loss = 0.19 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:44:37.640749: step 334430, loss = 0.21 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:44:38.474501: step 334440, loss = 0.22 (1535.2 examples/sec; 0.083 sec/batch)
2017-06-02 10:44:39.331696: step 334450, loss = 0.18 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:44:40.182389: step 334460, loss = 0.16 (1504.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:44:41.046466: step 334470, loss = 0.15 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:44:41.913459: step 334480, loss = 0.17 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:44:42.766256: step 334490, loss = 0.17 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:44:43.723167: step 334500, loss = 0.17 (1337.6 examples/sec; 0.096 sec/batch)
2017-06-02 10:44:44.461407: step 334510, loss = 0.16 (1733.9 examples/sec; 0.074 sec/batch)
2017-06-02 10:44:45.301293: step 334520, loss = 0.17 (1524.0 examples/sec; 0.084 sec/batch)
2017-06-02 10:44:46.163475: step 334530, loss = 0.18 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:44:47.042452: step 334540, loss = 0.22 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:44:47.904040: step 334550, loss = 0.24 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:44:48.753326: step 334560, loss = 0.13 (1507.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:44:49.607607: step 334570, loss = 0.15 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:44:50.465525: step 334580, loss = 0.13 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:44:51.343974: step 334590, loss = 0.16 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:44:52.292225: step 334600, loss = 0.16 (1349.9 examples/sec; 0.095 sec/batch)
2017-06-02 10:44:53.070282: step 334610, loss = 0.17 (1645.1 examples/sec; 0.078 sec/batch)
2017-06-02 10:44:53.946354: step 334620, loss = 0.13 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:44:54.814217: step 334630, loss = 0.18 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:44:55.649198: step 334640, loss = 0.16 (1533.0 examples/sec; 0.083 sec/batch)
2017-06-02 10:44:56.497567: step 334650, loss = 0.15 (1508.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:44:57.362440: step 334660, loss = 0.19 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:44:58.214410: step 334670, loss = 0.17 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:44:59.086001: step 334680, loss = 0.17 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:44:59.982672: step 334690, loss = 0.16 (1427.5 examples/sec; 0.090 sec/batch)
2017-06-02 10:45:00.960774: step 334700, loss = 0.18 (1308.6 examples/sec; 0.098 sec/batch)
2017-06-02 10:45:01.739258: step 334710, loss = 0.15 (1644.2 examples/sec; 0.078 sec/batch)
2017-06-02 10:45:02.606496: step 334720, loss = 0.15 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:45:03.469360: step 334730, loss = 0.17 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:45:04.324873: step 334740, loss = 0.14 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:45:05.205521: step 334750, loss = 0.18 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:45:06.078423: step 334760, loss = 0.13 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:45:06.970824: step 334770, loss = 0.14 (1434.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:45:07.849262: step 334780, loss = 0.14 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:45:08.696588: step 334790, loss = 0.19 (1510.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:45:09.680566: step 334800, loss = 0.23 (1300.8 examples/sec; 0.098 sec/batch)
2017-06-02 10:45:10.455496: step 334810, loss = 0.18 (1651.8 examples/sec; 0.077 sec/batch)
2017-06-02 10:45:11.330716: step 334820, loss = 0.15 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:45:12.210429: step 334830, loss = 0.15 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:45:13.071914: step 334840, loss = 0.18 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:45:13.944677: step 334850, loss = 0.14 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:45:14.815153: step 334860, loss = 0.17 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:45:15.664205: step 334870, loss = 0.18 (1507.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:45:16.532559: step 334880, loss = 0.20 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:45:17.405330: step 334890, loss = 0.16 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:45:18.374200: step 334900, loss = 0.20 (1321.1 examples/sec; 0.097 sec/batch)
2017-06-02 10:45:19.105626: step 334910, loss = 0.17 (1750.0 examples/sec; 0.073 sec/batch)
2017-06-02 10:45:19.962182: step 334920, loss = 0.14 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:45:20.835223: step 334930, loss = 0.15 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:45:21.700772: step 334940, loss = 0.20 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:45:22.581952: step 334950, loss = 0.20 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:45:23.459025: step 334960, loss = 0.14 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:45:24.319550: step 334970, loss = 0.14 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:45:25.161944: step 334980, loss = 0.18 (1519.5 examples/sec; 0.084 sec/batch)
2017-06-02 10:45:26.035801: step 334990, loss = 0.17 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:45:27.001537: step 335000, loss = 0.17 (1325.4 examples/sec; 0.097 sec/batch)
2017-06-02 10:45:27.760239: step 335010, loss = 0.19 (1687.1 examples/sec; 0.076 sec/batch)
2017-06-02 10:45:28.623531: step 335020, loss = 0.16 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:45:29.495657: step 335030, loss = 0.19 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:45:30.357468: step 335040, loss = 0.15 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:45:31.241370: step 335050, loss = 0.20 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:45:32.114863: step 335060, loss = 0.17 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:45:32.953825: step 335070, loss = 0.15 (1525.7 examples/sec; 0.084 sec/batch)
2017-06-02 10:45:33.806502: step 335080, loss = 0.16 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:45:34.667467: step 335090, loss = 0.14 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:45:35.654918: step 335100, loss = 0.18 (1296.3 examples/sec; 0.099 sec/batch)
2017-06-02 10:45:36.404232: step 335110, loss = 0.19 (1708.3 examples/sec; 0.075 sec/batch)
2017-06-02 10:45:37.264680: step 335120, loss = 0.16 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:45:38.102768: step 335130, loss = 0.22 (1527.3 examples/sec; 0.084 sec/batch)
2017-06-02 10:45:38.926233: step 335140, loss = 0.18 (1554.4 examples/sec; 0.082 sec/batch)
2017-06-02 10:45:39.797868: step 335150, loss = 0.19 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:45:40.654641: step 335160, loss = 0.13 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:45:41.542842: step 335170, loss = 0.20 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 10:45:42.397577: step 335180, loss = 0.13 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:45:43.284886: step 335190, loss = 0.20 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:45:44.254392: step 335200, loss = 0.30 (1320.3 examples/sec; 0.097 sec/batch)
2017-06-02 10:45:45.027328: step 335210, loss = 0.15 (1656.0 examples/sec; 0.077 sec/batch)
2017-06-02 10:45:45.890677: step 335220, loss = 0.18 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:45:46.732613: step 335230, loss = 0.16 (1520.3 examples/sec; 0.084 sec/batch)
2017-06-02 10:45:47.589683: step 335240, loss = 0.15 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:45:48.461943: step 335250, loss = 0.13 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:45:49.336874: step 335260, loss = 0.21 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:45:50.188105: step 335270, loss = 0.16 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:45:51.042039: step 335280, loss = 0.18 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:45:51.936598: step 335290, loss = 0.21 (1430.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:45:52.919651: step 335300, loss = 0.13 (1302.1 examples/sec; 0.098 sec/batch)
2017-06-02 10:45:53.685742: step 335310, loss = 0.14 (1670.8 examples/sec; 0.077 sec/batch)
2017-06-02 10:45:54.523822: step 335320, loss = 0.12 (1527.3 examples/sec; 0.084 sec/batch)
2017-06-02 10:45:55.380576: step 335330, loss = 0.24 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:45:56.238452: step 335340, loss = 0.17 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:45:57.101209: step 335350, loss = 0.19 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:45:57.977836: step 335360, loss = 0.15 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:45:58.860438: step 335370, loss = 0.17 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:45:59.699325: step 335380, loss = 0.18 (1525.8 examples/sec; 0.084 sec/batch)
2017-06-02 10:46:00.561152: step 335390, loss = 0.19 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:46:01.540448: step 335400, loss = 0.15 (1307.1 examples/sec; 0.098 sec/batch)
2017-06-02 10:46:02.322566: step 335410, loss = 0.18 (1636.6 examples/sec; 0.078 sec/batch)
2017-06-02 10:46:03.193599: step 335420, loss = 0.16 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:46:04.068187: step 335430, loss = 0.18 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:46:04.916245: step 335440, loss = 0.18 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:46:05.788765: step 335450, loss = 0.17 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:46:06.671401: step 335460, loss = 0.21 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:46:07.518618: step 335470, loss = 0.16 (1510.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:46:08.395410: step 335480, loss = 0.16 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:46:09.258475: step 335490, loss = 0.16 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:46:10.223361: step 335500, loss = 0.17 (1326.6 examples/sec; 0.096 sec/batch)
2017-06-02 10:46:11.011246: step 335510, loss = 0.17 (1624.6 examples/sec; 0.079 sec/batch)
2017-06-02 10:46:11.872746: step 335520, loss = 0.15 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:46:12.737465: step 335530, loss = 0.18 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:46:13.590134: step 335540, loss = 0.17 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:46:14.445468: step 335550, loss = 0.23 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:46:15.334489: step 335560, loss = 0.18 (1439.8 examples/sec; 0.089 sec/batch)
2017-06-02 10:46:16.204445: step 335570, loss = 0.15 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:46:17.050205: step 335580, loss = 0.17 (1513.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:46:17.929257: step 335590, loss = 0.15 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:46:18.897678: step 335600, loss = 0.19 (1321.7 examples/sec; 0.097 sec/batch)
2017-06-02 10:46:19.663302: step 335610, loss = 0.14 (1671.8 examples/sec; 0.077 sec/batch)
2017-06-02 10:46:20.533647: step 335620, loss = 0.14 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:46:21.365474: step 335630, loss = 0.16 (1538.8 examples/sec; 0.083 sec/batch)
2017-06-02 10:46:22.211931: step 335640, loss = 0.15 (1512.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:46:23.082312: step 335650, loss = 0.14 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:46:23.957348: step 335660, loss = 0.16 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:46:24.797395: step 335670, loss = 0.16 (1523.7 examples/sec; 0.084 sec/batch)
2017-06-02 10:46:25.652980: step 335680, loss = 0.14 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:46:26.490952: step 335690, loss = 0.17 (1527.5 examples/sec; 0.084 sec/batch)
2017-06-02 10:46:27.477194: step 335700, loss = 0.17 (1297.8 examples/sec; 0.099 sec/batch)
2017-06-02 10:46:28.256301: step 335710, loss = 0.14 (1642.9 examples/sec; 0.078 sec/batch)
2017-06-02 10:46:29.103533: step 335720, loss = 0.18 (1510.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:46:29.944325: step 335730, loss = 0.14 (1522.4 examples/sec; 0.084 sec/batch)
2017-06-02 10:46:30.811922: step 335740, loss = 0.17 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:46:31.703327: step 335750, loss = 0.14 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:46:32.547919: step 335760, loss = 0.13 (1515.5 examples/sec; 0.084 sec/batch)
2017-06-02 10:46:33.403970: step 335770, loss = 0.16 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:46:34.267716: step 335780, loss = 0.21 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:46:35.116379: step 335790, loss = 0.15 (1508.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:46:36.123978: step 335800, loss = 0.16 (1270.3 examples/sec; 0.101 sec/batch)
2017-06-02 10:46:36.847075: step 335810, loss = 0.17 (1770.2 examples/sec; 0.072 sec/batch)
2017-06-02 10:46:37.724138: step 335820, loss = 0.17 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:46:38.599257: step 335830, loss = 0.17 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:46:39.464067: step 335840, loss = 0.21 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:46:40.318141: step 335850, loss = 0.15 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:46:41.182579: step 335860, loss = 0.14 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:46:42.039638: step 335870, loss = 0.16 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:46:42.906541: step 335880, loss = 0.17 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:46:43.793649: step 335890, loss = 0.19 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:46:44.768432: step 335900, loss = 0.14 (1313.1 examples/sec; 0.097 sec/batch)
2017-06-02 10:46:45.525952: step 335910, loss = 0.19 (1689.7 examples/sec; 0.076 sec/batch)
2017-06-02 10:46:46.372023: step 335920, loss = 0.14 (1512.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:46:47.216955: step 335930, loss = 0.20 (1514.9 examples/sec; 0.084 sec/batch)
2017-06-02 10:46:48.085782: step 335940, loss = 0.19 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:46:48.929577: step 335950, loss = 0.17 (1517.0 examples/sec; 0.084 sec/batch)
2017-06-02 10:46:49.768095: step 335960, loss = 0.15 (1526.5 examples/sec; 0.084 sec/batch)
2017-06-02 10:46:50.661348: step 335970, loss = 0.18 (1433.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:46:51.516423: step 335980, loss = 0.18 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:46:52.350888: step 335990, loss = 0.15 (1533.9 examples/sec; 0.083 sec/batch)
2017-06-02 10:46:53.373832: step 336000, loss = 0.21 (1251.3 examples/sec; 0.102 sec/batch)
2017-06-02 10:46:54.081384: step 336010, loss = 0.18 (1809.1 examples/sec; 0.071 sec/batch)
2017-06-02 10:46:54.939979: step 336020, loss = 0.17 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:46:55.801092: step 336030, loss = 0.16 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:46:56.666386: step 336040, loss = 0.19 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:46:57.515203: step 336050, loss = 0.19 (1508.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:46:58.392443: step 336060, loss = 0.25 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:46:59.247358: step 336070, loss = 0.20 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:47:00.089626: step 336080, loss = 0.15 (1519.7 examples/sec; 0.084 sec/batch)
2017-06-02 10:47:00.946083: step 336090, loss = 0.21 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:47:01.906994: step 336100, loss = 0.15 (1332.1 examples/sec; 0.096 sec/batch)
2017-06-02 10:47:02.628699: step 336110, loss = 0.16 (1773.6 examples/sec; 0.072 sec/batch)
2017-06-02 10:47:03.513509: step 336120, loss = 0.16 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:47:04.403780: step 336130, loss = 0.13 (1437.7 examples/sec; 0.089 sec/batch)
2017-06-02 10:47:05.239280: step 336140, loss = 0.16 (1532.0 examples/sec; 0.084 sec/batch)
2017-06-02 10:47:06.111568: step 336150, loss = 0.18 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:47:06.983317: step 336160, loss = 0.14 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:47:07.840682: step 336170, loss = 0.16 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:47:08.710052: step 336180, loss = 0.16 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:47:09.588555: step 336190, loss = 0.17 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:47:10.555204: step 336200, loss = 0.17 (1324.2 examples/sec; 0.097 sec/batch)
2017-06-02 10:47:11.342781: step 336210, loss = 0.18 (1625.2 examples/sec; 0.079 sec/batch)
2017-06-02 10:47:12.224820: step 336220, loss = 0.17 (1451.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:47:13.114100: step 336230, loss = 0.19 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:47:13.967681: step 336240, loss = 0.14 (1499.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:47:14.812858: step 336250, loss = 0.16 (1514.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:47:15.698542: step 336260, loss = 0.20 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:47:16.563529: step 336270, loss = 0.14 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:47:17.443772: step 336280, loss = 0.14 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:47:18.291376: step 336290, loss = 0.19 (1510.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:47:19.297456: step 336300, loss = 0.20 (1272.3 examples/sec; 0.101 sec/batch)
2017-06-02 10:47:20.028789: step 336310, loss = 0.16 (1750.2 examples/sec; 0.073 sec/batch)
2017-06-02 10:47:20.912942: step 336320, loss = 0.14 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:47:21.748160: step 336330, loss = 0.14 (1532.5 examples/sec; 0.084 sec/batch)
2017-06-02 10:47:22.600728: step 336340, loss = 0.16 (1501.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:47:23.467775: step 336350, loss = 0.16 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:47:24.339414: step 336360, loss = 0.16 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:47:25.187476: step 336370, loss = 0.16 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:47:26.055019: step 336380, loss = 0.15 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:47:26.907108: step 336390, loss = 0.15 (1502.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:47:27.887628: step 336400, loss = 0.21 (1305.4 examples/sec; 0.098 sec/batch)
2017-06-02 10:47:28.656323: step 336410, loss = 0.13 (1665.2 examples/sec; 0.077 sec/batch)
2017-06-02 10:47:29.515521: step 336420, loss = 0.18 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:47:30.366844: step 336430, loss = 0.15 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:47:31.226358: step 336440, loss = 0.18 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:47:32.111479: step 336450, loss = 0.16 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 10:47:32.980161: step 336460, loss = 0.16 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:47:33.804962: step 336470, loss = 0.16 (1551.9 examples/sec; 0.082 sec/batch)
2017-06-02 10:47:34.663344: step 336480, loss = 0.17 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:47:35.525841: step 336490, loss = 0.15 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:47:36.512966: step 336500, loss = 0.14 (1296.7 examples/sec; 0.099 sec/batch)
2017-06-02 10:47:37.288701: step 336510, loss = 0.15 (1650.1 examples/sec; 0.078 sec/batch)
2017-06-02 10:47:38.176837: step 336520, loss = 0.17 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:47:39.046500: step 336530, loss = 0.17 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:47:39.916096: step 336540, loss = 0.24 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:47:40.795451: step 336550, loss = 0.18 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:47:41.682443: step 336560, loss = 0.15 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 10:47:42.541225: step 336570, loss = 0.25 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:47:43.409667: step 336580, loss = 0.19 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:47:44.265490: step 336590, loss = 0.18 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:47:45.200822: step 336600, loss = 0.14 (1368.5 examples/sec; 0.094 sec/batch)
2017-06-02 10:47:45.959829: step 336610, loss = 0.15 (1686.4 examples/sec; 0.076 sec/batch)
2017-06-02 10:47:46.809235: step 336620, loss = 0.19 (1506.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:47:47.679650: step 336630, loss = 0.17 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:47:48.562323: step 336640, loss = 0.17 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:47:49.427017: step 336650, loss = 0.15 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:47:50.276367: step 336660, loss = 0.14 (1507.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:47:51.149458: step 336670, loss = 0.14 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:47:52.004079: step 336680, loss = 0.16 (1497.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:47:52.876234: step 336690, loss = 0.16 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:47:53.840825: step 336700, loss = 0.15 (1327.0 examples/sec; 0.096 sec/batch)
2017-06-02 10:47:54.597392: step 336710, loss = 0.13 (1691.8 examples/sec; 0.076 sec/batch)
2017-06-02 10:47:55.489160: step 336720, loss = 0.18 (1435.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:47:56.338411: step 336730, loss = 0.16 (1507.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:47:57.170298: step 336740, loss = 0.17 (1538.6 examples/sec; 0.083 sec/batch)
2017-06-02 10:47:58.059852: step 336750, loss = 0.14 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:47:58.912095: step 336760, loss = 0.18 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:47:59.762232: step 336770, loss = 0.18 (1505.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:48:00.643418: step 336780, loss = 0.16 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:48:01.506997: step 336790, loss = 0.18 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:48:02.479939: step 336800, loss = 0.15 (1315.6 examples/sec; 0.097 sec/batch)
2017-06-02 10:48:03.266552: step 336810, loss = 0.18 (1627.2 examples/sec; 0.079 sec/batch)
2017-06-02 10:48:04.155676: step 336820, loss = 0.16 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:48:05.024015: step 336830, loss = 0.17 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:48:05.881065: step 336840, loss = 0.20 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:48:06.739478: step 336850, loss = 0.13 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:48:07.605917: step 336860, loss = 0.19 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:48:08.471562: step 336870, loss = 0.16 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:48:09.318702: step 336880, loss = 0.13 (1511.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:48:10.161795: step 336890, loss = 0.15 (1518.2 examples/sec; 0.084 sec/batch)
2017-06-02 10:48:11.127766: step 336900, loss = 0.20 (1325.1 examples/sec; 0.097 sec/batch)
2017-06-02 10:48:11.887478: step 336910, loss = 0.20 (1684.9 examples/sec; 0.076 sec/batch)
2017-06-02 10:48:12.751083: step 336920, loss = 0.13 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:48:13.618207: step 336930, loss = 0.15 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:48:14.466152: step 336940, loss = 0.16 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:48:15.326418: step 336950, loss = 0.15 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:48:16.199136: step 336960, loss = 0.15 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:48:17.079312: step 336970, loss = 0.18 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:48:17.953538: step 336980, loss = 0.17 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:48:18.809138: step 336990, loss = 0.16 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:48:19.758565: step 337000, loss = 0.19 (1348.2 examples/sec; 0.095 sec/batch)
2017-06-02 10:48:20.525572: step 337010, loss = 0.18 (1668.8 examples/sec; 0.077 sec/batch)
2017-06-02 10:48:21.407718: step 337020, loss = 0.16 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:48:22.292074: step 337030, loss = 0.17 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:48:23.165588: step 337040, loss = 0.15 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:48:24.029304: step 337050, loss = 0.14 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:48:24.896505: step 337060, loss = 0.17 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:48:25.770967: step 337070, loss = 0.18 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:48:26.635777: step 337080, loss = 0.16 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:48:27.511240: step 337090, loss = 0.16 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:48:28.490798: step 337100, loss = 0.21 (1306.7 examples/sec; 0.098 sec/batch)
2017-06-02 10:48:29.243858: step 337110, loss = 0.14 (1699.7 examples/sec; 0.075 sec/batch)
2017-06-02 10:48:30.092794: step 337120, loss = 0.14 (1507.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:48:30.944686: step 337130, loss = 0.15 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:48:31.817508: step 337140, loss = 0.14 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:48:32.698823: step 337150, loss = 0.16 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:48:33.552805: step 337160, loss = 0.16 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:48:34.431466: step 337170, loss = 0.18 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:48:35.290409: step 337180, loss = 0.17 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:48:36.124001: step 337190, loss = 0.17 (1535.5 examples/sec; 0.083 sec/batch)
2017-06-02 10:48:37.066616: step 337200, loss = 0.16 (1357.9 examples/sec; 0.094 sec/batch)
2017-06-02 10:48:37.849640: step 337210, loss = 0.16 (1634.7 examples/sec; 0.078 sec/batch)
2017-06-02 10:48:38.691380: step 337220, loss = 0.17 (1520.7 examples/sec; 0.084 sec/batch)
2017-06-02 10:48:39.567592: step 337230, loss = 0.17 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:48:40.424979: step 337240, loss = 0.17 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:48:41.303369: step 337250, loss = 0.21 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:48:42.140995: step 337260, loss = 0.20 (1528.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:48:43.032813: step 337270, loss = 0.19 (1435.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:48:43.889686: step 337280, loss = 0.18 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:48:44.766858: step 337290, loss = 0.24 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:48:45.737811: step 337300, loss = 0.18 (1318.3 examples/sec; 0.097 sec/batch)
2017-06-02 10:48:46.514891: step 337310, loss = 0.19 (1647.2 examples/sec; 0.078 sec/batch)
2017-06-02 10:48:47.371249: step 337320, loss = 0.15 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:48:48.246087: step 337330, loss = 0.15 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:48:49.107138: step 337340, loss = 0.16 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:48:49.971213: step 337350, loss = 0.14 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:48:50.827428: step 337360, loss = 0.14 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:48:51.707082: step 337370, loss = 0.19 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:48:52.571464: step 337380, loss = 0.16 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:48:53.408687: step 337390, loss = 0.15 (1528.9 examples/sec; 0.084 sec/batch)
2017-06-02 10:48:54.373737: step 337400, loss = 0.16 (1326.4 examples/sec; 0.097 sec/batch)
2017-06-02 10:48:55.132372: step 337410, loss = 0.13 (1687.2 examples/sec; 0.076 sec/batch)
2017-06-02 10:48:55.982242: step 337420, loss = 0.16 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:48:56.820561: step 337430, loss = 0.14 (1526.9 examples/sec; 0.084 sec/batch)
2017-06-02 10:48:57.680298: step 337440, loss = 0.13 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:48:58.548073: step 337450, loss = 0.19 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:48:59.412940: step 337460, loss = 0.14 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:49:00.297885: step 337470, loss = 0.16 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:49:01.146880: step 337480, loss = 0.15 (1507.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:49:02.035120: step 337490, loss = 0.15 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:49:02.995999: step 337500, loss = 0.17 (1332.1 examples/sec; 0.096 sec/batch)
2017-06-02 10:49:03.742784: step 337510, loss = 0.14 (1714.0 examples/sec; 0.075 sec/batch)
2017-06-02 10:49:04.616847: step 337520, loss = 0.19 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:49:05.473211: step 337530, loss = 0.14 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:49:06.351051: step 337540, loss = 0.19 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:49:07.207335: step 337550, loss = 0.20 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:49:08.067897: step 337560, loss = 0.16 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:49:08.968177: step 337570, loss = 0.20 (1421.8 examples/sec; 0.090 sec/batch)
2017-06-02 10:49:09.838583: step 337580, loss = 0.17 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:49:10.704386: step 337590, loss = 0.23 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:49:11.691171: step 337600, loss = 0.22 (1297.1 examples/sec; 0.099 sec/batch)
2017-06-02 10:49:12.467190: step 337610, loss = 0.15 (1649.4 examples/sec; 0.078 sec/batch)
2017-06-02 10:49:13.325210: step 337620, loss = 0.15 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:49:14.171572: step 337630, loss = 0.13 (1512.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:49:15.018760: step 337640, loss = 0.17 (1510.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:49:15.856682: step 337650, loss = 0.19 (1527.6 examples/sec; 0.084 sec/batch)
2017-06-02 10:49:16.714573: step 337660, loss = 0.14 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:49:17.555525: step 337670, loss = 0.16 (1522.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:49:18.399428: step 337680, loss = 0.16 (1516.8 examples/sec; 0.084 sec/batch)
2017-06-02 10:49:19.275373: step 337690, loss = 0.22 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:49:20.251991: step 337700, loss = 0.17 (1310.6 examples/sec; 0.098 sec/batch)
2017-06-02 10:49:20.996343: step 337710, loss = 0.19 (1719.6 examples/sec; 0.074 sec/batch)
2017-06-02 10:49:21.846848: step 337720, loss = 0.17 (1505.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:49:22.733095: step 337730, loss = 0.17 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:49:23.600894: step 337740, loss = 0.19 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:49:24.463863: step 337750, loss = 0.16 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:49:25.321011: step 337760, loss = 0.19 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:49:26.181892: step 337770, loss = 0.15 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:49:27.044185: step 337780, loss = 0.16 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:49:27.928178: step 337790, loss = 0.15 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:49:28.899443: step 337800, loss = 0.17 (1317.9 examples/sec; 0.097 sec/batch)
2017-06-02 10:49:29.658495: step 337810, loss = 0.15 (1686.3 examples/sec; 0.076 sec/batch)
2017-06-02 10:49:30.525904: step 337820, loss = 0.18 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:49:31.389319: step 337830, loss = 0.14 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:49:32.242971: step 337840, loss = 0.22 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:49:33.107691: step 337850, loss = 0.14 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:49:33.961901: step 337860, loss = 0.14 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:49:34.854567: step 337870, loss = 0.17 (1433.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:49:35.705835: step 337880, loss = 0.16 (1503.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:49:36.554654: step 337890, loss = 0.21 (1508.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:49:37.535561: step 337900, loss = 0.16 (1304.9 examples/sec; 0.098 sec/batch)
2017-06-02 10:49:38.285442: step 337910, loss = 0.14 (1706.9 examples/sec; 0.075 sec/batch)
2017-06-02 10:49:39.141945: step 337920, loss = 0.15 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:49:40.013172: step 337930, loss = 0.16 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:49:40.860987: step 337940, loss = 0.19 (1509.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:49:41.755718: step 337950, loss = 0.14 (1430.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:49:42.646285: step 337960, loss = 0.15 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:49:43.526689: step 337970, loss = 0.15 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:49:44.391885: step 337980, loss = 0.18 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:49:45.280049: step 337990, loss = 0.17 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:49:46.264447: step 338000, loss = 0.14 (1300.3 examples/sec; 0.098 sec/batch)
2017-06-02 10:49:47.043856: step 338010, loss = 0.17 (1642.3 examples/sec; 0.078 sec/batch)
2017-06-02 10:49:47.930951: step 338020, loss = 0.19 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:49:48.811939: step 338030, loss = 0.14 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:49:49.682952: step 338040, loss = 0.17 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:49:50.569841: step 338050, loss = 0.19 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:49:51.454945: step 338060, loss = 0.16 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 10:49:52.306228: step 338070, loss = 0.16 (1503.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:49:53.259632: step 338080, loss = 0.17 (1342.5 examples/sec; 0.095 sec/batch)
2017-06-02 10:49:54.133713: step 338090, loss = 0.14 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:49:55.108857: step 338100, loss = 0.18 (1312.6 examples/sec; 0.098 sec/batch)
2017-06-02 10:49:55.909030: step 338110, loss = 0.16 (1599.7 examples/sec; 0.080 sec/batch)
2017-06-02 10:49:56.782511: step 338120, loss = 0.14 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:49:57.645411: step 338130, loss = 0.17 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:49:58.540908: step 338140, loss = 0.14 (1429.4 examples/sec; 0.090 sec/batch)
2017-06-02 10:49:59.411706: step 338150, loss = 0.15 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:50:00.288303: step 338160, loss = 0.16 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:50:01.115375: step 338170, loss = 0.16 (1547.6 examples/sec; 0.083 sec/batch)
2017-06-02 10:50:01.960507: step 338180, loss = 0.17 (1514.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:50:02.804012: step 338190, loss = 0.14 (1517.5 examples/sec; 0.084 sec/batch)
2017-06-02 10:50:03.739932: step 338200, loss = 0.17 (1367.6 examples/sec; 0.094 sec/batch)
2017-06-02 10:50:04.503778: step 338210, loss = 0.18 (1675.7 examples/sec; 0.076 sec/batch)
2017-06-02 10:50:05.359493: step 338220, loss = 0.16 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:50:06.218362: step 338230, loss = 0.18 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:50:07.100923: step 338240, loss = 0.14 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:50:07.971830: step 338250, loss = 0.17 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:50:08.842801: step 338260, loss = 0.16 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:50:09.721279: step 338270, loss = 0.14 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:50:10.601152: step 338280, loss = 0.18 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:50:11.486332: step 338290, loss = 0.16 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:50:12.475767: step 338300, loss = 0.15 (1293.7 examples/sec; 0.099 sec/batch)
2017-06-02 10:50:13.253126: step 338310, loss = 0.18 (1646.6 examples/sec; 0.078 sec/batch)
2017-06-02 10:50:14.148390: step 338320, loss = 0.20 (1429.8 examples/sec; 0.090 sec/batch)
2017-06-02 10:50:15.017296: step 338330, loss = 0.19 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:50:15.871080: step 338340, loss = 0.19 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:50:16.732686: step 338350, loss = 0.16 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:50:17.639313: step 338360, loss = 0.20 (1411.8 examples/sec; 0.091 sec/batch)
2017-06-02 10:50:18.492360: step 338370, loss = 0.14 (1500.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:50:19.370298: step 338380, loss = 0.20 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:50:20.235504: step 338390, loss = 0.20 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:50:21.200869: step 338400, loss = 0.17 (1325.9 examples/sec; 0.097 sec/batch)
2017-06-02 10:50:21.979059: step 338410, loss = 0.20 (1644.8 examples/sec; 0.078 sec/batch)
2017-06-02 10:50:22.852480: step 338420, loss = 0.17 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:50:23.699824: step 338430, loss = 0.21 (1510.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:50:24.560417: step 338440, loss = 0.20 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:50:25.419147: step 338450, loss = 0.15 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:50:26.298545: step 338460, loss = 0.15 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:50:27.148986: step 338470, loss = 0.15 (1505.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:50:27.993242: step 338480, loss = 0.19 (1516.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:50:28.854924: step 338490, loss = 0.13 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:50:29.790962: step 338500, loss = 0.21 (1367.5 examples/sec; 0.094 sec/batch)
2017-06-02 10:50:30.545280: step 338510, loss = 0.19 (1696.9 examples/sec; 0.075 sec/batch)
2017-06-02 10:50:31.390739: step 338520, loss = 0.18 (1514.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:50:32.255796: step 338530, loss = 0.18 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:50:33.098099: step 338540, loss = 0.15 (1519.6 examples/sec; 0.084 sec/batch)
2017-06-02 10:50:33.971849: step 338550, loss = 0.14 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:50:34.835202: step 338560, loss = 0.25 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:50:35.740116: step 338570, loss = 0.16 (1414.5 examples/sec; 0.090 sec/batch)
2017-06-02 10:50:36.608817: step 338580, loss = 0.17 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:50:37.443130: step 338590, loss = 0.16 (1534.2 examples/sec; 0.083 sec/batch)
2017-06-02 10:50:38.422032: step 338600, loss = 0.15 (1307.6 examples/sec; 0.098 sec/batch)
2017-06-02 10:50:39.175094: step 338610, loss = 0.14 (1699.7 examples/sec; 0.075 sec/batch)
2017-06-02 10:50:40.051825: step 338620, loss = 0.20 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:50:40.910097: step 338630, loss = 0.13 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:50:41.785338: step 338640, loss = 0.20 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:50:42.655078: step 338650, loss = 0.20 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:50:43.517049: step 338660, loss = 0.18 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:50:44.407574: step 338670, loss = 0.15 (1437.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:50:45.267111: step 338680, loss = 0.16 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:50:46.101467: step 338690, loss = 0.16 (1534.1 examples/sec; 0.083 sec/batch)
2017-06-02 10:50:47.075927: step 338700, loss = 0.14 (1313.6 examples/sec; 0.097 sec/batch)
2017-06-02 10:50:47.845630: step 338710, loss = 0.16 (1663.0 examples/sec; 0.077 sec/batch)
2017-06-02 10:50:48.713840: step 338720, loss = 0.13 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:50:49.594185: step 338730, loss = 0.18 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:50:50.455159: step 338740, loss = 0.16 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:50:51.311786: step 338750, loss = 0.20 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:50:52.205866: step 338760, loss = 0.16 (1431.7 examples/sec; 0.089 sec/batch)
2017-06-02 10:50:53.060560: step 338770, loss = 0.15 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:50:53.912334: step 338780, loss = 0.15 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:50:54.786810: step 338790, loss = 0.21 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:50:55.747950: step 338800, loss = 0.17 (1331.8 examples/sec; 0.096 sec/batch)
2017-06-02 10:50:56.513094: step 338810, loss = 0.16 (1672.9 examples/sec; 0.077 sec/batch)
2017-06-02 10:50:57.356238: step 338820, loss = 0.16 (1518.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:50:58.218817: step 338830, loss = 0.20 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:50:59.122705: step 338840, loss = 0.14 (1416.1 examples/sec; 0.090 sec/batch)
2017-06-02 10:50:59.981158: step 338850, loss = 0.15 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:51:00.844692: step 338860, loss = 0.16 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:51:01.713856: step 338870, loss = 0.16 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:51:02.580959: step 338880, loss = 0.12 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:51:03.425609: step 338890, loss = 0.19 (1515.4 examples/sec; 0.084 sec/batch)
2017-06-02 10:51:04.424555: step 338900, loss = 0.13 (1281.3 examples/sec; 0.100 sec/batch)
2017-06-02 10:51:05.169682: step 338910, loss = 0.18 (1717.9 examples/sec; 0.075 sec/batch)
2017-06-02 10:51:06.040491: step 338920, loss = 0.20 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:51:06.908336: step 338930, loss = 0.18 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:51:07.779135: step 338940, loss = 0.20 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:51:08.666332: step 338950, loss = 0.23 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 10:51:09.520779: step 338960, loss = 0.14 (1498.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:51:10.378291: step 338970, loss = 0.17 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:51:11.233395: step 338980, loss = 0.15 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:51:12.069326: step 338990, loss = 0.16 (1531.2 examples/sec; 0.084 sec/batch)
2017-06-02 10:51:13.049048: step 339000, loss = 0.18 (1306.5 examples/sec; 0.098 sec/batch)
2017-06-02 10:51:13.818099: step 339010, loss = 0.18 (1664.4 examples/sec; 0.077 sec/batch)
2017-06-02 10:51:14.688089: step 339020, loss = 0.15 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:51:15.565441: step 339030, loss = 0.14 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:51:16.420298: step 339040, loss = 0.16 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:51:17.295715: step 339050, loss = 0.17 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:51:18.145960: step 339060, loss = 0.17 (1505.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:51:19.005714: step 339070, loss = 0.16 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:51:19.877963: step 339080, loss = 0.17 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:51:20.719517: step 339090, loss = 0.16 (1521.0 examples/sec; 0.084 sec/batch)
2017-06-02 10:51:21.703911: step 339100, loss = 0.14 (1300.3 examples/sec; 0.098 sec/batch)
2017-06-02 10:51:22.448671: step 339110, loss = 0.17 (1718.7 examples/sec; 0.074 sec/batch)
2017-06-02 10:51:23.308865: step 339120, loss = 0.14 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:51:24.147457: step 339130, loss = 0.15 (1526.4 examples/sec; 0.084 sec/batch)
2017-06-02 10:51:25.008380: step 339140, loss = 0.12 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:51:25.877423: step 339150, loss = 0.18 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:51:26.752311: step 339160, loss = 0.20 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:51:27.638125: step 339170, loss = 0.19 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:51:28.503920: step 339180, loss = 0.20 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:51:29.367522: step 339190, loss = 0.19 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:51:30.332615: step 339200, loss = 0.20 (1326.3 examples/sec; 0.097 sec/batch)
2017-06-02 10:51:31.110353: step 339210, loss = 0.17 (1645.8 examples/sec; 0.078 sec/batch)
2017-06-02 10:51:31.953427: step 339220, loss = 0.13 (1518.3 examples/sec; 0.084 sec/batch)
2017-06-02 10:51:32.815822: step 339230, loss = 0.18 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:51:33.685497: step 339240, loss = 0.17 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:51:34.554910: step 339250, loss = 0.13 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:51:35.433241: step 339260, loss = 0.20 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:51:36.292901: step 339270, loss = 0.23 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:51:37.158026: step 339280, loss = 0.14 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:51:38.018304: step 339290, loss = 0.18 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:51:38.982387: step 339300, loss = 0.17 (1327.7 examples/sec; 0.096 sec/batch)
2017-06-02 10:51:39.755482: step 339310, loss = 0.12 (1655.7 examples/sec; 0.077 sec/batch)
2017-06-02 10:51:40.625657: step 339320, loss = 0.20 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:51:41.506279: step 339330, loss = 0.17 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:51:42.378199: step 339340, loss = 0.15 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:51:43.230330: step 339350, loss = 0.15 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:51:44.110618: step 339360, loss = 0.20 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:51:44.974366: step 339370, loss = 0.15 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:51:45.844909: step 339380, loss = 0.23 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:51:46.697968: step 339390, loss = 0.20 (1500.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:51:47.738486: step 339400, loss = 0.18 (1230.1 examples/sec; 0.104 sec/batch)
2017-06-02 10:51:48.472715: step 339410, loss = 0.16 (1743.3 examples/sec; 0.073 sec/batch)
2017-06-02 10:51:49.348020: step 339420, loss = 0.19 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:51:50.235281: step 339430, loss = 0.19 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:51:51.117235: step 339440, loss = 0.16 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:51:52.010040: step 339450, loss = 0.21 (1433.7 examples/sec; 0.089 sec/batch)
2017-06-02 10:51:52.884737: step 339460, loss = 0.20 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:51:53.765904: step 339470, loss = 0.16 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:51:54.646392: step 339480, loss = 0.16 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:51:55.530446: step 339490, loss = 0.26 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:51:56.551285: step 339500, loss = 0.17 (1253.9 examples/sec; 0.102 sec/batch)
2017-06-02 10:51:57.334335: step 339510, loss = 0.17 (1634.6 examples/sec; 0.078 sec/batch)
2017-06-02 10:51:58.195105: step 339520, loss = 0.16 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:51:59.054767: step 339530, loss = 0.18 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:51:59.920542: step 339540, loss = 0.15 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:52:00.815773: step 339550, loss = 0.15 (1429.8 examples/sec; 0.090 sec/batch)
2017-06-02 10:52:01.699360: step 339560, loss = 0.16 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:52:02.551787: step 339570, loss = 0.16 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:52:03.443782: step 339580, loss = 0.13 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:52:04.328179: step 339590, loss = 0.19 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:52:05.303342: step 339600, loss = 0.19 (1312.6 examples/sec; 0.098 sec/batch)
2017-06-02 10:52:06.075922: step 339610, loss = 0.19 (1656.8 examples/sec; 0.077 sec/batch)
2017-06-02 10:52:06.919963: step 339620, loss = 0.15 (1516.5 examples/sec; 0.084 sec/batch)
2017-06-02 10:52:07.799331: step 339630, loss = 0.14 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:52:08.670559: step 339640, loss = 0.16 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:52:09.557877: step 339650, loss = 0.20 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 10:52:10.444133: step 339660, loss = 0.16 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:52:11.303154: step 339670, loss = 0.18 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:52:12.166989: step 339680, loss = 0.19 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:52:13.054155: step 339690, loss = 0.19 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 10:52:14.045041: step 339700, loss = 0.22 (1291.8 examples/sec; 0.099 sec/batch)
2017-06-02 10:52:14.803233: step 339710, loss = 0.19 (1688.3 examples/sec; 0.076 sec/batch)
2017-06-02 10:52:15.703226: step 339720, loss = 0.12 (1422.2 examples/sec; 0.090 sec/batch)
2017-06-02 10:52:16.604966: step 339730, loss = 0.17 (1419.5 examples/sec; 0.090 sec/batch)
2017-06-02 10:52:17.458192: step 339740, loss = 0.15 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:52:18.342096: step 339750, loss = 0.18 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:52:19.234135: step 339760, loss = 0.19 (1434.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:52:20.120822: step 339770, loss = 0.17 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:52:20.968149: step 339780, loss = 0.14 (1510.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:52:21.840434: step 339790, loss = 0.14 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:52:22.862405: step 339800, loss = 0.16 (1252.5 examples/sec; 0.102 sec/batch)
2017-06-02 10:52:23.554050: step 339810, loss = 0.16 (1850.7 examples/sec; 0.069 sec/batch)
2017-06-02 10:52:24.411617: step 339820, loss = 0.16 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:52:25.292001: step 339830, loss = 0.16 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:52:26.163432: step 339840, loss = 0.26 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:52:27.056244: step 339850, loss = 0.18 (1433.7 examples/sec; 0.089 sec/batch)
2017-06-02 10:52:27.946936: step 339860, loss = 0.18 (1437.1 examples/sec; 0.089 sec/batch)
2017-06-02 10:52:28.826546: step 339870, loss = 0.25 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:52:29.676116: step 339880, loss = 0.20 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:52:30.553356: step 339890, loss = 0.15 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:52:31.544290: step 339900, loss = 0.17 (1291.7 examples/sec; 0.099 sec/batch)
2017-06-02 10:52:32.311662: step 339910, loss = 0.14 (1668.0 examples/sec; 0.077 sec/batch)
2017-06-02 10:52:33.193557: step 339920, loss = 0.14 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:52:34.082781: step 339930, loss = 0.16 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:52:34.978608: step 339940, loss = 0.16 (1428.8 examples/sec; 0.090 sec/batch)
2017-06-02 10:52:35.821996: step 339950, loss = 0.13 (1517.7 examples/sec; 0.084 sec/batch)
2017-06-02 10:52:36.712094: step 339960, loss = 0.14 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:52:37.603821: step 339970, loss = 0.16 (1435.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:52:38.451620: step 339980, loss = 0.15 (1509.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:52:39.324282: step 339990, loss = 0.13 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:52:40.300343: step 340000, loss = 0.21 (1311.4 examples/sec; 0.098 sec/batch)
2017-06-02 10:52:41.091236: step 340010, loss = 0.15 (1618.4 examples/sec; 0.079 sec/batch)
2017-06-02 10:52:41.957004: step 340020, loss = 0.23 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:52:42.800790: step 340030, loss = 0.18 (1517.0 examples/sec; 0.084 sec/batch)
2017-06-02 10:52:43.686097: step 340040, loss = 0.17 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 10:52:44.568062: step 340050, loss = 0.19 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:52:45.447586: step 340060, loss = 0.17 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:52:46.326115: step 340070, loss = 0.20 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:52:47.236106: step 340080, loss = 0.14 (1406.6 examples/sec; 0.091 sec/batch)
2017-06-02 10:52:48.138419: step 340090, loss = 0.15 (1418.6 examples/sec; 0.090 sec/batch)
2017-06-02 10:52:49.120081: step 340100, loss = 0.21 (1303.9 examples/sec; 0.098 sec/batch)
2017-06-02 10:52:49.922413: step 340110, loss = 0.14 (1595.4 examples/sec; 0.080 sec/batch)
2017-06-02 10:52:50.784190: step 340120, loss = 0.14 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:52:51.650969: step 340130, loss = 0.12 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:52:52.520700: step 340140, loss = 0.20 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:52:53.371095: step 340150, loss = 0.16 (1505.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:52:54.211827: step 340160, loss = 0.14 (1522.5 examples/sec; 0.084 sec/batch)
2017-06-02 10:52:55.079152: step 340170, loss = 0.15 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:52:55.946805: step 340180, loss = 0.17 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:52:56.821002: step 340190, loss = 0.14 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:52:57.819448: step 340200, loss = 0.16 (1282.0 examples/sec; 0.100 sec/batch)
2017-06-02 10:52:58.577206: step 340210, loss = 0.19 (1689.2 examples/sec; 0.076 sec/batch)
2017-06-02 10:52:59.446931: step 340220, loss = 0.16 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:53:00.311258: step 340230, loss = 0.17 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:53:01.195074: step 340240, loss = 0.17 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:53:02.077570: step 340250, loss = 0.24 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:53:02.934714: step 340260, loss = 0.18 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:53:03.799039: step 340270, loss = 0.17 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:53:04.662663: step 340280, loss = 0.14 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:53:05.555995: step 340290, loss = 0.21 (1432.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:53:06.550165: step 340300, loss = 0.17 (1287.5 examples/sec; 0.099 sec/batch)
2017-06-02 10:53:07.302879: step 340310, loss = 0.16 (1700.5 examples/sec; 0.075 sec/batch)
2017-06-02 10:53:08.164346: step 340320, loss = 0.15 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:53:09.044285: step 340330, loss = 0.19 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:53:09.906718: step 340340, loss = 0.17 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:53:10.763116: step 340350, loss = 0.17 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:53:11.627408: step 340360, loss = 0.19 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:53:12.518577: step 340370, loss = 0.16 (1436.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:53:13.374911: step 340380, loss = 0.17 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:53:14.230285: step 340390, loss = 0.12 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:53:15.185289: step 340400, loss = 0.21 (1340.3 examples/sec; 0.095 sec/batch)
2017-06-02 10:53:15.958534: step 340410, loss = 0.16 (1655.3 examples/sec; 0.077 sec/batch)
2017-06-02 10:53:16.839004: step 340420, loss = 0.14 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:53:17.715942: step 340430, loss = 0.16 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:53:18.573269: step 340440, loss = 0.18 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:53:19.460582: step 340450, loss = 0.13 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 10:53:20.351392: step 340460, loss = 0.14 (1436.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:53:21.205090: step 340470, loss = 0.16 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:53:22.072280: step 340480, loss = 0.18 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:53:22.932485: step 340490, loss = 0.13 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:53:23.894544: step 340500, loss = 0.19 (1330.5 examples/sec; 0.096 sec/batch)
2017-06-02 10:53:24.705205: step 340510, loss = 0.14 (1579.0 examples/sec; 0.081 sec/batch)
2017-06-02 10:53:25.577015: step 340520, loss = 0.14 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:53:26.454181: step 340530, loss = 0.18 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:53:27.350983: step 340540, loss = 0.17 (1427.3 examples/sec; 0.090 sec/batch)
2017-06-02 10:53:28.194197: step 340550, loss = 0.20 (1518.0 examples/sec; 0.084 sec/batch)
2017-06-02 10:53:29.071834: step 340560, loss = 0.19 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:53:29.910859: step 340570, loss = 0.19 (1525.6 examples/sec; 0.084 sec/batch)
2017-06-02 10:53:30.786587: step 340580, loss = 0.18 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:53:31.651744: step 340590, loss = 0.14 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:53:32.631499: step 340600, loss = 0.16 (1306.4 examples/sec; 0.098 sec/batch)
2017-06-02 10:53:33.398081: step 340610, loss = 0.16 (1669.8 examples/sec; 0.077 sec/batch)
2017-06-02 10:53:34.272422: step 340620, loss = 0.13 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:53:35.149927: step 340630, loss = 0.14 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:53:36.012262: step 340640, loss = 0.16 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:53:36.853990: step 340650, loss = 0.19 (1520.7 examples/sec; 0.084 sec/batch)
2017-06-02 10:53:37.761657: step 340660, loss = 0.18 (1410.2 examples/sec; 0.091 sec/batch)
2017-06-02 10:53:38.606461: step 340670, loss = 0.15 (1515.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:53:39.475450: step 340680, loss = 0.15 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:53:40.340343: step 340690, loss = 0.18 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:53:41.312858: step 340700, loss = 0.20 (1316.2 examples/sec; 0.097 sec/batch)
2017-06-02 10:53:42.085148: step 340710, loss = 0.17 (1657.4 examples/sec; 0.077 sec/batch)
2017-06-02 10:53:42.959891: step 340720, loss = 0.17 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:53:43.822409: step 340730, loss = 0.16 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:53:44.695268: step 340740, loss = 0.20 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:53:45.552842: step 340750, loss = 0.21 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:53:46.417018: step 340760, loss = 0.14 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:53:47.278253: step 340770, loss = 0.13 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:53:48.131096: step 340780, loss = 0.19 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:53:48.975827: step 340790, loss = 0.15 (1515.3 examples/sec; 0.084 sec/batch)
2017-06-02 10:53:49.942713: step 340800, loss = 0.12 (1323.8 examples/sec; 0.097 sec/batch)
2017-06-02 10:53:50.694525: step 340810, loss = 0.16 (1702.6 examples/sec; 0.075 sec/batch)
2017-06-02 10:53:51.561433: step 340820, loss = 0.17 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:53:52.429188: step 340830, loss = 0.16 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:53:53.292679: step 340840, loss = 0.15 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:53:54.171019: step 340850, loss = 0.17 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:53:55.008241: step 340860, loss = 0.20 (1528.9 examples/sec; 0.084 sec/batch)
2017-06-02 10:53:55.857705: step 340870, loss = 0.15 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:53:56.723981: step 340880, loss = 0.15 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:53:57.600058: step 340890, loss = 0.18 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:53:58.626754: step 340900, loss = 0.14 (1246.7 examples/sec; 0.103 sec/batch)
2017-06-02 10:53:59.368715: step 340910, loss = 0.17 (1725.1 examples/sec; 0.074 sec/batch)
2017-06-02 10:54:00.243064: step 340920, loss = 0.20 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:54:01.101536: step 340930, loss = 0.16 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:54:01.954238: step 340940, loss = 0.15 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:54:02.826366: step 340950, loss = 0.18 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:54:03.693331: step 340960, loss = 0.13 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:54:04.558870: step 340970, loss = 0.16 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:54:05.448667: step 340980, loss = 0.15 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 10:54:06.286134: step 340990, loss = 0.13 (1528.4 examples/sec; 0.084 sec/batch)
2017-06-02 10:54:07.261820: step 341000, loss = 0.23 (1311.9 examples/sec; 0.098 sec/batch)
2017-06-02 10:54:08.003090: step 341010, loss = 0.15 (1726.8 examples/sec; 0.074 sec/batch)
2017-06-02 10:54:08.898446: step 341020, loss = 0.15 (1429.6 examples/sec; 0.090 sec/batch)
2017-06-02 10:54:09.764636: step 341030, loss = 0.14 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:54:10.627475: step 341040, loss = 0.18 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:54:11.517697: step 341050, loss = 0.18 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 10:54:12.382440: step 341060, loss = 0.17 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:54:13.222624: step 341070, loss = 0.21 (1523.4 examples/sec; 0.084 sec/batch)
2017-06-02 10:54:14.099019: step 341080, loss = 0.18 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:54:14.967620: step 341090, loss = 0.19 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:54:15.950905: step 341100, loss = 0.14 (1301.8 examples/sec; 0.098 sec/batch)
2017-06-02 10:54:16.675454: step 341110, loss = 0.14 (1766.6 examples/sec; 0.072 sec/batch)
2017-06-02 10:54:17.535930: step 341120, loss = 0.14 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:54:18.420908: step 341130, loss = 0.18 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:54:19.284562: step 341140, loss = 0.19 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:54:20.145276: step 341150, loss = 0.16 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:54:21.010972: step 341160, loss = 0.16 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:54:21.879413: step 341170, loss = 0.17 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:54:22.736616: step 341180, loss = 0.17 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:54:23.620585: step 341190, loss = 0.15 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:54:24.635670: step 341200, loss = 0.20 (1261.0 examples/sec; 0.102 sec/batch)
2017-06-02 10:54:25.364900: step 341210, loss = 0.19 (1755.3 examples/sec; 0.073 sec/batch)
2017-06-02 10:54:26.238477: step 341220, loss = 0.18 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:54:27.118163: step 341230, loss = 0.13 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:54:28.001434: step 341240, loss = 0.16 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:54:28.872480: step 341250, loss = 0.23 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:54:29.750495: step 341260, loss = 0.18 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:54:30.604198: step 341270, loss = 0.17 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:54:31.491561: step 341280, loss = 0.14 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 10:54:32.358124: step 341290, loss = 0.17 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:54:33.343494: step 341300, loss = 0.18 (1299.0 examples/sec; 0.099 sec/batch)
2017-06-02 10:54:34.122225: step 341310, loss = 0.17 (1643.7 examples/sec; 0.078 sec/batch)
2017-06-02 10:54:34.992508: step 341320, loss = 0.18 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:54:35.857347: step 341330, loss = 0.19 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:54:36.711918: step 341340, loss = 0.19 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:54:37.566061: step 341350, loss = 0.12 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:54:38.447776: step 341360, loss = 0.18 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:54:39.297357: step 341370, loss = 0.15 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:54:40.161932: step 341380, loss = 0.18 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:54:40.994596: step 341390, loss = 0.16 (1537.2 examples/sec; 0.083 sec/batch)
2017-06-02 10:54:41.968309: step 341400, loss = 0.15 (1314.5 examples/sec; 0.097 sec/batch)
2017-06-02 10:54:42.722387: step 341410, loss = 0.20 (1697.4 examples/sec; 0.075 sec/batch)
2017-06-02 10:54:43.598761: step 341420, loss = 0.17 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:54:44.476962: step 341430, loss = 0.21 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:54:45.357907: step 341440, loss = 0.19 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:54:46.251853: step 341450, loss = 0.16 (1431.8 examples/sec; 0.089 sec/batch)
2017-06-02 10:54:47.109462: step 341460, loss = 0.16 (1492.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:54:47.969681: step 341470, loss = 0.14 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:54:48.838419: step 341480, loss = 0.20 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:54:49.701353: step 341490, loss = 0.19 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:54:50.663475: step 341500, loss = 0.18 (1330.4 examples/sec; 0.096 sec/batch)
2017-06-02 10:54:51.435071: step 341510, loss = 0.16 (1658.9 examples/sec; 0.077 sec/batch)
2017-06-02 10:54:52.308882: step 341520, loss = 0.15 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:54:53.178488: step 341530, loss = 0.14 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:54:54.046963: step 341540, loss = 0.13 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:54:54.916344: step 341550, loss = 0.18 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:54:55.796969: step 341560, loss = 0.17 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:54:56.680630: step 341570, loss = 0.15 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:54:57.547928: step 341580, loss = 0.18 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:54:58.403705: step 341590, loss = 0.16 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:54:59.371329: step 341600, loss = 0.16 (1322.8 examples/sec; 0.097 sec/batch)
2017-06-02 10:55:00.112745: step 341610, loss = 0.16 (1726.4 examples/sec; 0.074 sec/batch)
2017-06-02 10:55:00.998944: step 341620, loss = 0.13 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:55:01.869316: step 341630, loss = 0.17 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:55:02.722715: step 341640, loss = 0.13 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:55:03.608026: step 341650, loss = 0.15 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 10:55:04.467489: step 341660, loss = 0.19 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:55:05.314691: step 341670, loss = 0.15 (1510.9 examples/sec; 0.085 sec/batch)
2017-06-02 10:55:06.161031: step 341680, loss = 0.16 (1512.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:55:07.028136: step 341690, loss = 0.19 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:55:07.972064: step 341700, loss = 0.16 (1356.0 examples/sec; 0.094 sec/batch)
2017-06-02 10:55:08.733616: step 341710, loss = 0.18 (1680.8 examples/sec; 0.076 sec/batch)
2017-06-02 10:55:09.606796: step 341720, loss = 0.16 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:55:10.473643: step 341730, loss = 0.21 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:55:11.325766: step 341740, loss = 0.14 (1502.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:55:12.191049: step 341750, loss = 0.14 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:55:13.046136: step 341760, loss = 0.12 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:55:13.923223: step 341770, loss = 0.16 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:55:14.812651: step 341780, loss = 0.18 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 10:55:15.697515: step 341790, loss = 0.13 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:55:16.663543: step 341800, loss = 0.15 (1325.0 examples/sec; 0.097 sec/batch)
2017-06-02 10:55:17.466519: step 341810, loss = 0.20 (1594.1 examples/sec; 0.080 sec/batch)
2017-06-02 10:55:18.348176: step 341820, loss = 0.20 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:55:19.204277: step 341830, loss = 0.15 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:55:20.060456: step 341840, loss = 0.17 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:55:20.925803: step 341850, loss = 0.15 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:55:21.802673: step 341860, loss = 0.21 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:55:22.694474: step 341870, loss = 0.18 (1435.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:55:23.567020: step 341880, loss = 0.22 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:55:24.445766: step 341890, loss = 0.18 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:55:25.435741: step 341900, loss = 0.16 (1293.0 examples/sec; 0.099 sec/batch)
2017-06-02 10:55:26.197608: step 341910, loss = 0.16 (1680.1 examples/sec; 0.076 sec/batch)
2017-06-02 10:55:27.082108: step 341920, loss = 0.14 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:55:27.951246: step 341930, loss = 0.15 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:55:28.813994: step 341940, loss = 0.16 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:55:29.694922: step 341950, loss = 0.13 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:55:30.560429: step 341960, loss = 0.22 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:55:31.425948: step 341970, loss = 0.17 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:55:32.299863: step 341980, loss = 0.15 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:55:33.163680: step 341990, loss = 0.16 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:55:34.164716: step 342000, loss = 0.15 (1278.7 examples/sec; 0.100 sec/batch)
2017-06-02 10:55:34.918603: step 342010, loss = 0.15 (1697.9 examples/sec; 0.075 sec/batch)
2017-06-02 10:55:35.778749: step 342020, loss = 0.15 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:55:36.636744: step 342030, loss = 0.15 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:55:37.495066: step 342040, loss = 0.18 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:55:38.365603: step 342050, loss = 0.22 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:55:39.250882: step 342060, loss = 0.14 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 10:55:40.106579: step 342070, loss = 0.15 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:55:40.971671: step 342080, loss = 0.18 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:55:41.828814: step 342090, loss = 0.16 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:55:42.800018: step 342100, loss = 0.13 (1318.0 examples/sec; 0.097 sec/batch)
2017-06-02 10:55:43.594556: step 342110, loss = 0.18 (1611.0 examples/sec; 0.079 sec/batch)
2017-06-02 10:55:44.488011: step 342120, loss = 0.17 (1432.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:55:45.339204: step 342130, loss = 0.21 (1503.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:55:46.174245: step 342140, loss = 0.15 (1532.9 examples/sec; 0.084 sec/batch)
2017-06-02 10:55:47.002862: step 342150, loss = 0.16 (1544.7 examples/sec; 0.083 sec/batch)
2017-06-02 10:55:47.848696: step 342160, loss = 0.15 (1513.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:55:48.705290: step 342170, loss = 0.17 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:55:49.586602: step 342180, loss = 0.20 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:55:50.449583: step 342190, loss = 0.21 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:55:51.422682: step 342200, loss = 0.17 (1315.4 examples/sec; 0.097 sec/batch)
2017-06-02 10:55:52.191466: step 342210, loss = 0.20 (1665.0 examples/sec; 0.077 sec/batch)
2017-06-02 10:55:53.032251: step 342220, loss = 0.14 (1522.4 examples/sec; 0.084 sec/batch)
2017-06-02 10:55:53.885272: step 342230, loss = 0.18 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 10:55:54.752014: step 342240, loss = 0.16 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:55:55.594490: step 342250, loss = 0.16 (1519.3 examples/sec; 0.084 sec/batch)
2017-06-02 10:55:56.447430: step 342260, loss = 0.20 (1500.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:55:57.317892: step 342270, loss = 0.14 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:55:58.178819: step 342280, loss = 0.15 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:55:59.023223: step 342290, loss = 0.18 (1515.9 examples/sec; 0.084 sec/batch)
2017-06-02 10:55:59.999193: step 342300, loss = 0.17 (1311.5 examples/sec; 0.098 sec/batch)
2017-06-02 10:56:00.782594: step 342310, loss = 0.16 (1633.9 examples/sec; 0.078 sec/batch)
2017-06-02 10:56:01.646298: step 342320, loss = 0.14 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:56:02.508993: step 342330, loss = 0.15 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:56:03.369764: step 342340, loss = 0.16 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:56:04.226010: step 342350, loss = 0.21 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:56:05.094829: step 342360, loss = 0.20 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:56:05.965781: step 342370, loss = 0.15 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:56:06.829067: step 342380, loss = 0.16 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:56:07.672172: step 342390, loss = 0.20 (1518.2 examples/sec; 0.084 sec/batch)
2017-06-02 10:56:08.611289: step 342400, loss = 0.17 (1363.0 examples/sec; 0.094 sec/batch)
2017-06-02 10:56:09.395837: step 342410, loss = 0.15 (1631.5 examples/sec; 0.078 sec/batch)
2017-06-02 10:56:10.254677: step 342420, loss = 0.16 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:56:11.126211: step 342430, loss = 0.17 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:56:11.994567: step 342440, loss = 0.15 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:56:12.871299: step 342450, loss = 0.16 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:56:13.723513: step 342460, loss = 0.13 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:56:14.579535: step 342470, loss = 0.18 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:56:15.404176: step 342480, loss = 0.15 (1552.2 examples/sec; 0.082 sec/batch)
2017-06-02 10:56:16.274886: step 342490, loss = 0.15 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:56:17.293005: step 342500, loss = 0.16 (1257.2 examples/sec; 0.102 sec/batch)
2017-06-02 10:56:18.029375: step 342510, loss = 0.16 (1738.2 examples/sec; 0.074 sec/batch)
2017-06-02 10:56:18.896700: step 342520, loss = 0.17 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:56:19.777204: step 342530, loss = 0.18 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:56:20.642054: step 342540, loss = 0.14 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:56:21.536953: step 342550, loss = 0.18 (1430.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:56:22.408956: step 342560, loss = 0.15 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:56:23.279645: step 342570, loss = 0.17 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:56:24.153721: step 342580, loss = 0.16 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:56:25.029657: step 342590, loss = 0.14 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:56:26.004305: step 342600, loss = 0.13 (1313.3 examples/sec; 0.097 sec/batch)
2017-06-02 10:56:26.754924: step 342610, loss = 0.16 (1705.3 examples/sec; 0.075 sec/batch)
2017-06-02 10:56:27.651248: step 342620, loss = 0.15 (1428.1 examples/sec; 0.090 sec/batch)
2017-06-02 10:56:28.528885: step 342630, loss = 0.14 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:56:29.374436: step 342640, loss = 0.21 (1513.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:56:30.210900: step 342650, loss = 0.13 (1530.2 examples/sec; 0.084 sec/batch)
2017-06-02 10:56:31.085712: step 342660, loss = 0.14 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:56:31.941607: step 342670, loss = 0.17 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:56:32.797656: step 342680, loss = 0.14 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:56:33.661271: step 342690, loss = 0.21 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:56:34.668757: step 342700, loss = 0.18 (1270.5 examples/sec; 0.101 sec/batch)
2017-06-02 10:56:35.380495: step 342710, loss = 0.14 (1798.4 examples/sec; 0.071 sec/batch)
2017-06-02 10:56:36.251752: step 342720, loss = 0.17 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:56:37.117658: step 342730, loss = 0.20 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:56:37.987623: step 342740, loss = 0.16 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:56:38.865694: step 342750, loss = 0.15 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:56:39.709045: step 342760, loss = 0.13 (1517.7 examples/sec; 0.084 sec/batch)
2017-06-02 10:56:40.571310: step 342770, loss = 0.18 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:56:41.430695: step 342780, loss = 0.14 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:56:42.295648: step 342790, loss = 0.17 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:56:43.312884: step 342800, loss = 0.15 (1258.3 examples/sec; 0.102 sec/batch)
2017-06-02 10:56:44.076995: step 342810, loss = 0.20 (1675.1 examples/sec; 0.076 sec/batch)
2017-06-02 10:56:44.950551: step 342820, loss = 0.27 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:56:45.829829: step 342830, loss = 0.14 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:56:46.742282: step 342840, loss = 0.17 (1402.8 examples/sec; 0.091 sec/batch)
2017-06-02 10:56:47.631600: step 342850, loss = 0.16 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:56:48.492618: step 342860, loss = 0.14 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:56:49.374873: step 342870, loss = 0.18 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:56:50.245535: step 342880, loss = 0.24 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:56:51.120106: step 342890, loss = 0.15 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:56:52.111900: step 342900, loss = 0.14 (1290.6 examples/sec; 0.099 sec/batch)
2017-06-02 10:56:52.917341: step 342910, loss = 0.15 (1589.2 examples/sec; 0.081 sec/batch)
2017-06-02 10:56:53.781573: step 342920, loss = 0.15 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:56:54.651906: step 342930, loss = 0.15 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:56:55.511785: step 342940, loss = 0.17 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:56:56.364647: step 342950, loss = 0.14 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:56:57.251296: step 342960, loss = 0.18 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 10:56:58.109450: step 342970, loss = 0.13 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:56:58.964865: step 342980, loss = 0.18 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:56:59.833078: step 342990, loss = 0.12 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:57:00.813502: step 343000, loss = 0.20 (1305.6 examples/sec; 0.098 sec/batch)
2017-06-02 10:57:01.561906: step 343010, loss = 0.16 (1710.3 examples/sec; 0.075 sec/batch)
2017-06-02 10:57:02.405602: step 343020, loss = 0.17 (1517.1 examples/sec; 0.084 sec/batch)
2017-06-02 10:57:03.292753: step 343030, loss = 0.13 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 10:57:04.170608: step 343040, loss = 0.18 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:57:05.027501: step 343050, loss = 0.16 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:57:05.883893: step 343060, loss = 0.14 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:57:06.759053: step 343070, loss = 0.16 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:57:07.633346: step 343080, loss = 0.17 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:57:08.515914: step 343090, loss = 0.19 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:57:09.495078: step 343100, loss = 0.18 (1307.2 examples/sec; 0.098 sec/batch)
2017-06-02 10:57:10.266711: step 343110, loss = 0.15 (1658.8 examples/sec; 0.077 sec/batch)
2017-06-02 10:57:11.148264: step 343120, loss = 0.24 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:57:12.022289: step 343130, loss = 0.16 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:57:12.894646: step 343140, loss = 0.15 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:57:13.737346: step 343150, loss = 0.14 (1518.9 examples/sec; 0.084 sec/batch)
2017-06-02 10:57:14.596644: step 343160, loss = 0.20 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:57:15.466522: step 343170, loss = 0.17 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:57:16.348586: step 343180, loss = 0.18 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:57:17.217180: step 343190, loss = 0.16 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:57:18.178793: step 343200, loss = 0.26 (1331.1 examples/sec; 0.096 sec/batch)
2017-06-02 10:57:18.958778: step 343210, loss = 0.16 (1641.1 examples/sec; 0.078 sec/batch)
2017-06-02 10:57:19.821215: step 343220, loss = 0.20 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:57:20.701368: step 343230, loss = 0.20 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:57:21.555793: step 343240, loss = 0.22 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:57:22.417563: step 343250, loss = 0.19 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:57:23.294831: step 343260, loss = 0.17 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:57:24.140091: step 343270, loss = 0.15 (1514.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:57:25.021718: step 343280, loss = 0.17 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:57:25.872572: step 343290, loss = 0.17 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 10:57:26.843581: step 343300, loss = 0.19 (1318.2 examples/sec; 0.097 sec/batch)
2017-06-02 10:57:27.626137: step 343310, loss = 0.17 (1635.7 examples/sec; 0.078 sec/batch)
2017-06-02 10:57:28.478490: step 343320, loss = 0.16 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:57:29.328507: step 343330, loss = 0.16 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:57:30.180231: step 343340, loss = 0.14 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:57:31.039083: step 343350, loss = 0.19 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:57:31.936015: step 343360, loss = 0.17 (1427.1 examples/sec; 0.090 sec/batch)
2017-06-02 10:57:32.803169: step 343370, loss = 0.15 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:57:33.678891: step 343380, loss = 0.15 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:57:34.550373: step 343390, loss = 0.18 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:57:35.515965: step 343400, loss = 0.15 (1325.6 examples/sec; 0.097 sec/batch)
2017-06-02 10:57:36.254048: step 343410, loss = 0.17 (1734.2 examples/sec; 0.074 sec/batch)
2017-06-02 10:57:37.138515: step 343420, loss = 0.16 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:57:38.009181: step 343430, loss = 0.16 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:57:38.893520: step 343440, loss = 0.14 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:57:39.773657: step 343450, loss = 0.14 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:57:40.644293: step 343460, loss = 0.18 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:57:41.496114: step 343470, loss = 0.17 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:57:42.366641: step 343480, loss = 0.19 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:57:43.257981: step 343490, loss = 0.20 (1436.0 examples/sec; 0.089 sec/batch)
2017-06-02 10:57:44.228002: step 343500, loss = 0.13 (1319.6 examples/sec; 0.097 sec/batch)
2017-06-02 10:57:45.009289: step 343510, loss = 0.15 (1638.4 examples/sec; 0.078 sec/batch)
2017-06-02 10:57:45.891247: step 343520, loss = 0.12 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:57:46.767059: step 343530, loss = 0.16 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:57:47.616403: step 343540, loss = 0.17 (1507.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:57:48.491205: step 343550, loss = 0.14 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:57:49.359983: step 343560, loss = 0.14 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:57:50.232826: step 343570, loss = 0.22 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:57:51.111697: step 343580, loss = 0.14 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:57:51.990914: step 343590, loss = 0.14 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:57:52.981973: step 343600, loss = 0.20 (1291.6 examples/sec; 0.099 sec/batch)
2017-06-02 10:57:53.727128: step 343610, loss = 0.19 (1717.8 examples/sec; 0.075 sec/batch)
2017-06-02 10:57:54.603014: step 343620, loss = 0.19 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:57:55.480314: step 343630, loss = 0.16 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:57:56.342762: step 343640, loss = 0.14 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:57:57.213013: step 343650, loss = 0.18 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 10:57:58.096877: step 343660, loss = 0.14 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:57:58.974359: step 343670, loss = 0.17 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:57:59.825726: step 343680, loss = 0.14 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:58:00.687850: step 343690, loss = 0.14 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:58:01.673100: step 343700, loss = 0.12 (1299.2 examples/sec; 0.099 sec/batch)
2017-06-02 10:58:02.459666: step 343710, loss = 0.16 (1627.3 examples/sec; 0.079 sec/batch)
2017-06-02 10:58:03.323619: step 343720, loss = 0.24 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:58:04.166586: step 343730, loss = 0.16 (1518.5 examples/sec; 0.084 sec/batch)
2017-06-02 10:58:05.043592: step 343740, loss = 0.16 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:58:05.907765: step 343750, loss = 0.16 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:58:06.779580: step 343760, loss = 0.16 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:58:07.646715: step 343770, loss = 0.13 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:58:08.504239: step 343780, loss = 0.14 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:58:09.371194: step 343790, loss = 0.14 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:58:10.334141: step 343800, loss = 0.17 (1329.2 examples/sec; 0.096 sec/batch)
2017-06-02 10:58:11.079112: step 343810, loss = 0.18 (1718.2 examples/sec; 0.074 sec/batch)
2017-06-02 10:58:11.926382: step 343820, loss = 0.20 (1510.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:58:12.778565: step 343830, loss = 0.14 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:58:13.683817: step 343840, loss = 0.16 (1414.0 examples/sec; 0.091 sec/batch)
2017-06-02 10:58:14.551490: step 343850, loss = 0.14 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:58:15.422757: step 343860, loss = 0.17 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:58:16.307101: step 343870, loss = 0.19 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:58:17.167018: step 343880, loss = 0.19 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:58:18.027328: step 343890, loss = 0.15 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:58:19.007389: step 343900, loss = 0.20 (1306.1 examples/sec; 0.098 sec/batch)
2017-06-02 10:58:19.813901: step 343910, loss = 0.18 (1587.1 examples/sec; 0.081 sec/batch)
2017-06-02 10:58:20.668516: step 343920, loss = 0.15 (1497.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:58:21.555312: step 343930, loss = 0.14 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 10:58:22.443090: step 343940, loss = 0.18 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 10:58:23.322954: step 343950, loss = 0.18 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:58:24.179724: step 343960, loss = 0.16 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:58:25.063646: step 343970, loss = 0.18 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:58:25.934333: step 343980, loss = 0.15 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:58:26.807790: step 343990, loss = 0.15 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:58:27.785996: step 344000, loss = 0.23 (1308.5 examples/sec; 0.098 sec/batch)
2017-06-02 10:58:28.543565: step 344010, loss = 0.17 (1689.6 examples/sec; 0.076 sec/batch)
2017-06-02 10:58:29.377403: step 344020, loss = 0.17 (1535.1 examples/sec; 0.083 sec/batch)
2017-06-02 10:58:30.243357: step 344030, loss = 0.18 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:58:31.106891: step 344040, loss = 0.16 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:58:31.965369: step 344050, loss = 0.17 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:58:32.835616: step 344060, loss = 0.18 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:58:33.703067: step 344070, loss = 0.19 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:58:34.603399: step 344080, loss = 0.16 (1421.7 examples/sec; 0.090 sec/batch)
2017-06-02 10:58:35.478480: step 344090, loss = 0.15 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:58:36.443977: step 344100, loss = 0.15 (1325.7 examples/sec; 0.097 sec/batch)
2017-06-02 10:58:37.196946: step 344110, loss = 0.15 (1699.9 examples/sec; 0.075 sec/batch)
2017-06-02 10:58:38.079963: step 344120, loss = 0.14 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:58:38.947077: step 344130, loss = 0.20 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:58:39.815318: step 344140, loss = 0.23 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:58:40.690763: step 344150, loss = 0.15 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:58:41.561158: step 344160, loss = 0.15 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:58:42.442522: step 344170, loss = 0.18 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:58:43.331201: step 344180, loss = 0.14 (1440.3 examples/sec; 0.089 sec/batch)
2017-06-02 10:58:44.200246: step 344190, loss = 0.14 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:58:45.221327: step 344200, loss = 0.14 (1253.6 examples/sec; 0.102 sec/batch)
2017-06-02 10:58:45.945073: step 344210, loss = 0.23 (1768.6 examples/sec; 0.072 sec/batch)
2017-06-02 10:58:46.825091: step 344220, loss = 0.19 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:58:47.706965: step 344230, loss = 0.18 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:58:48.582078: step 344240, loss = 0.16 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:58:49.488819: step 344250, loss = 0.12 (1411.7 examples/sec; 0.091 sec/batch)
2017-06-02 10:58:50.361940: step 344260, loss = 0.18 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:58:51.210173: step 344270, loss = 0.14 (1509.0 examples/sec; 0.085 sec/batch)
2017-06-02 10:58:52.084218: step 344280, loss = 0.13 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:58:52.922656: step 344290, loss = 0.18 (1526.6 examples/sec; 0.084 sec/batch)
2017-06-02 10:58:53.931530: step 344300, loss = 0.18 (1268.7 examples/sec; 0.101 sec/batch)
2017-06-02 10:58:54.649365: step 344310, loss = 0.16 (1783.1 examples/sec; 0.072 sec/batch)
2017-06-02 10:58:55.516451: step 344320, loss = 0.13 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 10:58:56.367915: step 344330, loss = 0.16 (1503.3 examples/sec; 0.085 sec/batch)
2017-06-02 10:58:57.221140: step 344340, loss = 0.17 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:58:58.080276: step 344350, loss = 0.16 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:58:58.927554: step 344360, loss = 0.18 (1510.7 examples/sec; 0.085 sec/batch)
2017-06-02 10:58:59.790376: step 344370, loss = 0.14 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 10:59:00.626338: step 344380, loss = 0.17 (1531.2 examples/sec; 0.084 sec/batch)
2017-06-02 10:59:01.502752: step 344390, loss = 0.15 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:59:02.445039: step 344400, loss = 0.27 (1358.4 examples/sec; 0.094 sec/batch)
2017-06-02 10:59:03.227467: step 344410, loss = 0.15 (1635.9 examples/sec; 0.078 sec/batch)
2017-06-02 10:59:04.100195: step 344420, loss = 0.16 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:59:04.957684: step 344430, loss = 0.21 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 10:59:05.816843: step 344440, loss = 0.25 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:59:06.683820: step 344450, loss = 0.21 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 10:59:07.565946: step 344460, loss = 0.16 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 10:59:08.431050: step 344470, loss = 0.16 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:59:09.302114: step 344480, loss = 0.17 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:59:10.178999: step 344490, loss = 0.15 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:59:11.221232: step 344500, loss = 0.19 (1228.1 examples/sec; 0.104 sec/batch)
2017-06-02 10:59:11.903557: step 344510, loss = 0.18 (1876.0 examples/sec; 0.068 sec/batch)
2017-06-02 10:59:12.796060: step 344520, loss = 0.15 (1434.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:59:13.680084: step 344530, loss = 0.15 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 10:59:14.553814: step 344540, loss = 0.14 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:59:15.438667: step 344550, loss = 0.16 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:59:16.327433: step 344560, loss = 0.17 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:59:17.211026: step 344570, loss = 0.16 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:59:18.081246: step 344580, loss = 0.14 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 10:59:18.971325: step 344590, loss = 0.16 (1438.1 examples/sec; 0.089 sec/batch)
2017-06-02 10:59:19.962427: step 344600, loss = 0.17 (1291.5 examples/sec; 0.099 sec/batch)
2017-06-02 10:59:20.723157: step 344610, loss = 0.15 (1682.6 examples/sec; 0.076 sec/batch)
2017-06-02 10:59:21.626574: step 344620, loss = 0.17 (1416.9 examples/sec; 0.090 sec/batch)
2017-06-02 10:59:22.508626: step 344630, loss = 0.15 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 10:59:23.379307: step 344640, loss = 0.15 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:59:24.260344: step 344650, loss = 0.16 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 10:59:25.119413: step 344660, loss = 0.17 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 10:59:25.968130: step 344670, loss = 0.17 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 10:59:26.821909: step 344680, loss = 0.16 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:59:27.700023: step 344690, loss = 0.20 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 10:59:28.658700: step 344700, loss = 0.18 (1335.2 examples/sec; 0.096 sec/batch)
2017-06-02 10:59:29.440305: step 344710, loss = 0.14 (1637.7 examples/sec; 0.078 sec/batch)
2017-06-02 10:59:30.322171: step 344720, loss = 0.17 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 10:59:31.171088: step 344730, loss = 0.17 (1507.8 examples/sec; 0.085 sec/batch)
2017-06-02 10:59:32.057392: step 344740, loss = 0.25 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 10:59:32.917872: step 344750, loss = 0.14 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:59:33.783848: step 344760, loss = 0.16 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 10:59:34.653707: step 344770, loss = 0.15 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 10:59:35.515493: step 344780, loss = 0.13 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 10:59:36.399786: step 344790, loss = 0.14 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 10:59:37.412368: step 344800, loss = 0.14 (1264.1 examples/sec; 0.101 sec/batch)
2017-06-02 10:59:38.145870: step 344810, loss = 0.19 (1745.1 examples/sec; 0.073 sec/batch)
2017-06-02 10:59:38.997409: step 344820, loss = 0.18 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 10:59:39.862351: step 344830, loss = 0.19 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 10:59:40.739461: step 344840, loss = 0.18 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 10:59:41.590814: step 344850, loss = 0.15 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 10:59:42.461765: step 344860, loss = 0.18 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:59:43.283699: step 344870, loss = 0.18 (1557.3 examples/sec; 0.082 sec/batch)
2017-06-02 10:59:44.156401: step 344880, loss = 0.22 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 10:59:45.036021: step 344890, loss = 0.15 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:59:46.007149: step 344900, loss = 0.18 (1318.0 examples/sec; 0.097 sec/batch)
2017-06-02 10:59:46.795980: step 344910, loss = 0.15 (1622.7 examples/sec; 0.079 sec/batch)
2017-06-02 10:59:47.660968: step 344920, loss = 0.17 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 10:59:48.520374: step 344930, loss = 0.13 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 10:59:49.427548: step 344940, loss = 0.15 (1411.0 examples/sec; 0.091 sec/batch)
2017-06-02 10:59:50.303535: step 344950, loss = 0.18 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 10:59:51.166886: step 344960, loss = 0.20 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 10:59:52.031626: step 344970, loss = 0.18 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 10:59:52.963318: step 344980, loss = 0.16 (1373.8 examples/sec; 0.093 sec/batch)
2017-06-02 10:59:53.819427: step 344990, loss = 0.17 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 10:59:54.776541: step 345000, loss = 0.18 (1337.4 examples/sec; 0.096 sec/batch)
2017-06-02 10:59:55.553820: step 345010, loss = 0.15 (1646.8 examples/sec; 0.078 sec/batch)
2017-06-02 10:59:56.429558: step 345020, loss = 0.13 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 10:59:57.301906: step 345030, loss = 0.15 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 10:59:58.175276: step 345040, loss = 0.15 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 10:59:59.046596: step 345050, loss = 0.18 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 10:59:59.903584: step 345060, loss = 0.15 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:00:00.772567: step 345070, loss = 0.15 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:00:01.642152: step 345080, loss = 0.19 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:00:02.502058: step 345090, loss = 0.19 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:00:03.464906: step 345100, loss = 0.18 (1329.4 examples/sec; 0.096 sec/batch)
2017-06-02 11:00:04.218623: step 345110, loss = 0.16 (1698.3 examples/sec; 0.075 sec/batch)
2017-06-02 11:00:05.069146: step 345120, loss = 0.19 (1505.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:00:05.934495: step 345130, loss = 0.23 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:00:06.786140: step 345140, loss = 0.17 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:00:07.644221: step 345150, loss = 0.15 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:00:08.494579: step 345160, loss = 0.17 (1505.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:00:09.354807: step 345170, loss = 0.18 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:00:10.229279: step 345180, loss = 0.18 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:00:11.093165: step 345190, loss = 0.18 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:00:12.058224: step 345200, loss = 0.17 (1326.3 examples/sec; 0.097 sec/batch)
2017-06-02 11:00:12.811792: step 345210, loss = 0.20 (1698.6 examples/sec; 0.075 sec/batch)
2017-06-02 11:00:13.671936: step 345220, loss = 0.16 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:00:14.532004: step 345230, loss = 0.16 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:00:15.413011: step 345240, loss = 0.13 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:00:16.277528: step 345250, loss = 0.15 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:00:17.145697: step 345260, loss = 0.23 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:00:18.006468: step 345270, loss = 0.13 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:00:18.883271: step 345280, loss = 0.19 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:00:19.764321: step 345290, loss = 0.18 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:00:20.734156: step 345300, loss = 0.14 (1319.8 examples/sec; 0.097 sec/batch)
2017-06-02 11:00:21.506646: step 345310, loss = 0.13 (1657.0 examples/sec; 0.077 sec/batch)
2017-06-02 11:00:22.392797: step 345320, loss = 0.13 (1444.5 examples/sec; 0.089 sec/batch)
2017-06-02 11:00:23.258188: step 345330, loss = 0.17 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:00:24.123054: step 345340, loss = 0.20 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:00:25.019121: step 345350, loss = 0.18 (1428.5 examples/sec; 0.090 sec/batch)
2017-06-02 11:00:25.877359: step 345360, loss = 0.15 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:00:26.758240: step 345370, loss = 0.16 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:00:27.634016: step 345380, loss = 0.15 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:00:28.492212: step 345390, loss = 0.16 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:00:29.444554: step 345400, loss = 0.15 (1344.0 examples/sec; 0.095 sec/batch)
2017-06-02 11:00:30.201752: step 345410, loss = 0.15 (1690.5 examples/sec; 0.076 sec/batch)
2017-06-02 11:00:31.070081: step 345420, loss = 0.17 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:00:31.945645: step 345430, loss = 0.12 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:00:32.811139: step 345440, loss = 0.14 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:00:33.687026: step 345450, loss = 0.15 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:00:34.522378: step 345460, loss = 0.18 (1532.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:00:35.367099: step 345470, loss = 0.14 (1515.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:00:36.240175: step 345480, loss = 0.14 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:00:37.116321: step 345490, loss = 0.18 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:00:38.085430: step 345500, loss = 0.14 (1320.8 examples/sec; 0.097 sec/batch)
2017-06-02 11:00:38.835306: step 345510, loss = 0.19 (1706.9 examples/sec; 0.075 sec/batch)
2017-06-02 11:00:39.677517: step 345520, loss = 0.16 (1519.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:00:40.518569: step 345530, loss = 0.13 (1521.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:00:41.382108: step 345540, loss = 0.19 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:00:42.250805: step 345550, loss = 0.14 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:00:43.111527: step 345560, loss = 0.17 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:00:43.954396: step 345570, loss = 0.22 (1518.6 examples/sec; 0.084 sec/batch)
2017-06-02 11:00:44.813523: step 345580, loss = 0.13 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:00:45.656825: step 345590, loss = 0.17 (1517.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:00:46.614399: step 345600, loss = 0.14 (1336.7 examples/sec; 0.096 sec/batch)
2017-06-02 11:00:47.371367: step 345610, loss = 0.19 (1691.0 examples/sec; 0.076 sec/batch)
2017-06-02 11:00:48.214496: step 345620, loss = 0.20 (1518.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:00:49.058576: step 345630, loss = 0.15 (1516.4 examples/sec; 0.084 sec/batch)
2017-06-02 11:00:49.910621: step 345640, loss = 0.16 (1502.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:00:50.755253: step 345650, loss = 0.19 (1515.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:00:51.611168: step 345660, loss = 0.20 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:00:52.482009: step 345670, loss = 0.14 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:00:53.347629: step 345680, loss = 0.14 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:00:54.227663: step 345690, loss = 0.22 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:00:55.243410: step 345700, loss = 0.18 (1260.2 examples/sec; 0.102 sec/batch)
2017-06-02 11:00:55.982600: step 345710, loss = 0.14 (1731.6 examples/sec; 0.074 sec/batch)
2017-06-02 11:00:56.831767: step 345720, loss = 0.14 (1507.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:00:57.692425: step 345730, loss = 0.21 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:00:58.559831: step 345740, loss = 0.18 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:00:59.431857: step 345750, loss = 0.14 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:01:00.284253: step 345760, loss = 0.20 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:01:01.139895: step 345770, loss = 0.15 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:01:01.982031: step 345780, loss = 0.12 (1519.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:01:02.830553: step 345790, loss = 0.16 (1508.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:01:03.823982: step 345800, loss = 0.16 (1288.5 examples/sec; 0.099 sec/batch)
2017-06-02 11:01:04.548271: step 345810, loss = 0.14 (1767.3 examples/sec; 0.072 sec/batch)
2017-06-02 11:01:05.402820: step 345820, loss = 0.15 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:01:06.258042: step 345830, loss = 0.18 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:01:07.142750: step 345840, loss = 0.18 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:01:07.990379: step 345850, loss = 0.15 (1510.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:01:08.874752: step 345860, loss = 0.15 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:01:09.743485: step 345870, loss = 0.17 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:01:10.598768: step 345880, loss = 0.21 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:01:11.450187: step 345890, loss = 0.15 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:01:12.430442: step 345900, loss = 0.18 (1305.8 examples/sec; 0.098 sec/batch)
2017-06-02 11:01:13.193513: step 345910, loss = 0.16 (1677.4 examples/sec; 0.076 sec/batch)
2017-06-02 11:01:14.034603: step 345920, loss = 0.16 (1521.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:01:14.897041: step 345930, loss = 0.14 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:01:15.786881: step 345940, loss = 0.17 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 11:01:16.653682: step 345950, loss = 0.15 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:01:17.501375: step 345960, loss = 0.20 (1510.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:01:18.366889: step 345970, loss = 0.17 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:01:19.234972: step 345980, loss = 0.15 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:01:20.111698: step 345990, loss = 0.15 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:01:21.070858: step 346000, loss = 0.22 (1334.5 examples/sec; 0.096 sec/batch)
2017-06-02 11:01:21.857035: step 346010, loss = 0.15 (1628.2 examples/sec; 0.079 sec/batch)
2017-06-02 11:01:22.705376: step 346020, loss = 0.17 (1508.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:01:23.570029: step 346030, loss = 0.20 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:01:24.443283: step 346040, loss = 0.16 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:01:25.291246: step 346050, loss = 0.18 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:01:26.162496: step 346060, loss = 0.21 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:01:27.043335: step 346070, loss = 0.18 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:01:27.929676: step 346080, loss = 0.15 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 11:01:28.798450: step 346090, loss = 0.19 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:01:29.765500: step 346100, loss = 0.14 (1323.6 examples/sec; 0.097 sec/batch)
2017-06-02 11:01:30.520932: step 346110, loss = 0.17 (1694.4 examples/sec; 0.076 sec/batch)
2017-06-02 11:01:31.392344: step 346120, loss = 0.19 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:01:32.276850: step 346130, loss = 0.19 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:01:33.155963: step 346140, loss = 0.19 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:01:34.028148: step 346150, loss = 0.16 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:01:34.916311: step 346160, loss = 0.19 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 11:01:35.804249: step 346170, loss = 0.16 (1441.5 examples/sec; 0.089 sec/batch)
2017-06-02 11:01:36.645644: step 346180, loss = 0.15 (1521.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:01:37.502515: step 346190, loss = 0.19 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:01:38.471424: step 346200, loss = 0.17 (1321.1 examples/sec; 0.097 sec/batch)
2017-06-02 11:01:39.246541: step 346210, loss = 0.14 (1651.4 examples/sec; 0.078 sec/batch)
2017-06-02 11:01:40.125095: step 346220, loss = 0.17 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:01:40.992180: step 346230, loss = 0.19 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:01:41.853922: step 346240, loss = 0.15 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:01:42.738430: step 346250, loss = 0.14 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:01:43.609667: step 346260, loss = 0.16 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:01:44.458583: step 346270, loss = 0.19 (1507.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:01:45.299325: step 346280, loss = 0.14 (1522.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:01:46.164595: step 346290, loss = 0.15 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:01:47.138940: step 346300, loss = 0.14 (1313.7 examples/sec; 0.097 sec/batch)
2017-06-02 11:01:47.891710: step 346310, loss = 0.19 (1700.4 examples/sec; 0.075 sec/batch)
2017-06-02 11:01:48.752208: step 346320, loss = 0.13 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:01:49.639622: step 346330, loss = 0.15 (1442.4 examples/sec; 0.089 sec/batch)
2017-06-02 11:01:50.495856: step 346340, loss = 0.22 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:01:51.357402: step 346350, loss = 0.15 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:01:52.216494: step 346360, loss = 0.18 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:01:53.089977: step 346370, loss = 0.18 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:01:53.957080: step 346380, loss = 0.18 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:01:54.812042: step 346390, loss = 0.16 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:01:55.809047: step 346400, loss = 0.15 (1283.8 examples/sec; 0.100 sec/batch)
2017-06-02 11:01:56.569267: step 346410, loss = 0.14 (1683.7 examples/sec; 0.076 sec/batch)
2017-06-02 11:01:57.448957: step 346420, loss = 0.14 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:01:58.345757: step 346430, loss = 0.18 (1427.3 examples/sec; 0.090 sec/batch)
2017-06-02 11:01:59.208275: step 346440, loss = 0.15 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:02:00.087568: step 346450, loss = 0.13 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:02:00.940450: step 346460, loss = 0.15 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:02:01.826291: step 346470, loss = 0.18 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 11:02:02.704314: step 346480, loss = 0.11 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:02:03.533405: step 346490, loss = 0.15 (1543.9 examples/sec; 0.083 sec/batch)
2017-06-02 11:02:04.511905: step 346500, loss = 0.14 (1308.1 examples/sec; 0.098 sec/batch)
2017-06-02 11:02:05.262080: step 346510, loss = 0.19 (1706.3 examples/sec; 0.075 sec/batch)
2017-06-02 11:02:06.141112: step 346520, loss = 0.21 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:02:06.997449: step 346530, loss = 0.14 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:02:07.866288: step 346540, loss = 0.14 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:02:08.719575: step 346550, loss = 0.21 (1500.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:02:09.563903: step 346560, loss = 0.16 (1516.0 examples/sec; 0.084 sec/batch)
2017-06-02 11:02:10.437234: step 346570, loss = 0.15 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:02:11.284403: step 346580, loss = 0.18 (1510.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:02:12.135034: step 346590, loss = 0.18 (1504.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:02:13.087621: step 346600, loss = 0.17 (1343.7 examples/sec; 0.095 sec/batch)
2017-06-02 11:02:13.873757: step 346610, loss = 0.22 (1628.2 examples/sec; 0.079 sec/batch)
2017-06-02 11:02:14.731022: step 346620, loss = 0.18 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:02:15.615094: step 346630, loss = 0.18 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:02:16.476230: step 346640, loss = 0.18 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:02:17.357796: step 346650, loss = 0.16 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:02:18.189653: step 346660, loss = 0.18 (1538.7 examples/sec; 0.083 sec/batch)
2017-06-02 11:02:19.051910: step 346670, loss = 0.16 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:02:19.908929: step 346680, loss = 0.17 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:02:20.797500: step 346690, loss = 0.15 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 11:02:21.799921: step 346700, loss = 0.16 (1276.9 examples/sec; 0.100 sec/batch)
2017-06-02 11:02:22.551223: step 346710, loss = 0.20 (1703.7 examples/sec; 0.075 sec/batch)
2017-06-02 11:02:23.414403: step 346720, loss = 0.18 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:02:24.255801: step 346730, loss = 0.15 (1521.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:02:25.127454: step 346740, loss = 0.13 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:02:25.975064: step 346750, loss = 0.16 (1510.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:02:26.835211: step 346760, loss = 0.15 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:02:27.714691: step 346770, loss = 0.15 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:02:28.587900: step 346780, loss = 0.15 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:02:29.447836: step 346790, loss = 0.12 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:02:30.410856: step 346800, loss = 0.12 (1329.1 examples/sec; 0.096 sec/batch)
2017-06-02 11:02:31.170066: step 346810, loss = 0.23 (1686.0 examples/sec; 0.076 sec/batch)
2017-06-02 11:02:32.036107: step 346820, loss = 0.17 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:02:32.902534: step 346830, loss = 0.17 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:02:33.768064: step 346840, loss = 0.17 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:02:34.630766: step 346850, loss = 0.15 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:02:35.508398: step 346860, loss = 0.16 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:02:36.398651: step 346870, loss = 0.19 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 11:02:37.270076: step 346880, loss = 0.14 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:02:38.156113: step 346890, loss = 0.19 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:02:39.134106: step 346900, loss = 0.17 (1308.8 examples/sec; 0.098 sec/batch)
2017-06-02 11:02:39.903002: step 346910, loss = 0.14 (1664.8 examples/sec; 0.077 sec/batch)
2017-06-02 11:02:40.765545: step 346920, loss = 0.19 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:02:41.613993: step 346930, loss = 0.19 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:02:42.490837: step 346940, loss = 0.18 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:02:43.379460: step 346950, loss = 0.17 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 11:02:44.247814: step 346960, loss = 0.15 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:02:45.088682: step 346970, loss = 0.16 (1522.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:02:45.953262: step 346980, loss = 0.14 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:02:46.822176: step 346990, loss = 0.12 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:02:47.817087: step 347000, loss = 0.21 (1286.6 examples/sec; 0.099 sec/batch)
2017-06-02 11:02:48.542608: step 347010, loss = 0.15 (1764.3 examples/sec; 0.073 sec/batch)
2017-06-02 11:02:49.397431: step 347020, loss = 0.17 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:02:50.282586: step 347030, loss = 0.14 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 11:02:51.165710: step 347040, loss = 0.19 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:02:52.051386: step 347050, loss = 0.15 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 11:02:52.935534: step 347060, loss = 0.18 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:02:53.817267: step 347070, loss = 0.18 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:02:54.698389: step 347080, loss = 0.13 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:02:55.557922: step 347090, loss = 0.17 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:02:56.532124: step 347100, loss = 0.20 (1313.9 examples/sec; 0.097 sec/batch)
2017-06-02 11:02:57.323384: step 347110, loss = 0.21 (1617.7 examples/sec; 0.079 sec/batch)
2017-06-02 11:02:58.201131: step 347120, loss = 0.12 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:02:59.056601: step 347130, loss = 0.17 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:02:59.955496: step 347140, loss = 0.18 (1424.0 examples/sec; 0.090 sec/batch)
2017-06-02 11:03:00.839108: step 347150, loss = 0.18 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:03:01.729319: step 347160, loss = 0.21 (1437.9 examples/sec; 0.089 sec/batch)
2017-06-02 11:03:02.609565: step 347170, loss = 0.15 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:03:03.476669: step 347180, loss = 0.17 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:03:04.315370: step 347190, loss = 0.17 (1526.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:03:05.326241: step 347200, loss = 0.17 (1266.2 examples/sec; 0.101 sec/batch)
2017-06-02 11:03:06.100741: step 347210, loss = 0.17 (1652.7 examples/sec; 0.077 sec/batch)
2017-06-02 11:03:06.972598: step 347220, loss = 0.16 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:03:07.870609: step 347230, loss = 0.16 (1425.4 examples/sec; 0.090 sec/batch)
2017-06-02 11:03:08.761607: step 347240, loss = 0.15 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:03:09.623813: step 347250, loss = 0.14 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:03:10.493010: step 347260, loss = 0.21 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:03:11.378573: step 347270, loss = 0.14 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 11:03:12.252537: step 347280, loss = 0.23 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:03:13.137694: step 347290, loss = 0.14 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 11:03:14.121467: step 347300, loss = 0.14 (1301.1 examples/sec; 0.098 sec/batch)
2017-06-02 11:03:14.889403: step 347310, loss = 0.16 (1666.8 examples/sec; 0.077 sec/batch)
2017-06-02 11:03:15.754111: step 347320, loss = 0.16 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:03:16.626069: step 347330, loss = 0.15 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:03:17.486432: step 347340, loss = 0.18 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:03:18.332994: step 347350, loss = 0.17 (1512.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:03:19.193142: step 347360, loss = 0.15 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:03:20.064093: step 347370, loss = 0.18 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:03:20.913622: step 347380, loss = 0.17 (1506.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:03:21.792250: step 347390, loss = 0.15 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:03:22.772054: step 347400, loss = 0.19 (1306.4 examples/sec; 0.098 sec/batch)
2017-06-02 11:03:23.537784: step 347410, loss = 0.15 (1671.6 examples/sec; 0.077 sec/batch)
2017-06-02 11:03:24.416613: step 347420, loss = 0.14 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:03:25.280066: step 347430, loss = 0.17 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:03:26.139771: step 347440, loss = 0.18 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:03:27.014394: step 347450, loss = 0.15 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:03:27.903680: step 347460, loss = 0.15 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 11:03:28.751078: step 347470, loss = 0.20 (1510.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:03:29.631160: step 347480, loss = 0.16 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:03:30.479790: step 347490, loss = 0.18 (1508.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:03:31.522516: step 347500, loss = 0.15 (1227.6 examples/sec; 0.104 sec/batch)
2017-06-02 11:03:32.247891: step 347510, loss = 0.18 (1764.6 examples/sec; 0.073 sec/batch)
2017-06-02 11:03:33.102223: step 347520, loss = 0.13 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:03:33.953789: step 347530, loss = 0.14 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:03:34.804543: step 347540, loss = 0.19 (1504.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:03:35.653478: step 347550, loss = 0.16 (1507.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:03:36.521037: step 347560, loss = 0.17 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:03:37.370907: step 347570, loss = 0.15 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:03:38.247794: step 347580, loss = 0.14 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:03:39.110005: step 347590, loss = 0.15 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:03:40.086167: step 347600, loss = 0.14 (1311.3 examples/sec; 0.098 sec/batch)
2017-06-02 11:03:40.850714: step 347610, loss = 0.17 (1674.2 examples/sec; 0.076 sec/batch)
2017-06-02 11:03:41.727985: step 347620, loss = 0.18 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:03:42.581888: step 347630, loss = 0.20 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:03:43.409242: step 347640, loss = 0.18 (1547.1 examples/sec; 0.083 sec/batch)
2017-06-02 11:03:44.270774: step 347650, loss = 0.18 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:03:45.141267: step 347660, loss = 0.16 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:03:46.020260: step 347670, loss = 0.18 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:03:46.872776: step 347680, loss = 0.19 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:03:47.751317: step 347690, loss = 0.12 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:03:48.702848: step 347700, loss = 0.13 (1345.2 examples/sec; 0.095 sec/batch)
2017-06-02 11:03:49.454811: step 347710, loss = 0.20 (1702.2 examples/sec; 0.075 sec/batch)
2017-06-02 11:03:50.301332: step 347720, loss = 0.16 (1512.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:03:51.133171: step 347730, loss = 0.18 (1538.8 examples/sec; 0.083 sec/batch)
2017-06-02 11:03:52.025007: step 347740, loss = 0.16 (1435.2 examples/sec; 0.089 sec/batch)
2017-06-02 11:03:52.906227: step 347750, loss = 0.22 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:03:53.751683: step 347760, loss = 0.17 (1514.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:03:54.621816: step 347770, loss = 0.16 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:03:55.467402: step 347780, loss = 0.20 (1513.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:03:56.319108: step 347790, loss = 0.20 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:03:57.291086: step 347800, loss = 0.16 (1316.9 examples/sec; 0.097 sec/batch)
2017-06-02 11:03:58.050930: step 347810, loss = 0.18 (1684.6 examples/sec; 0.076 sec/batch)
2017-06-02 11:03:58.932117: step 347820, loss = 0.19 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:03:59.791369: step 347830, loss = 0.20 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:04:00.623744: step 347840, loss = 0.15 (1537.8 examples/sec; 0.083 sec/batch)
2017-06-02 11:04:01.522676: step 347850, loss = 0.23 (1423.9 examples/sec; 0.090 sec/batch)
2017-06-02 11:04:02.380729: step 347860, loss = 0.19 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:04:03.275309: step 347870, loss = 0.16 (1430.8 examples/sec; 0.089 sec/batch)
2017-06-02 11:04:04.136886: step 347880, loss = 0.17 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:04:05.011582: step 347890, loss = 0.14 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:04:06.004063: step 347900, loss = 0.17 (1289.7 examples/sec; 0.099 sec/batch)
2017-06-02 11:04:06.744451: step 347910, loss = 0.21 (1728.8 examples/sec; 0.074 sec/batch)
2017-06-02 11:04:07.596652: step 347920, loss = 0.12 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:04:08.476257: step 347930, loss = 0.16 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:04:09.315197: step 347940, loss = 0.18 (1525.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:04:10.170904: step 347950, loss = 0.18 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:04:11.031965: step 347960, loss = 0.14 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:04:11.918798: step 347970, loss = 0.17 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 11:04:12.795102: step 347980, loss = 0.15 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:04:13.657099: step 347990, loss = 0.21 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:04:14.620530: step 348000, loss = 0.20 (1328.6 examples/sec; 0.096 sec/batch)
2017-06-02 11:04:15.359505: step 348010, loss = 0.16 (1732.2 examples/sec; 0.074 sec/batch)
2017-06-02 11:04:16.224773: step 348020, loss = 0.16 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:04:17.101434: step 348030, loss = 0.15 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:04:17.979806: step 348040, loss = 0.13 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:04:18.821165: step 348050, loss = 0.14 (1521.4 examples/sec; 0.084 sec/batch)
2017-06-02 11:04:19.691498: step 348060, loss = 0.15 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:04:20.558724: step 348070, loss = 0.14 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:04:21.421265: step 348080, loss = 0.17 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:04:22.273138: step 348090, loss = 0.14 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:04:23.222210: step 348100, loss = 0.16 (1348.7 examples/sec; 0.095 sec/batch)
2017-06-02 11:04:23.984814: step 348110, loss = 0.13 (1678.5 examples/sec; 0.076 sec/batch)
2017-06-02 11:04:24.816102: step 348120, loss = 0.19 (1539.8 examples/sec; 0.083 sec/batch)
2017-06-02 11:04:25.666662: step 348130, loss = 0.17 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:04:26.535977: step 348140, loss = 0.15 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:04:27.384796: step 348150, loss = 0.16 (1508.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:04:28.233559: step 348160, loss = 0.20 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:04:29.098313: step 348170, loss = 0.12 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:04:29.967248: step 348180, loss = 0.17 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:04:30.842252: step 348190, loss = 0.15 (1462.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:04:31.827202: step 348200, loss = 0.16 (1299.6 examples/sec; 0.098 sec/batch)
2017-06-02 11:04:32.594847: step 348210, loss = 0.18 (1667.4 examples/sec; 0.077 sec/batch)
2017-06-02 11:04:33.455229: step 348220, loss = 0.20 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:04:34.312302: step 348230, loss = 0.17 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:04:35.184458: step 348240, loss = 0.14 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:04:36.039383: step 348250, loss = 0.15 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:04:36.922124: step 348260, loss = 0.15 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:04:37.784967: step 348270, loss = 0.17 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:04:38.662641: step 348280, loss = 0.19 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:04:39.544606: step 348290, loss = 0.20 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:04:40.513913: step 348300, loss = 0.16 (1320.6 examples/sec; 0.097 sec/batch)
2017-06-02 11:04:41.277962: step 348310, loss = 0.19 (1675.3 examples/sec; 0.076 sec/batch)
2017-06-02 11:04:42.146994: step 348320, loss = 0.18 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:04:42.994971: step 348330, loss = 0.16 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:04:43.872599: step 348340, loss = 0.15 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:04:44.738267: step 348350, loss = 0.18 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:04:45.575417: step 348360, loss = 0.15 (1529.0 examples/sec; 0.084 sec/batch)
2017-06-02 11:04:46.424295: step 348370, loss = 0.18 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:04:47.291209: step 348380, loss = 0.16 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:04:48.151345: step 348390, loss = 0.15 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:04:49.105012: step 348400, loss = 0.15 (1342.2 examples/sec; 0.095 sec/batch)
2017-06-02 11:04:49.847741: step 348410, loss = 0.18 (1723.4 examples/sec; 0.074 sec/batch)
2017-06-02 11:04:50.682532: step 348420, loss = 0.16 (1533.3 examples/sec; 0.083 sec/batch)
2017-06-02 11:04:51.522908: step 348430, loss = 0.15 (1523.1 examples/sec; 0.084 sec/batch)
2017-06-02 11:04:52.405481: step 348440, loss = 0.16 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:04:53.258897: step 348450, loss = 0.19 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:04:54.124304: step 348460, loss = 0.18 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:04:54.981144: step 348470, loss = 0.17 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:04:55.827999: step 348480, loss = 0.14 (1511.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:04:56.666315: step 348490, loss = 0.19 (1526.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:04:57.674622: step 348500, loss = 0.13 (1269.5 examples/sec; 0.101 sec/batch)
2017-06-02 11:04:58.381513: step 348510, loss = 0.13 (1810.7 examples/sec; 0.071 sec/batch)
2017-06-02 11:04:59.249794: step 348520, loss = 0.21 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:05:00.110063: step 348530, loss = 0.22 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:05:00.932680: step 348540, loss = 0.18 (1556.0 examples/sec; 0.082 sec/batch)
2017-06-02 11:05:01.782503: step 348550, loss = 0.19 (1506.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:05:02.648247: step 348560, loss = 0.14 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:05:03.515703: step 348570, loss = 0.16 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:05:04.357988: step 348580, loss = 0.15 (1519.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:05:05.211335: step 348590, loss = 0.16 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:05:06.173506: step 348600, loss = 0.17 (1330.3 examples/sec; 0.096 sec/batch)
2017-06-02 11:05:06.942195: step 348610, loss = 0.14 (1665.2 examples/sec; 0.077 sec/batch)
2017-06-02 11:05:07.830399: step 348620, loss = 0.17 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 11:05:08.680125: step 348630, loss = 0.15 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:05:09.556011: step 348640, loss = 0.12 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:05:10.395352: step 348650, loss = 0.14 (1525.0 examples/sec; 0.084 sec/batch)
2017-06-02 11:05:11.239503: step 348660, loss = 0.15 (1516.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:05:12.081361: step 348670, loss = 0.12 (1520.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:05:12.943791: step 348680, loss = 0.19 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:05:13.804621: step 348690, loss = 0.19 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:05:14.763850: step 348700, loss = 0.19 (1334.4 examples/sec; 0.096 sec/batch)
2017-06-02 11:05:15.541242: step 348710, loss = 0.18 (1646.5 examples/sec; 0.078 sec/batch)
2017-06-02 11:05:16.395216: step 348720, loss = 0.17 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:05:17.265460: step 348730, loss = 0.19 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:05:18.148380: step 348740, loss = 0.23 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:05:19.004491: step 348750, loss = 0.13 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:05:19.857509: step 348760, loss = 0.15 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:05:20.748013: step 348770, loss = 0.16 (1437.4 examples/sec; 0.089 sec/batch)
2017-06-02 11:05:21.623489: step 348780, loss = 0.13 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:05:22.500182: step 348790, loss = 0.17 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:05:23.501202: step 348800, loss = 0.16 (1278.7 examples/sec; 0.100 sec/batch)
2017-06-02 11:05:24.271511: step 348810, loss = 0.14 (1661.7 examples/sec; 0.077 sec/batch)
2017-06-02 11:05:25.139623: step 348820, loss = 0.13 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:05:26.001769: step 348830, loss = 0.17 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:05:26.864234: step 348840, loss = 0.15 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:05:27.723417: step 348850, loss = 0.15 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:05:28.592161: step 348860, loss = 0.16 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:05:29.455670: step 348870, loss = 0.15 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:05:30.298532: step 348880, loss = 0.17 (1518.6 examples/sec; 0.084 sec/batch)
2017-06-02 11:05:31.140372: step 348890, loss = 0.22 (1520.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:05:32.123515: step 348900, loss = 0.17 (1301.9 examples/sec; 0.098 sec/batch)
2017-06-02 11:05:32.901312: step 348910, loss = 0.14 (1645.7 examples/sec; 0.078 sec/batch)
2017-06-02 11:05:33.744929: step 348920, loss = 0.19 (1517.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:05:34.612297: step 348930, loss = 0.13 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:05:35.471187: step 348940, loss = 0.14 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:05:36.345611: step 348950, loss = 0.15 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:05:37.216884: step 348960, loss = 0.16 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:05:38.085441: step 348970, loss = 0.13 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:05:38.943777: step 348980, loss = 0.17 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:05:39.821849: step 348990, loss = 0.13 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:05:40.778278: step 349000, loss = 0.20 (1338.3 examples/sec; 0.096 sec/batch)
2017-06-02 11:05:41.540155: step 349010, loss = 0.15 (1680.0 examples/sec; 0.076 sec/batch)
2017-06-02 11:05:42.413926: step 349020, loss = 0.18 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:05:43.281374: step 349030, loss = 0.15 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:05:44.149656: step 349040, loss = 0.18 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:05:45.006556: step 349050, loss = 0.16 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:05:45.882005: step 349060, loss = 0.14 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:05:46.760593: step 349070, loss = 0.17 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:05:47.620072: step 349080, loss = 0.14 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:05:48.510676: step 349090, loss = 0.13 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 11:05:49.522445: step 349100, loss = 0.14 (1265.1 examples/sec; 0.101 sec/batch)
2017-06-02 11:05:50.245566: step 349110, loss = 0.13 (1770.1 examples/sec; 0.072 sec/batch)
2017-06-02 11:05:51.111421: step 349120, loss = 0.17 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:05:51.983419: step 349130, loss = 0.14 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:05:52.839905: step 349140, loss = 0.17 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:05:53.718838: step 349150, loss = 0.15 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:05:54.579993: step 349160, loss = 0.16 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:05:55.434060: step 349170, loss = 0.16 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:05:56.312423: step 349180, loss = 0.16 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:05:57.199816: step 349190, loss = 0.20 (1442.4 examples/sec; 0.089 sec/batch)
2017-06-02 11:05:58.194641: step 349200, loss = 0.16 (1286.7 examples/sec; 0.099 sec/batch)
2017-06-02 11:05:58.944459: step 349210, loss = 0.13 (1707.1 examples/sec; 0.075 sec/batch)
2017-06-02 11:05:59.783967: step 349220, loss = 0.15 (1524.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:06:00.640996: step 349230, loss = 0.18 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:06:01.491884: step 349240, loss = 0.14 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:06:02.372719: step 349250, loss = 0.16 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:06:03.235169: step 349260, loss = 0.18 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:06:04.110132: step 349270, loss = 0.22 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:06:04.973861: step 349280, loss = 0.22 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:06:05.864168: step 349290, loss = 0.21 (1437.7 examples/sec; 0.089 sec/batch)
2017-06-02 11:06:06.851113: step 349300, loss = 0.13 (1296.9 examples/sec; 0.099 sec/batch)
2017-06-02 11:06:07.603580: step 349310, loss = 0.15 (1701.1 examples/sec; 0.075 sec/batch)
2017-06-02 11:06:08.506410: step 349320, loss = 0.17 (1417.8 examples/sec; 0.090 sec/batch)
2017-06-02 11:06:09.382804: step 349330, loss = 0.13 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:06:10.256959: step 349340, loss = 0.17 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:06:11.140486: step 349350, loss = 0.14 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:06:12.016652: step 349360, loss = 0.19 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:06:12.868069: step 349370, loss = 0.23 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:06:13.709209: step 349380, loss = 0.12 (1521.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:06:14.573381: step 349390, loss = 0.15 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:06:15.533798: step 349400, loss = 0.14 (1332.7 examples/sec; 0.096 sec/batch)
2017-06-02 11:06:16.312472: step 349410, loss = 0.15 (1643.8 examples/sec; 0.078 sec/batch)
2017-06-02 11:06:17.173788: step 349420, loss = 0.19 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:06:18.044163: step 349430, loss = 0.16 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:06:18.920154: step 349440, loss = 0.16 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:06:19.801118: step 349450, loss = 0.19 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:06:20.688318: step 349460, loss = 0.22 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 11:06:21.561374: step 349470, loss = 0.13 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:06:22.449042: step 349480, loss = 0.23 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 11:06:23.321629: step 349490, loss = 0.12 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:06:24.290740: step 349500, loss = 0.23 (1320.8 examples/sec; 0.097 sec/batch)
2017-06-02 11:06:25.033958: step 349510, loss = 0.17 (1722.2 examples/sec; 0.074 sec/batch)
2017-06-02 11:06:25.878298: step 349520, loss = 0.17 (1516.0 examples/sec; 0.084 sec/batch)
2017-06-02 11:06:26.740679: step 349530, loss = 0.15 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:06:27.635814: step 349540, loss = 0.18 (1430.0 examples/sec; 0.090 sec/batch)
2017-06-02 11:06:28.503350: step 349550, loss = 0.16 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:06:29.364726: step 349560, loss = 0.14 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:06:30.208627: step 349570, loss = 0.13 (1516.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:06:31.055791: step 349580, loss = 0.17 (1510.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:06:31.924328: step 349590, loss = 0.19 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:06:32.876680: step 349600, loss = 0.16 (1344.0 examples/sec; 0.095 sec/batch)
2017-06-02 11:06:33.645065: step 349610, loss = 0.16 (1665.8 examples/sec; 0.077 sec/batch)
2017-06-02 11:06:34.488943: step 349620, loss = 0.16 (1516.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:06:35.369641: step 349630, loss = 0.13 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:06:36.249001: step 349640, loss = 0.20 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:06:37.126786: step 349650, loss = 0.15 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:06:37.975629: step 349660, loss = 0.15 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:06:38.855433: step 349670, loss = 0.16 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:06:39.730192: step 349680, loss = 0.14 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:06:40.604160: step 349690, loss = 0.15 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:06:41.581718: step 349700, loss = 0.18 (1309.4 examples/sec; 0.098 sec/batch)
2017-06-02 11:06:42.335949: step 349710, loss = 0.15 (1697.1 examples/sec; 0.075 sec/batch)
2017-06-02 11:06:43.214096: step 349720, loss = 0.22 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:06:44.087825: step 349730, loss = 0.17 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:06:44.944029: step 349740, loss = 0.18 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:06:45.826464: step 349750, loss = 0.13 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:06:46.716328: step 349760, loss = 0.17 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 11:06:47.607048: step 349770, loss = 0.17 (1437.0 examples/sec; 0.089 sec/batch)
2017-06-02 11:06:48.485156: step 349780, loss = 0.16 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:06:49.367864: step 349790, loss = 0.15 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:06:50.354580: step 349800, loss = 0.22 (1297.2 examples/sec; 0.099 sec/batch)
2017-06-02 11:06:51.138382: step 349810, loss = 0.16 (1633.1 examples/sec; 0.078 sec/batch)
2017-06-02 11:06:52.015383: step 349820, loss = 0.19 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:06:52.875232: step 349830, loss = 0.16 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:06:53.750851: step 349840, loss = 0.17 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:06:54.617341: step 349850, loss = 0.15 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:06:55.492928: step 349860, loss = 0.18 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:06:56.365211: step 349870, loss = 0.19 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:06:57.229190: step 349880, loss = 0.14 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:06:58.091305: step 349890, loss = 0.14 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:06:59.067614: step 349900, loss = 0.15 (1311.1 examples/sec; 0.098 sec/batch)
2017-06-02 11:06:59.819498: step 349910, loss = 0.20 (1702.4 examples/sec; 0.075 sec/batch)
2017-06-02 11:07:00.693864: step 349920, loss = 0.14 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:07:01.528841: step 349930, loss = 0.23 (1533.0 examples/sec; 0.083 sec/batch)
2017-06-02 11:07:02.403371: step 349940, loss = 0.18 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:07:03.241888: step 349950, loss = 0.18 (1526.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:07:04.102480: step 349960, loss = 0.15 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:07:04.943847: step 349970, loss = 0.23 (1521.4 examples/sec; 0.084 sec/batch)
2017-06-02 11:07:05.822586: step 349980, loss = 0.18 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:07:06.675913: step 349990, loss = 0.14 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:07:07.636874: step 350000, loss = 0.15 (1332.0 examples/sec; 0.096 sec/batch)
2017-06-02 11:07:08.368221: step 350010, loss = 0.14 (1750.2 examples/sec; 0.073 sec/batch)
2017-06-02 11:07:09.242787: step 350020, loss = 0.13 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:07:10.117607: step 350030, loss = 0.22 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:07:10.970279: step 350040, loss = 0.12 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:07:11.818171: step 350050, loss = 0.18 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:07:12.676341: step 350060, loss = 0.18 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:07:13.549450: step 350070, loss = 0.17 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:07:14.431891: step 350080, loss = 0.18 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:07:15.292996: step 350090, loss = 0.15 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:07:16.238456: step 350100, loss = 0.12 (1353.8 examples/sec; 0.095 sec/batch)
2017-06-02 11:07:17.028773: step 350110, loss = 0.15 (1619.7 examples/sec; 0.079 sec/batch)
2017-06-02 11:07:17.874215: step 350120, loss = 0.16 (1513.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:07:18.727070: step 350130, loss = 0.14 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:07:19.610954: step 350140, loss = 0.16 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:07:20.471460: step 350150, loss = 0.16 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:07:21.342551: step 350160, loss = 0.16 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:07:22.234030: step 350170, loss = 0.16 (1435.8 examples/sec; 0.089 sec/batch)
2017-06-02 11:07:23.095560: step 350180, loss = 0.20 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:07:23.943680: step 350190, loss = 0.16 (1509.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:07:24.922121: step 350200, loss = 0.16 (1308.2 examples/sec; 0.098 sec/batch)
2017-06-02 11:07:25.687988: step 350210, loss = 0.14 (1671.4 examples/sec; 0.077 sec/batch)
2017-06-02 11:07:26.541665: step 350220, loss = 0.13 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:07:27.392579: step 350230, loss = 0.21 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:07:28.266489: step 350240, loss = 0.15 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:07:29.122752: step 350250, loss = 0.12 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:07:29.986372: step 350260, loss = 0.16 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:07:30.843379: step 350270, loss = 0.19 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:07:31.702250: step 350280, loss = 0.19 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:07:32.556719: step 350290, loss = 0.16 (1498.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:07:33.536809: step 350300, loss = 0.18 (1306.0 examples/sec; 0.098 sec/batch)
2017-06-02 11:07:34.280353: step 350310, loss = 0.15 (1721.5 examples/sec; 0.074 sec/batch)
2017-06-02 11:07:35.151972: step 350320, loss = 0.19 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:07:36.031943: step 350330, loss = 0.18 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:07:36.901234: step 350340, loss = 0.20 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:07:37.785853: step 350350, loss = 0.16 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:07:38.661935: step 350360, loss = 0.14 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:07:39.514922: step 350370, loss = 0.15 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:07:40.391556: step 350380, loss = 0.18 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:07:41.266871: step 350390, loss = 0.19 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:07:42.245375: step 350400, loss = 0.14 (1308.1 examples/sec; 0.098 sec/batch)
2017-06-02 11:07:43.017303: step 350410, loss = 0.13 (1658.2 examples/sec; 0.077 sec/batch)
2017-06-02 11:07:43.870018: step 350420, loss = 0.20 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:07:44.738304: step 350430, loss = 0.20 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:07:45.593295: step 350440, loss = 0.14 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:07:46.477845: step 350450, loss = 0.20 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:07:47.361483: step 350460, loss = 0.19 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:07:48.217698: step 350470, loss = 0.19 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:07:49.076159: step 350480, loss = 0.16 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:07:49.960017: step 350490, loss = 0.14 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:07:50.942125: step 350500, loss = 0.24 (1303.3 examples/sec; 0.098 sec/batch)
2017-06-02 11:07:51.686441: step 350510, loss = 0.17 (1719.7 examples/sec; 0.074 sec/batch)
2017-06-02 11:07:52.554425: step 350520, loss = 0.16 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:07:53.438096: step 350530, loss = 0.16 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:07:54.297575: step 350540, loss = 0.17 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:07:55.163793: step 350550, loss = 0.16 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:07:56.041594: step 350560, loss = 0.18 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:07:56.911811: step 350570, loss = 0.19 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:07:57.790702: step 350580, loss = 0.17 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:07:58.651337: step 350590, loss = 0.15 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:07:59.606465: step 350600, loss = 0.17 (1340.1 examples/sec; 0.096 sec/batch)
2017-06-02 11:08:00.360667: step 350610, loss = 0.15 (1697.2 examples/sec; 0.075 sec/batch)
2017-06-02 11:08:01.236496: step 350620, loss = 0.16 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:08:02.088219: step 350630, loss = 0.15 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:08:02.961610: step 350640, loss = 0.15 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:08:03.821728: step 350650, loss = 0.12 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:08:04.684980: step 350660, loss = 0.17 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:08:05.567910: step 350670, loss = 0.24 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:08:06.411698: step 350680, loss = 0.18 (1517.0 examples/sec; 0.084 sec/batch)
2017-06-02 11:08:07.248598: step 350690, loss = 0.12 (1529.4 examples/sec; 0.084 sec/batch)
2017-06-02 11:08:08.222364: step 350700, loss = 0.14 (1314.5 examples/sec; 0.097 sec/batch)
2017-06-02 11:08:09.000479: step 350710, loss = 0.15 (1645.0 examples/sec; 0.078 sec/batch)
2017-06-02 11:08:09.866825: step 350720, loss = 0.11 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:08:10.744522: step 350730, loss = 0.13 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:08:11.613355: step 350740, loss = 0.17 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:08:12.485663: step 350750, loss = 0.13 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:08:13.340231: step 350760, loss = 0.15 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:08:14.206677: step 350770, loss = 0.14 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:08:15.085302: step 350780, loss = 0.13 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:08:15.956630: step 350790, loss = 0.18 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:08:16.922278: step 350800, loss = 0.14 (1325.5 examples/sec; 0.097 sec/batch)
2017-06-02 11:08:17.707619: step 350810, loss = 0.21 (1629.9 examples/sec; 0.079 sec/batch)
2017-06-02 11:08:18.570068: step 350820, loss = 0.15 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:08:19.447345: step 350830, loss = 0.15 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:08:20.323156: step 350840, loss = 0.14 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:08:21.178258: step 350850, loss = 0.14 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:08:22.011544: step 350860, loss = 0.18 (1536.1 examples/sec; 0.083 sec/batch)
2017-06-02 11:08:22.879275: step 350870, loss = 0.15 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:08:23.766136: step 350880, loss = 0.19 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 11:08:24.602349: step 350890, loss = 0.14 (1530.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:08:25.637681: step 350900, loss = 0.16 (1236.3 examples/sec; 0.104 sec/batch)
2017-06-02 11:08:26.326769: step 350910, loss = 0.14 (1857.6 examples/sec; 0.069 sec/batch)
2017-06-02 11:08:27.167421: step 350920, loss = 0.18 (1522.6 examples/sec; 0.084 sec/batch)
2017-06-02 11:08:28.033284: step 350930, loss = 0.18 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:08:28.923628: step 350940, loss = 0.14 (1437.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:08:29.769724: step 350950, loss = 0.22 (1512.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:08:30.658022: step 350960, loss = 0.20 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 11:08:31.517630: step 350970, loss = 0.15 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:08:32.365870: step 350980, loss = 0.15 (1509.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:08:33.226629: step 350990, loss = 0.14 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:08:34.193372: step 351000, loss = 0.15 (1324.0 examples/sec; 0.097 sec/batch)
2017-06-02 11:08:34.944011: step 351010, loss = 0.18 (1705.2 examples/sec; 0.075 sec/batch)
2017-06-02 11:08:35.807056: step 351020, loss = 0.18 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:08:36.694306: step 351030, loss = 0.17 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 11:08:37.575815: step 351040, loss = 0.13 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:08:38.449148: step 351050, loss = 0.16 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:08:39.320239: step 351060, loss = 0.17 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:08:40.193533: step 351070, loss = 0.19 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:08:41.055129: step 351080, loss = 0.13 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:08:41.920575: step 351090, loss = 0.15 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:08:42.879812: step 351100, loss = 0.14 (1334.4 examples/sec; 0.096 sec/batch)
2017-06-02 11:08:43.635377: step 351110, loss = 0.15 (1694.1 examples/sec; 0.076 sec/batch)
2017-06-02 11:08:44.479089: step 351120, loss = 0.13 (1517.1 examples/sec; 0.084 sec/batch)
2017-06-02 11:08:45.327717: step 351130, loss = 0.15 (1508.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:08:46.205276: step 351140, loss = 0.16 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:08:47.044041: step 351150, loss = 0.21 (1526.1 examples/sec; 0.084 sec/batch)
2017-06-02 11:08:47.922676: step 351160, loss = 0.14 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:08:48.769312: step 351170, loss = 0.21 (1511.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:08:49.621419: step 351180, loss = 0.12 (1502.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:08:50.478380: step 351190, loss = 0.16 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:08:51.438056: step 351200, loss = 0.17 (1333.8 examples/sec; 0.096 sec/batch)
2017-06-02 11:08:52.211451: step 351210, loss = 0.12 (1655.0 examples/sec; 0.077 sec/batch)
2017-06-02 11:08:53.080321: step 351220, loss = 0.16 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:08:53.932289: step 351230, loss = 0.22 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:08:54.786410: step 351240, loss = 0.17 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:08:55.640685: step 351250, loss = 0.12 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:08:56.480748: step 351260, loss = 0.18 (1523.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:08:57.367791: step 351270, loss = 0.17 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 11:08:58.215990: step 351280, loss = 0.16 (1509.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:08:59.079542: step 351290, loss = 0.18 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:09:00.055320: step 351300, loss = 0.18 (1311.7 examples/sec; 0.098 sec/batch)
2017-06-02 11:09:00.797894: step 351310, loss = 0.17 (1723.7 examples/sec; 0.074 sec/batch)
2017-06-02 11:09:01.665679: step 351320, loss = 0.15 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:09:02.533135: step 351330, loss = 0.17 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:09:03.418037: step 351340, loss = 0.16 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:09:04.291899: step 351350, loss = 0.12 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:09:05.163930: step 351360, loss = 0.14 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:09:06.044902: step 351370, loss = 0.15 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:09:06.899670: step 351380, loss = 0.17 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:09:07.736544: step 351390, loss = 0.15 (1529.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:09:08.720511: step 351400, loss = 0.16 (1300.8 examples/sec; 0.098 sec/batch)
2017-06-02 11:09:09.483559: step 351410, loss = 0.13 (1677.5 examples/sec; 0.076 sec/batch)
2017-06-02 11:09:10.336153: step 351420, loss = 0.16 (1501.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:09:11.206238: step 351430, loss = 0.18 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:09:12.077483: step 351440, loss = 0.14 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:09:12.950422: step 351450, loss = 0.15 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:09:13.816891: step 351460, loss = 0.16 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:09:14.665871: step 351470, loss = 0.16 (1507.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:09:15.558432: step 351480, loss = 0.21 (1434.1 examples/sec; 0.089 sec/batch)
2017-06-02 11:09:16.441875: step 351490, loss = 0.16 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:09:17.417353: step 351500, loss = 0.15 (1312.2 examples/sec; 0.098 sec/batch)
2017-06-02 11:09:18.205610: step 351510, loss = 0.16 (1623.8 examples/sec; 0.079 sec/batch)
2017-06-02 11:09:19.065615: step 351520, loss = 0.24 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:09:19.948123: step 351530, loss = 0.24 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:09:20.807999: step 351540, loss = 0.18 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:09:21.652508: step 351550, loss = 0.14 (1515.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:09:22.484313: step 351560, loss = 0.17 (1538.8 examples/sec; 0.083 sec/batch)
2017-06-02 11:09:23.362117: step 351570, loss = 0.19 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:09:24.242366: step 351580, loss = 0.16 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:09:25.113318: step 351590, loss = 0.21 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:09:26.099947: step 351600, loss = 0.13 (1297.3 examples/sec; 0.099 sec/batch)
2017-06-02 11:09:26.863069: step 351610, loss = 0.16 (1677.3 examples/sec; 0.076 sec/batch)
2017-06-02 11:09:27.741340: step 351620, loss = 0.17 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:09:28.597559: step 351630, loss = 0.17 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:09:29.477386: step 351640, loss = 0.18 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:09:30.361197: step 351650, loss = 0.19 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:09:31.237433: step 351660, loss = 0.17 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:09:32.115204: step 351670, loss = 0.14 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:09:33.004894: step 351680, loss = 0.19 (1438.7 examples/sec; 0.089 sec/batch)
2017-06-02 11:09:33.873796: step 351690, loss = 0.16 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:09:34.839964: step 351700, loss = 0.19 (1324.8 examples/sec; 0.097 sec/batch)
2017-06-02 11:09:35.608453: step 351710, loss = 0.16 (1665.6 examples/sec; 0.077 sec/batch)
2017-06-02 11:09:36.479586: step 351720, loss = 0.20 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:09:37.333560: step 351730, loss = 0.18 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:09:38.179886: step 351740, loss = 0.19 (1512.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:09:39.051182: step 351750, loss = 0.18 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:09:39.947404: step 351760, loss = 0.16 (1428.2 examples/sec; 0.090 sec/batch)
2017-06-02 11:09:40.805179: step 351770, loss = 0.19 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:09:41.670028: step 351780, loss = 0.15 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:09:42.514431: step 351790, loss = 0.18 (1515.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:09:43.501160: step 351800, loss = 0.16 (1297.2 examples/sec; 0.099 sec/batch)
2017-06-02 11:09:44.245937: step 351810, loss = 0.15 (1718.6 examples/sec; 0.074 sec/batch)
2017-06-02 11:09:45.128627: step 351820, loss = 0.14 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:09:45.977209: step 351830, loss = 0.17 (1508.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:09:46.827554: step 351840, loss = 0.15 (1505.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:09:47.702130: step 351850, loss = 0.17 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:09:48.555652: step 351860, loss = 0.15 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:09:49.370909: step 351870, loss = 0.18 (1570.1 examples/sec; 0.082 sec/batch)
2017-06-02 11:09:50.219068: step 351880, loss = 0.22 (1509.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:09:51.062294: step 351890, loss = 0.16 (1518.0 examples/sec; 0.084 sec/batch)
2017-06-02 11:09:52.030867: step 351900, loss = 0.19 (1321.5 examples/sec; 0.097 sec/batch)
2017-06-02 11:09:52.856855: step 351910, loss = 0.19 (1549.7 examples/sec; 0.083 sec/batch)
2017-06-02 11:09:53.735324: step 351920, loss = 0.14 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:09:54.609942: step 351930, loss = 0.18 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:09:55.471032: step 351940, loss = 0.19 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:09:56.349399: step 351950, loss = 0.20 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:09:57.239374: step 351960, loss = 0.16 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 11:09:58.125752: step 351970, loss = 0.15 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 11:09:58.986243: step 351980, loss = 0.17 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:09:59.822933: step 351990, loss = 0.17 (1529.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:10:00.784283: step 352000, loss = 0.16 (1331.5 examples/sec; 0.096 sec/batch)
2017-06-02 11:10:01.529460: step 352010, loss = 0.18 (1717.7 examples/sec; 0.075 sec/batch)
2017-06-02 11:10:02.393126: step 352020, loss = 0.14 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:10:03.260827: step 352030, loss = 0.16 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:10:04.129226: step 352040, loss = 0.18 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:10:05.004074: step 352050, loss = 0.16 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:10:05.862110: step 352060, loss = 0.19 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:10:06.717588: step 352070, loss = 0.18 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:10:07.587345: step 352080, loss = 0.17 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:10:08.446202: step 352090, loss = 0.17 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:10:09.449206: step 352100, loss = 0.15 (1276.2 examples/sec; 0.100 sec/batch)
2017-06-02 11:10:10.223272: step 352110, loss = 0.12 (1653.6 examples/sec; 0.077 sec/batch)
2017-06-02 11:10:11.087634: step 352120, loss = 0.14 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:10:11.934054: step 352130, loss = 0.16 (1512.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:10:12.800328: step 352140, loss = 0.16 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:10:13.684195: step 352150, loss = 0.14 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:10:14.539352: step 352160, loss = 0.17 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:10:15.424373: step 352170, loss = 0.16 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 11:10:16.298106: step 352180, loss = 0.15 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:10:17.150888: step 352190, loss = 0.16 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:10:18.150292: step 352200, loss = 0.18 (1280.8 examples/sec; 0.100 sec/batch)
2017-06-02 11:10:18.889982: step 352210, loss = 0.13 (1730.5 examples/sec; 0.074 sec/batch)
2017-06-02 11:10:19.762320: step 352220, loss = 0.16 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:10:20.629191: step 352230, loss = 0.15 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:10:21.531164: step 352240, loss = 0.15 (1419.1 examples/sec; 0.090 sec/batch)
2017-06-02 11:10:22.384667: step 352250, loss = 0.12 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:10:23.236890: step 352260, loss = 0.16 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:10:24.111037: step 352270, loss = 0.16 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:10:24.992123: step 352280, loss = 0.18 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:10:25.856101: step 352290, loss = 0.14 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:10:26.815410: step 352300, loss = 0.23 (1334.3 examples/sec; 0.096 sec/batch)
2017-06-02 11:10:27.573851: step 352310, loss = 0.15 (1687.7 examples/sec; 0.076 sec/batch)
2017-06-02 11:10:28.442858: step 352320, loss = 0.15 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:10:29.282655: step 352330, loss = 0.13 (1524.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:10:30.162702: step 352340, loss = 0.16 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:10:31.038168: step 352350, loss = 0.15 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:10:31.889511: step 352360, loss = 0.18 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:10:32.759708: step 352370, loss = 0.15 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:10:33.627679: step 352380, loss = 0.17 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:10:34.487539: step 352390, loss = 0.17 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:10:35.469476: step 352400, loss = 0.16 (1303.5 examples/sec; 0.098 sec/batch)
2017-06-02 11:10:36.231118: step 352410, loss = 0.15 (1680.6 examples/sec; 0.076 sec/batch)
2017-06-02 11:10:37.132695: step 352420, loss = 0.15 (1419.7 examples/sec; 0.090 sec/batch)
2017-06-02 11:10:37.999020: step 352430, loss = 0.12 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:10:38.854332: step 352440, loss = 0.16 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:10:39.698825: step 352450, loss = 0.18 (1515.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:10:40.571383: step 352460, loss = 0.13 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:10:41.437122: step 352470, loss = 0.16 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:10:42.289099: step 352480, loss = 0.19 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:10:43.155955: step 352490, loss = 0.17 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:10:44.111065: step 352500, loss = 0.15 (1340.1 examples/sec; 0.096 sec/batch)
2017-06-02 11:10:44.883187: step 352510, loss = 0.19 (1657.8 examples/sec; 0.077 sec/batch)
2017-06-02 11:10:45.739473: step 352520, loss = 0.14 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:10:46.593627: step 352530, loss = 0.20 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:10:47.422673: step 352540, loss = 0.17 (1544.0 examples/sec; 0.083 sec/batch)
2017-06-02 11:10:48.306535: step 352550, loss = 0.18 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:10:49.171934: step 352560, loss = 0.16 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:10:50.037237: step 352570, loss = 0.14 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:10:50.907408: step 352580, loss = 0.16 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:10:51.761550: step 352590, loss = 0.18 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:10:52.700683: step 352600, loss = 0.15 (1363.0 examples/sec; 0.094 sec/batch)
2017-06-02 11:10:53.483896: step 352610, loss = 0.13 (1634.3 examples/sec; 0.078 sec/batch)
2017-06-02 11:10:54.329893: step 352620, loss = 0.15 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:10:55.195232: step 352630, loss = 0.16 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:10:56.065239: step 352640, loss = 0.18 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:10:56.908262: step 352650, loss = 0.14 (1518.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:10:57.775635: step 352660, loss = 0.15 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:10:58.634749: step 352670, loss = 0.17 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:10:59.501071: step 352680, loss = 0.19 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:11:00.357159: step 352690, loss = 0.14 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:11:01.337404: step 352700, loss = 0.16 (1305.8 examples/sec; 0.098 sec/batch)
2017-06-02 11:11:02.097166: step 352710, loss = 0.15 (1684.7 examples/sec; 0.076 sec/batch)
2017-06-02 11:11:02.933857: step 352720, loss = 0.13 (1529.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:11:03.807329: step 352730, loss = 0.19 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:11:04.695425: step 352740, loss = 0.15 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 11:11:05.561953: step 352750, loss = 0.16 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:11:06.420179: step 352760, loss = 0.17 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:11:07.292608: step 352770, loss = 0.16 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:11:08.163610: step 352780, loss = 0.15 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:11:09.021943: step 352790, loss = 0.13 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:11:09.993957: step 352800, loss = 0.18 (1316.8 examples/sec; 0.097 sec/batch)
2017-06-02 11:11:10.754685: step 352810, loss = 0.13 (1682.6 examples/sec; 0.076 sec/batch)
2017-06-02 11:11:11.615335: step 352820, loss = 0.16 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:11:12.468445: step 352830, loss = 0.16 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:11:13.338195: step 352840, loss = 0.16 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:11:14.208239: step 352850, loss = 0.14 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:11:15.077326: step 352860, loss = 0.18 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:11:15.934942: step 352870, loss = 0.15 (1492.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:11:16.801772: step 352880, loss = 0.14 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:11:17.631861: step 352890, loss = 0.14 (1542.0 examples/sec; 0.083 sec/batch)
2017-06-02 11:11:18.603307: step 352900, loss = 0.13 (1317.6 examples/sec; 0.097 sec/batch)
2017-06-02 11:11:19.349941: step 352910, loss = 0.18 (1714.4 examples/sec; 0.075 sec/batch)
2017-06-02 11:11:20.203336: step 352920, loss = 0.15 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:11:21.108920: step 352930, loss = 0.15 (1413.5 examples/sec; 0.091 sec/batch)
2017-06-02 11:11:21.960650: step 352940, loss = 0.14 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:11:22.823616: step 352950, loss = 0.19 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:11:23.716194: step 352960, loss = 0.15 (1434.0 examples/sec; 0.089 sec/batch)
2017-06-02 11:11:24.585943: step 352970, loss = 0.16 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:11:25.475787: step 352980, loss = 0.17 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 11:11:26.336036: step 352990, loss = 0.15 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:11:27.322194: step 353000, loss = 0.17 (1298.0 examples/sec; 0.099 sec/batch)
2017-06-02 11:11:28.095227: step 353010, loss = 0.16 (1655.8 examples/sec; 0.077 sec/batch)
2017-06-02 11:11:28.953894: step 353020, loss = 0.17 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:11:29.832985: step 353030, loss = 0.20 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:11:30.686286: step 353040, loss = 0.17 (1500.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:11:31.586658: step 353050, loss = 0.16 (1421.6 examples/sec; 0.090 sec/batch)
2017-06-02 11:11:32.483253: step 353060, loss = 0.16 (1427.6 examples/sec; 0.090 sec/batch)
2017-06-02 11:11:33.337334: step 353070, loss = 0.14 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:11:34.204632: step 353080, loss = 0.13 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:11:35.069437: step 353090, loss = 0.12 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:11:36.042034: step 353100, loss = 0.18 (1316.0 examples/sec; 0.097 sec/batch)
2017-06-02 11:11:36.810664: step 353110, loss = 0.17 (1665.3 examples/sec; 0.077 sec/batch)
2017-06-02 11:11:37.688478: step 353120, loss = 0.19 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:11:38.552386: step 353130, loss = 0.14 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:11:39.417842: step 353140, loss = 0.17 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:11:40.286277: step 353150, loss = 0.15 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:11:41.146894: step 353160, loss = 0.17 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:11:42.008975: step 353170, loss = 0.15 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:11:42.885627: step 353180, loss = 0.18 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:11:43.748864: step 353190, loss = 0.16 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:11:44.734457: step 353200, loss = 0.14 (1298.7 examples/sec; 0.099 sec/batch)
2017-06-02 11:11:45.518042: step 353210, loss = 0.17 (1633.5 examples/sec; 0.078 sec/batch)
2017-06-02 11:11:46.376315: step 353220, loss = 0.18 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:11:47.247634: step 353230, loss = 0.16 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:11:48.124025: step 353240, loss = 0.15 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:11:48.983109: step 353250, loss = 0.16 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:11:49.857377: step 353260, loss = 0.15 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:11:50.718749: step 353270, loss = 0.20 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:11:51.559637: step 353280, loss = 0.17 (1522.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:11:52.407102: step 353290, loss = 0.20 (1510.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:11:53.361340: step 353300, loss = 0.17 (1341.4 examples/sec; 0.095 sec/batch)
2017-06-02 11:11:54.110384: step 353310, loss = 0.16 (1708.8 examples/sec; 0.075 sec/batch)
2017-06-02 11:11:54.977678: step 353320, loss = 0.20 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:11:55.817664: step 353330, loss = 0.13 (1523.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:11:56.681015: step 353340, loss = 0.18 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:11:57.541424: step 353350, loss = 0.14 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:11:58.425611: step 353360, loss = 0.20 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:11:59.291190: step 353370, loss = 0.16 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:12:00.158067: step 353380, loss = 0.16 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:12:01.027244: step 353390, loss = 0.14 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:12:02.014396: step 353400, loss = 0.17 (1296.7 examples/sec; 0.099 sec/batch)
2017-06-02 11:12:02.779554: step 353410, loss = 0.16 (1672.9 examples/sec; 0.077 sec/batch)
2017-06-02 11:12:03.642802: step 353420, loss = 0.15 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:12:04.500504: step 353430, loss = 0.14 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:12:05.397749: step 353440, loss = 0.13 (1426.6 examples/sec; 0.090 sec/batch)
2017-06-02 11:12:06.264757: step 353450, loss = 0.15 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:12:07.142616: step 353460, loss = 0.19 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:12:07.991970: step 353470, loss = 0.13 (1507.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:12:08.858769: step 353480, loss = 0.18 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:12:09.726317: step 353490, loss = 0.17 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:12:10.705243: step 353500, loss = 0.17 (1307.6 examples/sec; 0.098 sec/batch)
2017-06-02 11:12:11.441036: step 353510, loss = 0.12 (1739.6 examples/sec; 0.074 sec/batch)
2017-06-02 11:12:12.308885: step 353520, loss = 0.14 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:12:13.174517: step 353530, loss = 0.14 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:12:14.039253: step 353540, loss = 0.12 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:12:14.889012: step 353550, loss = 0.17 (1506.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:12:15.740924: step 353560, loss = 0.14 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:12:16.571549: step 353570, loss = 0.20 (1541.0 examples/sec; 0.083 sec/batch)
2017-06-02 11:12:17.437419: step 353580, loss = 0.18 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:12:18.286954: step 353590, loss = 0.17 (1506.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:12:19.245023: step 353600, loss = 0.14 (1336.0 examples/sec; 0.096 sec/batch)
2017-06-02 11:12:20.031597: step 353610, loss = 0.17 (1627.3 examples/sec; 0.079 sec/batch)
2017-06-02 11:12:20.888665: step 353620, loss = 0.14 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:12:21.756430: step 353630, loss = 0.17 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:12:22.612851: step 353640, loss = 0.13 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:12:23.500803: step 353650, loss = 0.20 (1441.5 examples/sec; 0.089 sec/batch)
2017-06-02 11:12:24.368448: step 353660, loss = 0.19 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:12:25.231417: step 353670, loss = 0.14 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:12:26.085796: step 353680, loss = 0.14 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:12:26.947910: step 353690, loss = 0.18 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:12:27.921446: step 353700, loss = 0.17 (1314.8 examples/sec; 0.097 sec/batch)
2017-06-02 11:12:28.689994: step 353710, loss = 0.18 (1665.5 examples/sec; 0.077 sec/batch)
2017-06-02 11:12:29.565478: step 353720, loss = 0.16 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:12:30.442419: step 353730, loss = 0.19 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:12:31.317135: step 353740, loss = 0.18 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:12:32.192843: step 353750, loss = 0.14 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:12:33.036502: step 353760, loss = 0.21 (1517.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:12:33.901993: step 353770, loss = 0.13 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:12:34.778269: step 353780, loss = 0.18 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:12:35.614513: step 353790, loss = 0.14 (1530.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:12:36.611035: step 353800, loss = 0.24 (1284.5 examples/sec; 0.100 sec/batch)
2017-06-02 11:12:37.343472: step 353810, loss = 0.17 (1747.6 examples/sec; 0.073 sec/batch)
2017-06-02 11:12:38.216227: step 353820, loss = 0.18 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:12:39.075945: step 353830, loss = 0.16 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:12:39.961006: step 353840, loss = 0.23 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 11:12:40.817065: step 353850, loss = 0.14 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:12:41.689827: step 353860, loss = 0.20 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:12:42.558596: step 353870, loss = 0.17 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:12:43.412009: step 353880, loss = 0.20 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:12:44.256316: step 353890, loss = 0.13 (1516.0 examples/sec; 0.084 sec/batch)
2017-06-02 11:12:45.192529: step 353900, loss = 0.18 (1367.2 examples/sec; 0.094 sec/batch)
2017-06-02 11:12:45.963091: step 353910, loss = 0.13 (1661.1 examples/sec; 0.077 sec/batch)
2017-06-02 11:12:46.833542: step 353920, loss = 0.18 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:12:47.689071: step 353930, loss = 0.18 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:12:48.548633: step 353940, loss = 0.15 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:12:49.417063: step 353950, loss = 0.15 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:12:50.269168: step 353960, loss = 0.21 (1502.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:12:51.122869: step 353970, loss = 0.18 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:12:51.980615: step 353980, loss = 0.13 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:12:52.852101: step 353990, loss = 0.14 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:12:53.785993: step 354000, loss = 0.15 (1370.6 examples/sec; 0.093 sec/batch)
2017-06-02 11:12:54.568509: step 354010, loss = 0.16 (1635.7 examples/sec; 0.078 sec/batch)
2017-06-02 11:12:55.401537: step 354020, loss = 0.15 (1536.6 examples/sec; 0.083 sec/batch)
2017-06-02 11:12:56.234405: step 354030, loss = 0.23 (1536.9 examples/sec; 0.083 sec/batch)
2017-06-02 11:12:57.103239: step 354040, loss = 0.16 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:12:57.981578: step 354050, loss = 0.19 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:12:58.841423: step 354060, loss = 0.18 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:12:59.722033: step 354070, loss = 0.19 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:13:00.596803: step 354080, loss = 0.16 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:13:01.470445: step 354090, loss = 0.16 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:13:02.456218: step 354100, loss = 0.13 (1298.5 examples/sec; 0.099 sec/batch)
2017-06-02 11:13:03.229890: step 354110, loss = 0.15 (1654.4 examples/sec; 0.077 sec/batch)
2017-06-02 11:13:04.102201: step 354120, loss = 0.18 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:13:04.949020: step 354130, loss = 0.13 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:13:05.806256: step 354140, loss = 0.13 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:13:06.695542: step 354150, loss = 0.18 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 11:13:07.547134: step 354160, loss = 0.12 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:13:08.387802: step 354170, loss = 0.21 (1522.6 examples/sec; 0.084 sec/batch)
2017-06-02 11:13:09.246458: step 354180, loss = 0.14 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:13:10.135813: step 354190, loss = 0.18 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 11:13:11.112681: step 354200, loss = 0.15 (1310.3 examples/sec; 0.098 sec/batch)
2017-06-02 11:13:11.888865: step 354210, loss = 0.23 (1649.1 examples/sec; 0.078 sec/batch)
2017-06-02 11:13:12.729135: step 354220, loss = 0.19 (1523.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:13:13.620057: step 354230, loss = 0.16 (1436.7 examples/sec; 0.089 sec/batch)
2017-06-02 11:13:14.482857: step 354240, loss = 0.12 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:13:15.366288: step 354250, loss = 0.14 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:13:16.217377: step 354260, loss = 0.18 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:13:17.099725: step 354270, loss = 0.14 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:13:17.987523: step 354280, loss = 0.21 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 11:13:18.859186: step 354290, loss = 0.16 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:13:19.844485: step 354300, loss = 0.16 (1299.1 examples/sec; 0.099 sec/batch)
2017-06-02 11:13:20.611144: step 354310, loss = 0.16 (1669.6 examples/sec; 0.077 sec/batch)
2017-06-02 11:13:21.477589: step 354320, loss = 0.16 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:13:22.346784: step 354330, loss = 0.17 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:13:23.239391: step 354340, loss = 0.18 (1434.0 examples/sec; 0.089 sec/batch)
2017-06-02 11:13:24.081652: step 354350, loss = 0.13 (1519.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:13:24.892705: step 354360, loss = 0.14 (1578.2 examples/sec; 0.081 sec/batch)
2017-06-02 11:13:25.754489: step 354370, loss = 0.20 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:13:26.613877: step 354380, loss = 0.19 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:13:27.482320: step 354390, loss = 0.16 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:13:28.466324: step 354400, loss = 0.17 (1300.8 examples/sec; 0.098 sec/batch)
2017-06-02 11:13:29.213864: step 354410, loss = 0.16 (1712.3 examples/sec; 0.075 sec/batch)
2017-06-02 11:13:30.102411: step 354420, loss = 0.19 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:13:30.994333: step 354430, loss = 0.15 (1435.1 examples/sec; 0.089 sec/batch)
2017-06-02 11:13:31.877383: step 354440, loss = 0.15 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:13:32.729548: step 354450, loss = 0.19 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:13:33.597405: step 354460, loss = 0.21 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:13:34.437915: step 354470, loss = 0.12 (1522.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:13:35.281743: step 354480, loss = 0.14 (1516.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:13:36.147033: step 354490, loss = 0.12 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:13:37.107272: step 354500, loss = 0.15 (1333.0 examples/sec; 0.096 sec/batch)
2017-06-02 11:13:37.891096: step 354510, loss = 0.18 (1633.1 examples/sec; 0.078 sec/batch)
2017-06-02 11:13:38.772536: step 354520, loss = 0.16 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:13:39.629077: step 354530, loss = 0.16 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:13:40.502646: step 354540, loss = 0.17 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:13:41.376683: step 354550, loss = 0.14 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:13:42.275657: step 354560, loss = 0.15 (1423.8 examples/sec; 0.090 sec/batch)
2017-06-02 11:13:43.125320: step 354570, loss = 0.14 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:13:43.992845: step 354580, loss = 0.18 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:13:44.861434: step 354590, loss = 0.15 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:13:45.849844: step 354600, loss = 0.14 (1295.0 examples/sec; 0.099 sec/batch)
2017-06-02 11:13:46.605352: step 354610, loss = 0.16 (1694.2 examples/sec; 0.076 sec/batch)
2017-06-02 11:13:47.456380: step 354620, loss = 0.16 (1504.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:13:48.315930: step 354630, loss = 0.13 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:13:49.177018: step 354640, loss = 0.19 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:13:50.065953: step 354650, loss = 0.13 (1439.9 examples/sec; 0.089 sec/batch)
2017-06-02 11:13:50.913352: step 354660, loss = 0.19 (1510.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:13:51.760841: step 354670, loss = 0.19 (1510.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:13:52.642986: step 354680, loss = 0.15 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:13:53.499980: step 354690, loss = 0.13 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:13:54.442801: step 354700, loss = 0.19 (1357.6 examples/sec; 0.094 sec/batch)
2017-06-02 11:13:55.220502: step 354710, loss = 0.12 (1645.9 examples/sec; 0.078 sec/batch)
2017-06-02 11:13:56.077225: step 354720, loss = 0.13 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:13:56.953942: step 354730, loss = 0.17 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:13:57.815389: step 354740, loss = 0.19 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:13:58.684473: step 354750, loss = 0.14 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:13:59.536321: step 354760, loss = 0.15 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:14:00.406806: step 354770, loss = 0.16 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:14:01.267853: step 354780, loss = 0.15 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:14:02.122132: step 354790, loss = 0.15 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:14:03.070920: step 354800, loss = 0.15 (1349.1 examples/sec; 0.095 sec/batch)
2017-06-02 11:14:03.814450: step 354810, loss = 0.19 (1721.5 examples/sec; 0.074 sec/batch)
2017-06-02 11:14:04.680827: step 354820, loss = 0.14 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:14:05.543415: step 354830, loss = 0.19 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:14:06.422830: step 354840, loss = 0.17 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:14:07.323198: step 354850, loss = 0.16 (1421.6 examples/sec; 0.090 sec/batch)
2017-06-02 11:14:08.189805: step 354860, loss = 0.17 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:14:09.060290: step 354870, loss = 0.15 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:14:09.914484: step 354880, loss = 0.16 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:14:10.760722: step 354890, loss = 0.15 (1512.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:14:11.760746: step 354900, loss = 0.19 (1280.0 examples/sec; 0.100 sec/batch)
2017-06-02 11:14:12.505595: step 354910, loss = 0.17 (1718.5 examples/sec; 0.074 sec/batch)
2017-06-02 11:14:13.351908: step 354920, loss = 0.19 (1512.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:14:14.241711: step 354930, loss = 0.14 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 11:14:15.126419: step 354940, loss = 0.22 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:14:15.990514: step 354950, loss = 0.18 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:14:16.888466: step 354960, loss = 0.17 (1425.4 examples/sec; 0.090 sec/batch)
2017-06-02 11:14:17.727178: step 354970, loss = 0.14 (1526.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:14:18.586830: step 354980, loss = 0.14 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:14:19.467351: step 354990, loss = 0.19 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:14:20.408340: step 355000, loss = 0.14 (1360.3 examples/sec; 0.094 sec/batch)
2017-06-02 11:14:21.172057: step 355010, loss = 0.21 (1676.0 examples/sec; 0.076 sec/batch)
2017-06-02 11:14:22.060483: step 355020, loss = 0.13 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 11:14:22.925115: step 355030, loss = 0.16 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:14:23.801158: step 355040, loss = 0.20 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:14:24.685894: step 355050, loss = 0.16 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:14:25.556490: step 355060, loss = 0.12 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:14:26.414421: step 355070, loss = 0.15 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:14:27.288662: step 355080, loss = 0.20 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:14:28.148687: step 355090, loss = 0.13 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:14:29.137613: step 355100, loss = 0.18 (1294.3 examples/sec; 0.099 sec/batch)
2017-06-02 11:14:29.899548: step 355110, loss = 0.14 (1680.0 examples/sec; 0.076 sec/batch)
2017-06-02 11:14:30.772066: step 355120, loss = 0.14 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:14:31.625379: step 355130, loss = 0.16 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:14:32.486454: step 355140, loss = 0.16 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:14:33.352065: step 355150, loss = 0.15 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:14:34.222337: step 355160, loss = 0.15 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:14:35.079075: step 355170, loss = 0.17 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:14:35.921841: step 355180, loss = 0.13 (1518.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:14:36.776374: step 355190, loss = 0.14 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:14:37.743872: step 355200, loss = 0.15 (1323.0 examples/sec; 0.097 sec/batch)
2017-06-02 11:14:38.497184: step 355210, loss = 0.20 (1699.2 examples/sec; 0.075 sec/batch)
2017-06-02 11:14:39.357394: step 355220, loss = 0.16 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:14:40.211465: step 355230, loss = 0.23 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:14:41.092710: step 355240, loss = 0.17 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:14:41.937640: step 355250, loss = 0.11 (1514.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:14:42.805239: step 355260, loss = 0.15 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:14:43.679329: step 355270, loss = 0.17 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:14:44.541664: step 355280, loss = 0.13 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:14:45.403823: step 355290, loss = 0.11 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:14:46.361840: step 355300, loss = 0.18 (1336.1 examples/sec; 0.096 sec/batch)
2017-06-02 11:14:47.129889: step 355310, loss = 0.20 (1666.6 examples/sec; 0.077 sec/batch)
2017-06-02 11:14:47.987325: step 355320, loss = 0.13 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:14:48.848982: step 355330, loss = 0.18 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:14:49.736122: step 355340, loss = 0.13 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 11:14:50.627310: step 355350, loss = 0.17 (1436.3 examples/sec; 0.089 sec/batch)
2017-06-02 11:14:51.466890: step 355360, loss = 0.15 (1524.6 examples/sec; 0.084 sec/batch)
2017-06-02 11:14:52.345314: step 355370, loss = 0.18 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:14:53.219649: step 355380, loss = 0.14 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:14:54.104215: step 355390, loss = 0.18 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:14:55.083848: step 355400, loss = 0.13 (1306.6 examples/sec; 0.098 sec/batch)
2017-06-02 11:14:55.845502: step 355410, loss = 0.14 (1680.6 examples/sec; 0.076 sec/batch)
2017-06-02 11:14:56.722447: step 355420, loss = 0.16 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:14:57.570007: step 355430, loss = 0.17 (1510.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:14:58.418099: step 355440, loss = 0.18 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:14:59.269246: step 355450, loss = 0.13 (1503.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:15:00.131940: step 355460, loss = 0.17 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:15:00.995905: step 355470, loss = 0.15 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:15:01.842678: step 355480, loss = 0.16 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:15:02.702513: step 355490, loss = 0.15 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:15:03.690843: step 355500, loss = 0.15 (1295.1 examples/sec; 0.099 sec/batch)
2017-06-02 11:15:04.448896: step 355510, loss = 0.23 (1688.5 examples/sec; 0.076 sec/batch)
2017-06-02 11:15:05.307485: step 355520, loss = 0.17 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:15:06.177040: step 355530, loss = 0.15 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:15:07.023821: step 355540, loss = 0.16 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:15:07.888686: step 355550, loss = 0.15 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:15:08.742815: step 355560, loss = 0.17 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:15:09.583714: step 355570, loss = 0.15 (1522.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:15:10.436653: step 355580, loss = 0.16 (1500.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:15:11.289705: step 355590, loss = 0.20 (1500.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:15:12.231620: step 355600, loss = 0.14 (1358.9 examples/sec; 0.094 sec/batch)
2017-06-02 11:15:13.017015: step 355610, loss = 0.20 (1629.8 examples/sec; 0.079 sec/batch)
2017-06-02 11:15:13.876794: step 355620, loss = 0.13 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:15:14.732117: step 355630, loss = 0.18 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:15:15.607254: step 355640, loss = 0.15 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:15:16.461633: step 355650, loss = 0.17 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:15:17.347274: step 355660, loss = 0.15 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 11:15:18.185997: step 355670, loss = 0.19 (1526.1 examples/sec; 0.084 sec/batch)
2017-06-02 11:15:19.045900: step 355680, loss = 0.20 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:15:19.926042: step 355690, loss = 0.15 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:15:20.872357: step 355700, loss = 0.14 (1352.6 examples/sec; 0.095 sec/batch)
2017-06-02 11:15:21.651510: step 355710, loss = 0.19 (1642.8 examples/sec; 0.078 sec/batch)
2017-06-02 11:15:22.497950: step 355720, loss = 0.16 (1512.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:15:23.363943: step 355730, loss = 0.17 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:15:24.235136: step 355740, loss = 0.13 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:15:25.095937: step 355750, loss = 0.15 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:15:25.977115: step 355760, loss = 0.19 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:15:26.848535: step 355770, loss = 0.14 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:15:27.719636: step 355780, loss = 0.17 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:15:28.596502: step 355790, loss = 0.15 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:15:29.621614: step 355800, loss = 0.13 (1248.6 examples/sec; 0.103 sec/batch)
2017-06-02 11:15:30.338345: step 355810, loss = 0.18 (1785.9 examples/sec; 0.072 sec/batch)
2017-06-02 11:15:31.210847: step 355820, loss = 0.18 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:15:32.053019: step 355830, loss = 0.18 (1519.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:15:32.911920: step 355840, loss = 0.16 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:15:33.792864: step 355850, loss = 0.17 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:15:34.661846: step 355860, loss = 0.20 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:15:35.522087: step 355870, loss = 0.15 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:15:36.402664: step 355880, loss = 0.17 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:15:37.260603: step 355890, loss = 0.16 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:15:38.234451: step 355900, loss = 0.13 (1314.4 examples/sec; 0.097 sec/batch)
2017-06-02 11:15:39.012962: step 355910, loss = 0.16 (1644.2 examples/sec; 0.078 sec/batch)
2017-06-02 11:15:39.900229: step 355920, loss = 0.17 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:15:40.783753: step 355930, loss = 0.14 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:15:41.645675: step 355940, loss = 0.15 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:15:42.507259: step 355950, loss = 0.15 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:15:43.368546: step 355960, loss = 0.15 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:15:44.234728: step 355970, loss = 0.13 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:15:45.084630: step 355980, loss = 0.15 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:15:45.960132: step 355990, loss = 0.14 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:15:46.984265: step 356000, loss = 0.17 (1249.8 examples/sec; 0.102 sec/batch)
2017-06-02 11:15:47.702291: step 356010, loss = 0.17 (1782.7 examples/sec; 0.072 sec/batch)
2017-06-02 11:15:48.587118: step 356020, loss = 0.14 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:15:49.471944: step 356030, loss = 0.13 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:15:50.339130: step 356040, loss = 0.14 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:15:51.244320: step 356050, loss = 0.18 (1414.1 examples/sec; 0.091 sec/batch)
2017-06-02 11:15:52.110763: step 356060, loss = 0.19 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:15:52.968438: step 356070, loss = 0.17 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:15:53.808245: step 356080, loss = 0.14 (1524.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:15:54.672022: step 356090, loss = 0.17 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:15:55.667249: step 356100, loss = 0.15 (1286.1 examples/sec; 0.100 sec/batch)
2017-06-02 11:15:56.441256: step 356110, loss = 0.16 (1653.8 examples/sec; 0.077 sec/batch)
2017-06-02 11:15:57.312110: step 356120, loss = 0.14 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:15:58.175581: step 356130, loss = 0.14 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:15:59.047837: step 356140, loss = 0.17 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:15:59.949308: step 356150, loss = 0.15 (1419.9 examples/sec; 0.090 sec/batch)
2017-06-02 11:16:00.820101: step 356160, loss = 0.16 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:16:01.677520: step 356170, loss = 0.18 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:16:02.549971: step 356180, loss = 0.16 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:16:03.406700: step 356190, loss = 0.15 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:16:04.375527: step 356200, loss = 0.11 (1321.2 examples/sec; 0.097 sec/batch)
2017-06-02 11:16:05.164385: step 356210, loss = 0.12 (1622.6 examples/sec; 0.079 sec/batch)
2017-06-02 11:16:06.038497: step 356220, loss = 0.19 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:16:06.894170: step 356230, loss = 0.16 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:16:07.758679: step 356240, loss = 0.13 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:16:08.632061: step 356250, loss = 0.13 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:16:09.487102: step 356260, loss = 0.13 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:16:10.332753: step 356270, loss = 0.22 (1513.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:16:11.206868: step 356280, loss = 0.14 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:16:12.057229: step 356290, loss = 0.17 (1505.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:16:13.034489: step 356300, loss = 0.18 (1309.8 examples/sec; 0.098 sec/batch)
2017-06-02 11:16:13.787504: step 356310, loss = 0.13 (1699.8 examples/sec; 0.075 sec/batch)
2017-06-02 11:16:14.659097: step 356320, loss = 0.16 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:16:15.495408: step 356330, loss = 0.17 (1530.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:16:16.357714: step 356340, loss = 0.27 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:16:17.207700: step 356350, loss = 0.15 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:16:18.053764: step 356360, loss = 0.15 (1512.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:16:18.915016: step 356370, loss = 0.15 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:16:19.772348: step 356380, loss = 0.18 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:16:20.629246: step 356390, loss = 0.19 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:16:21.620659: step 356400, loss = 0.16 (1291.1 examples/sec; 0.099 sec/batch)
2017-06-02 11:16:22.377685: step 356410, loss = 0.18 (1690.8 examples/sec; 0.076 sec/batch)
2017-06-02 11:16:23.271244: step 356420, loss = 0.18 (1432.5 examples/sec; 0.089 sec/batch)
2017-06-02 11:16:24.149954: step 356430, loss = 0.18 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:16:25.030519: step 356440, loss = 0.13 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:16:25.897287: step 356450, loss = 0.19 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:16:26.775559: step 356460, loss = 0.16 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:16:27.630869: step 356470, loss = 0.12 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:16:28.507866: step 356480, loss = 0.16 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:16:29.369818: step 356490, loss = 0.18 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:16:30.358608: step 356500, loss = 0.16 (1294.5 examples/sec; 0.099 sec/batch)
2017-06-02 11:16:31.124653: step 356510, loss = 0.18 (1671.0 examples/sec; 0.077 sec/batch)
2017-06-02 11:16:32.004650: step 356520, loss = 0.15 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:16:32.879838: step 356530, loss = 0.11 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:16:33.742319: step 356540, loss = 0.20 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:16:34.600376: step 356550, loss = 0.13 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:16:35.500271: step 356560, loss = 0.14 (1422.4 examples/sec; 0.090 sec/batch)
2017-06-02 11:16:36.354428: step 356570, loss = 0.16 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:16:37.212454: step 356580, loss = 0.13 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:16:38.070894: step 356590, loss = 0.16 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:16:39.054920: step 356600, loss = 0.15 (1300.8 examples/sec; 0.098 sec/batch)
2017-06-02 11:16:39.845232: step 356610, loss = 0.16 (1619.6 examples/sec; 0.079 sec/batch)
2017-06-02 11:16:40.708784: step 356620, loss = 0.21 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:16:41.564705: step 356630, loss = 0.15 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:16:42.424532: step 356640, loss = 0.15 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:16:43.315140: step 356650, loss = 0.16 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 11:16:44.172395: step 356660, loss = 0.13 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:16:45.068156: step 356670, loss = 0.15 (1429.0 examples/sec; 0.090 sec/batch)
2017-06-02 11:16:45.927581: step 356680, loss = 0.17 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:16:46.791452: step 356690, loss = 0.15 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:16:47.753742: step 356700, loss = 0.16 (1330.2 examples/sec; 0.096 sec/batch)
2017-06-02 11:16:48.528472: step 356710, loss = 0.19 (1652.1 examples/sec; 0.077 sec/batch)
2017-06-02 11:16:49.366349: step 356720, loss = 0.20 (1527.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:16:50.206832: step 356730, loss = 0.16 (1522.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:16:51.066005: step 356740, loss = 0.17 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:16:51.923135: step 356750, loss = 0.16 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:16:52.786998: step 356760, loss = 0.16 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:16:53.680812: step 356770, loss = 0.13 (1432.0 examples/sec; 0.089 sec/batch)
2017-06-02 11:16:54.546569: step 356780, loss = 0.24 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:16:55.398873: step 356790, loss = 0.15 (1501.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:16:56.356686: step 356800, loss = 0.18 (1336.4 examples/sec; 0.096 sec/batch)
2017-06-02 11:16:57.138242: step 356810, loss = 0.15 (1637.8 examples/sec; 0.078 sec/batch)
2017-06-02 11:16:57.999917: step 356820, loss = 0.14 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:16:58.831697: step 356830, loss = 0.20 (1538.9 examples/sec; 0.083 sec/batch)
2017-06-02 11:16:59.696334: step 356840, loss = 0.18 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:17:00.557017: step 356850, loss = 0.13 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:17:01.448206: step 356860, loss = 0.23 (1436.3 examples/sec; 0.089 sec/batch)
2017-06-02 11:17:02.312474: step 356870, loss = 0.17 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:17:03.189939: step 356880, loss = 0.15 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:17:04.066914: step 356890, loss = 0.18 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:17:05.021863: step 356900, loss = 0.17 (1340.4 examples/sec; 0.095 sec/batch)
2017-06-02 11:17:05.775803: step 356910, loss = 0.16 (1697.7 examples/sec; 0.075 sec/batch)
2017-06-02 11:17:06.634368: step 356920, loss = 0.15 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:17:07.509231: step 356930, loss = 0.21 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:17:08.386923: step 356940, loss = 0.17 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:17:09.223783: step 356950, loss = 0.17 (1529.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:17:10.081128: step 356960, loss = 0.14 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:17:10.930550: step 356970, loss = 0.17 (1506.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:17:11.774943: step 356980, loss = 0.14 (1515.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:17:12.633610: step 356990, loss = 0.15 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:17:13.588253: step 357000, loss = 0.16 (1340.8 examples/sec; 0.095 sec/batch)
2017-06-02 11:17:14.333066: step 357010, loss = 0.17 (1718.5 examples/sec; 0.074 sec/batch)
2017-06-02 11:17:15.178004: step 357020, loss = 0.16 (1514.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:17:16.050876: step 357030, loss = 0.15 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:17:16.903974: step 357040, loss = 0.18 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:17:17.759935: step 357050, loss = 0.17 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:17:18.638380: step 357060, loss = 0.16 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:17:19.499423: step 357070, loss = 0.14 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:17:20.380274: step 357080, loss = 0.16 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:17:21.233747: step 357090, loss = 0.13 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:17:22.172356: step 357100, loss = 0.14 (1363.7 examples/sec; 0.094 sec/batch)
2017-06-02 11:17:22.916664: step 357110, loss = 0.12 (1719.7 examples/sec; 0.074 sec/batch)
2017-06-02 11:17:23.776215: step 357120, loss = 0.17 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:17:24.620998: step 357130, loss = 0.17 (1515.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:17:25.484924: step 357140, loss = 0.14 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:17:26.345780: step 357150, loss = 0.18 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:17:27.216654: step 357160, loss = 0.13 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:17:28.067620: step 357170, loss = 0.18 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:17:28.934900: step 357180, loss = 0.19 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:17:29.776784: step 357190, loss = 0.13 (1520.4 examples/sec; 0.084 sec/batch)
2017-06-02 11:17:30.771562: step 357200, loss = 0.16 (1286.7 examples/sec; 0.099 sec/batch)
2017-06-02 11:17:31.508702: step 357210, loss = 0.16 (1736.4 examples/sec; 0.074 sec/batch)
2017-06-02 11:17:32.360281: step 357220, loss = 0.18 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:17:33.207513: step 357230, loss = 0.12 (1510.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:17:34.055290: step 357240, loss = 0.18 (1509.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:17:34.926988: step 357250, loss = 0.16 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:17:35.788247: step 357260, loss = 0.16 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:17:36.675673: step 357270, loss = 0.14 (1442.4 examples/sec; 0.089 sec/batch)
2017-06-02 11:17:37.538748: step 357280, loss = 0.18 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:17:38.430686: step 357290, loss = 0.15 (1435.1 examples/sec; 0.089 sec/batch)
2017-06-02 11:17:39.425490: step 357300, loss = 0.15 (1286.7 examples/sec; 0.099 sec/batch)
2017-06-02 11:17:40.172567: step 357310, loss = 0.13 (1713.4 examples/sec; 0.075 sec/batch)
2017-06-02 11:17:41.046715: step 357320, loss = 0.22 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:17:41.918612: step 357330, loss = 0.21 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:17:42.774236: step 357340, loss = 0.16 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:17:43.636172: step 357350, loss = 0.15 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:17:44.499343: step 357360, loss = 0.14 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:17:45.368147: step 357370, loss = 0.19 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:17:46.238798: step 357380, loss = 0.16 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:17:47.113286: step 357390, loss = 0.14 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:17:48.113667: step 357400, loss = 0.17 (1279.5 examples/sec; 0.100 sec/batch)
2017-06-02 11:17:48.851523: step 357410, loss = 0.13 (1734.8 examples/sec; 0.074 sec/batch)
2017-06-02 11:17:49.673266: step 357420, loss = 0.13 (1557.7 examples/sec; 0.082 sec/batch)
2017-06-02 11:17:50.508305: step 357430, loss = 0.12 (1532.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:17:51.392898: step 357440, loss = 0.14 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:17:52.244336: step 357450, loss = 0.21 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:17:53.098121: step 357460, loss = 0.19 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:17:53.959143: step 357470, loss = 0.12 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:17:54.844890: step 357480, loss = 0.14 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 11:17:55.705149: step 357490, loss = 0.15 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:17:56.687507: step 357500, loss = 0.14 (1303.0 examples/sec; 0.098 sec/batch)
2017-06-02 11:17:57.451720: step 357510, loss = 0.15 (1674.9 examples/sec; 0.076 sec/batch)
2017-06-02 11:17:58.326384: step 357520, loss = 0.21 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:17:59.201793: step 357530, loss = 0.21 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:18:00.071097: step 357540, loss = 0.18 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:18:00.955822: step 357550, loss = 0.16 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:18:01.814493: step 357560, loss = 0.16 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:18:02.679593: step 357570, loss = 0.17 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:18:03.522181: step 357580, loss = 0.17 (1519.1 examples/sec; 0.084 sec/batch)
2017-06-02 11:18:04.371540: step 357590, loss = 0.16 (1507.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:18:05.351460: step 357600, loss = 0.18 (1306.2 examples/sec; 0.098 sec/batch)
2017-06-02 11:18:06.112895: step 357610, loss = 0.20 (1681.0 examples/sec; 0.076 sec/batch)
2017-06-02 11:18:06.972217: step 357620, loss = 0.16 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:18:07.831753: step 357630, loss = 0.13 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:18:08.718913: step 357640, loss = 0.16 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 11:18:09.611237: step 357650, loss = 0.15 (1434.5 examples/sec; 0.089 sec/batch)
2017-06-02 11:18:10.497882: step 357660, loss = 0.19 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 11:18:11.347760: step 357670, loss = 0.17 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:18:12.207976: step 357680, loss = 0.11 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:18:13.059531: step 357690, loss = 0.14 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:18:14.014926: step 357700, loss = 0.15 (1339.8 examples/sec; 0.096 sec/batch)
2017-06-02 11:18:14.794148: step 357710, loss = 0.14 (1642.7 examples/sec; 0.078 sec/batch)
2017-06-02 11:18:15.646976: step 357720, loss = 0.15 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:18:16.521393: step 357730, loss = 0.16 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:18:17.412846: step 357740, loss = 0.14 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 11:18:18.273745: step 357750, loss = 0.17 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:18:19.146308: step 357760, loss = 0.14 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:18:20.026932: step 357770, loss = 0.15 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:18:20.881113: step 357780, loss = 0.16 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:18:21.743813: step 357790, loss = 0.19 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:18:22.723689: step 357800, loss = 0.15 (1306.3 examples/sec; 0.098 sec/batch)
2017-06-02 11:18:23.486595: step 357810, loss = 0.19 (1677.8 examples/sec; 0.076 sec/batch)
2017-06-02 11:18:24.351429: step 357820, loss = 0.14 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:18:25.215777: step 357830, loss = 0.14 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:18:26.075081: step 357840, loss = 0.15 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:18:26.950522: step 357850, loss = 0.16 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:18:27.812656: step 357860, loss = 0.16 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:18:28.698416: step 357870, loss = 0.19 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 11:18:29.541127: step 357880, loss = 0.19 (1518.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:18:30.418630: step 357890, loss = 0.17 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:18:31.401145: step 357900, loss = 0.16 (1302.8 examples/sec; 0.098 sec/batch)
2017-06-02 11:18:32.187207: step 357910, loss = 0.13 (1628.4 examples/sec; 0.079 sec/batch)
2017-06-02 11:18:33.043512: step 357920, loss = 0.13 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:18:33.911755: step 357930, loss = 0.18 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:18:34.798583: step 357940, loss = 0.17 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 11:18:35.659610: step 357950, loss = 0.19 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:18:36.523467: step 357960, loss = 0.12 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:18:37.386120: step 357970, loss = 0.16 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:18:38.255703: step 357980, loss = 0.17 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:18:39.118087: step 357990, loss = 0.13 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:18:40.065253: step 358000, loss = 0.21 (1351.4 examples/sec; 0.095 sec/batch)
2017-06-02 11:18:40.837511: step 358010, loss = 0.15 (1657.5 examples/sec; 0.077 sec/batch)
2017-06-02 11:18:41.695210: step 358020, loss = 0.15 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:18:42.579367: step 358030, loss = 0.15 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:18:43.454111: step 358040, loss = 0.11 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:18:44.337692: step 358050, loss = 0.12 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:18:45.188449: step 358060, loss = 0.17 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:18:46.054754: step 358070, loss = 0.17 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:18:46.905644: step 358080, loss = 0.17 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:18:47.784886: step 358090, loss = 0.19 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:18:48.759632: step 358100, loss = 0.15 (1313.2 examples/sec; 0.097 sec/batch)
2017-06-02 11:18:49.506208: step 358110, loss = 0.13 (1714.5 examples/sec; 0.075 sec/batch)
2017-06-02 11:18:50.374236: step 358120, loss = 0.16 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:18:51.239219: step 358130, loss = 0.14 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:18:52.115595: step 358140, loss = 0.16 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:18:52.994261: step 358150, loss = 0.13 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:18:53.881425: step 358160, loss = 0.19 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 11:18:54.772082: step 358170, loss = 0.14 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 11:18:55.668873: step 358180, loss = 0.14 (1427.3 examples/sec; 0.090 sec/batch)
2017-06-02 11:18:56.538923: step 358190, loss = 0.19 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:18:57.479982: step 358200, loss = 0.15 (1360.2 examples/sec; 0.094 sec/batch)
2017-06-02 11:18:58.268684: step 358210, loss = 0.17 (1622.9 examples/sec; 0.079 sec/batch)
2017-06-02 11:18:59.134364: step 358220, loss = 0.14 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:18:59.995620: step 358230, loss = 0.16 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:19:00.864113: step 358240, loss = 0.17 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:19:01.745142: step 358250, loss = 0.19 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:19:02.612256: step 358260, loss = 0.22 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:19:03.452766: step 358270, loss = 0.16 (1522.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:19:04.329471: step 358280, loss = 0.13 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:19:05.226123: step 358290, loss = 0.15 (1427.5 examples/sec; 0.090 sec/batch)
2017-06-02 11:19:06.253306: step 358300, loss = 0.13 (1246.1 examples/sec; 0.103 sec/batch)
2017-06-02 11:19:06.996386: step 358310, loss = 0.16 (1722.6 examples/sec; 0.074 sec/batch)
2017-06-02 11:19:07.852759: step 358320, loss = 0.16 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:19:08.721330: step 358330, loss = 0.18 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:19:09.583547: step 358340, loss = 0.15 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:19:10.454690: step 358350, loss = 0.14 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:19:11.317568: step 358360, loss = 0.14 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:19:12.154978: step 358370, loss = 0.15 (1528.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:19:13.022335: step 358380, loss = 0.21 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:19:13.902911: step 358390, loss = 0.17 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:19:14.852554: step 358400, loss = 0.21 (1347.9 examples/sec; 0.095 sec/batch)
2017-06-02 11:19:15.605508: step 358410, loss = 0.23 (1700.0 examples/sec; 0.075 sec/batch)
2017-06-02 11:19:16.465338: step 358420, loss = 0.14 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:19:17.315934: step 358430, loss = 0.13 (1504.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:19:18.174745: step 358440, loss = 0.15 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:19:19.021355: step 358450, loss = 0.17 (1511.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:19:19.889986: step 358460, loss = 0.16 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:19:20.750906: step 358470, loss = 0.15 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:19:21.605617: step 358480, loss = 0.17 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:19:22.456632: step 358490, loss = 0.13 (1504.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:19:23.426897: step 358500, loss = 0.17 (1319.2 examples/sec; 0.097 sec/batch)
2017-06-02 11:19:24.198271: step 358510, loss = 0.12 (1659.4 examples/sec; 0.077 sec/batch)
2017-06-02 11:19:25.016357: step 358520, loss = 0.13 (1564.7 examples/sec; 0.082 sec/batch)
2017-06-02 11:19:25.866652: step 358530, loss = 0.15 (1505.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:19:26.731449: step 358540, loss = 0.15 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:19:27.610643: step 358550, loss = 0.12 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:19:28.467280: step 358560, loss = 0.14 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:19:29.321821: step 358570, loss = 0.14 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:19:30.208993: step 358580, loss = 0.20 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 11:19:31.064365: step 358590, loss = 0.13 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:19:32.030012: step 358600, loss = 0.11 (1325.5 examples/sec; 0.097 sec/batch)
2017-06-02 11:19:32.794522: step 358610, loss = 0.17 (1674.3 examples/sec; 0.076 sec/batch)
2017-06-02 11:19:33.624959: step 358620, loss = 0.13 (1541.4 examples/sec; 0.083 sec/batch)
2017-06-02 11:19:34.465192: step 358630, loss = 0.20 (1523.4 examples/sec; 0.084 sec/batch)
2017-06-02 11:19:35.345147: step 358640, loss = 0.16 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:19:36.222437: step 358650, loss = 0.13 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:19:37.075777: step 358660, loss = 0.17 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:19:37.938600: step 358670, loss = 0.18 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:19:38.801679: step 358680, loss = 0.15 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:19:39.651251: step 358690, loss = 0.17 (1506.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:19:40.608928: step 358700, loss = 0.16 (1336.6 examples/sec; 0.096 sec/batch)
2017-06-02 11:19:41.379064: step 358710, loss = 0.17 (1662.0 examples/sec; 0.077 sec/batch)
2017-06-02 11:19:42.222083: step 358720, loss = 0.12 (1518.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:19:43.081205: step 358730, loss = 0.20 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:19:43.949115: step 358740, loss = 0.14 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:19:44.830070: step 358750, loss = 0.18 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:19:45.687157: step 358760, loss = 0.16 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:19:46.547313: step 358770, loss = 0.13 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:19:47.407191: step 358780, loss = 0.13 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:19:48.277414: step 358790, loss = 0.15 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:19:49.224231: step 358800, loss = 0.13 (1351.9 examples/sec; 0.095 sec/batch)
2017-06-02 11:19:49.990713: step 358810, loss = 0.19 (1670.0 examples/sec; 0.077 sec/batch)
2017-06-02 11:19:50.840702: step 358820, loss = 0.16 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:19:51.717136: step 358830, loss = 0.18 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:19:52.563010: step 358840, loss = 0.17 (1513.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:19:53.513115: step 358850, loss = 0.18 (1347.2 examples/sec; 0.095 sec/batch)
2017-06-02 11:19:54.349660: step 358860, loss = 0.18 (1530.1 examples/sec; 0.084 sec/batch)
2017-06-02 11:19:55.175928: step 358870, loss = 0.14 (1549.1 examples/sec; 0.083 sec/batch)
2017-06-02 11:19:56.007876: step 358880, loss = 0.14 (1538.6 examples/sec; 0.083 sec/batch)
2017-06-02 11:19:56.878289: step 358890, loss = 0.13 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:19:57.861980: step 358900, loss = 0.15 (1301.2 examples/sec; 0.098 sec/batch)
2017-06-02 11:19:58.611668: step 358910, loss = 0.21 (1707.4 examples/sec; 0.075 sec/batch)
2017-06-02 11:19:59.451609: step 358920, loss = 0.15 (1523.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:20:00.299872: step 358930, loss = 0.14 (1509.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:20:01.186212: step 358940, loss = 0.15 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 11:20:02.045670: step 358950, loss = 0.18 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:20:02.901859: step 358960, loss = 0.24 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:20:03.769784: step 358970, loss = 0.11 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:20:04.634208: step 358980, loss = 0.16 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:20:05.510525: step 358990, loss = 0.15 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:20:06.467385: step 359000, loss = 0.16 (1337.7 examples/sec; 0.096 sec/batch)
2017-06-02 11:20:07.232526: step 359010, loss = 0.13 (1672.9 examples/sec; 0.077 sec/batch)
2017-06-02 11:20:08.106635: step 359020, loss = 0.17 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:20:08.988750: step 359030, loss = 0.21 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:20:09.857457: step 359040, loss = 0.13 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:20:10.716204: step 359050, loss = 0.13 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:20:11.586628: step 359060, loss = 0.13 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:20:12.455569: step 359070, loss = 0.12 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:20:13.322014: step 359080, loss = 0.15 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:20:14.194092: step 359090, loss = 0.15 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:20:15.202748: step 359100, loss = 0.16 (1269.0 examples/sec; 0.101 sec/batch)
2017-06-02 11:20:15.950452: step 359110, loss = 0.16 (1711.9 examples/sec; 0.075 sec/batch)
2017-06-02 11:20:16.806081: step 359120, loss = 0.15 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:20:17.681985: step 359130, loss = 0.14 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:20:18.561985: step 359140, loss = 0.12 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:20:19.417039: step 359150, loss = 0.16 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:20:20.293060: step 359160, loss = 0.18 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:20:21.165955: step 359170, loss = 0.16 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:20:22.056249: step 359180, loss = 0.19 (1437.7 examples/sec; 0.089 sec/batch)
2017-06-02 11:20:22.887270: step 359190, loss = 0.15 (1540.3 examples/sec; 0.083 sec/batch)
2017-06-02 11:20:23.867599: step 359200, loss = 0.21 (1305.7 examples/sec; 0.098 sec/batch)
2017-06-02 11:20:24.630037: step 359210, loss = 0.15 (1678.8 examples/sec; 0.076 sec/batch)
2017-06-02 11:20:25.509535: step 359220, loss = 0.17 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:20:26.383144: step 359230, loss = 0.18 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:20:27.253144: step 359240, loss = 0.13 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:20:28.091080: step 359250, loss = 0.15 (1527.6 examples/sec; 0.084 sec/batch)
2017-06-02 11:20:28.956813: step 359260, loss = 0.19 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:20:29.813755: step 359270, loss = 0.16 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:20:30.672725: step 359280, loss = 0.15 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:20:31.544019: step 359290, loss = 0.16 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:20:32.525831: step 359300, loss = 0.18 (1303.7 examples/sec; 0.098 sec/batch)
2017-06-02 11:20:33.292964: step 359310, loss = 0.16 (1668.5 examples/sec; 0.077 sec/batch)
2017-06-02 11:20:34.147275: step 359320, loss = 0.22 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:20:35.031370: step 359330, loss = 0.18 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:20:35.894083: step 359340, loss = 0.15 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:20:36.762060: step 359350, loss = 0.17 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:20:37.624746: step 359360, loss = 0.18 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:20:38.516550: step 359370, loss = 0.12 (1435.3 examples/sec; 0.089 sec/batch)
2017-06-02 11:20:39.377143: step 359380, loss = 0.18 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:20:40.217875: step 359390, loss = 0.14 (1522.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:20:41.171060: step 359400, loss = 0.15 (1342.9 examples/sec; 0.095 sec/batch)
2017-06-02 11:20:41.932444: step 359410, loss = 0.14 (1681.1 examples/sec; 0.076 sec/batch)
2017-06-02 11:20:42.785482: step 359420, loss = 0.14 (1500.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:20:43.612920: step 359430, loss = 0.17 (1546.9 examples/sec; 0.083 sec/batch)
2017-06-02 11:20:44.450450: step 359440, loss = 0.21 (1528.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:20:45.309693: step 359450, loss = 0.14 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:20:46.185558: step 359460, loss = 0.15 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:20:47.054672: step 359470, loss = 0.18 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:20:47.919307: step 359480, loss = 0.16 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:20:48.774193: step 359490, loss = 0.17 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:20:49.726893: step 359500, loss = 0.18 (1343.6 examples/sec; 0.095 sec/batch)
2017-06-02 11:20:50.514376: step 359510, loss = 0.15 (1625.4 examples/sec; 0.079 sec/batch)
2017-06-02 11:20:51.366714: step 359520, loss = 0.16 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:20:52.216291: step 359530, loss = 0.15 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:20:53.085175: step 359540, loss = 0.15 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:20:53.930918: step 359550, loss = 0.20 (1513.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:20:54.784608: step 359560, loss = 0.20 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:20:55.627512: step 359570, loss = 0.17 (1518.6 examples/sec; 0.084 sec/batch)
2017-06-02 11:20:56.472927: step 359580, loss = 0.15 (1514.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:20:57.344538: step 359590, loss = 0.12 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:20:58.313546: step 359600, loss = 0.14 (1320.9 examples/sec; 0.097 sec/batch)
2017-06-02 11:20:59.065073: step 359610, loss = 0.14 (1703.3 examples/sec; 0.075 sec/batch)
2017-06-02 11:20:59.914205: step 359620, loss = 0.18 (1507.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:21:00.761501: step 359630, loss = 0.20 (1510.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:21:01.649946: step 359640, loss = 0.20 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 11:21:02.516379: step 359650, loss = 0.15 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:21:03.368913: step 359660, loss = 0.20 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:21:04.209529: step 359670, loss = 0.13 (1522.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:21:05.066035: step 359680, loss = 0.17 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:21:05.922458: step 359690, loss = 0.17 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:21:06.884174: step 359700, loss = 0.16 (1331.0 examples/sec; 0.096 sec/batch)
2017-06-02 11:21:07.652622: step 359710, loss = 0.19 (1665.7 examples/sec; 0.077 sec/batch)
2017-06-02 11:21:08.517982: step 359720, loss = 0.14 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:21:09.386144: step 359730, loss = 0.18 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:21:10.227307: step 359740, loss = 0.13 (1521.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:21:11.089881: step 359750, loss = 0.17 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:21:11.945692: step 359760, loss = 0.22 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:21:12.809955: step 359770, loss = 0.17 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:21:13.694775: step 359780, loss = 0.18 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:21:14.552942: step 359790, loss = 0.13 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:21:15.533405: step 359800, loss = 0.19 (1305.5 examples/sec; 0.098 sec/batch)
2017-06-02 11:21:16.289115: step 359810, loss = 0.14 (1693.8 examples/sec; 0.076 sec/batch)
2017-06-02 11:21:17.130329: step 359820, loss = 0.13 (1521.6 examples/sec; 0.084 sec/batch)
2017-06-02 11:21:17.976976: step 359830, loss = 0.12 (1511.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:21:18.827974: step 359840, loss = 0.13 (1504.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:21:19.663260: step 359850, loss = 0.14 (1532.4 examples/sec; 0.084 sec/batch)
2017-06-02 11:21:20.504207: step 359860, loss = 0.16 (1522.1 examples/sec; 0.084 sec/batch)
2017-06-02 11:21:21.389665: step 359870, loss = 0.15 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:21:22.246613: step 359880, loss = 0.12 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:21:23.151302: step 359890, loss = 0.20 (1414.9 examples/sec; 0.090 sec/batch)
2017-06-02 11:21:24.145876: step 359900, loss = 0.18 (1287.0 examples/sec; 0.099 sec/batch)
2017-06-02 11:21:24.911323: step 359910, loss = 0.16 (1672.2 examples/sec; 0.077 sec/batch)
2017-06-02 11:21:25.769394: step 359920, loss = 0.14 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:21:26.591707: step 359930, loss = 0.18 (1556.6 examples/sec; 0.082 sec/batch)
2017-06-02 11:21:27.440664: step 359940, loss = 0.23 (1507.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:21:28.305657: step 359950, loss = 0.15 (1479.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:21:29.138366: step 359960, loss = 0.12 (1537.1 examples/sec; 0.083 sec/batch)
2017-06-02 11:21:29.979924: step 359970, loss = 0.13 (1521.0 examples/sec; 0.084 sec/batch)
2017-06-02 11:21:30.855995: step 359980, loss = 0.13 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:21:31.718358: step 359990, loss = 0.15 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:21:32.715149: step 360000, loss = 0.18 (1284.1 examples/sec; 0.100 sec/batch)
2017-06-02 11:21:33.426841: step 360010, loss = 0.20 (1798.5 examples/sec; 0.071 sec/batch)
2017-06-02 11:21:34.317169: step 360020, loss = 0.16 (1437.7 examples/sec; 0.089 sec/batch)
2017-06-02 11:21:35.182028: step 360030, loss = 0.14 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:21:36.065174: step 360040, loss = 0.15 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:21:36.932488: step 360050, loss = 0.13 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:21:37.812692: step 360060, loss = 0.16 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:21:38.642881: step 360070, loss = 0.17 (1541.8 examples/sec; 0.083 sec/batch)
2017-06-02 11:21:39.515332: step 360080, loss = 0.13 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:21:40.395073: step 360090, loss = 0.17 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:21:41.352688: step 360100, loss = 0.12 (1336.6 examples/sec; 0.096 sec/batch)
2017-06-02 11:21:42.105662: step 360110, loss = 0.19 (1699.9 examples/sec; 0.075 sec/batch)
2017-06-02 11:21:42.957185: step 360120, loss = 0.19 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:21:43.833700: step 360130, loss = 0.14 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:21:44.693274: step 360140, loss = 0.15 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:21:45.568757: step 360150, loss = 0.15 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:21:46.400285: step 360160, loss = 0.13 (1539.3 examples/sec; 0.083 sec/batch)
2017-06-02 11:21:47.257328: step 360170, loss = 0.14 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:21:48.133403: step 360180, loss = 0.15 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:21:48.999677: step 360190, loss = 0.13 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:21:49.963224: step 360200, loss = 0.17 (1328.4 examples/sec; 0.096 sec/batch)
2017-06-02 11:21:50.720394: step 360210, loss = 0.11 (1690.5 examples/sec; 0.076 sec/batch)
2017-06-02 11:21:51.589626: step 360220, loss = 0.16 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:21:52.451978: step 360230, loss = 0.18 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:21:53.335128: step 360240, loss = 0.13 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:21:54.213109: step 360250, loss = 0.13 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:21:55.085667: step 360260, loss = 0.14 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:21:55.944921: step 360270, loss = 0.14 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:21:56.808975: step 360280, loss = 0.17 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:21:57.649776: step 360290, loss = 0.17 (1522.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:21:58.616999: step 360300, loss = 0.13 (1323.4 examples/sec; 0.097 sec/batch)
2017-06-02 11:21:59.359321: step 360310, loss = 0.20 (1724.3 examples/sec; 0.074 sec/batch)
2017-06-02 11:22:00.204101: step 360320, loss = 0.12 (1515.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:22:01.074090: step 360330, loss = 0.17 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:22:01.926812: step 360340, loss = 0.12 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:22:02.816143: step 360350, loss = 0.16 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 11:22:03.675602: step 360360, loss = 0.18 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:22:04.527577: step 360370, loss = 0.15 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:22:05.386469: step 360380, loss = 0.21 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:22:06.251690: step 360390, loss = 0.15 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:22:07.243200: step 360400, loss = 0.16 (1291.0 examples/sec; 0.099 sec/batch)
2017-06-02 11:22:07.975131: step 360410, loss = 0.16 (1748.8 examples/sec; 0.073 sec/batch)
2017-06-02 11:22:08.828682: step 360420, loss = 0.14 (1499.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:22:09.681002: step 360430, loss = 0.12 (1501.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:22:10.536510: step 360440, loss = 0.20 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:22:11.403837: step 360450, loss = 0.15 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:22:12.255958: step 360460, loss = 0.13 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:22:13.109983: step 360470, loss = 0.18 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:22:13.963501: step 360480, loss = 0.17 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:22:14.816843: step 360490, loss = 0.15 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:22:15.768829: step 360500, loss = 0.14 (1344.5 examples/sec; 0.095 sec/batch)
2017-06-02 11:22:16.533888: step 360510, loss = 0.16 (1673.1 examples/sec; 0.077 sec/batch)
2017-06-02 11:22:17.400471: step 360520, loss = 0.15 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:22:18.274790: step 360530, loss = 0.14 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:22:19.115945: step 360540, loss = 0.16 (1521.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:22:19.979857: step 360550, loss = 0.15 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:22:20.841031: step 360560, loss = 0.12 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:22:21.697871: step 360570, loss = 0.12 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:22:22.571938: step 360580, loss = 0.13 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:22:23.416185: step 360590, loss = 0.14 (1516.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:22:24.378556: step 360600, loss = 0.15 (1330.1 examples/sec; 0.096 sec/batch)
2017-06-02 11:22:25.121937: step 360610, loss = 0.12 (1721.9 examples/sec; 0.074 sec/batch)
2017-06-02 11:22:25.976940: step 360620, loss = 0.16 (1497.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:22:26.818875: step 360630, loss = 0.18 (1520.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:22:27.696012: step 360640, loss = 0.16 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:22:28.550547: step 360650, loss = 0.20 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:22:29.437242: step 360660, loss = 0.21 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:22:30.280173: step 360670, loss = 0.14 (1518.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:22:31.166792: step 360680, loss = 0.16 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 11:22:32.021850: step 360690, loss = 0.16 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:22:32.989390: step 360700, loss = 0.12 (1322.9 examples/sec; 0.097 sec/batch)
2017-06-02 11:22:33.748882: step 360710, loss = 0.14 (1685.3 examples/sec; 0.076 sec/batch)
2017-06-02 11:22:34.620810: step 360720, loss = 0.19 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:22:35.485595: step 360730, loss = 0.14 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:22:36.329138: step 360740, loss = 0.13 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 11:22:37.190441: step 360750, loss = 0.19 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:22:38.051267: step 360760, loss = 0.17 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:22:38.939036: step 360770, loss = 0.13 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 11:22:39.824046: step 360780, loss = 0.18 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 11:22:40.680915: step 360790, loss = 0.19 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:22:41.664993: step 360800, loss = 0.18 (1300.7 examples/sec; 0.098 sec/batch)
2017-06-02 11:22:42.416633: step 360810, loss = 0.14 (1702.9 examples/sec; 0.075 sec/batch)
2017-06-02 11:22:43.286113: step 360820, loss = 0.16 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:22:44.159335: step 360830, loss = 0.13 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:22:44.991445: step 360840, loss = 0.16 (1538.3 examples/sec; 0.083 sec/batch)
2017-06-02 11:22:45.847365: step 360850, loss = 0.19 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:22:46.705824: step 360860, loss = 0.14 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:22:47.585094: step 360870, loss = 0.16 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:22:48.431550: step 360880, loss = 0.15 (1512.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:22:49.270463: step 360890, loss = 0.14 (1525.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:22:50.220095: step 360900, loss = 0.15 (1347.9 examples/sec; 0.095 sec/batch)
2017-06-02 11:22:50.993318: step 360910, loss = 0.15 (1655.4 examples/sec; 0.077 sec/batch)
2017-06-02 11:22:51.868882: step 360920, loss = 0.14 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:22:52.718666: step 360930, loss = 0.14 (1506.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:22:53.560162: step 360940, loss = 0.14 (1521.1 examples/sec; 0.084 sec/batch)
2017-06-02 11:22:54.420192: step 360950, loss = 0.19 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:22:55.321830: step 360960, loss = 0.15 (1419.6 examples/sec; 0.090 sec/batch)
2017-06-02 11:22:56.210623: step 360970, loss = 0.14 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 11:22:57.063896: step 360980, loss = 0.15 (1500.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:22:57.934701: step 360990, loss = 0.15 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:22:58.889549: step 361000, loss = 0.19 (1340.5 examples/sec; 0.095 sec/batch)
2017-06-02 11:22:59.669435: step 361010, loss = 0.13 (1641.3 examples/sec; 0.078 sec/batch)
2017-06-02 11:23:00.530918: step 361020, loss = 0.15 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:23:01.400193: step 361030, loss = 0.18 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:23:02.252096: step 361040, loss = 0.18 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:23:03.122030: step 361050, loss = 0.15 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:23:03.983297: step 361060, loss = 0.13 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:23:04.859785: step 361070, loss = 0.14 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:23:05.757535: step 361080, loss = 0.22 (1425.8 examples/sec; 0.090 sec/batch)
2017-06-02 11:23:06.627166: step 361090, loss = 0.15 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:23:07.606041: step 361100, loss = 0.17 (1307.6 examples/sec; 0.098 sec/batch)
2017-06-02 11:23:08.351088: step 361110, loss = 0.17 (1718.0 examples/sec; 0.075 sec/batch)
2017-06-02 11:23:09.239029: step 361120, loss = 0.16 (1441.5 examples/sec; 0.089 sec/batch)
2017-06-02 11:23:10.111122: step 361130, loss = 0.12 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:23:10.972286: step 361140, loss = 0.18 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:23:11.857837: step 361150, loss = 0.14 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 11:23:12.715711: step 361160, loss = 0.14 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:23:13.580689: step 361170, loss = 0.16 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:23:14.437121: step 361180, loss = 0.13 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:23:15.295882: step 361190, loss = 0.15 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:23:16.268298: step 361200, loss = 0.14 (1316.3 examples/sec; 0.097 sec/batch)
2017-06-02 11:23:17.036538: step 361210, loss = 0.13 (1666.2 examples/sec; 0.077 sec/batch)
2017-06-02 11:23:17.916473: step 361220, loss = 0.13 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:23:18.772795: step 361230, loss = 0.16 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:23:19.617931: step 361240, loss = 0.12 (1514.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:23:20.456896: step 361250, loss = 0.15 (1525.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:23:21.316670: step 361260, loss = 0.15 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:23:22.189907: step 361270, loss = 0.16 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:23:23.052699: step 361280, loss = 0.13 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:23:23.913763: step 361290, loss = 0.19 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:23:24.869766: step 361300, loss = 0.20 (1338.9 examples/sec; 0.096 sec/batch)
2017-06-02 11:23:25.640269: step 361310, loss = 0.14 (1661.3 examples/sec; 0.077 sec/batch)
2017-06-02 11:23:26.493095: step 361320, loss = 0.20 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:23:27.360575: step 361330, loss = 0.14 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:23:28.197305: step 361340, loss = 0.15 (1529.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:23:29.048882: step 361350, loss = 0.16 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:23:29.934434: step 361360, loss = 0.17 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 11:23:30.816942: step 361370, loss = 0.16 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:23:31.690013: step 361380, loss = 0.19 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:23:32.561326: step 361390, loss = 0.15 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:23:33.580045: step 361400, loss = 0.14 (1256.5 examples/sec; 0.102 sec/batch)
2017-06-02 11:23:34.293918: step 361410, loss = 0.15 (1793.1 examples/sec; 0.071 sec/batch)
2017-06-02 11:23:35.146662: step 361420, loss = 0.16 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:23:36.033921: step 361430, loss = 0.13 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:23:36.870468: step 361440, loss = 0.12 (1530.1 examples/sec; 0.084 sec/batch)
2017-06-02 11:23:37.736181: step 361450, loss = 0.13 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:23:38.590022: step 361460, loss = 0.16 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:23:39.441283: step 361470, loss = 0.19 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:23:40.323950: step 361480, loss = 0.14 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:23:41.208739: step 361490, loss = 0.18 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:23:42.160008: step 361500, loss = 0.16 (1345.6 examples/sec; 0.095 sec/batch)
2017-06-02 11:23:42.947402: step 361510, loss = 0.20 (1625.6 examples/sec; 0.079 sec/batch)
2017-06-02 11:23:43.784958: step 361520, loss = 0.21 (1528.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:23:44.659970: step 361530, loss = 0.12 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:23:45.522155: step 361540, loss = 0.16 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:23:46.362454: step 361550, loss = 0.21 (1523.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:23:47.224712: step 361560, loss = 0.17 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:23:48.104634: step 361570, loss = 0.15 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:23:48.984335: step 361580, loss = 0.15 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:23:49.870161: step 361590, loss = 0.16 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 11:23:50.839386: step 361600, loss = 0.15 (1320.6 examples/sec; 0.097 sec/batch)
2017-06-02 11:23:51.612233: step 361610, loss = 0.17 (1656.2 examples/sec; 0.077 sec/batch)
2017-06-02 11:23:52.501915: step 361620, loss = 0.14 (1438.7 examples/sec; 0.089 sec/batch)
2017-06-02 11:23:53.377041: step 361630, loss = 0.17 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:23:54.239363: step 361640, loss = 0.15 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:23:55.091569: step 361650, loss = 0.18 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:23:55.958636: step 361660, loss = 0.15 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:23:56.837720: step 361670, loss = 0.14 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:23:57.705470: step 361680, loss = 0.14 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:23:58.547140: step 361690, loss = 0.16 (1520.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:23:59.517475: step 361700, loss = 0.15 (1319.1 examples/sec; 0.097 sec/batch)
2017-06-02 11:24:00.289771: step 361710, loss = 0.14 (1657.4 examples/sec; 0.077 sec/batch)
2017-06-02 11:24:01.131000: step 361720, loss = 0.13 (1521.6 examples/sec; 0.084 sec/batch)
2017-06-02 11:24:01.993515: step 361730, loss = 0.14 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:24:02.870561: step 361740, loss = 0.21 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:24:03.748036: step 361750, loss = 0.22 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:24:04.624164: step 361760, loss = 0.12 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:24:05.485197: step 361770, loss = 0.18 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:24:06.363764: step 361780, loss = 0.17 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:24:07.253462: step 361790, loss = 0.20 (1438.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:24:08.218190: step 361800, loss = 0.16 (1326.8 examples/sec; 0.096 sec/batch)
2017-06-02 11:24:08.993729: step 361810, loss = 0.18 (1650.5 examples/sec; 0.078 sec/batch)
2017-06-02 11:24:09.847870: step 361820, loss = 0.17 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:24:10.720505: step 361830, loss = 0.17 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:24:11.572308: step 361840, loss = 0.15 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:24:12.419331: step 361850, loss = 0.17 (1511.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:24:13.322552: step 361860, loss = 0.18 (1417.2 examples/sec; 0.090 sec/batch)
2017-06-02 11:24:14.207245: step 361870, loss = 0.13 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:24:15.071155: step 361880, loss = 0.16 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:24:15.927134: step 361890, loss = 0.17 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:24:16.903415: step 361900, loss = 0.18 (1311.1 examples/sec; 0.098 sec/batch)
2017-06-02 11:24:17.651430: step 361910, loss = 0.14 (1711.2 examples/sec; 0.075 sec/batch)
2017-06-02 11:24:18.512610: step 361920, loss = 0.12 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:24:19.371232: step 361930, loss = 0.16 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:24:20.217869: step 361940, loss = 0.15 (1511.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:24:21.097570: step 361950, loss = 0.16 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:24:21.943694: step 361960, loss = 0.18 (1512.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:24:22.822193: step 361970, loss = 0.17 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:24:23.663695: step 361980, loss = 0.16 (1521.1 examples/sec; 0.084 sec/batch)
2017-06-02 11:24:24.509081: step 361990, loss = 0.15 (1514.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:24:25.488442: step 362000, loss = 0.14 (1307.0 examples/sec; 0.098 sec/batch)
2017-06-02 11:24:26.251474: step 362010, loss = 0.17 (1677.5 examples/sec; 0.076 sec/batch)
2017-06-02 11:24:27.106565: step 362020, loss = 0.21 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:24:27.991788: step 362030, loss = 0.17 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 11:24:28.860844: step 362040, loss = 0.16 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:24:29.689952: step 362050, loss = 0.15 (1543.8 examples/sec; 0.083 sec/batch)
2017-06-02 11:24:30.552854: step 362060, loss = 0.12 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:24:31.427190: step 362070, loss = 0.13 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:24:32.298206: step 362080, loss = 0.19 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:24:33.162297: step 362090, loss = 0.15 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:24:34.129988: step 362100, loss = 0.11 (1322.7 examples/sec; 0.097 sec/batch)
2017-06-02 11:24:34.897388: step 362110, loss = 0.15 (1668.0 examples/sec; 0.077 sec/batch)
2017-06-02 11:24:35.783270: step 362120, loss = 0.15 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 11:24:36.632235: step 362130, loss = 0.18 (1507.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:24:37.494951: step 362140, loss = 0.15 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:24:38.341096: step 362150, loss = 0.13 (1512.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:24:39.235474: step 362160, loss = 0.17 (1431.2 examples/sec; 0.089 sec/batch)
2017-06-02 11:24:40.109015: step 362170, loss = 0.15 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:24:40.961903: step 362180, loss = 0.21 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:24:41.815280: step 362190, loss = 0.14 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:24:42.753401: step 362200, loss = 0.16 (1364.4 examples/sec; 0.094 sec/batch)
2017-06-02 11:24:43.528088: step 362210, loss = 0.16 (1652.3 examples/sec; 0.077 sec/batch)
2017-06-02 11:24:44.391786: step 362220, loss = 0.16 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:24:45.273975: step 362230, loss = 0.12 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:24:46.149264: step 362240, loss = 0.13 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:24:47.001946: step 362250, loss = 0.15 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:24:47.865020: step 362260, loss = 0.12 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:24:48.727381: step 362270, loss = 0.15 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:24:49.579130: step 362280, loss = 0.14 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:24:50.451092: step 362290, loss = 0.15 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:24:51.443366: step 362300, loss = 0.14 (1290.0 examples/sec; 0.099 sec/batch)
2017-06-02 11:24:52.192171: step 362310, loss = 0.17 (1709.4 examples/sec; 0.075 sec/batch)
2017-06-02 11:24:53.066725: step 362320, loss = 0.13 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:24:53.949477: step 362330, loss = 0.16 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:24:54.811416: step 362340, loss = 0.15 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:24:55.668158: step 362350, loss = 0.15 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:24:56.560688: step 362360, loss = 0.20 (1434.1 examples/sec; 0.089 sec/batch)
2017-06-02 11:24:57.423242: step 362370, loss = 0.13 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:24:58.273879: step 362380, loss = 0.16 (1504.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:24:59.137268: step 362390, loss = 0.15 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:25:00.105691: step 362400, loss = 0.14 (1321.7 examples/sec; 0.097 sec/batch)
2017-06-02 11:25:00.866788: step 362410, loss = 0.14 (1681.8 examples/sec; 0.076 sec/batch)
2017-06-02 11:25:01.728638: step 362420, loss = 0.14 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:25:02.618096: step 362430, loss = 0.14 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 11:25:03.486839: step 362440, loss = 0.14 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:25:04.367745: step 362450, loss = 0.19 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:25:05.224523: step 362460, loss = 0.18 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:25:06.065301: step 362470, loss = 0.17 (1522.4 examples/sec; 0.084 sec/batch)
2017-06-02 11:25:06.962390: step 362480, loss = 0.20 (1426.9 examples/sec; 0.090 sec/batch)
2017-06-02 11:25:07.840865: step 362490, loss = 0.17 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:25:08.821093: step 362500, loss = 0.19 (1305.8 examples/sec; 0.098 sec/batch)
2017-06-02 11:25:09.593922: step 362510, loss = 0.15 (1656.2 examples/sec; 0.077 sec/batch)
2017-06-02 11:25:10.474865: step 362520, loss = 0.13 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:25:11.378303: step 362530, loss = 0.14 (1416.8 examples/sec; 0.090 sec/batch)
2017-06-02 11:25:12.248098: step 362540, loss = 0.15 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:25:13.126945: step 362550, loss = 0.15 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:25:14.002424: step 362560, loss = 0.18 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:25:14.882773: step 362570, loss = 0.18 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:25:15.749092: step 362580, loss = 0.18 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:25:16.612294: step 362590, loss = 0.12 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:25:17.537385: step 362600, loss = 0.15 (1383.6 examples/sec; 0.093 sec/batch)
2017-06-02 11:25:18.333228: step 362610, loss = 0.11 (1608.4 examples/sec; 0.080 sec/batch)
2017-06-02 11:25:19.225878: step 362620, loss = 0.15 (1433.9 examples/sec; 0.089 sec/batch)
2017-06-02 11:25:20.090273: step 362630, loss = 0.16 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:25:20.961231: step 362640, loss = 0.17 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:25:21.808212: step 362650, loss = 0.14 (1511.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:25:22.676599: step 362660, loss = 0.13 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:25:23.553562: step 362670, loss = 0.16 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:25:24.417375: step 362680, loss = 0.13 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:25:25.303534: step 362690, loss = 0.12 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 11:25:26.289429: step 362700, loss = 0.17 (1298.3 examples/sec; 0.099 sec/batch)
2017-06-02 11:25:27.054768: step 362710, loss = 0.16 (1672.5 examples/sec; 0.077 sec/batch)
2017-06-02 11:25:27.944106: step 362720, loss = 0.15 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 11:25:28.814599: step 362730, loss = 0.14 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:25:29.694610: step 362740, loss = 0.14 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:25:30.544874: step 362750, loss = 0.13 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:25:31.391239: step 362760, loss = 0.20 (1512.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:25:32.259917: step 362770, loss = 0.19 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:25:33.130019: step 362780, loss = 0.16 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:25:34.007932: step 362790, loss = 0.14 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:25:34.967224: step 362800, loss = 0.16 (1334.3 examples/sec; 0.096 sec/batch)
2017-06-02 11:25:35.728017: step 362810, loss = 0.13 (1682.4 examples/sec; 0.076 sec/batch)
2017-06-02 11:25:36.568484: step 362820, loss = 0.15 (1523.0 examples/sec; 0.084 sec/batch)
2017-06-02 11:25:37.418868: step 362830, loss = 0.17 (1505.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:25:38.252653: step 362840, loss = 0.13 (1535.2 examples/sec; 0.083 sec/batch)
2017-06-02 11:25:39.090634: step 362850, loss = 0.14 (1527.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:25:39.963429: step 362860, loss = 0.14 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:25:40.831018: step 362870, loss = 0.20 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:25:41.683124: step 362880, loss = 0.13 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:25:42.532415: step 362890, loss = 0.13 (1507.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:25:43.530780: step 362900, loss = 0.19 (1282.1 examples/sec; 0.100 sec/batch)
2017-06-02 11:25:44.281342: step 362910, loss = 0.12 (1705.4 examples/sec; 0.075 sec/batch)
2017-06-02 11:25:45.141416: step 362920, loss = 0.15 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:25:45.997273: step 362930, loss = 0.22 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:25:46.853933: step 362940, loss = 0.14 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:25:47.681083: step 362950, loss = 0.16 (1547.5 examples/sec; 0.083 sec/batch)
2017-06-02 11:25:48.538226: step 362960, loss = 0.18 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:25:49.398266: step 362970, loss = 0.15 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:25:50.246252: step 362980, loss = 0.15 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:25:51.068051: step 362990, loss = 0.19 (1557.6 examples/sec; 0.082 sec/batch)
2017-06-02 11:25:52.060937: step 363000, loss = 0.18 (1289.2 examples/sec; 0.099 sec/batch)
2017-06-02 11:25:52.824575: step 363010, loss = 0.14 (1676.2 examples/sec; 0.076 sec/batch)
2017-06-02 11:25:53.701060: step 363020, loss = 0.22 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:25:54.577882: step 363030, loss = 0.18 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:25:55.443343: step 363040, loss = 0.15 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:25:56.323826: step 363050, loss = 0.17 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:25:57.191963: step 363060, loss = 0.15 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:25:58.025756: step 363070, loss = 0.22 (1535.2 examples/sec; 0.083 sec/batch)
2017-06-02 11:25:58.877369: step 363080, loss = 0.17 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:25:59.730364: step 363090, loss = 0.16 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:26:00.697798: step 363100, loss = 0.13 (1323.1 examples/sec; 0.097 sec/batch)
2017-06-02 11:26:01.443498: step 363110, loss = 0.12 (1716.5 examples/sec; 0.075 sec/batch)
2017-06-02 11:26:02.322584: step 363120, loss = 0.12 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:26:03.159213: step 363130, loss = 0.19 (1530.0 examples/sec; 0.084 sec/batch)
2017-06-02 11:26:04.019922: step 363140, loss = 0.13 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:26:04.865194: step 363150, loss = 0.15 (1514.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:26:05.731825: step 363160, loss = 0.14 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:26:06.603846: step 363170, loss = 0.20 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:26:07.473109: step 363180, loss = 0.13 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:26:08.332016: step 363190, loss = 0.18 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:26:09.288741: step 363200, loss = 0.14 (1337.9 examples/sec; 0.096 sec/batch)
2017-06-02 11:26:10.031122: step 363210, loss = 0.19 (1724.1 examples/sec; 0.074 sec/batch)
2017-06-02 11:26:10.919626: step 363220, loss = 0.14 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:26:11.785262: step 363230, loss = 0.16 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:26:12.664549: step 363240, loss = 0.19 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:26:13.532974: step 363250, loss = 0.15 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:26:14.391045: step 363260, loss = 0.13 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:26:15.250224: step 363270, loss = 0.16 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:26:16.113789: step 363280, loss = 0.14 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:26:17.005821: step 363290, loss = 0.17 (1434.9 examples/sec; 0.089 sec/batch)
2017-06-02 11:26:17.984257: step 363300, loss = 0.16 (1308.2 examples/sec; 0.098 sec/batch)
2017-06-02 11:26:18.747579: step 363310, loss = 0.12 (1676.9 examples/sec; 0.076 sec/batch)
2017-06-02 11:26:19.593186: step 363320, loss = 0.15 (1513.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:26:20.459644: step 363330, loss = 0.13 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:26:21.314805: step 363340, loss = 0.14 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:26:22.190728: step 363350, loss = 0.20 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:26:23.050081: step 363360, loss = 0.18 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:26:23.920020: step 363370, loss = 0.15 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:26:24.787683: step 363380, loss = 0.20 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:26:25.670723: step 363390, loss = 0.18 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:26:26.629829: step 363400, loss = 0.14 (1334.6 examples/sec; 0.096 sec/batch)
2017-06-02 11:26:27.384756: step 363410, loss = 0.19 (1695.5 examples/sec; 0.075 sec/batch)
2017-06-02 11:26:28.266644: step 363420, loss = 0.14 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:26:29.133844: step 363430, loss = 0.17 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:26:29.986322: step 363440, loss = 0.14 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:26:30.848576: step 363450, loss = 0.17 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:26:31.708283: step 363460, loss = 0.18 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:26:32.591928: step 363470, loss = 0.16 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:26:33.433116: step 363480, loss = 0.15 (1521.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:26:34.266175: step 363490, loss = 0.15 (1536.5 examples/sec; 0.083 sec/batch)
2017-06-02 11:26:35.219082: step 363500, loss = 0.14 (1343.3 examples/sec; 0.095 sec/batch)
2017-06-02 11:26:36.005590: step 363510, loss = 0.21 (1627.5 examples/sec; 0.079 sec/batch)
2017-06-02 11:26:36.865642: step 363520, loss = 0.17 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:26:37.754367: step 363530, loss = 0.14 (1440.3 examples/sec; 0.089 sec/batch)
2017-06-02 11:26:38.612551: step 363540, loss = 0.12 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:26:39.471263: step 363550, loss = 0.14 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:26:40.356041: step 363560, loss = 0.17 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:26:41.205958: step 363570, loss = 0.15 (1506.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:26:42.071922: step 363580, loss = 0.13 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:26:42.947390: step 363590, loss = 0.14 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:26:43.907674: step 363600, loss = 0.21 (1332.9 examples/sec; 0.096 sec/batch)
2017-06-02 11:26:44.678532: step 363610, loss = 0.13 (1660.5 examples/sec; 0.077 sec/batch)
2017-06-02 11:26:45.537708: step 363620, loss = 0.17 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:26:46.420478: step 363630, loss = 0.15 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:26:47.329882: step 363640, loss = 0.16 (1407.5 examples/sec; 0.091 sec/batch)
2017-06-02 11:26:48.178951: step 363650, loss = 0.15 (1507.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:26:49.055974: step 363660, loss = 0.22 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:26:49.921864: step 363670, loss = 0.15 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:26:50.773586: step 363680, loss = 0.13 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:26:51.649087: step 363690, loss = 0.16 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:26:52.665195: step 363700, loss = 0.21 (1259.7 examples/sec; 0.102 sec/batch)
2017-06-02 11:26:53.392502: step 363710, loss = 0.14 (1759.9 examples/sec; 0.073 sec/batch)
2017-06-02 11:26:54.247691: step 363720, loss = 0.15 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:26:55.128959: step 363730, loss = 0.18 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:26:55.983956: step 363740, loss = 0.21 (1497.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:26:56.884623: step 363750, loss = 0.13 (1421.2 examples/sec; 0.090 sec/batch)
2017-06-02 11:26:57.769425: step 363760, loss = 0.19 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:26:58.652532: step 363770, loss = 0.15 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:26:59.534855: step 363780, loss = 0.14 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:27:00.408364: step 363790, loss = 0.17 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:27:01.423711: step 363800, loss = 0.13 (1260.7 examples/sec; 0.102 sec/batch)
2017-06-02 11:27:02.155448: step 363810, loss = 0.12 (1749.2 examples/sec; 0.073 sec/batch)
2017-06-02 11:27:02.998070: step 363820, loss = 0.21 (1519.1 examples/sec; 0.084 sec/batch)
2017-06-02 11:27:03.853158: step 363830, loss = 0.20 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:27:04.714168: step 363840, loss = 0.14 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:27:05.602476: step 363850, loss = 0.12 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 11:27:06.466148: step 363860, loss = 0.18 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:27:07.336043: step 363870, loss = 0.15 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:27:08.211197: step 363880, loss = 0.14 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:27:09.065168: step 363890, loss = 0.13 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:27:10.038616: step 363900, loss = 0.21 (1314.9 examples/sec; 0.097 sec/batch)
2017-06-02 11:27:10.825398: step 363910, loss = 0.17 (1626.9 examples/sec; 0.079 sec/batch)
2017-06-02 11:27:11.666054: step 363920, loss = 0.14 (1522.6 examples/sec; 0.084 sec/batch)
2017-06-02 11:27:12.540031: step 363930, loss = 0.14 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:27:13.401294: step 363940, loss = 0.17 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:27:14.243270: step 363950, loss = 0.24 (1520.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:27:15.094403: step 363960, loss = 0.20 (1503.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:27:15.934607: step 363970, loss = 0.14 (1523.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:27:16.797343: step 363980, loss = 0.14 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:27:17.669974: step 363990, loss = 0.16 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:27:18.648325: step 364000, loss = 0.16 (1308.3 examples/sec; 0.098 sec/batch)
2017-06-02 11:27:19.410171: step 364010, loss = 0.19 (1680.1 examples/sec; 0.076 sec/batch)
2017-06-02 11:27:20.301185: step 364020, loss = 0.17 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:27:21.165915: step 364030, loss = 0.14 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:27:22.043411: step 364040, loss = 0.17 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:27:22.919911: step 364050, loss = 0.14 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:27:23.815105: step 364060, loss = 0.15 (1429.8 examples/sec; 0.090 sec/batch)
2017-06-02 11:27:24.689406: step 364070, loss = 0.16 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:27:25.571117: step 364080, loss = 0.16 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:27:26.448923: step 364090, loss = 0.12 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:27:27.451523: step 364100, loss = 0.17 (1276.7 examples/sec; 0.100 sec/batch)
2017-06-02 11:27:28.214441: step 364110, loss = 0.14 (1677.8 examples/sec; 0.076 sec/batch)
2017-06-02 11:27:29.093573: step 364120, loss = 0.19 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:27:29.959561: step 364130, loss = 0.17 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:27:30.842548: step 364140, loss = 0.15 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:27:31.688211: step 364150, loss = 0.18 (1513.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:27:32.556826: step 364160, loss = 0.12 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:27:33.414716: step 364170, loss = 0.16 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:27:34.278126: step 364180, loss = 0.18 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:27:35.127996: step 364190, loss = 0.16 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:27:36.119719: step 364200, loss = 0.14 (1290.7 examples/sec; 0.099 sec/batch)
2017-06-02 11:27:36.845056: step 364210, loss = 0.15 (1764.7 examples/sec; 0.073 sec/batch)
2017-06-02 11:27:37.703259: step 364220, loss = 0.14 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:27:38.566468: step 364230, loss = 0.14 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:27:39.416447: step 364240, loss = 0.14 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:27:40.272185: step 364250, loss = 0.12 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:27:41.114811: step 364260, loss = 0.13 (1519.1 examples/sec; 0.084 sec/batch)
2017-06-02 11:27:41.987102: step 364270, loss = 0.18 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:27:42.849106: step 364280, loss = 0.15 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:27:43.719883: step 364290, loss = 0.13 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:27:44.693091: step 364300, loss = 0.16 (1315.2 examples/sec; 0.097 sec/batch)
2017-06-02 11:27:45.466107: step 364310, loss = 0.12 (1655.8 examples/sec; 0.077 sec/batch)
2017-06-02 11:27:46.334122: step 364320, loss = 0.12 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:27:47.209029: step 364330, loss = 0.15 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:27:48.065510: step 364340, loss = 0.12 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:27:48.945793: step 364350, loss = 0.16 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:27:49.836778: step 364360, loss = 0.18 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:27:50.709251: step 364370, loss = 0.16 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:27:51.585814: step 364380, loss = 0.15 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:27:52.458422: step 364390, loss = 0.14 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:27:53.423900: step 364400, loss = 0.12 (1325.8 examples/sec; 0.097 sec/batch)
2017-06-02 11:27:54.200427: step 364410, loss = 0.19 (1648.4 examples/sec; 0.078 sec/batch)
2017-06-02 11:27:55.075644: step 364420, loss = 0.16 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:27:55.943542: step 364430, loss = 0.16 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:27:56.807934: step 364440, loss = 0.18 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:27:57.649823: step 364450, loss = 0.15 (1520.4 examples/sec; 0.084 sec/batch)
2017-06-02 11:27:58.504660: step 364460, loss = 0.15 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:27:59.330696: step 364470, loss = 0.17 (1549.6 examples/sec; 0.083 sec/batch)
2017-06-02 11:28:00.199556: step 364480, loss = 0.15 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:28:01.070057: step 364490, loss = 0.15 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:28:02.035679: step 364500, loss = 0.16 (1325.6 examples/sec; 0.097 sec/batch)
2017-06-02 11:28:02.805136: step 364510, loss = 0.17 (1663.5 examples/sec; 0.077 sec/batch)
2017-06-02 11:28:03.688200: step 364520, loss = 0.17 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:28:04.556337: step 364530, loss = 0.20 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:28:05.449539: step 364540, loss = 0.14 (1433.0 examples/sec; 0.089 sec/batch)
2017-06-02 11:28:06.299874: step 364550, loss = 0.14 (1505.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:28:07.135891: step 364560, loss = 0.12 (1531.1 examples/sec; 0.084 sec/batch)
2017-06-02 11:28:08.031291: step 364570, loss = 0.17 (1429.5 examples/sec; 0.090 sec/batch)
2017-06-02 11:28:08.881574: step 364580, loss = 0.12 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:28:09.741035: step 364590, loss = 0.13 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:28:10.690489: step 364600, loss = 0.17 (1348.1 examples/sec; 0.095 sec/batch)
2017-06-02 11:28:11.469406: step 364610, loss = 0.12 (1643.3 examples/sec; 0.078 sec/batch)
2017-06-02 11:28:12.323425: step 364620, loss = 0.15 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:28:13.195791: step 364630, loss = 0.18 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:28:14.043394: step 364640, loss = 0.16 (1510.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:28:14.943550: step 364650, loss = 0.15 (1422.0 examples/sec; 0.090 sec/batch)
2017-06-02 11:28:15.821510: step 364660, loss = 0.13 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:28:16.685751: step 364670, loss = 0.14 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:28:17.561125: step 364680, loss = 0.20 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:28:18.397079: step 364690, loss = 0.18 (1531.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:28:19.379549: step 364700, loss = 0.13 (1302.8 examples/sec; 0.098 sec/batch)
2017-06-02 11:28:20.139096: step 364710, loss = 0.16 (1685.2 examples/sec; 0.076 sec/batch)
2017-06-02 11:28:21.000052: step 364720, loss = 0.23 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:28:21.860971: step 364730, loss = 0.13 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:28:22.714483: step 364740, loss = 0.14 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:28:23.584227: step 364750, loss = 0.14 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:28:24.473516: step 364760, loss = 0.18 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 11:28:25.347413: step 364770, loss = 0.15 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:28:26.241146: step 364780, loss = 0.15 (1432.2 examples/sec; 0.089 sec/batch)
2017-06-02 11:28:27.088094: step 364790, loss = 0.16 (1511.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:28:28.053135: step 364800, loss = 0.12 (1326.4 examples/sec; 0.097 sec/batch)
2017-06-02 11:28:28.805765: step 364810, loss = 0.14 (1700.7 examples/sec; 0.075 sec/batch)
2017-06-02 11:28:29.663648: step 364820, loss = 0.14 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:28:30.519971: step 364830, loss = 0.17 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:28:31.376466: step 364840, loss = 0.12 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:28:32.227399: step 364850, loss = 0.17 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:28:33.098836: step 364860, loss = 0.13 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:28:33.977197: step 364870, loss = 0.11 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:28:34.842414: step 364880, loss = 0.18 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:28:35.683935: step 364890, loss = 0.16 (1521.1 examples/sec; 0.084 sec/batch)
2017-06-02 11:28:36.661803: step 364900, loss = 0.16 (1309.0 examples/sec; 0.098 sec/batch)
2017-06-02 11:28:37.412123: step 364910, loss = 0.18 (1706.0 examples/sec; 0.075 sec/batch)
2017-06-02 11:28:38.283455: step 364920, loss = 0.15 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:28:39.157604: step 364930, loss = 0.14 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:28:40.019912: step 364940, loss = 0.17 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:28:40.883690: step 364950, loss = 0.12 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:28:41.761452: step 364960, loss = 0.17 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:28:42.629914: step 364970, loss = 0.14 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:28:43.494025: step 364980, loss = 0.13 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:28:44.384054: step 364990, loss = 0.18 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 11:28:45.346559: step 365000, loss = 0.16 (1329.9 examples/sec; 0.096 sec/batch)
2017-06-02 11:28:46.092506: step 365010, loss = 0.15 (1715.9 examples/sec; 0.075 sec/batch)
2017-06-02 11:28:46.964002: step 365020, loss = 0.19 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:28:47.839660: step 365030, loss = 0.17 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:28:48.745378: step 365040, loss = 0.15 (1413.2 examples/sec; 0.091 sec/batch)
2017-06-02 11:28:49.606326: step 365050, loss = 0.15 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:28:50.449288: step 365060, loss = 0.12 (1518.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:28:51.330778: step 365070, loss = 0.15 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:28:52.197267: step 365080, loss = 0.16 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:28:53.044120: step 365090, loss = 0.13 (1511.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:28:53.994119: step 365100, loss = 0.16 (1347.4 examples/sec; 0.095 sec/batch)
2017-06-02 11:28:54.751859: step 365110, loss = 0.13 (1689.2 examples/sec; 0.076 sec/batch)
2017-06-02 11:28:55.650993: step 365120, loss = 0.16 (1423.6 examples/sec; 0.090 sec/batch)
2017-06-02 11:28:56.528171: step 365130, loss = 0.12 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:28:57.420397: step 365140, loss = 0.14 (1434.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:28:58.269267: step 365150, loss = 0.16 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:28:59.120907: step 365160, loss = 0.13 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:28:59.991880: step 365170, loss = 0.15 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:29:00.864868: step 365180, loss = 0.15 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:29:01.721276: step 365190, loss = 0.18 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:29:02.681225: step 365200, loss = 0.17 (1333.4 examples/sec; 0.096 sec/batch)
2017-06-02 11:29:03.464626: step 365210, loss = 0.13 (1633.9 examples/sec; 0.078 sec/batch)
2017-06-02 11:29:04.336037: step 365220, loss = 0.16 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:29:05.201438: step 365230, loss = 0.11 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:29:06.071632: step 365240, loss = 0.17 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:29:06.934994: step 365250, loss = 0.14 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:29:07.831568: step 365260, loss = 0.11 (1427.7 examples/sec; 0.090 sec/batch)
2017-06-02 11:29:08.691001: step 365270, loss = 0.21 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:29:09.567006: step 365280, loss = 0.20 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:29:10.438918: step 365290, loss = 0.11 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:29:11.402758: step 365300, loss = 0.13 (1328.0 examples/sec; 0.096 sec/batch)
2017-06-02 11:29:12.156253: step 365310, loss = 0.17 (1698.7 examples/sec; 0.075 sec/batch)
2017-06-02 11:29:12.989705: step 365320, loss = 0.16 (1535.8 examples/sec; 0.083 sec/batch)
2017-06-02 11:29:13.825945: step 365330, loss = 0.16 (1530.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:29:14.680821: step 365340, loss = 0.16 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:29:15.520486: step 365350, loss = 0.17 (1524.4 examples/sec; 0.084 sec/batch)
2017-06-02 11:29:16.404488: step 365360, loss = 0.16 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:29:17.257814: step 365370, loss = 0.15 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:29:18.105804: step 365380, loss = 0.16 (1509.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:29:18.982267: step 365390, loss = 0.11 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:29:19.964990: step 365400, loss = 0.14 (1302.5 examples/sec; 0.098 sec/batch)
2017-06-02 11:29:20.748022: step 365410, loss = 0.22 (1634.7 examples/sec; 0.078 sec/batch)
2017-06-02 11:29:21.625890: step 365420, loss = 0.18 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:29:22.488585: step 365430, loss = 0.15 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:29:23.354673: step 365440, loss = 0.25 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:29:24.238478: step 365450, loss = 0.13 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:29:25.126030: step 365460, loss = 0.15 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 11:29:26.002598: step 365470, loss = 0.20 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:29:26.846615: step 365480, loss = 0.18 (1516.6 examples/sec; 0.084 sec/batch)
2017-06-02 11:29:27.730434: step 365490, loss = 0.15 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:29:28.704598: step 365500, loss = 0.13 (1313.9 examples/sec; 0.097 sec/batch)
2017-06-02 11:29:29.463922: step 365510, loss = 0.18 (1685.7 examples/sec; 0.076 sec/batch)
2017-06-02 11:29:30.350058: step 365520, loss = 0.12 (1444.5 examples/sec; 0.089 sec/batch)
2017-06-02 11:29:31.244798: step 365530, loss = 0.12 (1430.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:29:32.109920: step 365540, loss = 0.12 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:29:32.984556: step 365550, loss = 0.17 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:29:33.852571: step 365560, loss = 0.18 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:29:34.732796: step 365570, loss = 0.16 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:29:35.617423: step 365580, loss = 0.12 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:29:36.480774: step 365590, loss = 0.14 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:29:37.463846: step 365600, loss = 0.15 (1302.0 examples/sec; 0.098 sec/batch)
2017-06-02 11:29:38.235171: step 365610, loss = 0.21 (1659.5 examples/sec; 0.077 sec/batch)
2017-06-02 11:29:39.070334: step 365620, loss = 0.17 (1532.6 examples/sec; 0.084 sec/batch)
2017-06-02 11:29:39.913415: step 365630, loss = 0.17 (1518.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:29:40.776826: step 365640, loss = 0.12 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:29:41.626775: step 365650, loss = 0.14 (1506.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:29:42.514128: step 365660, loss = 0.15 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 11:29:43.361994: step 365670, loss = 0.15 (1509.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:29:44.246716: step 365680, loss = 0.13 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:29:45.132414: step 365690, loss = 0.14 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 11:29:46.100522: step 365700, loss = 0.13 (1322.2 examples/sec; 0.097 sec/batch)
2017-06-02 11:29:46.863209: step 365710, loss = 0.14 (1678.3 examples/sec; 0.076 sec/batch)
2017-06-02 11:29:47.717060: step 365720, loss = 0.20 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:29:48.574601: step 365730, loss = 0.18 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:29:49.443672: step 365740, loss = 0.15 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:29:50.306210: step 365750, loss = 0.16 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:29:51.203096: step 365760, loss = 0.13 (1427.2 examples/sec; 0.090 sec/batch)
2017-06-02 11:29:52.084817: step 365770, loss = 0.14 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:29:53.005191: step 365780, loss = 0.16 (1390.7 examples/sec; 0.092 sec/batch)
2017-06-02 11:29:53.829910: step 365790, loss = 0.17 (1552.0 examples/sec; 0.082 sec/batch)
2017-06-02 11:29:54.808186: step 365800, loss = 0.17 (1308.4 examples/sec; 0.098 sec/batch)
2017-06-02 11:29:55.569741: step 365810, loss = 0.16 (1680.8 examples/sec; 0.076 sec/batch)
2017-06-02 11:29:56.426048: step 365820, loss = 0.15 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:29:57.284495: step 365830, loss = 0.13 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:29:58.141688: step 365840, loss = 0.22 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:29:59.006204: step 365850, loss = 0.14 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:29:59.883480: step 365860, loss = 0.16 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:30:00.746054: step 365870, loss = 0.17 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:30:01.594492: step 365880, loss = 0.16 (1508.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:30:02.426114: step 365890, loss = 0.15 (1539.2 examples/sec; 0.083 sec/batch)
2017-06-02 11:30:03.401794: step 365900, loss = 0.19 (1311.9 examples/sec; 0.098 sec/batch)
2017-06-02 11:30:04.147351: step 365910, loss = 0.17 (1716.8 examples/sec; 0.075 sec/batch)
2017-06-02 11:30:05.025606: step 365920, loss = 0.15 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:30:05.881885: step 365930, loss = 0.14 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:30:06.765403: step 365940, loss = 0.17 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:30:07.629837: step 365950, loss = 0.17 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:30:08.517825: step 365960, loss = 0.14 (1441.5 examples/sec; 0.089 sec/batch)
2017-06-02 11:30:09.406021: step 365970, loss = 0.19 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 11:30:10.253399: step 365980, loss = 0.17 (1510.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:30:11.141865: step 365990, loss = 0.13 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 11:30:12.092879: step 366000, loss = 0.16 (1345.9 examples/sec; 0.095 sec/batch)
2017-06-02 11:30:12.862721: step 366010, loss = 0.13 (1662.7 examples/sec; 0.077 sec/batch)
2017-06-02 11:30:13.743461: step 366020, loss = 0.15 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:30:14.608913: step 366030, loss = 0.18 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:30:15.483321: step 366040, loss = 0.14 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:30:16.333186: step 366050, loss = 0.15 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:30:17.193903: step 366060, loss = 0.15 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:30:18.076191: step 366070, loss = 0.15 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:30:18.911494: step 366080, loss = 0.17 (1532.4 examples/sec; 0.084 sec/batch)
2017-06-02 11:30:19.766102: step 366090, loss = 0.12 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:30:20.730858: step 366100, loss = 0.23 (1326.8 examples/sec; 0.096 sec/batch)
2017-06-02 11:30:21.502275: step 366110, loss = 0.15 (1659.3 examples/sec; 0.077 sec/batch)
2017-06-02 11:30:22.370877: step 366120, loss = 0.13 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:30:23.252990: step 366130, loss = 0.14 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:30:24.121401: step 366140, loss = 0.13 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:30:24.946195: step 366150, loss = 0.15 (1551.9 examples/sec; 0.082 sec/batch)
2017-06-02 11:30:25.823240: step 366160, loss = 0.14 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:30:26.701464: step 366170, loss = 0.15 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:30:27.536907: step 366180, loss = 0.14 (1532.1 examples/sec; 0.084 sec/batch)
2017-06-02 11:30:28.372256: step 366190, loss = 0.14 (1532.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:30:29.335634: step 366200, loss = 0.14 (1328.7 examples/sec; 0.096 sec/batch)
2017-06-02 11:30:30.107351: step 366210, loss = 0.15 (1658.6 examples/sec; 0.077 sec/batch)
2017-06-02 11:30:30.946840: step 366220, loss = 0.17 (1524.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:30:31.810789: step 366230, loss = 0.16 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:30:32.652935: step 366240, loss = 0.12 (1519.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:30:33.530343: step 366250, loss = 0.15 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:30:34.390204: step 366260, loss = 0.14 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:30:35.271880: step 366270, loss = 0.15 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:30:36.157881: step 366280, loss = 0.14 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 11:30:37.016216: step 366290, loss = 0.15 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:30:37.988496: step 366300, loss = 0.16 (1316.5 examples/sec; 0.097 sec/batch)
2017-06-02 11:30:38.764219: step 366310, loss = 0.16 (1650.1 examples/sec; 0.078 sec/batch)
2017-06-02 11:30:39.615177: step 366320, loss = 0.13 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:30:40.460532: step 366330, loss = 0.17 (1514.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:30:41.313524: step 366340, loss = 0.12 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:30:42.180822: step 366350, loss = 0.15 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:30:43.028486: step 366360, loss = 0.14 (1510.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:30:43.875672: step 366370, loss = 0.19 (1510.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:30:44.735670: step 366380, loss = 0.20 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:30:45.607316: step 366390, loss = 0.17 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:30:46.577062: step 366400, loss = 0.16 (1319.9 examples/sec; 0.097 sec/batch)
2017-06-02 11:30:47.350393: step 366410, loss = 0.19 (1655.2 examples/sec; 0.077 sec/batch)
2017-06-02 11:30:48.223722: step 366420, loss = 0.15 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:30:49.093200: step 366430, loss = 0.16 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:30:49.956707: step 366440, loss = 0.14 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:30:50.839354: step 366450, loss = 0.12 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:30:51.684833: step 366460, loss = 0.14 (1513.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:30:52.548466: step 366470, loss = 0.19 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:30:53.416301: step 366480, loss = 0.17 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:30:54.307746: step 366490, loss = 0.17 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 11:30:55.309647: step 366500, loss = 0.13 (1277.6 examples/sec; 0.100 sec/batch)
2017-06-02 11:30:56.076196: step 366510, loss = 0.17 (1669.8 examples/sec; 0.077 sec/batch)
2017-06-02 11:30:56.927476: step 366520, loss = 0.15 (1503.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:30:57.780400: step 366530, loss = 0.14 (1500.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:30:58.647936: step 366540, loss = 0.21 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:30:59.511009: step 366550, loss = 0.26 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:31:00.393679: step 366560, loss = 0.23 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:31:01.266474: step 366570, loss = 0.16 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:31:02.124908: step 366580, loss = 0.15 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:31:03.002214: step 366590, loss = 0.16 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:31:03.980547: step 366600, loss = 0.17 (1308.3 examples/sec; 0.098 sec/batch)
2017-06-02 11:31:04.740189: step 366610, loss = 0.15 (1685.0 examples/sec; 0.076 sec/batch)
2017-06-02 11:31:05.614658: step 366620, loss = 0.20 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:31:06.490006: step 366630, loss = 0.19 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:31:07.363469: step 366640, loss = 0.13 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:31:08.204984: step 366650, loss = 0.16 (1521.1 examples/sec; 0.084 sec/batch)
2017-06-02 11:31:09.090871: step 366660, loss = 0.18 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 11:31:09.949803: step 366670, loss = 0.14 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:31:10.826811: step 366680, loss = 0.16 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:31:11.698459: step 366690, loss = 0.15 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:31:12.673394: step 366700, loss = 0.12 (1312.9 examples/sec; 0.097 sec/batch)
2017-06-02 11:31:13.479291: step 366710, loss = 0.13 (1588.3 examples/sec; 0.081 sec/batch)
2017-06-02 11:31:14.339280: step 366720, loss = 0.17 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:31:15.171824: step 366730, loss = 0.13 (1537.5 examples/sec; 0.083 sec/batch)
2017-06-02 11:31:16.009850: step 366740, loss = 0.17 (1527.4 examples/sec; 0.084 sec/batch)
2017-06-02 11:31:16.895562: step 366750, loss = 0.16 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 11:31:17.752533: step 366760, loss = 0.19 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:31:18.602657: step 366770, loss = 0.21 (1505.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:31:19.461532: step 366780, loss = 0.16 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:31:20.346182: step 366790, loss = 0.16 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:31:21.323018: step 366800, loss = 0.15 (1310.3 examples/sec; 0.098 sec/batch)
2017-06-02 11:31:22.085858: step 366810, loss = 0.15 (1678.0 examples/sec; 0.076 sec/batch)
2017-06-02 11:31:22.959513: step 366820, loss = 0.16 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:31:23.827555: step 366830, loss = 0.19 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:31:24.685464: step 366840, loss = 0.18 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:31:25.542164: step 366850, loss = 0.17 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:31:26.412925: step 366860, loss = 0.16 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:31:27.267842: step 366870, loss = 0.13 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:31:28.145143: step 366880, loss = 0.15 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:31:29.008580: step 366890, loss = 0.18 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:31:29.974937: step 366900, loss = 0.14 (1324.6 examples/sec; 0.097 sec/batch)
2017-06-02 11:31:30.725592: step 366910, loss = 0.15 (1705.2 examples/sec; 0.075 sec/batch)
2017-06-02 11:31:31.580703: step 366920, loss = 0.16 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:31:32.454613: step 366930, loss = 0.22 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:31:33.310652: step 366940, loss = 0.15 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:31:34.210070: step 366950, loss = 0.15 (1423.2 examples/sec; 0.090 sec/batch)
2017-06-02 11:31:35.056025: step 366960, loss = 0.16 (1513.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:31:35.911214: step 366970, loss = 0.15 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:31:36.776634: step 366980, loss = 0.19 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:31:37.628412: step 366990, loss = 0.16 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:31:38.594352: step 367000, loss = 0.13 (1325.1 examples/sec; 0.097 sec/batch)
2017-06-02 11:31:39.357569: step 367010, loss = 0.18 (1677.1 examples/sec; 0.076 sec/batch)
2017-06-02 11:31:40.217830: step 367020, loss = 0.22 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:31:41.104920: step 367030, loss = 0.14 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 11:31:41.989809: step 367040, loss = 0.13 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:31:42.864160: step 367050, loss = 0.18 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:31:43.744281: step 367060, loss = 0.18 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:31:44.620721: step 367070, loss = 0.14 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:31:45.470415: step 367080, loss = 0.17 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:31:46.326072: step 367090, loss = 0.12 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:31:47.295048: step 367100, loss = 0.15 (1321.0 examples/sec; 0.097 sec/batch)
2017-06-02 11:31:48.055725: step 367110, loss = 0.16 (1682.7 examples/sec; 0.076 sec/batch)
2017-06-02 11:31:48.919910: step 367120, loss = 0.14 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:31:49.810962: step 367130, loss = 0.21 (1436.5 examples/sec; 0.089 sec/batch)
2017-06-02 11:31:50.657692: step 367140, loss = 0.21 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:31:51.559251: step 367150, loss = 0.14 (1419.8 examples/sec; 0.090 sec/batch)
2017-06-02 11:31:52.439873: step 367160, loss = 0.14 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:31:53.289557: step 367170, loss = 0.17 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:31:54.171078: step 367180, loss = 0.18 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:31:55.023251: step 367190, loss = 0.16 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:31:55.976133: step 367200, loss = 0.12 (1343.3 examples/sec; 0.095 sec/batch)
2017-06-02 11:31:56.733649: step 367210, loss = 0.17 (1689.7 examples/sec; 0.076 sec/batch)
2017-06-02 11:31:57.592900: step 367220, loss = 0.17 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:31:58.448965: step 367230, loss = 0.17 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:31:59.308180: step 367240, loss = 0.15 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:32:00.159304: step 367250, loss = 0.16 (1503.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:32:01.031292: step 367260, loss = 0.20 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:32:01.910992: step 367270, loss = 0.13 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:32:02.764886: step 367280, loss = 0.14 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:32:03.639361: step 367290, loss = 0.13 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:32:04.609669: step 367300, loss = 0.14 (1319.1 examples/sec; 0.097 sec/batch)
2017-06-02 11:32:05.399651: step 367310, loss = 0.15 (1620.3 examples/sec; 0.079 sec/batch)
2017-06-02 11:32:06.268970: step 367320, loss = 0.12 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:32:07.139026: step 367330, loss = 0.15 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:32:07.998873: step 367340, loss = 0.13 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:32:08.847262: step 367350, loss = 0.20 (1508.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:32:09.735871: step 367360, loss = 0.16 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 11:32:10.613868: step 367370, loss = 0.21 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:32:11.491326: step 367380, loss = 0.17 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:32:12.352106: step 367390, loss = 0.15 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:32:13.332489: step 367400, loss = 0.14 (1305.6 examples/sec; 0.098 sec/batch)
2017-06-02 11:32:14.100843: step 367410, loss = 0.12 (1665.9 examples/sec; 0.077 sec/batch)
2017-06-02 11:32:14.974469: step 367420, loss = 0.16 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:32:15.853534: step 367430, loss = 0.23 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:32:16.728923: step 367440, loss = 0.16 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:32:17.571391: step 367450, loss = 0.13 (1519.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:32:18.434757: step 367460, loss = 0.12 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:32:19.291495: step 367470, loss = 0.22 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:32:20.170844: step 367480, loss = 0.11 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:32:21.052604: step 367490, loss = 0.16 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:32:22.007710: step 367500, loss = 0.11 (1340.2 examples/sec; 0.096 sec/batch)
2017-06-02 11:32:22.779211: step 367510, loss = 0.21 (1659.1 examples/sec; 0.077 sec/batch)
2017-06-02 11:32:23.630604: step 367520, loss = 0.14 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:32:24.467380: step 367530, loss = 0.20 (1529.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:32:25.328250: step 367540, loss = 0.15 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:32:26.202154: step 367550, loss = 0.14 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:32:27.089621: step 367560, loss = 0.14 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 11:32:27.948875: step 367570, loss = 0.17 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:32:28.805995: step 367580, loss = 0.15 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:32:29.658386: step 367590, loss = 0.15 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:32:30.637277: step 367600, loss = 0.14 (1307.6 examples/sec; 0.098 sec/batch)
2017-06-02 11:32:31.388810: step 367610, loss = 0.19 (1703.2 examples/sec; 0.075 sec/batch)
2017-06-02 11:32:32.267038: step 367620, loss = 0.19 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:32:33.104916: step 367630, loss = 0.15 (1527.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:32:33.968625: step 367640, loss = 0.18 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:32:34.822384: step 367650, loss = 0.18 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:32:35.696317: step 367660, loss = 0.21 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:32:36.557784: step 367670, loss = 0.12 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:32:37.409299: step 367680, loss = 0.13 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:32:38.265200: step 367690, loss = 0.15 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:32:39.231866: step 367700, loss = 0.14 (1324.1 examples/sec; 0.097 sec/batch)
2017-06-02 11:32:39.993991: step 367710, loss = 0.14 (1679.5 examples/sec; 0.076 sec/batch)
2017-06-02 11:32:40.843129: step 367720, loss = 0.11 (1507.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:32:41.732194: step 367730, loss = 0.15 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 11:32:42.567377: step 367740, loss = 0.14 (1532.6 examples/sec; 0.084 sec/batch)
2017-06-02 11:32:43.443252: step 367750, loss = 0.19 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:32:44.308881: step 367760, loss = 0.18 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:32:45.152024: step 367770, loss = 0.14 (1518.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:32:46.051457: step 367780, loss = 0.14 (1423.1 examples/sec; 0.090 sec/batch)
2017-06-02 11:32:46.925228: step 367790, loss = 0.13 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:32:47.916996: step 367800, loss = 0.12 (1290.6 examples/sec; 0.099 sec/batch)
2017-06-02 11:32:48.703534: step 367810, loss = 0.16 (1627.4 examples/sec; 0.079 sec/batch)
2017-06-02 11:32:49.549093: step 367820, loss = 0.16 (1513.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:32:50.434149: step 367830, loss = 0.16 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 11:32:51.329985: step 367840, loss = 0.12 (1428.8 examples/sec; 0.090 sec/batch)
2017-06-02 11:32:52.207715: step 367850, loss = 0.19 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:32:53.101880: step 367860, loss = 0.15 (1431.5 examples/sec; 0.089 sec/batch)
2017-06-02 11:32:53.962268: step 367870, loss = 0.14 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:32:54.815255: step 367880, loss = 0.21 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:32:55.657987: step 367890, loss = 0.19 (1518.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:32:56.632409: step 367900, loss = 0.14 (1313.6 examples/sec; 0.097 sec/batch)
2017-06-02 11:32:57.373897: step 367910, loss = 0.15 (1726.3 examples/sec; 0.074 sec/batch)
2017-06-02 11:32:58.215467: step 367920, loss = 0.13 (1521.0 examples/sec; 0.084 sec/batch)
2017-06-02 11:32:59.085070: step 367930, loss = 0.18 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:32:59.970875: step 367940, loss = 0.19 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 11:33:00.845217: step 367950, loss = 0.19 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:33:01.712496: step 367960, loss = 0.15 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:33:02.555662: step 367970, loss = 0.12 (1518.1 examples/sec; 0.084 sec/batch)
2017-06-02 11:33:03.425457: step 367980, loss = 0.16 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:33:04.270759: step 367990, loss = 0.16 (1514.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:33:05.216221: step 368000, loss = 0.14 (1353.8 examples/sec; 0.095 sec/batch)
2017-06-02 11:33:05.982308: step 368010, loss = 0.17 (1670.8 examples/sec; 0.077 sec/batch)
2017-06-02 11:33:06.810868: step 368020, loss = 0.20 (1544.8 examples/sec; 0.083 sec/batch)
2017-06-02 11:33:07.689302: step 368030, loss = 0.15 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:33:08.553373: step 368040, loss = 0.19 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:33:09.411306: step 368050, loss = 0.13 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:33:10.272854: step 368060, loss = 0.17 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:33:11.100476: step 368070, loss = 0.14 (1546.6 examples/sec; 0.083 sec/batch)
2017-06-02 11:33:11.942560: step 368080, loss = 0.17 (1520.0 examples/sec; 0.084 sec/batch)
2017-06-02 11:33:12.801675: step 368090, loss = 0.19 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:33:13.786117: step 368100, loss = 0.14 (1300.2 examples/sec; 0.098 sec/batch)
2017-06-02 11:33:14.567441: step 368110, loss = 0.15 (1638.3 examples/sec; 0.078 sec/batch)
2017-06-02 11:33:15.443610: step 368120, loss = 0.15 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:33:16.297855: step 368130, loss = 0.15 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:33:17.165751: step 368140, loss = 0.15 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:33:18.011217: step 368150, loss = 0.18 (1513.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:33:18.894108: step 368160, loss = 0.11 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:33:19.761194: step 368170, loss = 0.17 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:33:20.647861: step 368180, loss = 0.14 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:33:21.494283: step 368190, loss = 0.17 (1512.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:33:22.519054: step 368200, loss = 0.20 (1249.1 examples/sec; 0.102 sec/batch)
2017-06-02 11:33:23.230586: step 368210, loss = 0.16 (1799.0 examples/sec; 0.071 sec/batch)
2017-06-02 11:33:24.114715: step 368220, loss = 0.16 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:33:24.973429: step 368230, loss = 0.13 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:33:25.816808: step 368240, loss = 0.14 (1517.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:33:26.699093: step 368250, loss = 0.18 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:33:27.583853: step 368260, loss = 0.16 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:33:28.464507: step 368270, loss = 0.17 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:33:29.338672: step 368280, loss = 0.16 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:33:30.214451: step 368290, loss = 0.17 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:33:31.194773: step 368300, loss = 0.22 (1305.7 examples/sec; 0.098 sec/batch)
2017-06-02 11:33:31.956620: step 368310, loss = 0.17 (1680.1 examples/sec; 0.076 sec/batch)
2017-06-02 11:33:32.806284: step 368320, loss = 0.21 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:33:33.676173: step 368330, loss = 0.19 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:33:34.527252: step 368340, loss = 0.18 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:33:35.390599: step 368350, loss = 0.14 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:33:36.260601: step 368360, loss = 0.18 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:33:37.118182: step 368370, loss = 0.15 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:33:37.996265: step 368380, loss = 0.20 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:33:38.835077: step 368390, loss = 0.19 (1526.0 examples/sec; 0.084 sec/batch)
2017-06-02 11:33:39.791287: step 368400, loss = 0.18 (1338.6 examples/sec; 0.096 sec/batch)
2017-06-02 11:33:40.552560: step 368410, loss = 0.13 (1681.4 examples/sec; 0.076 sec/batch)
2017-06-02 11:33:41.407244: step 368420, loss = 0.17 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:33:42.264602: step 368430, loss = 0.16 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:33:43.126171: step 368440, loss = 0.14 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:33:43.983039: step 368450, loss = 0.15 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:33:44.853871: step 368460, loss = 0.15 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:33:45.747702: step 368470, loss = 0.16 (1432.0 examples/sec; 0.089 sec/batch)
2017-06-02 11:33:46.603451: step 368480, loss = 0.18 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:33:47.474685: step 368490, loss = 0.12 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:33:48.434386: step 368500, loss = 0.13 (1333.8 examples/sec; 0.096 sec/batch)
2017-06-02 11:33:49.209743: step 368510, loss = 0.21 (1650.9 examples/sec; 0.078 sec/batch)
2017-06-02 11:33:50.080555: step 368520, loss = 0.11 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:33:50.956882: step 368530, loss = 0.16 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:33:51.828535: step 368540, loss = 0.20 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:33:52.700795: step 368550, loss = 0.18 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:33:53.583614: step 368560, loss = 0.16 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:33:54.464420: step 368570, loss = 0.20 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:33:55.361540: step 368580, loss = 0.16 (1426.8 examples/sec; 0.090 sec/batch)
2017-06-02 11:33:56.205648: step 368590, loss = 0.15 (1516.4 examples/sec; 0.084 sec/batch)
2017-06-02 11:33:57.154957: step 368600, loss = 0.16 (1348.4 examples/sec; 0.095 sec/batch)
2017-06-02 11:33:57.932255: step 368610, loss = 0.15 (1646.7 examples/sec; 0.078 sec/batch)
2017-06-02 11:33:58.804896: step 368620, loss = 0.18 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:33:59.659303: step 368630, loss = 0.17 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:34:00.521718: step 368640, loss = 0.18 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:34:01.362529: step 368650, loss = 0.13 (1522.4 examples/sec; 0.084 sec/batch)
2017-06-02 11:34:02.227586: step 368660, loss = 0.16 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:34:03.111757: step 368670, loss = 0.16 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:34:03.962848: step 368680, loss = 0.14 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:34:04.848486: step 368690, loss = 0.16 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 11:34:05.823097: step 368700, loss = 0.15 (1313.3 examples/sec; 0.097 sec/batch)
2017-06-02 11:34:06.583411: step 368710, loss = 0.18 (1683.5 examples/sec; 0.076 sec/batch)
2017-06-02 11:34:07.453646: step 368720, loss = 0.15 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:34:08.315202: step 368730, loss = 0.18 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:34:09.170179: step 368740, loss = 0.21 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:34:10.012191: step 368750, loss = 0.16 (1520.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:34:10.873800: step 368760, loss = 0.13 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:34:11.758382: step 368770, loss = 0.14 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:34:12.626352: step 368780, loss = 0.21 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:34:13.485557: step 368790, loss = 0.16 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:34:14.454471: step 368800, loss = 0.15 (1321.1 examples/sec; 0.097 sec/batch)
2017-06-02 11:34:15.209402: step 368810, loss = 0.15 (1695.5 examples/sec; 0.075 sec/batch)
2017-06-02 11:34:16.076216: step 368820, loss = 0.14 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:34:16.907751: step 368830, loss = 0.16 (1539.3 examples/sec; 0.083 sec/batch)
2017-06-02 11:34:17.764130: step 368840, loss = 0.20 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:34:18.613932: step 368850, loss = 0.20 (1506.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:34:19.483285: step 368860, loss = 0.16 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:34:20.333326: step 368870, loss = 0.18 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:34:21.201484: step 368880, loss = 0.16 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:34:22.072483: step 368890, loss = 0.14 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:34:23.047136: step 368900, loss = 0.13 (1313.3 examples/sec; 0.097 sec/batch)
2017-06-02 11:34:23.842999: step 368910, loss = 0.13 (1608.3 examples/sec; 0.080 sec/batch)
2017-06-02 11:34:24.715190: step 368920, loss = 0.14 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:34:25.576147: step 368930, loss = 0.12 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:34:26.456563: step 368940, loss = 0.21 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:34:27.322011: step 368950, loss = 0.18 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:34:28.184498: step 368960, loss = 0.14 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:34:29.049275: step 368970, loss = 0.20 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:34:29.932774: step 368980, loss = 0.14 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:34:30.780141: step 368990, loss = 0.17 (1510.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:34:31.752124: step 369000, loss = 0.15 (1316.9 examples/sec; 0.097 sec/batch)
2017-06-02 11:34:32.524921: step 369010, loss = 0.12 (1656.3 examples/sec; 0.077 sec/batch)
2017-06-02 11:34:33.402919: step 369020, loss = 0.15 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:34:34.271997: step 369030, loss = 0.15 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:34:35.127112: step 369040, loss = 0.14 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:34:35.980878: step 369050, loss = 0.15 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:34:36.856910: step 369060, loss = 0.15 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:34:37.702878: step 369070, loss = 0.16 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:34:38.579108: step 369080, loss = 0.14 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:34:39.420264: step 369090, loss = 0.13 (1521.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:34:40.371517: step 369100, loss = 0.15 (1345.6 examples/sec; 0.095 sec/batch)
2017-06-02 11:34:41.125713: step 369110, loss = 0.20 (1697.2 examples/sec; 0.075 sec/batch)
2017-06-02 11:34:42.011562: step 369120, loss = 0.13 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 11:34:42.872114: step 369130, loss = 0.15 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:34:43.751072: step 369140, loss = 0.19 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:34:44.585271: step 369150, loss = 0.17 (1534.4 examples/sec; 0.083 sec/batch)
2017-06-02 11:34:45.428174: step 369160, loss = 0.16 (1518.6 examples/sec; 0.084 sec/batch)
2017-06-02 11:34:46.295805: step 369170, loss = 0.17 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:34:47.162819: step 369180, loss = 0.16 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:34:48.013890: step 369190, loss = 0.17 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:34:49.000297: step 369200, loss = 0.17 (1297.6 examples/sec; 0.099 sec/batch)
2017-06-02 11:34:49.764318: step 369210, loss = 0.15 (1675.4 examples/sec; 0.076 sec/batch)
2017-06-02 11:34:50.606552: step 369220, loss = 0.17 (1519.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:34:51.484655: step 369230, loss = 0.15 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:34:52.375632: step 369240, loss = 0.17 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:34:53.239045: step 369250, loss = 0.12 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:34:54.108008: step 369260, loss = 0.17 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:34:54.953636: step 369270, loss = 0.18 (1513.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:34:55.831362: step 369280, loss = 0.13 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:34:56.690265: step 369290, loss = 0.13 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:34:57.652100: step 369300, loss = 0.13 (1330.8 examples/sec; 0.096 sec/batch)
2017-06-02 11:34:58.415164: step 369310, loss = 0.14 (1677.5 examples/sec; 0.076 sec/batch)
2017-06-02 11:34:59.276482: step 369320, loss = 0.15 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:35:00.114329: step 369330, loss = 0.14 (1527.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:35:00.959421: step 369340, loss = 0.13 (1514.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:35:01.834772: step 369350, loss = 0.16 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:35:02.728369: step 369360, loss = 0.14 (1432.4 examples/sec; 0.089 sec/batch)
2017-06-02 11:35:03.631269: step 369370, loss = 0.15 (1417.7 examples/sec; 0.090 sec/batch)
2017-06-02 11:35:04.514068: step 369380, loss = 0.17 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:35:05.399189: step 369390, loss = 0.12 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 11:35:06.379383: step 369400, loss = 0.15 (1305.9 examples/sec; 0.098 sec/batch)
2017-06-02 11:35:07.146871: step 369410, loss = 0.13 (1667.8 examples/sec; 0.077 sec/batch)
2017-06-02 11:35:08.003146: step 369420, loss = 0.15 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:35:08.850237: step 369430, loss = 0.20 (1511.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:35:09.709237: step 369440, loss = 0.20 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:35:10.583666: step 369450, loss = 0.17 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:35:11.444794: step 369460, loss = 0.14 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:35:12.331375: step 369470, loss = 0.15 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 11:35:13.191764: step 369480, loss = 0.19 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:35:14.054749: step 369490, loss = 0.16 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:35:15.017930: step 369500, loss = 0.13 (1328.9 examples/sec; 0.096 sec/batch)
2017-06-02 11:35:15.796164: step 369510, loss = 0.17 (1644.8 examples/sec; 0.078 sec/batch)
2017-06-02 11:35:16.665722: step 369520, loss = 0.14 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:35:17.538581: step 369530, loss = 0.15 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:35:18.394208: step 369540, loss = 0.20 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:35:19.260486: step 369550, loss = 0.13 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:35:20.123352: step 369560, loss = 0.13 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:35:21.009820: step 369570, loss = 0.18 (1443.9 examples/sec; 0.089 sec/batch)
2017-06-02 11:35:21.866311: step 369580, loss = 0.15 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:35:22.719433: step 369590, loss = 0.18 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:35:23.701357: step 369600, loss = 0.15 (1303.6 examples/sec; 0.098 sec/batch)
2017-06-02 11:35:24.488273: step 369610, loss = 0.21 (1626.6 examples/sec; 0.079 sec/batch)
2017-06-02 11:35:25.373582: step 369620, loss = 0.13 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 11:35:26.242222: step 369630, loss = 0.13 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:35:27.089111: step 369640, loss = 0.13 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:35:27.962449: step 369650, loss = 0.16 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:35:28.820398: step 369660, loss = 0.20 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:35:29.702039: step 369670, loss = 0.16 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:35:30.590127: step 369680, loss = 0.15 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 11:35:31.486206: step 369690, loss = 0.17 (1428.4 examples/sec; 0.090 sec/batch)
2017-06-02 11:35:32.540362: step 369700, loss = 0.14 (1214.2 examples/sec; 0.105 sec/batch)
2017-06-02 11:35:33.241240: step 369710, loss = 0.15 (1826.3 examples/sec; 0.070 sec/batch)
2017-06-02 11:35:34.108812: step 369720, loss = 0.14 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:35:34.994763: step 369730, loss = 0.18 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 11:35:35.848318: step 369740, loss = 0.11 (1499.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:35:36.722795: step 369750, loss = 0.14 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:35:37.583429: step 369760, loss = 0.14 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:35:38.462567: step 369770, loss = 0.18 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:35:39.336067: step 369780, loss = 0.16 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:35:40.194515: step 369790, loss = 0.13 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:35:41.169035: step 369800, loss = 0.15 (1313.5 examples/sec; 0.097 sec/batch)
2017-06-02 11:35:41.951135: step 369810, loss = 0.16 (1636.6 examples/sec; 0.078 sec/batch)
2017-06-02 11:35:42.815032: step 369820, loss = 0.17 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:35:43.683704: step 369830, loss = 0.11 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:35:44.537383: step 369840, loss = 0.15 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:35:45.409260: step 369850, loss = 0.17 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:35:46.268108: step 369860, loss = 0.12 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:35:47.156587: step 369870, loss = 0.18 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 11:35:48.007181: step 369880, loss = 0.19 (1504.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:35:48.887067: step 369890, loss = 0.17 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:35:49.867568: step 369900, loss = 0.16 (1305.5 examples/sec; 0.098 sec/batch)
2017-06-02 11:35:50.629110: step 369910, loss = 0.14 (1680.8 examples/sec; 0.076 sec/batch)
2017-06-02 11:35:51.506854: step 369920, loss = 0.19 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:35:52.368009: step 369930, loss = 0.19 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:35:53.230417: step 369940, loss = 0.12 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:35:54.115048: step 369950, loss = 0.20 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:35:54.981065: step 369960, loss = 0.16 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:35:55.859796: step 369970, loss = 0.15 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:35:56.727690: step 369980, loss = 0.14 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:35:57.615923: step 369990, loss = 0.14 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 11:35:58.609906: step 370000, loss = 0.13 (1287.7 examples/sec; 0.099 sec/batch)
2017-06-02 11:35:59.360022: step 370010, loss = 0.24 (1706.4 examples/sec; 0.075 sec/batch)
2017-06-02 11:36:00.215256: step 370020, loss = 0.20 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:36:01.087246: step 370030, loss = 0.18 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:36:01.944684: step 370040, loss = 0.14 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:36:02.787792: step 370050, loss = 0.14 (1518.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:36:03.638329: step 370060, loss = 0.14 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:36:04.502197: step 370070, loss = 0.17 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:36:05.363107: step 370080, loss = 0.17 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:36:06.213471: step 370090, loss = 0.14 (1505.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:36:07.174552: step 370100, loss = 0.18 (1331.8 examples/sec; 0.096 sec/batch)
2017-06-02 11:36:07.926683: step 370110, loss = 0.13 (1701.8 examples/sec; 0.075 sec/batch)
2017-06-02 11:36:08.781554: step 370120, loss = 0.13 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:36:09.664841: step 370130, loss = 0.19 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:36:10.528513: step 370140, loss = 0.16 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:36:11.423494: step 370150, loss = 0.15 (1430.2 examples/sec; 0.089 sec/batch)
2017-06-02 11:36:12.277115: step 370160, loss = 0.14 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:36:13.163998: step 370170, loss = 0.13 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 11:36:14.015284: step 370180, loss = 0.17 (1503.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:36:14.878066: step 370190, loss = 0.15 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:36:15.871137: step 370200, loss = 0.21 (1288.9 examples/sec; 0.099 sec/batch)
2017-06-02 11:36:16.646553: step 370210, loss = 0.17 (1650.8 examples/sec; 0.078 sec/batch)
2017-06-02 11:36:17.521655: step 370220, loss = 0.15 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:36:18.393597: step 370230, loss = 0.20 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:36:19.280825: step 370240, loss = 0.13 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 11:36:20.134886: step 370250, loss = 0.17 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:36:20.988214: step 370260, loss = 0.17 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:36:21.860859: step 370270, loss = 0.14 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:36:22.713975: step 370280, loss = 0.17 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:36:23.563746: step 370290, loss = 0.14 (1506.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:36:24.537586: step 370300, loss = 0.19 (1314.4 examples/sec; 0.097 sec/batch)
2017-06-02 11:36:25.299142: step 370310, loss = 0.17 (1680.8 examples/sec; 0.076 sec/batch)
2017-06-02 11:36:26.176033: step 370320, loss = 0.15 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:36:27.038593: step 370330, loss = 0.19 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:36:27.901367: step 370340, loss = 0.16 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:36:28.783267: step 370350, loss = 0.12 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:36:29.637591: step 370360, loss = 0.14 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:36:30.492768: step 370370, loss = 0.21 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:36:31.364703: step 370380, loss = 0.13 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:36:32.249474: step 370390, loss = 0.17 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:36:33.219352: step 370400, loss = 0.13 (1319.8 examples/sec; 0.097 sec/batch)
2017-06-02 11:36:33.992011: step 370410, loss = 0.12 (1656.6 examples/sec; 0.077 sec/batch)
2017-06-02 11:36:34.864020: step 370420, loss = 0.22 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:36:35.751954: step 370430, loss = 0.18 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:36:36.623013: step 370440, loss = 0.13 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:36:37.523287: step 370450, loss = 0.12 (1421.8 examples/sec; 0.090 sec/batch)
2017-06-02 11:36:38.425136: step 370460, loss = 0.15 (1419.3 examples/sec; 0.090 sec/batch)
2017-06-02 11:36:39.318617: step 370470, loss = 0.16 (1432.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:36:40.221909: step 370480, loss = 0.18 (1417.0 examples/sec; 0.090 sec/batch)
2017-06-02 11:36:41.101139: step 370490, loss = 0.16 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:36:42.093045: step 370500, loss = 0.16 (1290.4 examples/sec; 0.099 sec/batch)
2017-06-02 11:36:42.856924: step 370510, loss = 0.14 (1675.7 examples/sec; 0.076 sec/batch)
2017-06-02 11:36:43.725309: step 370520, loss = 0.12 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:36:44.576132: step 370530, loss = 0.16 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:36:45.432333: step 370540, loss = 0.15 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:36:46.299177: step 370550, loss = 0.14 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:36:47.171575: step 370560, loss = 0.15 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:36:48.044518: step 370570, loss = 0.14 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:36:48.911785: step 370580, loss = 0.15 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:36:49.783401: step 370590, loss = 0.14 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:36:50.757625: step 370600, loss = 0.14 (1313.9 examples/sec; 0.097 sec/batch)
2017-06-02 11:36:51.537396: step 370610, loss = 0.14 (1641.5 examples/sec; 0.078 sec/batch)
2017-06-02 11:36:52.380774: step 370620, loss = 0.13 (1517.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:36:53.249748: step 370630, loss = 0.14 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:36:54.133600: step 370640, loss = 0.14 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:36:54.975327: step 370650, loss = 0.17 (1520.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:36:55.858110: step 370660, loss = 0.16 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:36:56.721626: step 370670, loss = 0.13 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:36:57.587107: step 370680, loss = 0.14 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:36:58.454713: step 370690, loss = 0.16 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:36:59.405247: step 370700, loss = 0.12 (1346.6 examples/sec; 0.095 sec/batch)
2017-06-02 11:37:00.161685: step 370710, loss = 0.13 (1692.1 examples/sec; 0.076 sec/batch)
2017-06-02 11:37:01.025925: step 370720, loss = 0.17 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:37:01.892367: step 370730, loss = 0.15 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:37:02.770347: step 370740, loss = 0.16 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:37:03.632486: step 370750, loss = 0.15 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:37:04.516576: step 370760, loss = 0.14 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:37:05.389547: step 370770, loss = 0.18 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:37:06.255119: step 370780, loss = 0.13 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:37:07.114661: step 370790, loss = 0.16 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:37:08.080737: step 370800, loss = 0.24 (1324.9 examples/sec; 0.097 sec/batch)
2017-06-02 11:37:08.861536: step 370810, loss = 0.12 (1639.3 examples/sec; 0.078 sec/batch)
2017-06-02 11:37:09.731033: step 370820, loss = 0.16 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:37:10.590396: step 370830, loss = 0.22 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:37:11.459972: step 370840, loss = 0.13 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:37:12.342511: step 370850, loss = 0.16 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:37:13.236014: step 370860, loss = 0.14 (1432.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:37:14.105782: step 370870, loss = 0.12 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:37:14.976277: step 370880, loss = 0.12 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:37:15.855894: step 370890, loss = 0.13 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:37:16.833910: step 370900, loss = 0.12 (1308.8 examples/sec; 0.098 sec/batch)
2017-06-02 11:37:17.609384: step 370910, loss = 0.20 (1650.6 examples/sec; 0.078 sec/batch)
2017-06-02 11:37:18.498713: step 370920, loss = 0.13 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 11:37:19.360055: step 370930, loss = 0.22 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:37:20.236192: step 370940, loss = 0.21 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:37:21.132376: step 370950, loss = 0.17 (1428.3 examples/sec; 0.090 sec/batch)
2017-06-02 11:37:22.006579: step 370960, loss = 0.14 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:37:22.886249: step 370970, loss = 0.15 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:37:23.757198: step 370980, loss = 0.12 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:37:24.602819: step 370990, loss = 0.18 (1513.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:37:25.634895: step 371000, loss = 0.19 (1240.2 examples/sec; 0.103 sec/batch)
2017-06-02 11:37:26.343087: step 371010, loss = 0.19 (1807.4 examples/sec; 0.071 sec/batch)
2017-06-02 11:37:27.235074: step 371020, loss = 0.15 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 11:37:28.120277: step 371030, loss = 0.14 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 11:37:28.984548: step 371040, loss = 0.21 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:37:29.845750: step 371050, loss = 0.14 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:37:30.706687: step 371060, loss = 0.12 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:37:31.601344: step 371070, loss = 0.17 (1430.7 examples/sec; 0.089 sec/batch)
2017-06-02 11:37:32.450305: step 371080, loss = 0.21 (1507.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:37:33.282583: step 371090, loss = 0.13 (1537.9 examples/sec; 0.083 sec/batch)
2017-06-02 11:37:34.235702: step 371100, loss = 0.13 (1343.0 examples/sec; 0.095 sec/batch)
2017-06-02 11:37:34.998195: step 371110, loss = 0.16 (1678.7 examples/sec; 0.076 sec/batch)
2017-06-02 11:37:35.830456: step 371120, loss = 0.15 (1538.0 examples/sec; 0.083 sec/batch)
2017-06-02 11:37:36.687595: step 371130, loss = 0.15 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:37:37.541484: step 371140, loss = 0.16 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:37:38.407626: step 371150, loss = 0.19 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:37:39.282545: step 371160, loss = 0.15 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:37:40.154523: step 371170, loss = 0.18 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:37:41.050444: step 371180, loss = 0.17 (1428.7 examples/sec; 0.090 sec/batch)
2017-06-02 11:37:41.930245: step 371190, loss = 0.14 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:37:42.957804: step 371200, loss = 0.18 (1245.7 examples/sec; 0.103 sec/batch)
2017-06-02 11:37:43.642282: step 371210, loss = 0.19 (1872.4 examples/sec; 0.068 sec/batch)
2017-06-02 11:37:44.498710: step 371220, loss = 0.15 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:37:45.337112: step 371230, loss = 0.15 (1526.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:37:46.195387: step 371240, loss = 0.13 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:37:47.057132: step 371250, loss = 0.13 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:37:47.928692: step 371260, loss = 0.21 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:37:48.812697: step 371270, loss = 0.17 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:37:49.667381: step 371280, loss = 0.15 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:37:50.522188: step 371290, loss = 0.16 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:37:51.505685: step 371300, loss = 0.17 (1301.5 examples/sec; 0.098 sec/batch)
2017-06-02 11:37:52.290550: step 371310, loss = 0.15 (1630.8 examples/sec; 0.078 sec/batch)
2017-06-02 11:37:53.147140: step 371320, loss = 0.20 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:37:53.984179: step 371330, loss = 0.13 (1529.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:37:54.869837: step 371340, loss = 0.14 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 11:37:55.746898: step 371350, loss = 0.16 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:37:56.620629: step 371360, loss = 0.17 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:37:57.498781: step 371370, loss = 0.13 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:37:58.369136: step 371380, loss = 0.17 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:37:59.220277: step 371390, loss = 0.16 (1503.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:38:00.182792: step 371400, loss = 0.16 (1329.8 examples/sec; 0.096 sec/batch)
2017-06-02 11:38:00.958460: step 371410, loss = 0.13 (1650.2 examples/sec; 0.078 sec/batch)
2017-06-02 11:38:01.827449: step 371420, loss = 0.16 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:38:02.674106: step 371430, loss = 0.14 (1511.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:38:03.522168: step 371440, loss = 0.18 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:38:04.419273: step 371450, loss = 0.16 (1426.8 examples/sec; 0.090 sec/batch)
2017-06-02 11:38:05.290238: step 371460, loss = 0.17 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:38:06.148512: step 371470, loss = 0.15 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:38:07.017888: step 371480, loss = 0.14 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:38:07.904858: step 371490, loss = 0.20 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 11:38:08.891011: step 371500, loss = 0.15 (1298.0 examples/sec; 0.099 sec/batch)
2017-06-02 11:38:09.637259: step 371510, loss = 0.16 (1715.3 examples/sec; 0.075 sec/batch)
2017-06-02 11:38:10.485483: step 371520, loss = 0.18 (1509.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:38:11.357694: step 371530, loss = 0.12 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:38:12.214109: step 371540, loss = 0.14 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:38:13.093121: step 371550, loss = 0.14 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:38:13.976206: step 371560, loss = 0.17 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:38:14.827158: step 371570, loss = 0.25 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:38:15.716942: step 371580, loss = 0.19 (1438.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:38:16.584828: step 371590, loss = 0.15 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:38:17.558192: step 371600, loss = 0.15 (1315.0 examples/sec; 0.097 sec/batch)
2017-06-02 11:38:18.343403: step 371610, loss = 0.18 (1630.1 examples/sec; 0.079 sec/batch)
2017-06-02 11:38:19.198970: step 371620, loss = 0.19 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:38:20.077415: step 371630, loss = 0.12 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:38:20.959810: step 371640, loss = 0.15 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:38:21.837782: step 371650, loss = 0.19 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:38:22.709174: step 371660, loss = 0.14 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:38:23.580721: step 371670, loss = 0.15 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:38:24.422360: step 371680, loss = 0.16 (1520.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:38:25.291678: step 371690, loss = 0.16 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:38:26.249887: step 371700, loss = 0.15 (1335.8 examples/sec; 0.096 sec/batch)
2017-06-02 11:38:27.044554: step 371710, loss = 0.12 (1610.7 examples/sec; 0.079 sec/batch)
2017-06-02 11:38:27.905747: step 371720, loss = 0.11 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:38:28.792570: step 371730, loss = 0.17 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 11:38:29.664868: step 371740, loss = 0.13 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:38:30.515350: step 371750, loss = 0.15 (1505.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:38:31.381585: step 371760, loss = 0.13 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:38:32.259749: step 371770, loss = 0.13 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:38:33.115343: step 371780, loss = 0.18 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:38:33.983433: step 371790, loss = 0.14 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:38:35.003498: step 371800, loss = 0.19 (1254.8 examples/sec; 0.102 sec/batch)
2017-06-02 11:38:35.743130: step 371810, loss = 0.14 (1730.6 examples/sec; 0.074 sec/batch)
2017-06-02 11:38:36.612498: step 371820, loss = 0.12 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:38:37.494175: step 371830, loss = 0.14 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:38:38.352677: step 371840, loss = 0.15 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:38:39.234008: step 371850, loss = 0.15 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:38:40.111876: step 371860, loss = 0.16 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:38:40.972660: step 371870, loss = 0.17 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:38:41.848989: step 371880, loss = 0.15 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:38:42.698365: step 371890, loss = 0.18 (1507.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:38:43.676983: step 371900, loss = 0.14 (1308.0 examples/sec; 0.098 sec/batch)
2017-06-02 11:38:44.410541: step 371910, loss = 0.15 (1744.9 examples/sec; 0.073 sec/batch)
2017-06-02 11:38:45.280510: step 371920, loss = 0.15 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:38:46.149841: step 371930, loss = 0.19 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:38:47.024268: step 371940, loss = 0.13 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:38:47.876353: step 371950, loss = 0.14 (1502.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:38:48.767649: step 371960, loss = 0.18 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 11:38:49.679147: step 371970, loss = 0.14 (1404.3 examples/sec; 0.091 sec/batch)
2017-06-02 11:38:50.512819: step 371980, loss = 0.16 (1535.4 examples/sec; 0.083 sec/batch)
2017-06-02 11:38:51.374454: step 371990, loss = 0.12 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:38:52.342837: step 372000, loss = 0.12 (1321.8 examples/sec; 0.097 sec/batch)
2017-06-02 11:38:53.118764: step 372010, loss = 0.12 (1649.7 examples/sec; 0.078 sec/batch)
2017-06-02 11:38:53.969581: step 372020, loss = 0.14 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:38:54.841412: step 372030, loss = 0.19 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:38:55.709840: step 372040, loss = 0.15 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:38:56.572619: step 372050, loss = 0.16 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:38:57.454056: step 372060, loss = 0.15 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:38:58.314016: step 372070, loss = 0.14 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:38:59.157645: step 372080, loss = 0.18 (1517.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:39:00.030374: step 372090, loss = 0.13 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:39:01.007669: step 372100, loss = 0.14 (1309.7 examples/sec; 0.098 sec/batch)
2017-06-02 11:39:01.782175: step 372110, loss = 0.13 (1652.7 examples/sec; 0.077 sec/batch)
2017-06-02 11:39:02.626659: step 372120, loss = 0.13 (1515.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:39:03.480227: step 372130, loss = 0.15 (1499.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:39:04.340643: step 372140, loss = 0.12 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:39:05.210529: step 372150, loss = 0.16 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:39:06.063912: step 372160, loss = 0.15 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:39:06.932504: step 372170, loss = 0.17 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:39:07.790309: step 372180, loss = 0.14 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:39:08.668302: step 372190, loss = 0.13 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:39:09.666022: step 372200, loss = 0.16 (1282.9 examples/sec; 0.100 sec/batch)
2017-06-02 11:39:10.436168: step 372210, loss = 0.18 (1662.0 examples/sec; 0.077 sec/batch)
2017-06-02 11:39:11.283357: step 372220, loss = 0.14 (1510.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:39:12.179247: step 372230, loss = 0.14 (1428.8 examples/sec; 0.090 sec/batch)
2017-06-02 11:39:13.038354: step 372240, loss = 0.19 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:39:13.880215: step 372250, loss = 0.18 (1520.4 examples/sec; 0.084 sec/batch)
2017-06-02 11:39:14.764101: step 372260, loss = 0.15 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:39:15.614352: step 372270, loss = 0.13 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:39:16.494341: step 372280, loss = 0.16 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:39:17.368956: step 372290, loss = 0.16 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:39:18.361158: step 372300, loss = 0.12 (1290.0 examples/sec; 0.099 sec/batch)
2017-06-02 11:39:19.112047: step 372310, loss = 0.14 (1704.7 examples/sec; 0.075 sec/batch)
2017-06-02 11:39:20.004643: step 372320, loss = 0.12 (1434.0 examples/sec; 0.089 sec/batch)
2017-06-02 11:39:20.842221: step 372330, loss = 0.15 (1528.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:39:21.699683: step 372340, loss = 0.13 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:39:22.553697: step 372350, loss = 0.14 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:39:23.417940: step 372360, loss = 0.17 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:39:24.277656: step 372370, loss = 0.18 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:39:25.125552: step 372380, loss = 0.16 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:39:25.989252: step 372390, loss = 0.18 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:39:26.975366: step 372400, loss = 0.13 (1298.0 examples/sec; 0.099 sec/batch)
2017-06-02 11:39:27.755419: step 372410, loss = 0.15 (1640.9 examples/sec; 0.078 sec/batch)
2017-06-02 11:39:28.622673: step 372420, loss = 0.17 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:39:29.477683: step 372430, loss = 0.16 (1497.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:39:30.334666: step 372440, loss = 0.13 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:39:31.209040: step 372450, loss = 0.13 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:39:32.081640: step 372460, loss = 0.14 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:39:32.930101: step 372470, loss = 0.14 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:39:33.799369: step 372480, loss = 0.16 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:39:34.658871: step 372490, loss = 0.17 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:39:35.650013: step 372500, loss = 0.11 (1291.4 examples/sec; 0.099 sec/batch)
2017-06-02 11:39:36.426406: step 372510, loss = 0.12 (1648.7 examples/sec; 0.078 sec/batch)
2017-06-02 11:39:37.291588: step 372520, loss = 0.13 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:39:38.164645: step 372530, loss = 0.13 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:39:39.007567: step 372540, loss = 0.15 (1518.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:39:39.825873: step 372550, loss = 0.15 (1564.2 examples/sec; 0.082 sec/batch)
2017-06-02 11:39:40.679224: step 372560, loss = 0.19 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:39:41.507023: step 372570, loss = 0.11 (1546.3 examples/sec; 0.083 sec/batch)
2017-06-02 11:39:42.352767: step 372580, loss = 0.12 (1513.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:39:43.196899: step 372590, loss = 0.14 (1516.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:39:44.159562: step 372600, loss = 0.11 (1329.6 examples/sec; 0.096 sec/batch)
2017-06-02 11:39:44.941538: step 372610, loss = 0.21 (1636.9 examples/sec; 0.078 sec/batch)
2017-06-02 11:39:45.817919: step 372620, loss = 0.15 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:39:46.704343: step 372630, loss = 0.11 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 11:39:47.582073: step 372640, loss = 0.13 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:39:48.430997: step 372650, loss = 0.17 (1507.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:39:49.312306: step 372660, loss = 0.12 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:39:50.167134: step 372670, loss = 0.22 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:39:51.019208: step 372680, loss = 0.15 (1502.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:39:51.880476: step 372690, loss = 0.11 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:39:52.941567: step 372700, loss = 0.19 (1206.3 examples/sec; 0.106 sec/batch)
2017-06-02 11:39:53.678619: step 372710, loss = 0.18 (1736.6 examples/sec; 0.074 sec/batch)
2017-06-02 11:39:54.528036: step 372720, loss = 0.17 (1506.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:39:55.409156: step 372730, loss = 0.14 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:39:56.272575: step 372740, loss = 0.17 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:39:57.116504: step 372750, loss = 0.14 (1516.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:39:57.963782: step 372760, loss = 0.15 (1510.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:39:58.817127: step 372770, loss = 0.14 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:39:59.691802: step 372780, loss = 0.13 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:40:00.540923: step 372790, loss = 0.15 (1507.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:40:01.520848: step 372800, loss = 0.11 (1306.2 examples/sec; 0.098 sec/batch)
2017-06-02 11:40:02.289739: step 372810, loss = 0.15 (1664.7 examples/sec; 0.077 sec/batch)
2017-06-02 11:40:03.165226: step 372820, loss = 0.16 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:40:04.028357: step 372830, loss = 0.15 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:40:04.875194: step 372840, loss = 0.15 (1511.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:40:05.759180: step 372850, loss = 0.17 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:40:06.609405: step 372860, loss = 0.22 (1505.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:40:07.459398: step 372870, loss = 0.14 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:40:08.317500: step 372880, loss = 0.16 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:40:09.172784: step 372890, loss = 0.19 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:40:10.137440: step 372900, loss = 0.13 (1326.9 examples/sec; 0.096 sec/batch)
2017-06-02 11:40:10.890102: step 372910, loss = 0.14 (1700.6 examples/sec; 0.075 sec/batch)
2017-06-02 11:40:11.764321: step 372920, loss = 0.15 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:40:12.619970: step 372930, loss = 0.17 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:40:13.485378: step 372940, loss = 0.16 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:40:14.345918: step 372950, loss = 0.16 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:40:15.208700: step 372960, loss = 0.15 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:40:16.086145: step 372970, loss = 0.15 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:40:16.949747: step 372980, loss = 0.10 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:40:17.789296: step 372990, loss = 0.14 (1524.6 examples/sec; 0.084 sec/batch)
2017-06-02 11:40:18.781995: step 373000, loss = 0.19 (1289.4 examples/sec; 0.099 sec/batch)
2017-06-02 11:40:19.506534: step 373010, loss = 0.16 (1766.7 examples/sec; 0.072 sec/batch)
2017-06-02 11:40:20.391414: step 373020, loss = 0.17 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:40:21.243177: step 373030, loss = 0.19 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:40:22.102893: step 373040, loss = 0.12 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:40:22.983965: step 373050, loss = 0.16 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:40:23.832211: step 373060, loss = 0.12 (1509.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:40:24.697755: step 373070, loss = 0.14 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:40:25.565958: step 373080, loss = 0.12 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:40:26.460604: step 373090, loss = 0.15 (1430.7 examples/sec; 0.089 sec/batch)
2017-06-02 11:40:27.427571: step 373100, loss = 0.16 (1323.7 examples/sec; 0.097 sec/batch)
2017-06-02 11:40:28.200946: step 373110, loss = 0.18 (1655.1 examples/sec; 0.077 sec/batch)
2017-06-02 11:40:29.065367: step 373120, loss = 0.15 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:40:29.933781: step 373130, loss = 0.15 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:40:30.763794: step 373140, loss = 0.17 (1542.1 examples/sec; 0.083 sec/batch)
2017-06-02 11:40:31.638078: step 373150, loss = 0.22 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:40:32.528042: step 373160, loss = 0.16 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 11:40:33.387059: step 373170, loss = 0.13 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:40:34.242694: step 373180, loss = 0.13 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:40:35.095803: step 373190, loss = 0.15 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:40:36.045686: step 373200, loss = 0.15 (1347.5 examples/sec; 0.095 sec/batch)
2017-06-02 11:40:36.810563: step 373210, loss = 0.15 (1673.5 examples/sec; 0.076 sec/batch)
2017-06-02 11:40:37.681016: step 373220, loss = 0.15 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:40:38.581239: step 373230, loss = 0.13 (1421.9 examples/sec; 0.090 sec/batch)
2017-06-02 11:40:39.435175: step 373240, loss = 0.17 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:40:40.327107: step 373250, loss = 0.11 (1435.1 examples/sec; 0.089 sec/batch)
2017-06-02 11:40:41.175658: step 373260, loss = 0.16 (1508.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:40:42.053949: step 373270, loss = 0.13 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:40:42.924374: step 373280, loss = 0.17 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:40:43.768768: step 373290, loss = 0.15 (1515.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:40:44.739468: step 373300, loss = 0.12 (1318.6 examples/sec; 0.097 sec/batch)
2017-06-02 11:40:45.495100: step 373310, loss = 0.12 (1694.0 examples/sec; 0.076 sec/batch)
2017-06-02 11:40:46.373808: step 373320, loss = 0.16 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:40:47.234830: step 373330, loss = 0.14 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:40:48.104042: step 373340, loss = 0.12 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:40:48.983686: step 373350, loss = 0.16 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:40:49.850460: step 373360, loss = 0.13 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:40:50.735855: step 373370, loss = 0.19 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 11:40:51.579855: step 373380, loss = 0.15 (1516.6 examples/sec; 0.084 sec/batch)
2017-06-02 11:40:52.448220: step 373390, loss = 0.16 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:40:53.456059: step 373400, loss = 0.16 (1270.0 examples/sec; 0.101 sec/batch)
2017-06-02 11:40:54.214565: step 373410, loss = 0.14 (1687.5 examples/sec; 0.076 sec/batch)
2017-06-02 11:40:55.104936: step 373420, loss = 0.13 (1437.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:40:55.956831: step 373430, loss = 0.14 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:40:56.820968: step 373440, loss = 0.18 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:40:57.669456: step 373450, loss = 0.14 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:40:58.524241: step 373460, loss = 0.16 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:40:59.380874: step 373470, loss = 0.15 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:41:00.251981: step 373480, loss = 0.17 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:41:01.135051: step 373490, loss = 0.13 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:41:02.129060: step 373500, loss = 0.13 (1287.7 examples/sec; 0.099 sec/batch)
2017-06-02 11:41:02.871688: step 373510, loss = 0.14 (1723.6 examples/sec; 0.074 sec/batch)
2017-06-02 11:41:03.752032: step 373520, loss = 0.13 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:41:04.648221: step 373530, loss = 0.16 (1428.3 examples/sec; 0.090 sec/batch)
2017-06-02 11:41:05.512995: step 373540, loss = 0.12 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:41:06.398229: step 373550, loss = 0.14 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 11:41:07.268426: step 373560, loss = 0.19 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:41:08.146529: step 373570, loss = 0.14 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:41:09.013131: step 373580, loss = 0.17 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:41:09.901112: step 373590, loss = 0.14 (1441.5 examples/sec; 0.089 sec/batch)
2017-06-02 11:41:10.919931: step 373600, loss = 0.19 (1256.4 examples/sec; 0.102 sec/batch)
2017-06-02 11:41:11.686614: step 373610, loss = 0.20 (1669.5 examples/sec; 0.077 sec/batch)
2017-06-02 11:41:12.559293: step 373620, loss = 0.17 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:41:13.443522: step 373630, loss = 0.16 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:41:14.303260: step 373640, loss = 0.16 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:41:15.174251: step 373650, loss = 0.13 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:41:16.037703: step 373660, loss = 0.17 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:41:16.885385: step 373670, loss = 0.18 (1510.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:41:17.745778: step 373680, loss = 0.16 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:41:18.623567: step 373690, loss = 0.13 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:41:19.597042: step 373700, loss = 0.12 (1314.8 examples/sec; 0.097 sec/batch)
2017-06-02 11:41:20.359830: step 373710, loss = 0.15 (1678.1 examples/sec; 0.076 sec/batch)
2017-06-02 11:41:21.210726: step 373720, loss = 0.18 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:41:22.066995: step 373730, loss = 0.12 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:41:22.948432: step 373740, loss = 0.15 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:41:23.804174: step 373750, loss = 0.21 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:41:24.670995: step 373760, loss = 0.13 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:41:25.526312: step 373770, loss = 0.15 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:41:26.388004: step 373780, loss = 0.16 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:41:27.259857: step 373790, loss = 0.15 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:41:28.305446: step 373800, loss = 0.16 (1224.2 examples/sec; 0.105 sec/batch)
2017-06-02 11:41:29.011321: step 373810, loss = 0.13 (1813.4 examples/sec; 0.071 sec/batch)
2017-06-02 11:41:29.914744: step 373820, loss = 0.15 (1416.9 examples/sec; 0.090 sec/batch)
2017-06-02 11:41:30.797102: step 373830, loss = 0.17 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:41:31.685455: step 373840, loss = 0.18 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 11:41:32.580031: step 373850, loss = 0.14 (1430.8 examples/sec; 0.089 sec/batch)
2017-06-02 11:41:33.484394: step 373860, loss = 0.15 (1415.4 examples/sec; 0.090 sec/batch)
2017-06-02 11:41:34.351639: step 373870, loss = 0.16 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:41:35.211549: step 373880, loss = 0.19 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:41:36.061029: step 373890, loss = 0.17 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:41:37.047580: step 373900, loss = 0.19 (1297.4 examples/sec; 0.099 sec/batch)
2017-06-02 11:41:37.813778: step 373910, loss = 0.14 (1670.6 examples/sec; 0.077 sec/batch)
2017-06-02 11:41:38.691767: step 373920, loss = 0.15 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:41:39.556414: step 373930, loss = 0.15 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:41:40.433537: step 373940, loss = 0.12 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:41:41.293806: step 373950, loss = 0.16 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:41:42.150997: step 373960, loss = 0.15 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:41:43.033304: step 373970, loss = 0.16 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:41:43.893438: step 373980, loss = 0.13 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:41:44.786857: step 373990, loss = 0.12 (1432.7 examples/sec; 0.089 sec/batch)
2017-06-02 11:41:45.748333: step 374000, loss = 0.14 (1331.3 examples/sec; 0.096 sec/batch)
2017-06-02 11:41:46.509100: step 374010, loss = 0.14 (1682.5 examples/sec; 0.076 sec/batch)
2017-06-02 11:41:47.384247: step 374020, loss = 0.13 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:41:48.262979: step 374030, loss = 0.17 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:41:49.138530: step 374040, loss = 0.19 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:41:49.982338: step 374050, loss = 0.12 (1516.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:41:50.839031: step 374060, loss = 0.15 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:41:51.705240: step 374070, loss = 0.13 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:41:52.601164: step 374080, loss = 0.14 (1428.7 examples/sec; 0.090 sec/batch)
2017-06-02 11:41:53.474624: step 374090, loss = 0.14 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:41:54.454990: step 374100, loss = 0.13 (1305.6 examples/sec; 0.098 sec/batch)
2017-06-02 11:41:55.224074: step 374110, loss = 0.18 (1664.3 examples/sec; 0.077 sec/batch)
2017-06-02 11:41:56.078638: step 374120, loss = 0.15 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:41:56.947521: step 374130, loss = 0.15 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:41:57.822153: step 374140, loss = 0.14 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:41:58.671041: step 374150, loss = 0.13 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:41:59.568081: step 374160, loss = 0.15 (1426.9 examples/sec; 0.090 sec/batch)
2017-06-02 11:42:00.424022: step 374170, loss = 0.19 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:42:01.280427: step 374180, loss = 0.14 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:42:02.133309: step 374190, loss = 0.20 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:42:03.127171: step 374200, loss = 0.13 (1287.9 examples/sec; 0.099 sec/batch)
2017-06-02 11:42:03.881561: step 374210, loss = 0.13 (1696.7 examples/sec; 0.075 sec/batch)
2017-06-02 11:42:04.748327: step 374220, loss = 0.13 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:42:05.606540: step 374230, loss = 0.16 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:42:06.464195: step 374240, loss = 0.17 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:42:07.316235: step 374250, loss = 0.12 (1502.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:42:08.189018: step 374260, loss = 0.13 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:42:09.080189: step 374270, loss = 0.13 (1436.3 examples/sec; 0.089 sec/batch)
2017-06-02 11:42:09.917306: step 374280, loss = 0.11 (1529.1 examples/sec; 0.084 sec/batch)
2017-06-02 11:42:10.767422: step 374290, loss = 0.13 (1505.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:42:11.714033: step 374300, loss = 0.15 (1352.2 examples/sec; 0.095 sec/batch)
2017-06-02 11:42:12.476503: step 374310, loss = 0.15 (1678.8 examples/sec; 0.076 sec/batch)
2017-06-02 11:42:13.344128: step 374320, loss = 0.18 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:42:14.184642: step 374330, loss = 0.15 (1522.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:42:15.031329: step 374340, loss = 0.17 (1511.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:42:15.895633: step 374350, loss = 0.19 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:42:16.753622: step 374360, loss = 0.16 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:42:17.607943: step 374370, loss = 0.15 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:42:18.449599: step 374380, loss = 0.17 (1520.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:42:19.315910: step 374390, loss = 0.16 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:42:20.262015: step 374400, loss = 0.14 (1352.9 examples/sec; 0.095 sec/batch)
2017-06-02 11:42:21.019965: step 374410, loss = 0.17 (1688.8 examples/sec; 0.076 sec/batch)
2017-06-02 11:42:21.902906: step 374420, loss = 0.18 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:42:22.759220: step 374430, loss = 0.14 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:42:23.598991: step 374440, loss = 0.17 (1524.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:42:24.459334: step 374450, loss = 0.12 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:42:25.319147: step 374460, loss = 0.16 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:42:26.186605: step 374470, loss = 0.18 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:42:27.055566: step 374480, loss = 0.12 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:42:27.917800: step 374490, loss = 0.16 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:42:28.887052: step 374500, loss = 0.15 (1320.6 examples/sec; 0.097 sec/batch)
2017-06-02 11:42:29.648886: step 374510, loss = 0.13 (1680.2 examples/sec; 0.076 sec/batch)
2017-06-02 11:42:30.519712: step 374520, loss = 0.17 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:42:31.400186: step 374530, loss = 0.19 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:42:32.264047: step 374540, loss = 0.16 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:42:33.112005: step 374550, loss = 0.14 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:42:33.964008: step 374560, loss = 0.13 (1502.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:42:34.827440: step 374570, loss = 0.17 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:42:35.701700: step 374580, loss = 0.15 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:42:36.573560: step 374590, loss = 0.13 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:42:37.597827: step 374600, loss = 0.18 (1249.7 examples/sec; 0.102 sec/batch)
2017-06-02 11:42:38.307791: step 374610, loss = 0.14 (1802.9 examples/sec; 0.071 sec/batch)
2017-06-02 11:42:39.140092: step 374620, loss = 0.13 (1537.9 examples/sec; 0.083 sec/batch)
2017-06-02 11:42:40.002176: step 374630, loss = 0.12 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:42:40.882345: step 374640, loss = 0.18 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:42:41.758691: step 374650, loss = 0.20 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:42:42.641188: step 374660, loss = 0.15 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:42:43.489644: step 374670, loss = 0.15 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:42:44.350925: step 374680, loss = 0.18 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:42:45.192782: step 374690, loss = 0.13 (1520.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:42:46.151274: step 374700, loss = 0.15 (1335.4 examples/sec; 0.096 sec/batch)
2017-06-02 11:42:46.923505: step 374710, loss = 0.13 (1657.6 examples/sec; 0.077 sec/batch)
2017-06-02 11:42:47.789855: step 374720, loss = 0.16 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:42:48.628611: step 374730, loss = 0.16 (1526.1 examples/sec; 0.084 sec/batch)
2017-06-02 11:42:49.473905: step 374740, loss = 0.16 (1514.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:42:50.353697: step 374750, loss = 0.16 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:42:51.185814: step 374760, loss = 0.13 (1538.2 examples/sec; 0.083 sec/batch)
2017-06-02 11:42:52.057053: step 374770, loss = 0.13 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:42:52.925660: step 374780, loss = 0.16 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:42:53.790034: step 374790, loss = 0.16 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:42:54.750968: step 374800, loss = 0.17 (1332.0 examples/sec; 0.096 sec/batch)
2017-06-02 11:42:55.489222: step 374810, loss = 0.15 (1733.9 examples/sec; 0.074 sec/batch)
2017-06-02 11:42:56.366926: step 374820, loss = 0.16 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:42:57.248521: step 374830, loss = 0.13 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:42:58.126982: step 374840, loss = 0.17 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:42:59.023370: step 374850, loss = 0.14 (1427.9 examples/sec; 0.090 sec/batch)
2017-06-02 11:42:59.876646: step 374860, loss = 0.18 (1500.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:43:00.753211: step 374870, loss = 0.14 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:43:01.636670: step 374880, loss = 0.12 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:43:02.503346: step 374890, loss = 0.12 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:43:03.511578: step 374900, loss = 0.15 (1269.5 examples/sec; 0.101 sec/batch)
2017-06-02 11:43:04.281365: step 374910, loss = 0.13 (1662.8 examples/sec; 0.077 sec/batch)
2017-06-02 11:43:05.150090: step 374920, loss = 0.14 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:43:06.021531: step 374930, loss = 0.13 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:43:06.856802: step 374940, loss = 0.14 (1532.4 examples/sec; 0.084 sec/batch)
2017-06-02 11:43:07.734046: step 374950, loss = 0.21 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:43:08.625682: step 374960, loss = 0.15 (1435.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:43:09.486242: step 374970, loss = 0.14 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:43:10.361077: step 374980, loss = 0.16 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:43:11.230853: step 374990, loss = 0.20 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:43:12.192608: step 375000, loss = 0.14 (1330.9 examples/sec; 0.096 sec/batch)
2017-06-02 11:43:12.947222: step 375010, loss = 0.11 (1696.2 examples/sec; 0.075 sec/batch)
2017-06-02 11:43:13.806202: step 375020, loss = 0.14 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:43:14.671661: step 375030, loss = 0.16 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:43:15.561684: step 375040, loss = 0.16 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 11:43:16.405161: step 375050, loss = 0.15 (1517.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:43:17.279215: step 375060, loss = 0.13 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:43:18.144330: step 375070, loss = 0.12 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:43:18.992427: step 375080, loss = 0.14 (1509.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:43:19.857535: step 375090, loss = 0.18 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:43:20.835497: step 375100, loss = 0.11 (1308.9 examples/sec; 0.098 sec/batch)
2017-06-02 11:43:21.631910: step 375110, loss = 0.16 (1607.2 examples/sec; 0.080 sec/batch)
2017-06-02 11:43:22.491692: step 375120, loss = 0.16 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:43:23.374566: step 375130, loss = 0.22 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:43:24.219580: step 375140, loss = 0.14 (1514.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:43:25.069178: step 375150, loss = 0.16 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:43:25.930463: step 375160, loss = 0.24 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:43:26.781304: step 375170, loss = 0.16 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:43:27.649129: step 375180, loss = 0.18 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:43:28.518210: step 375190, loss = 0.13 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:43:29.499471: step 375200, loss = 0.16 (1304.4 examples/sec; 0.098 sec/batch)
2017-06-02 11:43:30.259814: step 375210, loss = 0.14 (1683.4 examples/sec; 0.076 sec/batch)
2017-06-02 11:43:31.101259: step 375220, loss = 0.14 (1521.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:43:31.933722: step 375230, loss = 0.14 (1537.6 examples/sec; 0.083 sec/batch)
2017-06-02 11:43:32.813258: step 375240, loss = 0.20 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:43:33.672457: step 375250, loss = 0.15 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:43:34.512515: step 375260, loss = 0.13 (1523.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:43:35.361456: step 375270, loss = 0.13 (1507.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:43:36.244200: step 375280, loss = 0.19 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:43:37.105334: step 375290, loss = 0.17 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:43:38.051779: step 375300, loss = 0.13 (1352.4 examples/sec; 0.095 sec/batch)
2017-06-02 11:43:38.811355: step 375310, loss = 0.13 (1685.1 examples/sec; 0.076 sec/batch)
2017-06-02 11:43:39.655223: step 375320, loss = 0.22 (1516.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:43:40.515987: step 375330, loss = 0.20 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:43:41.409224: step 375340, loss = 0.16 (1433.0 examples/sec; 0.089 sec/batch)
2017-06-02 11:43:42.306675: step 375350, loss = 0.17 (1426.2 examples/sec; 0.090 sec/batch)
2017-06-02 11:43:43.167244: step 375360, loss = 0.15 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:43:44.016970: step 375370, loss = 0.15 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:43:44.844342: step 375380, loss = 0.15 (1547.1 examples/sec; 0.083 sec/batch)
2017-06-02 11:43:45.690713: step 375390, loss = 0.14 (1512.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:43:46.671406: step 375400, loss = 0.14 (1305.2 examples/sec; 0.098 sec/batch)
2017-06-02 11:43:47.462894: step 375410, loss = 0.16 (1617.2 examples/sec; 0.079 sec/batch)
2017-06-02 11:43:48.336143: step 375420, loss = 0.14 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:43:49.205954: step 375430, loss = 0.15 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:43:50.064007: step 375440, loss = 0.16 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:43:50.924437: step 375450, loss = 0.12 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:43:51.787398: step 375460, loss = 0.15 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:43:52.632149: step 375470, loss = 0.14 (1515.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:43:53.517048: step 375480, loss = 0.19 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:43:54.407069: step 375490, loss = 0.13 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 11:43:55.391844: step 375500, loss = 0.12 (1299.8 examples/sec; 0.098 sec/batch)
2017-06-02 11:43:56.153984: step 375510, loss = 0.22 (1679.5 examples/sec; 0.076 sec/batch)
2017-06-02 11:43:57.011806: step 375520, loss = 0.14 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:43:57.864471: step 375530, loss = 0.16 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:43:58.714899: step 375540, loss = 0.14 (1505.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:43:59.579801: step 375550, loss = 0.12 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:44:00.417937: step 375560, loss = 0.14 (1527.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:44:01.243150: step 375570, loss = 0.15 (1551.1 examples/sec; 0.083 sec/batch)
2017-06-02 11:44:02.121666: step 375580, loss = 0.18 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:44:02.989006: step 375590, loss = 0.15 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:44:03.965507: step 375600, loss = 0.13 (1310.8 examples/sec; 0.098 sec/batch)
2017-06-02 11:44:04.753282: step 375610, loss = 0.16 (1624.8 examples/sec; 0.079 sec/batch)
2017-06-02 11:44:05.598202: step 375620, loss = 0.12 (1514.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:44:06.445063: step 375630, loss = 0.16 (1511.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:44:07.307075: step 375640, loss = 0.16 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:44:08.177021: step 375650, loss = 0.17 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:44:09.042057: step 375660, loss = 0.21 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:44:09.905709: step 375670, loss = 0.17 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:44:10.740053: step 375680, loss = 0.13 (1534.1 examples/sec; 0.083 sec/batch)
2017-06-02 11:44:11.609159: step 375690, loss = 0.13 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:44:12.574784: step 375700, loss = 0.13 (1325.6 examples/sec; 0.097 sec/batch)
2017-06-02 11:44:13.335207: step 375710, loss = 0.14 (1683.3 examples/sec; 0.076 sec/batch)
2017-06-02 11:44:14.198182: step 375720, loss = 0.15 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:44:15.072122: step 375730, loss = 0.17 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:44:15.932898: step 375740, loss = 0.11 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:44:16.796201: step 375750, loss = 0.14 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:44:17.648944: step 375760, loss = 0.14 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:44:18.532534: step 375770, loss = 0.14 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:44:19.410894: step 375780, loss = 0.17 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:44:20.297198: step 375790, loss = 0.17 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 11:44:21.269869: step 375800, loss = 0.13 (1316.0 examples/sec; 0.097 sec/batch)
2017-06-02 11:44:22.034498: step 375810, loss = 0.13 (1674.0 examples/sec; 0.076 sec/batch)
2017-06-02 11:44:22.899641: step 375820, loss = 0.13 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:44:23.742035: step 375830, loss = 0.17 (1519.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:44:24.620189: step 375840, loss = 0.19 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:44:25.497770: step 375850, loss = 0.17 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:44:26.372102: step 375860, loss = 0.12 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:44:27.238449: step 375870, loss = 0.15 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:44:28.083390: step 375880, loss = 0.13 (1514.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:44:28.954433: step 375890, loss = 0.18 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:44:29.964375: step 375900, loss = 0.18 (1267.4 examples/sec; 0.101 sec/batch)
2017-06-02 11:44:30.682845: step 375910, loss = 0.11 (1781.6 examples/sec; 0.072 sec/batch)
2017-06-02 11:44:31.539830: step 375920, loss = 0.12 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:44:32.392546: step 375930, loss = 0.21 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:44:33.230783: step 375940, loss = 0.13 (1527.0 examples/sec; 0.084 sec/batch)
2017-06-02 11:44:34.058579: step 375950, loss = 0.14 (1546.3 examples/sec; 0.083 sec/batch)
2017-06-02 11:44:34.893951: step 375960, loss = 0.14 (1532.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:44:35.756381: step 375970, loss = 0.16 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:44:36.604851: step 375980, loss = 0.12 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:44:37.445303: step 375990, loss = 0.15 (1523.0 examples/sec; 0.084 sec/batch)
2017-06-02 11:44:38.467986: step 376000, loss = 0.18 (1251.6 examples/sec; 0.102 sec/batch)
2017-06-02 11:44:39.167150: step 376010, loss = 0.13 (1830.8 examples/sec; 0.070 sec/batch)
2017-06-02 11:44:40.049099: step 376020, loss = 0.12 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:44:40.921599: step 376030, loss = 0.13 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:44:41.758108: step 376040, loss = 0.12 (1530.1 examples/sec; 0.084 sec/batch)
2017-06-02 11:44:42.619933: step 376050, loss = 0.10 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:44:43.501725: step 376060, loss = 0.23 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:44:44.374925: step 376070, loss = 0.13 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:44:45.250268: step 376080, loss = 0.15 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:44:46.100186: step 376090, loss = 0.15 (1506.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:44:47.112435: step 376100, loss = 0.12 (1264.5 examples/sec; 0.101 sec/batch)
2017-06-02 11:44:47.810873: step 376110, loss = 0.18 (1832.7 examples/sec; 0.070 sec/batch)
2017-06-02 11:44:48.641081: step 376120, loss = 0.20 (1541.8 examples/sec; 0.083 sec/batch)
2017-06-02 11:44:49.501112: step 376130, loss = 0.15 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:44:50.356970: step 376140, loss = 0.13 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:44:51.246285: step 376150, loss = 0.15 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 11:44:52.108836: step 376160, loss = 0.17 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:44:52.935837: step 376170, loss = 0.21 (1547.8 examples/sec; 0.083 sec/batch)
2017-06-02 11:44:53.798768: step 376180, loss = 0.15 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:44:54.666936: step 376190, loss = 0.16 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:44:55.658549: step 376200, loss = 0.15 (1290.8 examples/sec; 0.099 sec/batch)
2017-06-02 11:44:56.407172: step 376210, loss = 0.12 (1709.8 examples/sec; 0.075 sec/batch)
2017-06-02 11:44:57.289819: step 376220, loss = 0.15 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:44:58.116189: step 376230, loss = 0.14 (1548.9 examples/sec; 0.083 sec/batch)
2017-06-02 11:44:58.963057: step 376240, loss = 0.16 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:44:59.822982: step 376250, loss = 0.16 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:45:00.677796: step 376260, loss = 0.12 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:45:01.543276: step 376270, loss = 0.19 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:45:02.403821: step 376280, loss = 0.17 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:45:03.266573: step 376290, loss = 0.15 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:45:04.216161: step 376300, loss = 0.15 (1348.0 examples/sec; 0.095 sec/batch)
2017-06-02 11:45:04.961048: step 376310, loss = 0.14 (1718.4 examples/sec; 0.074 sec/batch)
2017-06-02 11:45:05.812277: step 376320, loss = 0.16 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:45:06.684624: step 376330, loss = 0.14 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:45:07.535324: step 376340, loss = 0.14 (1504.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:45:08.397165: step 376350, loss = 0.15 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:45:09.264493: step 376360, loss = 0.13 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:45:10.127300: step 376370, loss = 0.21 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:45:11.001695: step 376380, loss = 0.14 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:45:11.875679: step 376390, loss = 0.17 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:45:12.840895: step 376400, loss = 0.21 (1326.1 examples/sec; 0.097 sec/batch)
2017-06-02 11:45:13.612338: step 376410, loss = 0.17 (1659.2 examples/sec; 0.077 sec/batch)
2017-06-02 11:45:14.470105: step 376420, loss = 0.18 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:45:15.340976: step 376430, loss = 0.12 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:45:16.211041: step 376440, loss = 0.13 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:45:17.079097: step 376450, loss = 0.17 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:45:17.899770: step 376460, loss = 0.13 (1559.7 examples/sec; 0.082 sec/batch)
2017-06-02 11:45:18.761567: step 376470, loss = 0.17 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:45:19.612413: step 376480, loss = 0.15 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:45:20.480994: step 376490, loss = 0.16 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:45:21.462138: step 376500, loss = 0.15 (1304.6 examples/sec; 0.098 sec/batch)
2017-06-02 11:45:22.215947: step 376510, loss = 0.20 (1698.0 examples/sec; 0.075 sec/batch)
2017-06-02 11:45:23.081477: step 376520, loss = 0.18 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:45:23.902922: step 376530, loss = 0.15 (1558.2 examples/sec; 0.082 sec/batch)
2017-06-02 11:45:24.787354: step 376540, loss = 0.15 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:45:25.663443: step 376550, loss = 0.16 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:45:26.513368: step 376560, loss = 0.14 (1506.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:45:27.348920: step 376570, loss = 0.21 (1531.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:45:28.189349: step 376580, loss = 0.14 (1523.0 examples/sec; 0.084 sec/batch)
2017-06-02 11:45:29.065558: step 376590, loss = 0.12 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:45:30.020417: step 376600, loss = 0.14 (1340.5 examples/sec; 0.095 sec/batch)
2017-06-02 11:45:30.780947: step 376610, loss = 0.15 (1683.1 examples/sec; 0.076 sec/batch)
2017-06-02 11:45:31.627850: step 376620, loss = 0.15 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:45:32.481066: step 376630, loss = 0.14 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:45:33.318366: step 376640, loss = 0.14 (1528.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:45:34.178345: step 376650, loss = 0.20 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:45:35.062563: step 376660, loss = 0.16 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:45:35.919335: step 376670, loss = 0.16 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:45:36.790596: step 376680, loss = 0.14 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:45:37.646990: step 376690, loss = 0.17 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:45:38.608925: step 376700, loss = 0.20 (1330.6 examples/sec; 0.096 sec/batch)
2017-06-02 11:45:39.358760: step 376710, loss = 0.16 (1707.1 examples/sec; 0.075 sec/batch)
2017-06-02 11:45:40.221128: step 376720, loss = 0.12 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:45:41.074980: step 376730, loss = 0.15 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:45:41.931391: step 376740, loss = 0.17 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:45:42.777032: step 376750, loss = 0.15 (1513.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:45:43.637190: step 376760, loss = 0.17 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:45:44.511306: step 376770, loss = 0.16 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:45:45.369412: step 376780, loss = 0.14 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:45:46.232297: step 376790, loss = 0.18 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:45:47.238648: step 376800, loss = 0.20 (1271.9 examples/sec; 0.101 sec/batch)
2017-06-02 11:45:47.973992: step 376810, loss = 0.15 (1740.7 examples/sec; 0.074 sec/batch)
2017-06-02 11:45:48.836746: step 376820, loss = 0.15 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:45:49.712801: step 376830, loss = 0.18 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:45:50.583878: step 376840, loss = 0.16 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:45:51.442379: step 376850, loss = 0.17 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:45:52.285905: step 376860, loss = 0.15 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 11:45:53.160205: step 376870, loss = 0.17 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:45:54.026155: step 376880, loss = 0.18 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:45:54.893561: step 376890, loss = 0.17 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:45:55.877816: step 376900, loss = 0.18 (1300.5 examples/sec; 0.098 sec/batch)
2017-06-02 11:45:56.636652: step 376910, loss = 0.18 (1686.8 examples/sec; 0.076 sec/batch)
2017-06-02 11:45:57.508517: step 376920, loss = 0.14 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:45:58.391234: step 376930, loss = 0.13 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:45:59.245883: step 376940, loss = 0.15 (1497.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:46:00.129283: step 376950, loss = 0.14 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:46:01.003502: step 376960, loss = 0.16 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:46:01.875917: step 376970, loss = 0.16 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:46:02.740265: step 376980, loss = 0.15 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:46:03.595792: step 376990, loss = 0.17 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:46:04.552936: step 377000, loss = 0.14 (1337.3 examples/sec; 0.096 sec/batch)
2017-06-02 11:46:05.330767: step 377010, loss = 0.17 (1645.6 examples/sec; 0.078 sec/batch)
2017-06-02 11:46:06.174340: step 377020, loss = 0.14 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 11:46:07.025116: step 377030, loss = 0.16 (1505.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:46:07.904843: step 377040, loss = 0.12 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:46:08.760720: step 377050, loss = 0.18 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:46:09.605661: step 377060, loss = 0.16 (1514.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:46:10.491093: step 377070, loss = 0.18 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:46:11.339491: step 377080, loss = 0.12 (1508.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:46:12.227325: step 377090, loss = 0.16 (1441.7 examples/sec; 0.089 sec/batch)
2017-06-02 11:46:13.236238: step 377100, loss = 0.12 (1268.7 examples/sec; 0.101 sec/batch)
2017-06-02 11:46:14.000557: step 377110, loss = 0.17 (1674.7 examples/sec; 0.076 sec/batch)
2017-06-02 11:46:14.891643: step 377120, loss = 0.16 (1436.5 examples/sec; 0.089 sec/batch)
2017-06-02 11:46:15.764753: step 377130, loss = 0.21 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:46:16.646040: step 377140, loss = 0.13 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:46:17.529642: step 377150, loss = 0.16 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:46:18.386152: step 377160, loss = 0.15 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:46:19.252214: step 377170, loss = 0.17 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:46:20.112271: step 377180, loss = 0.19 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:46:20.981288: step 377190, loss = 0.19 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:46:21.945456: step 377200, loss = 0.17 (1327.6 examples/sec; 0.096 sec/batch)
2017-06-02 11:46:22.699420: step 377210, loss = 0.14 (1697.7 examples/sec; 0.075 sec/batch)
2017-06-02 11:46:23.571335: step 377220, loss = 0.12 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:46:24.446120: step 377230, loss = 0.19 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:46:25.291595: step 377240, loss = 0.12 (1513.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:46:26.150847: step 377250, loss = 0.14 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:46:27.027349: step 377260, loss = 0.21 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:46:27.879656: step 377270, loss = 0.16 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:46:28.730221: step 377280, loss = 0.15 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:46:29.597145: step 377290, loss = 0.15 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:46:30.567588: step 377300, loss = 0.16 (1319.0 examples/sec; 0.097 sec/batch)
2017-06-02 11:46:31.324927: step 377310, loss = 0.16 (1690.1 examples/sec; 0.076 sec/batch)
2017-06-02 11:46:32.184020: step 377320, loss = 0.16 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:46:33.058986: step 377330, loss = 0.14 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:46:33.928945: step 377340, loss = 0.15 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:46:34.784021: step 377350, loss = 0.14 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:46:35.630814: step 377360, loss = 0.14 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:46:36.484237: step 377370, loss = 0.14 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:46:37.311757: step 377380, loss = 0.14 (1546.8 examples/sec; 0.083 sec/batch)
2017-06-02 11:46:38.160101: step 377390, loss = 0.18 (1508.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:46:39.176049: step 377400, loss = 0.11 (1259.9 examples/sec; 0.102 sec/batch)
2017-06-02 11:46:39.911388: step 377410, loss = 0.13 (1740.7 examples/sec; 0.074 sec/batch)
2017-06-02 11:46:40.768093: step 377420, loss = 0.17 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:46:41.620283: step 377430, loss = 0.11 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:46:42.457974: step 377440, loss = 0.17 (1528.0 examples/sec; 0.084 sec/batch)
2017-06-02 11:46:43.284486: step 377450, loss = 0.14 (1548.7 examples/sec; 0.083 sec/batch)
2017-06-02 11:46:44.139231: step 377460, loss = 0.12 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:46:44.999229: step 377470, loss = 0.16 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:46:45.859001: step 377480, loss = 0.16 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:46:46.729461: step 377490, loss = 0.14 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:46:47.716608: step 377500, loss = 0.17 (1296.7 examples/sec; 0.099 sec/batch)
2017-06-02 11:46:48.452962: step 377510, loss = 0.15 (1738.3 examples/sec; 0.074 sec/batch)
2017-06-02 11:46:49.311850: step 377520, loss = 0.16 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:46:50.171441: step 377530, loss = 0.14 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:46:51.041103: step 377540, loss = 0.15 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:46:51.903808: step 377550, loss = 0.13 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:46:52.778744: step 377560, loss = 0.13 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:46:53.644722: step 377570, loss = 0.12 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:46:54.526964: step 377580, loss = 0.13 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:46:55.412338: step 377590, loss = 0.17 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 11:46:56.391379: step 377600, loss = 0.15 (1307.4 examples/sec; 0.098 sec/batch)
2017-06-02 11:46:57.152690: step 377610, loss = 0.14 (1681.3 examples/sec; 0.076 sec/batch)
2017-06-02 11:46:58.033913: step 377620, loss = 0.15 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:46:58.880789: step 377630, loss = 0.16 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:46:59.767807: step 377640, loss = 0.15 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 11:47:00.627756: step 377650, loss = 0.17 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:47:01.485706: step 377660, loss = 0.15 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:47:02.360313: step 377670, loss = 0.15 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:47:03.208071: step 377680, loss = 0.11 (1509.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:47:04.099939: step 377690, loss = 0.14 (1435.2 examples/sec; 0.089 sec/batch)
2017-06-02 11:47:05.098383: step 377700, loss = 0.19 (1282.0 examples/sec; 0.100 sec/batch)
2017-06-02 11:47:05.906305: step 377710, loss = 0.16 (1584.3 examples/sec; 0.081 sec/batch)
2017-06-02 11:47:06.779574: step 377720, loss = 0.15 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:47:07.651542: step 377730, loss = 0.20 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:47:08.516022: step 377740, loss = 0.21 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:47:09.371706: step 377750, loss = 0.11 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:47:10.244834: step 377760, loss = 0.15 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:47:11.065424: step 377770, loss = 0.13 (1559.9 examples/sec; 0.082 sec/batch)
2017-06-02 11:47:11.929150: step 377780, loss = 0.14 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:47:12.792917: step 377790, loss = 0.16 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:47:13.737691: step 377800, loss = 0.14 (1354.8 examples/sec; 0.094 sec/batch)
2017-06-02 11:47:14.502515: step 377810, loss = 0.11 (1673.6 examples/sec; 0.076 sec/batch)
2017-06-02 11:47:15.376056: step 377820, loss = 0.15 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:47:16.242654: step 377830, loss = 0.16 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:47:17.096513: step 377840, loss = 0.15 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:47:17.947903: step 377850, loss = 0.13 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:47:18.806728: step 377860, loss = 0.16 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:47:19.661930: step 377870, loss = 0.15 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:47:20.497962: step 377880, loss = 0.18 (1531.0 examples/sec; 0.084 sec/batch)
2017-06-02 11:47:21.334668: step 377890, loss = 0.14 (1529.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:47:22.292211: step 377900, loss = 0.12 (1336.8 examples/sec; 0.096 sec/batch)
2017-06-02 11:47:23.041827: step 377910, loss = 0.15 (1707.5 examples/sec; 0.075 sec/batch)
2017-06-02 11:47:23.899767: step 377920, loss = 0.14 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:47:24.746519: step 377930, loss = 0.15 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:47:25.630231: step 377940, loss = 0.14 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:47:26.490956: step 377950, loss = 0.23 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:47:27.351941: step 377960, loss = 0.15 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:47:28.188593: step 377970, loss = 0.16 (1529.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:47:29.046311: step 377980, loss = 0.19 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:47:29.888888: step 377990, loss = 0.14 (1519.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:47:30.857986: step 378000, loss = 0.16 (1320.8 examples/sec; 0.097 sec/batch)
2017-06-02 11:47:31.619743: step 378010, loss = 0.14 (1680.3 examples/sec; 0.076 sec/batch)
2017-06-02 11:47:32.470293: step 378020, loss = 0.13 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:47:33.330723: step 378030, loss = 0.16 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:47:34.198425: step 378040, loss = 0.15 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:47:35.047573: step 378050, loss = 0.15 (1507.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:47:35.928313: step 378060, loss = 0.18 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:47:36.764768: step 378070, loss = 0.18 (1530.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:47:37.608794: step 378080, loss = 0.21 (1516.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:47:38.481943: step 378090, loss = 0.14 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:47:39.445592: step 378100, loss = 0.16 (1328.3 examples/sec; 0.096 sec/batch)
2017-06-02 11:47:40.208221: step 378110, loss = 0.14 (1678.4 examples/sec; 0.076 sec/batch)
2017-06-02 11:47:41.068796: step 378120, loss = 0.16 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:47:41.912337: step 378130, loss = 0.14 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 11:47:42.750947: step 378140, loss = 0.17 (1526.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:47:43.605588: step 378150, loss = 0.13 (1497.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:47:44.482772: step 378160, loss = 0.15 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:47:45.349809: step 378170, loss = 0.13 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:47:46.198931: step 378180, loss = 0.16 (1507.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:47:47.059746: step 378190, loss = 0.14 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:47:48.003228: step 378200, loss = 0.12 (1356.7 examples/sec; 0.094 sec/batch)
2017-06-02 11:47:48.775494: step 378210, loss = 0.13 (1657.5 examples/sec; 0.077 sec/batch)
2017-06-02 11:47:49.650090: step 378220, loss = 0.14 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:47:50.490684: step 378230, loss = 0.13 (1522.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:47:51.357088: step 378240, loss = 0.16 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:47:52.227839: step 378250, loss = 0.22 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:47:53.079715: step 378260, loss = 0.15 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:47:53.898027: step 378270, loss = 0.14 (1564.2 examples/sec; 0.082 sec/batch)
2017-06-02 11:47:54.758267: step 378280, loss = 0.16 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:47:55.590799: step 378290, loss = 0.13 (1537.5 examples/sec; 0.083 sec/batch)
2017-06-02 11:47:56.572018: step 378300, loss = 0.19 (1304.5 examples/sec; 0.098 sec/batch)
2017-06-02 11:47:57.296251: step 378310, loss = 0.15 (1767.4 examples/sec; 0.072 sec/batch)
2017-06-02 11:47:58.170147: step 378320, loss = 0.19 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:47:59.041824: step 378330, loss = 0.14 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:47:59.913636: step 378340, loss = 0.17 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:48:00.746837: step 378350, loss = 0.16 (1536.2 examples/sec; 0.083 sec/batch)
2017-06-02 11:48:01.590221: step 378360, loss = 0.14 (1517.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:48:02.440584: step 378370, loss = 0.15 (1505.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:48:03.315187: step 378380, loss = 0.20 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:48:04.193339: step 378390, loss = 0.14 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:48:05.179859: step 378400, loss = 0.19 (1297.5 examples/sec; 0.099 sec/batch)
2017-06-02 11:48:05.940145: step 378410, loss = 0.16 (1683.6 examples/sec; 0.076 sec/batch)
2017-06-02 11:48:06.820494: step 378420, loss = 0.17 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:48:07.701703: step 378430, loss = 0.15 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:48:08.577508: step 378440, loss = 0.12 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:48:09.446061: step 378450, loss = 0.16 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:48:10.309155: step 378460, loss = 0.15 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:48:11.155066: step 378470, loss = 0.17 (1513.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:48:12.026875: step 378480, loss = 0.19 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:48:12.881179: step 378490, loss = 0.13 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:48:13.876018: step 378500, loss = 0.14 (1286.6 examples/sec; 0.099 sec/batch)
2017-06-02 11:48:14.562921: step 378510, loss = 0.13 (1863.5 examples/sec; 0.069 sec/batch)
2017-06-02 11:48:15.413051: step 378520, loss = 0.22 (1505.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:48:16.272289: step 378530, loss = 0.11 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:48:17.131170: step 378540, loss = 0.18 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:48:17.976047: step 378550, loss = 0.15 (1515.0 examples/sec; 0.084 sec/batch)
2017-06-02 11:48:18.839240: step 378560, loss = 0.15 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:48:19.689648: step 378570, loss = 0.13 (1505.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:48:20.555638: step 378580, loss = 0.13 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:48:21.414108: step 378590, loss = 0.17 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:48:22.376447: step 378600, loss = 0.18 (1330.1 examples/sec; 0.096 sec/batch)
2017-06-02 11:48:23.131493: step 378610, loss = 0.15 (1695.3 examples/sec; 0.076 sec/batch)
2017-06-02 11:48:24.006756: step 378620, loss = 0.18 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:48:24.887705: step 378630, loss = 0.15 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:48:25.765978: step 378640, loss = 0.20 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:48:26.621564: step 378650, loss = 0.13 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:48:27.468717: step 378660, loss = 0.12 (1510.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:48:28.334684: step 378670, loss = 0.13 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:48:29.200924: step 378680, loss = 0.15 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:48:30.066800: step 378690, loss = 0.17 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:48:31.050308: step 378700, loss = 0.14 (1301.5 examples/sec; 0.098 sec/batch)
2017-06-02 11:48:31.835544: step 378710, loss = 0.15 (1630.1 examples/sec; 0.079 sec/batch)
2017-06-02 11:48:32.689534: step 378720, loss = 0.16 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:48:33.549373: step 378730, loss = 0.11 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:48:34.386352: step 378740, loss = 0.15 (1529.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:48:35.238385: step 378750, loss = 0.16 (1502.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:48:36.090340: step 378760, loss = 0.13 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:48:36.965631: step 378770, loss = 0.13 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:48:37.833334: step 378780, loss = 0.15 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:48:38.709475: step 378790, loss = 0.15 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:48:39.671708: step 378800, loss = 0.16 (1330.2 examples/sec; 0.096 sec/batch)
2017-06-02 11:48:40.427989: step 378810, loss = 0.12 (1692.5 examples/sec; 0.076 sec/batch)
2017-06-02 11:48:41.282918: step 378820, loss = 0.11 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:48:42.147544: step 378830, loss = 0.22 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:48:42.997568: step 378840, loss = 0.18 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:48:43.850362: step 378850, loss = 0.16 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:48:44.683262: step 378860, loss = 0.13 (1536.8 examples/sec; 0.083 sec/batch)
2017-06-02 11:48:45.566927: step 378870, loss = 0.14 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:48:46.416006: step 378880, loss = 0.14 (1507.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:48:47.284815: step 378890, loss = 0.16 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:48:48.288809: step 378900, loss = 0.19 (1274.9 examples/sec; 0.100 sec/batch)
2017-06-02 11:48:49.032449: step 378910, loss = 0.14 (1721.3 examples/sec; 0.074 sec/batch)
2017-06-02 11:48:49.900367: step 378920, loss = 0.16 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:48:50.758970: step 378930, loss = 0.14 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:48:51.626262: step 378940, loss = 0.20 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:48:52.490922: step 378950, loss = 0.12 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:48:53.318607: step 378960, loss = 0.18 (1546.5 examples/sec; 0.083 sec/batch)
2017-06-02 11:48:54.186833: step 378970, loss = 0.12 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:48:55.053748: step 378980, loss = 0.12 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:48:55.932014: step 378990, loss = 0.16 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:48:56.931795: step 379000, loss = 0.16 (1280.3 examples/sec; 0.100 sec/batch)
2017-06-02 11:48:57.673308: step 379010, loss = 0.17 (1726.2 examples/sec; 0.074 sec/batch)
2017-06-02 11:48:58.526053: step 379020, loss = 0.16 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:48:59.403882: step 379030, loss = 0.13 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:49:00.303183: step 379040, loss = 0.21 (1423.3 examples/sec; 0.090 sec/batch)
2017-06-02 11:49:01.166463: step 379050, loss = 0.15 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:49:02.045068: step 379060, loss = 0.15 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:49:02.941452: step 379070, loss = 0.22 (1428.0 examples/sec; 0.090 sec/batch)
2017-06-02 11:49:03.824998: step 379080, loss = 0.18 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:49:04.680858: step 379090, loss = 0.19 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:49:05.642995: step 379100, loss = 0.15 (1330.4 examples/sec; 0.096 sec/batch)
2017-06-02 11:49:06.402182: step 379110, loss = 0.14 (1686.0 examples/sec; 0.076 sec/batch)
2017-06-02 11:49:07.280683: step 379120, loss = 0.15 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:49:08.154088: step 379130, loss = 0.14 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:49:08.989821: step 379140, loss = 0.17 (1531.6 examples/sec; 0.084 sec/batch)
2017-06-02 11:49:09.859322: step 379150, loss = 0.16 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:49:10.707877: step 379160, loss = 0.14 (1508.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:49:11.578140: step 379170, loss = 0.13 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:49:12.430308: step 379180, loss = 0.14 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:49:13.299705: step 379190, loss = 0.14 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:49:14.255241: step 379200, loss = 0.12 (1339.6 examples/sec; 0.096 sec/batch)
2017-06-02 11:49:15.000864: step 379210, loss = 0.15 (1716.7 examples/sec; 0.075 sec/batch)
2017-06-02 11:49:15.864213: step 379220, loss = 0.16 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:49:16.732958: step 379230, loss = 0.17 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:49:17.600134: step 379240, loss = 0.14 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:49:18.491633: step 379250, loss = 0.15 (1435.8 examples/sec; 0.089 sec/batch)
2017-06-02 11:49:19.374042: step 379260, loss = 0.13 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:49:20.239279: step 379270, loss = 0.14 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:49:21.084244: step 379280, loss = 0.15 (1514.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:49:21.959565: step 379290, loss = 0.12 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:49:22.907048: step 379300, loss = 0.13 (1350.9 examples/sec; 0.095 sec/batch)
2017-06-02 11:49:23.668905: step 379310, loss = 0.14 (1680.1 examples/sec; 0.076 sec/batch)
2017-06-02 11:49:24.548859: step 379320, loss = 0.14 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:49:25.433617: step 379330, loss = 0.14 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:49:26.288261: step 379340, loss = 0.12 (1497.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:49:27.163355: step 379350, loss = 0.24 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:49:28.025627: step 379360, loss = 0.15 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:49:28.856221: step 379370, loss = 0.13 (1541.1 examples/sec; 0.083 sec/batch)
2017-06-02 11:49:29.709037: step 379380, loss = 0.19 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:49:30.561848: step 379390, loss = 0.13 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:49:31.553196: step 379400, loss = 0.14 (1291.2 examples/sec; 0.099 sec/batch)
2017-06-02 11:49:32.324142: step 379410, loss = 0.17 (1660.3 examples/sec; 0.077 sec/batch)
2017-06-02 11:49:33.173208: step 379420, loss = 0.14 (1507.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:49:34.039608: step 379430, loss = 0.15 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:49:34.909739: step 379440, loss = 0.13 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:49:35.782835: step 379450, loss = 0.17 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:49:36.630928: step 379460, loss = 0.14 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:49:37.466643: step 379470, loss = 0.13 (1531.6 examples/sec; 0.084 sec/batch)
2017-06-02 11:49:38.311483: step 379480, loss = 0.13 (1515.1 examples/sec; 0.084 sec/batch)
2017-06-02 11:49:39.136893: step 379490, loss = 0.12 (1550.8 examples/sec; 0.083 sec/batch)
2017-06-02 11:49:40.102267: step 379500, loss = 0.15 (1325.9 examples/sec; 0.097 sec/batch)
2017-06-02 11:49:40.890483: step 379510, loss = 0.16 (1623.9 examples/sec; 0.079 sec/batch)
2017-06-02 11:49:41.768031: step 379520, loss = 0.13 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:49:42.641262: step 379530, loss = 0.16 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:49:43.500931: step 379540, loss = 0.16 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:49:44.385843: step 379550, loss = 0.19 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:49:45.248387: step 379560, loss = 0.14 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:49:46.123092: step 379570, loss = 0.17 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:49:47.006251: step 379580, loss = 0.17 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:49:47.857257: step 379590, loss = 0.20 (1504.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:49:48.822471: step 379600, loss = 0.15 (1326.1 examples/sec; 0.097 sec/batch)
2017-06-02 11:49:49.604376: step 379610, loss = 0.21 (1637.0 examples/sec; 0.078 sec/batch)
2017-06-02 11:49:50.445077: step 379620, loss = 0.13 (1522.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:49:51.301311: step 379630, loss = 0.19 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:49:52.176154: step 379640, loss = 0.11 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:49:53.109903: step 379650, loss = 0.13 (1370.8 examples/sec; 0.093 sec/batch)
2017-06-02 11:49:53.978186: step 379660, loss = 0.21 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:49:54.842546: step 379670, loss = 0.12 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:49:55.720141: step 379680, loss = 0.11 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:49:56.573975: step 379690, loss = 0.15 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:49:57.551491: step 379700, loss = 0.14 (1309.4 examples/sec; 0.098 sec/batch)
2017-06-02 11:49:58.316771: step 379710, loss = 0.17 (1672.6 examples/sec; 0.077 sec/batch)
2017-06-02 11:49:59.187226: step 379720, loss = 0.17 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:50:00.023893: step 379730, loss = 0.14 (1529.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:50:00.863655: step 379740, loss = 0.15 (1524.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:50:01.720061: step 379750, loss = 0.15 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:50:02.588181: step 379760, loss = 0.16 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:50:03.426822: step 379770, loss = 0.15 (1526.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:50:04.287696: step 379780, loss = 0.13 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:50:05.154591: step 379790, loss = 0.17 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:50:06.114225: step 379800, loss = 0.15 (1333.9 examples/sec; 0.096 sec/batch)
2017-06-02 11:50:06.886367: step 379810, loss = 0.13 (1657.7 examples/sec; 0.077 sec/batch)
2017-06-02 11:50:07.741641: step 379820, loss = 0.15 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:50:08.596115: step 379830, loss = 0.14 (1498.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:50:09.441987: step 379840, loss = 0.13 (1513.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:50:10.322413: step 379850, loss = 0.15 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:50:11.174845: step 379860, loss = 0.15 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:50:12.043562: step 379870, loss = 0.17 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:50:12.901057: step 379880, loss = 0.14 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:50:13.774035: step 379890, loss = 0.15 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:50:14.739504: step 379900, loss = 0.16 (1325.8 examples/sec; 0.097 sec/batch)
2017-06-02 11:50:15.516214: step 379910, loss = 0.13 (1648.0 examples/sec; 0.078 sec/batch)
2017-06-02 11:50:16.378053: step 379920, loss = 0.18 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:50:17.225222: step 379930, loss = 0.21 (1510.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:50:18.089303: step 379940, loss = 0.12 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:50:18.968397: step 379950, loss = 0.13 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:50:19.835904: step 379960, loss = 0.16 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:50:20.665873: step 379970, loss = 0.14 (1542.2 examples/sec; 0.083 sec/batch)
2017-06-02 11:50:21.530031: step 379980, loss = 0.14 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:50:22.384444: step 379990, loss = 0.15 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:50:23.357448: step 380000, loss = 0.15 (1315.5 examples/sec; 0.097 sec/batch)
2017-06-02 11:50:24.099719: step 380010, loss = 0.13 (1724.5 examples/sec; 0.074 sec/batch)
2017-06-02 11:50:24.945943: step 380020, loss = 0.14 (1512.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:50:25.812115: step 380030, loss = 0.13 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:50:26.657572: step 380040, loss = 0.14 (1514.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:50:27.481265: step 380050, loss = 0.19 (1554.0 examples/sec; 0.082 sec/batch)
2017-06-02 11:50:28.323995: step 380060, loss = 0.15 (1518.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:50:29.179633: step 380070, loss = 0.12 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:50:30.060050: step 380080, loss = 0.12 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:50:30.921035: step 380090, loss = 0.13 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:50:31.899002: step 380100, loss = 0.15 (1310.2 examples/sec; 0.098 sec/batch)
2017-06-02 11:50:32.668508: step 380110, loss = 0.15 (1661.2 examples/sec; 0.077 sec/batch)
2017-06-02 11:50:33.526095: step 380120, loss = 0.18 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:50:34.378485: step 380130, loss = 0.19 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:50:35.232341: step 380140, loss = 0.13 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:50:36.103600: step 380150, loss = 0.17 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:50:36.969932: step 380160, loss = 0.15 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:50:37.846858: step 380170, loss = 0.15 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:50:38.723875: step 380180, loss = 0.15 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:50:39.581275: step 380190, loss = 0.14 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:50:40.544878: step 380200, loss = 0.14 (1328.3 examples/sec; 0.096 sec/batch)
2017-06-02 11:50:41.289665: step 380210, loss = 0.14 (1718.6 examples/sec; 0.074 sec/batch)
2017-06-02 11:50:42.125742: step 380220, loss = 0.15 (1531.0 examples/sec; 0.084 sec/batch)
2017-06-02 11:50:43.001742: step 380230, loss = 0.17 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:50:43.867036: step 380240, loss = 0.15 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:50:44.732262: step 380250, loss = 0.15 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:50:45.587906: step 380260, loss = 0.14 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:50:46.472477: step 380270, loss = 0.17 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:50:47.336249: step 380280, loss = 0.19 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:50:48.182670: step 380290, loss = 0.14 (1512.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:50:49.131164: step 380300, loss = 0.15 (1349.5 examples/sec; 0.095 sec/batch)
2017-06-02 11:50:49.876847: step 380310, loss = 0.15 (1716.6 examples/sec; 0.075 sec/batch)
2017-06-02 11:50:50.731721: step 380320, loss = 0.16 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:50:51.568571: step 380330, loss = 0.20 (1529.6 examples/sec; 0.084 sec/batch)
2017-06-02 11:50:52.434105: step 380340, loss = 0.13 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:50:53.276659: step 380350, loss = 0.11 (1519.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:50:54.161627: step 380360, loss = 0.15 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:50:55.023263: step 380370, loss = 0.20 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:50:55.901625: step 380380, loss = 0.12 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:50:56.776981: step 380390, loss = 0.13 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:50:57.719853: step 380400, loss = 0.14 (1357.6 examples/sec; 0.094 sec/batch)
2017-06-02 11:50:58.506009: step 380410, loss = 0.16 (1628.2 examples/sec; 0.079 sec/batch)
2017-06-02 11:50:59.344859: step 380420, loss = 0.14 (1525.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:51:00.205834: step 380430, loss = 0.18 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:51:01.055507: step 380440, loss = 0.17 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:51:01.938172: step 380450, loss = 0.15 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:51:02.781305: step 380460, loss = 0.13 (1518.1 examples/sec; 0.084 sec/batch)
2017-06-02 11:51:03.631535: step 380470, loss = 0.16 (1505.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:51:04.485658: step 380480, loss = 0.16 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:51:05.348002: step 380490, loss = 0.13 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:51:06.307256: step 380500, loss = 0.17 (1334.4 examples/sec; 0.096 sec/batch)
2017-06-02 11:51:07.061424: step 380510, loss = 0.13 (1697.2 examples/sec; 0.075 sec/batch)
2017-06-02 11:51:07.936120: step 380520, loss = 0.18 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:51:08.787798: step 380530, loss = 0.17 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:51:09.640034: step 380540, loss = 0.16 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:51:10.495299: step 380550, loss = 0.22 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:51:11.336051: step 380560, loss = 0.14 (1522.4 examples/sec; 0.084 sec/batch)
2017-06-02 11:51:12.204035: step 380570, loss = 0.16 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:51:13.071882: step 380580, loss = 0.15 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:51:13.941926: step 380590, loss = 0.12 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:51:14.910608: step 380600, loss = 0.14 (1321.4 examples/sec; 0.097 sec/batch)
2017-06-02 11:51:15.651574: step 380610, loss = 0.15 (1727.5 examples/sec; 0.074 sec/batch)
2017-06-02 11:51:16.499690: step 380620, loss = 0.14 (1509.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:51:17.374167: step 380630, loss = 0.16 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:51:18.241259: step 380640, loss = 0.19 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:51:19.103568: step 380650, loss = 0.15 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:51:19.958998: step 380660, loss = 0.18 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:51:20.822953: step 380670, loss = 0.16 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:51:21.680997: step 380680, loss = 0.14 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:51:22.550040: step 380690, loss = 0.14 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:51:23.514564: step 380700, loss = 0.19 (1327.1 examples/sec; 0.096 sec/batch)
2017-06-02 11:51:24.263211: step 380710, loss = 0.11 (1709.8 examples/sec; 0.075 sec/batch)
2017-06-02 11:51:25.141916: step 380720, loss = 0.13 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:51:25.992317: step 380730, loss = 0.18 (1505.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:51:26.828851: step 380740, loss = 0.15 (1530.1 examples/sec; 0.084 sec/batch)
2017-06-02 11:51:27.693211: step 380750, loss = 0.19 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:51:28.549638: step 380760, loss = 0.16 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:51:29.422912: step 380770, loss = 0.16 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:51:30.296505: step 380780, loss = 0.18 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:51:31.152851: step 380790, loss = 0.15 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:51:32.156599: step 380800, loss = 0.15 (1275.2 examples/sec; 0.100 sec/batch)
2017-06-02 11:51:32.854364: step 380810, loss = 0.19 (1834.5 examples/sec; 0.070 sec/batch)
2017-06-02 11:51:33.708572: step 380820, loss = 0.17 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:51:34.562197: step 380830, loss = 0.14 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:51:35.465664: step 380840, loss = 0.17 (1416.8 examples/sec; 0.090 sec/batch)
2017-06-02 11:51:36.339648: step 380850, loss = 0.16 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:51:37.224060: step 380860, loss = 0.20 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:51:38.106755: step 380870, loss = 0.12 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:51:38.968487: step 380880, loss = 0.14 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:51:39.864553: step 380890, loss = 0.14 (1428.5 examples/sec; 0.090 sec/batch)
2017-06-02 11:51:40.866693: step 380900, loss = 0.15 (1277.3 examples/sec; 0.100 sec/batch)
2017-06-02 11:51:41.648121: step 380910, loss = 0.15 (1638.1 examples/sec; 0.078 sec/batch)
2017-06-02 11:51:42.512968: step 380920, loss = 0.13 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:51:43.394126: step 380930, loss = 0.14 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:51:44.261260: step 380940, loss = 0.18 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:51:45.109787: step 380950, loss = 0.13 (1508.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:51:45.949275: step 380960, loss = 0.18 (1524.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:51:46.789315: step 380970, loss = 0.13 (1523.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:51:47.635869: step 380980, loss = 0.14 (1512.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:51:48.488819: step 380990, loss = 0.13 (1500.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:51:49.463938: step 381000, loss = 0.20 (1312.7 examples/sec; 0.098 sec/batch)
2017-06-02 11:51:50.206353: step 381010, loss = 0.12 (1724.1 examples/sec; 0.074 sec/batch)
2017-06-02 11:51:51.050432: step 381020, loss = 0.18 (1516.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:51:51.895491: step 381030, loss = 0.13 (1514.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:51:52.774504: step 381040, loss = 0.12 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:51:53.635598: step 381050, loss = 0.15 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:51:54.488958: step 381060, loss = 0.15 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:51:55.351676: step 381070, loss = 0.12 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:51:56.230074: step 381080, loss = 0.12 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:51:57.082695: step 381090, loss = 0.17 (1501.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:51:58.092447: step 381100, loss = 0.12 (1267.6 examples/sec; 0.101 sec/batch)
2017-06-02 11:51:58.807034: step 381110, loss = 0.14 (1791.2 examples/sec; 0.071 sec/batch)
2017-06-02 11:51:59.686305: step 381120, loss = 0.15 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:52:00.533402: step 381130, loss = 0.16 (1511.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:52:01.386029: step 381140, loss = 0.15 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:52:02.255727: step 381150, loss = 0.12 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:52:03.109997: step 381160, loss = 0.13 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:52:03.973575: step 381170, loss = 0.15 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:52:04.834649: step 381180, loss = 0.15 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:52:05.715871: step 381190, loss = 0.21 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:52:06.678322: step 381200, loss = 0.19 (1330.0 examples/sec; 0.096 sec/batch)
2017-06-02 11:52:07.417994: step 381210, loss = 0.15 (1730.5 examples/sec; 0.074 sec/batch)
2017-06-02 11:52:08.307190: step 381220, loss = 0.16 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 11:52:09.199946: step 381230, loss = 0.15 (1433.8 examples/sec; 0.089 sec/batch)
2017-06-02 11:52:10.066609: step 381240, loss = 0.15 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:52:10.923243: step 381250, loss = 0.14 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:52:11.810625: step 381260, loss = 0.14 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 11:52:12.692393: step 381270, loss = 0.13 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:52:13.585387: step 381280, loss = 0.15 (1433.4 examples/sec; 0.089 sec/batch)
2017-06-02 11:52:14.464178: step 381290, loss = 0.13 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:52:15.465983: step 381300, loss = 0.17 (1277.7 examples/sec; 0.100 sec/batch)
2017-06-02 11:52:16.226651: step 381310, loss = 0.15 (1682.7 examples/sec; 0.076 sec/batch)
2017-06-02 11:52:17.102299: step 381320, loss = 0.16 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:52:17.982992: step 381330, loss = 0.14 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:52:18.865769: step 381340, loss = 0.20 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:52:19.735012: step 381350, loss = 0.18 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:52:20.602992: step 381360, loss = 0.19 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:52:21.448074: step 381370, loss = 0.14 (1514.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:52:22.306053: step 381380, loss = 0.15 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:52:23.182276: step 381390, loss = 0.17 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:52:24.209352: step 381400, loss = 0.18 (1246.3 examples/sec; 0.103 sec/batch)
2017-06-02 11:52:24.930549: step 381410, loss = 0.13 (1774.8 examples/sec; 0.072 sec/batch)
2017-06-02 11:52:25.750969: step 381420, loss = 0.19 (1560.2 examples/sec; 0.082 sec/batch)
2017-06-02 11:52:26.589377: step 381430, loss = 0.19 (1526.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:52:27.453862: step 381440, loss = 0.15 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:52:28.301334: step 381450, loss = 0.17 (1510.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:52:29.158454: step 381460, loss = 0.14 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:52:30.026810: step 381470, loss = 0.16 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:52:30.881232: step 381480, loss = 0.16 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:52:31.730038: step 381490, loss = 0.17 (1508.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:52:32.688766: step 381500, loss = 0.15 (1335.1 examples/sec; 0.096 sec/batch)
2017-06-02 11:52:33.450183: step 381510, loss = 0.15 (1681.1 examples/sec; 0.076 sec/batch)
2017-06-02 11:52:34.298932: step 381520, loss = 0.14 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:52:35.171727: step 381530, loss = 0.13 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:52:36.012492: step 381540, loss = 0.16 (1522.4 examples/sec; 0.084 sec/batch)
2017-06-02 11:52:36.866725: step 381550, loss = 0.16 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:52:37.711991: step 381560, loss = 0.17 (1514.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:52:38.556063: step 381570, loss = 0.15 (1516.4 examples/sec; 0.084 sec/batch)
2017-06-02 11:52:39.418884: step 381580, loss = 0.13 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:52:40.277615: step 381590, loss = 0.13 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:52:41.241369: step 381600, loss = 0.16 (1328.1 examples/sec; 0.096 sec/batch)
2017-06-02 11:52:42.008007: step 381610, loss = 0.12 (1669.6 examples/sec; 0.077 sec/batch)
2017-06-02 11:52:42.839833: step 381620, loss = 0.12 (1538.8 examples/sec; 0.083 sec/batch)
2017-06-02 11:52:43.681851: step 381630, loss = 0.17 (1520.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:52:44.535951: step 381640, loss = 0.14 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:52:45.384627: step 381650, loss = 0.17 (1508.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:52:46.258053: step 381660, loss = 0.15 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:52:47.146470: step 381670, loss = 0.15 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 11:52:47.995835: step 381680, loss = 0.18 (1507.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:52:48.837805: step 381690, loss = 0.19 (1520.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:52:49.799758: step 381700, loss = 0.17 (1330.6 examples/sec; 0.096 sec/batch)
2017-06-02 11:52:50.570149: step 381710, loss = 0.16 (1661.5 examples/sec; 0.077 sec/batch)
2017-06-02 11:52:51.441922: step 381720, loss = 0.11 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:52:52.273900: step 381730, loss = 0.16 (1538.5 examples/sec; 0.083 sec/batch)
2017-06-02 11:52:53.150344: step 381740, loss = 0.15 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:52:54.011456: step 381750, loss = 0.15 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:52:54.864705: step 381760, loss = 0.17 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:52:55.723213: step 381770, loss = 0.14 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:52:56.565440: step 381780, loss = 0.12 (1519.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:52:57.430338: step 381790, loss = 0.10 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:52:58.390273: step 381800, loss = 0.18 (1333.4 examples/sec; 0.096 sec/batch)
2017-06-02 11:52:59.152344: step 381810, loss = 0.14 (1679.6 examples/sec; 0.076 sec/batch)
2017-06-02 11:53:00.027383: step 381820, loss = 0.19 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:53:00.881770: step 381830, loss = 0.12 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:53:01.745898: step 381840, loss = 0.12 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:53:02.619983: step 381850, loss = 0.12 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:53:03.500133: step 381860, loss = 0.14 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:53:04.376709: step 381870, loss = 0.12 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:53:05.227646: step 381880, loss = 0.14 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:53:06.073845: step 381890, loss = 0.13 (1512.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:53:07.044972: step 381900, loss = 0.19 (1318.0 examples/sec; 0.097 sec/batch)
2017-06-02 11:53:07.781615: step 381910, loss = 0.14 (1737.6 examples/sec; 0.074 sec/batch)
2017-06-02 11:53:08.622677: step 381920, loss = 0.18 (1521.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:53:09.488432: step 381930, loss = 0.11 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:53:10.330949: step 381940, loss = 0.14 (1520.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:53:11.190050: step 381950, loss = 0.12 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:53:12.045564: step 381960, loss = 0.24 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:53:12.914119: step 381970, loss = 0.15 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:53:13.757627: step 381980, loss = 0.16 (1517.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:53:14.611044: step 381990, loss = 0.17 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:53:15.555891: step 382000, loss = 0.12 (1354.7 examples/sec; 0.094 sec/batch)
2017-06-02 11:53:16.322212: step 382010, loss = 0.12 (1670.3 examples/sec; 0.077 sec/batch)
2017-06-02 11:53:17.165766: step 382020, loss = 0.20 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 11:53:18.011451: step 382030, loss = 0.19 (1513.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:53:18.867758: step 382040, loss = 0.20 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:53:19.764007: step 382050, loss = 0.17 (1428.2 examples/sec; 0.090 sec/batch)
2017-06-02 11:53:20.627512: step 382060, loss = 0.13 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:53:21.473536: step 382070, loss = 0.18 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:53:22.348604: step 382080, loss = 0.17 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:53:23.227357: step 382090, loss = 0.19 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:53:24.188681: step 382100, loss = 0.13 (1331.5 examples/sec; 0.096 sec/batch)
2017-06-02 11:53:24.937220: step 382110, loss = 0.15 (1710.0 examples/sec; 0.075 sec/batch)
2017-06-02 11:53:25.790097: step 382120, loss = 0.12 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:53:26.650841: step 382130, loss = 0.13 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:53:27.500076: step 382140, loss = 0.15 (1507.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:53:28.362927: step 382150, loss = 0.12 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:53:29.215586: step 382160, loss = 0.14 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:53:30.059652: step 382170, loss = 0.12 (1516.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:53:30.930260: step 382180, loss = 0.15 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:53:31.806535: step 382190, loss = 0.12 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:53:32.775505: step 382200, loss = 0.17 (1321.0 examples/sec; 0.097 sec/batch)
2017-06-02 11:53:33.552408: step 382210, loss = 0.15 (1647.6 examples/sec; 0.078 sec/batch)
2017-06-02 11:53:34.423522: step 382220, loss = 0.12 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:53:35.275422: step 382230, loss = 0.14 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:53:36.160699: step 382240, loss = 0.13 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 11:53:37.028346: step 382250, loss = 0.11 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:53:37.869232: step 382260, loss = 0.14 (1522.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:53:38.717685: step 382270, loss = 0.15 (1508.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:53:39.610743: step 382280, loss = 0.16 (1433.3 examples/sec; 0.089 sec/batch)
2017-06-02 11:53:40.458475: step 382290, loss = 0.19 (1509.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:53:41.422378: step 382300, loss = 0.18 (1327.9 examples/sec; 0.096 sec/batch)
2017-06-02 11:53:42.204901: step 382310, loss = 0.14 (1635.7 examples/sec; 0.078 sec/batch)
2017-06-02 11:53:43.046444: step 382320, loss = 0.19 (1521.0 examples/sec; 0.084 sec/batch)
2017-06-02 11:53:43.911892: step 382330, loss = 0.19 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:53:44.785004: step 382340, loss = 0.14 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:53:45.629501: step 382350, loss = 0.18 (1515.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:53:46.484056: step 382360, loss = 0.11 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:53:47.325715: step 382370, loss = 0.12 (1520.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:53:48.229021: step 382380, loss = 0.18 (1417.0 examples/sec; 0.090 sec/batch)
2017-06-02 11:53:49.084164: step 382390, loss = 0.24 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:53:50.047390: step 382400, loss = 0.18 (1328.9 examples/sec; 0.096 sec/batch)
2017-06-02 11:53:50.800442: step 382410, loss = 0.17 (1699.8 examples/sec; 0.075 sec/batch)
2017-06-02 11:53:51.663353: step 382420, loss = 0.14 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:53:52.508066: step 382430, loss = 0.15 (1515.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:53:53.362099: step 382440, loss = 0.19 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:53:54.203926: step 382450, loss = 0.15 (1520.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:53:55.051357: step 382460, loss = 0.13 (1510.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:53:55.921968: step 382470, loss = 0.18 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:53:56.800532: step 382480, loss = 0.13 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:53:57.660066: step 382490, loss = 0.14 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:53:58.607231: step 382500, loss = 0.13 (1351.4 examples/sec; 0.095 sec/batch)
2017-06-02 11:53:59.384861: step 382510, loss = 0.13 (1646.0 examples/sec; 0.078 sec/batch)
2017-06-02 11:54:00.216034: step 382520, loss = 0.15 (1540.0 examples/sec; 0.083 sec/batch)
2017-06-02 11:54:01.070323: step 382530, loss = 0.14 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:54:01.947342: step 382540, loss = 0.18 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:54:02.819029: step 382550, loss = 0.13 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:54:03.688339: step 382560, loss = 0.18 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:54:04.574443: step 382570, loss = 0.15 (1444.5 examples/sec; 0.089 sec/batch)
2017-06-02 11:54:05.412517: step 382580, loss = 0.14 (1527.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:54:06.286708: step 382590, loss = 0.14 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:54:07.267419: step 382600, loss = 0.15 (1305.2 examples/sec; 0.098 sec/batch)
2017-06-02 11:54:08.049257: step 382610, loss = 0.15 (1637.2 examples/sec; 0.078 sec/batch)
2017-06-02 11:54:08.922292: step 382620, loss = 0.11 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:54:09.793801: step 382630, loss = 0.20 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:54:10.668107: step 382640, loss = 0.13 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:54:11.553011: step 382650, loss = 0.17 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:54:12.439661: step 382660, loss = 0.14 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:54:13.289401: step 382670, loss = 0.20 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:54:14.144115: step 382680, loss = 0.13 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:54:15.029151: step 382690, loss = 0.15 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 11:54:16.005709: step 382700, loss = 0.15 (1310.7 examples/sec; 0.098 sec/batch)
2017-06-02 11:54:16.789909: step 382710, loss = 0.16 (1632.2 examples/sec; 0.078 sec/batch)
2017-06-02 11:54:17.657176: step 382720, loss = 0.15 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:54:18.499400: step 382730, loss = 0.17 (1519.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:54:19.356937: step 382740, loss = 0.15 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:54:20.197671: step 382750, loss = 0.13 (1522.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:54:21.078355: step 382760, loss = 0.12 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:54:21.930999: step 382770, loss = 0.16 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:54:22.804877: step 382780, loss = 0.16 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:54:23.682262: step 382790, loss = 0.15 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:54:24.619848: step 382800, loss = 0.15 (1365.2 examples/sec; 0.094 sec/batch)
2017-06-02 11:54:25.393324: step 382810, loss = 0.14 (1654.9 examples/sec; 0.077 sec/batch)
2017-06-02 11:54:26.243821: step 382820, loss = 0.17 (1505.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:54:27.087606: step 382830, loss = 0.14 (1517.0 examples/sec; 0.084 sec/batch)
2017-06-02 11:54:27.975327: step 382840, loss = 0.13 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 11:54:28.830189: step 382850, loss = 0.16 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:54:29.686716: step 382860, loss = 0.14 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:54:30.563682: step 382870, loss = 0.13 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:54:31.439569: step 382880, loss = 0.14 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:54:32.306078: step 382890, loss = 0.14 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:54:33.314677: step 382900, loss = 0.15 (1269.1 examples/sec; 0.101 sec/batch)
2017-06-02 11:54:34.072186: step 382910, loss = 0.19 (1689.7 examples/sec; 0.076 sec/batch)
2017-06-02 11:54:34.921880: step 382920, loss = 0.14 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:54:35.776179: step 382930, loss = 0.20 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:54:36.653095: step 382940, loss = 0.17 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:54:37.506245: step 382950, loss = 0.12 (1500.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:54:38.355699: step 382960, loss = 0.14 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:54:39.201350: step 382970, loss = 0.16 (1513.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:54:40.045016: step 382980, loss = 0.17 (1517.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:54:40.898303: step 382990, loss = 0.13 (1500.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:54:41.859656: step 383000, loss = 0.16 (1331.5 examples/sec; 0.096 sec/batch)
2017-06-02 11:54:42.612629: step 383010, loss = 0.12 (1699.9 examples/sec; 0.075 sec/batch)
2017-06-02 11:54:43.466725: step 383020, loss = 0.16 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:54:44.343821: step 383030, loss = 0.15 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:54:45.180196: step 383040, loss = 0.18 (1530.4 examples/sec; 0.084 sec/batch)
2017-06-02 11:54:46.024653: step 383050, loss = 0.12 (1515.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:54:46.873910: step 383060, loss = 0.13 (1507.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:54:47.752800: step 383070, loss = 0.14 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:54:48.608689: step 383080, loss = 0.15 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:54:49.497787: step 383090, loss = 0.13 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:54:50.484807: step 383100, loss = 0.15 (1296.8 examples/sec; 0.099 sec/batch)
2017-06-02 11:54:51.250504: step 383110, loss = 0.12 (1671.7 examples/sec; 0.077 sec/batch)
2017-06-02 11:54:52.125856: step 383120, loss = 0.11 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:54:52.999083: step 383130, loss = 0.14 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:54:53.858201: step 383140, loss = 0.16 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:54:54.701782: step 383150, loss = 0.17 (1517.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:54:55.562421: step 383160, loss = 0.14 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:54:56.425773: step 383170, loss = 0.13 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:54:57.293944: step 383180, loss = 0.14 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:54:58.151977: step 383190, loss = 0.17 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:54:59.128705: step 383200, loss = 0.12 (1310.5 examples/sec; 0.098 sec/batch)
2017-06-02 11:54:59.877637: step 383210, loss = 0.11 (1709.1 examples/sec; 0.075 sec/batch)
2017-06-02 11:55:00.747376: step 383220, loss = 0.21 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:55:01.592420: step 383230, loss = 0.16 (1514.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:55:02.432211: step 383240, loss = 0.17 (1524.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:55:03.296455: step 383250, loss = 0.13 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:55:04.155116: step 383260, loss = 0.15 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:55:05.015937: step 383270, loss = 0.13 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:55:05.882501: step 383280, loss = 0.13 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:55:06.758612: step 383290, loss = 0.14 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:55:07.723439: step 383300, loss = 0.16 (1326.6 examples/sec; 0.096 sec/batch)
2017-06-02 11:55:08.517276: step 383310, loss = 0.10 (1612.4 examples/sec; 0.079 sec/batch)
2017-06-02 11:55:09.399029: step 383320, loss = 0.16 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:55:10.269803: step 383330, loss = 0.13 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:55:11.140247: step 383340, loss = 0.14 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:55:11.988507: step 383350, loss = 0.18 (1509.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:55:12.829905: step 383360, loss = 0.12 (1521.3 examples/sec; 0.084 sec/batch)
2017-06-02 11:55:13.700052: step 383370, loss = 0.15 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:55:14.562721: step 383380, loss = 0.18 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:55:15.443691: step 383390, loss = 0.18 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:55:16.424702: step 383400, loss = 0.15 (1304.8 examples/sec; 0.098 sec/batch)
2017-06-02 11:55:17.163481: step 383410, loss = 0.16 (1732.6 examples/sec; 0.074 sec/batch)
2017-06-02 11:55:18.036095: step 383420, loss = 0.19 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:55:18.905677: step 383430, loss = 0.18 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:55:19.752556: step 383440, loss = 0.18 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:55:20.643996: step 383450, loss = 0.15 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 11:55:21.511745: step 383460, loss = 0.14 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:55:22.361091: step 383470, loss = 0.14 (1507.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:55:23.225384: step 383480, loss = 0.15 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:55:24.082002: step 383490, loss = 0.22 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:55:25.027577: step 383500, loss = 0.14 (1353.7 examples/sec; 0.095 sec/batch)
2017-06-02 11:55:25.799852: step 383510, loss = 0.15 (1657.5 examples/sec; 0.077 sec/batch)
2017-06-02 11:55:26.653522: step 383520, loss = 0.17 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:55:27.511937: step 383530, loss = 0.14 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:55:28.381867: step 383540, loss = 0.18 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:55:29.212076: step 383550, loss = 0.20 (1541.8 examples/sec; 0.083 sec/batch)
2017-06-02 11:55:30.054361: step 383560, loss = 0.12 (1519.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:55:30.932258: step 383570, loss = 0.17 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:55:31.752692: step 383580, loss = 0.13 (1560.2 examples/sec; 0.082 sec/batch)
2017-06-02 11:55:32.614018: step 383590, loss = 0.15 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:55:33.559144: step 383600, loss = 0.11 (1354.3 examples/sec; 0.095 sec/batch)
2017-06-02 11:55:34.347632: step 383610, loss = 0.16 (1623.3 examples/sec; 0.079 sec/batch)
2017-06-02 11:55:35.190763: step 383620, loss = 0.14 (1518.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:55:36.036762: step 383630, loss = 0.14 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:55:36.877486: step 383640, loss = 0.10 (1522.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:55:37.765068: step 383650, loss = 0.12 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 11:55:38.631367: step 383660, loss = 0.16 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:55:39.488481: step 383670, loss = 0.19 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:55:40.338292: step 383680, loss = 0.15 (1506.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:55:41.191683: step 383690, loss = 0.13 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:55:42.143490: step 383700, loss = 0.19 (1344.8 examples/sec; 0.095 sec/batch)
2017-06-02 11:55:42.910455: step 383710, loss = 0.15 (1668.9 examples/sec; 0.077 sec/batch)
2017-06-02 11:55:43.767299: step 383720, loss = 0.12 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:55:44.613578: step 383730, loss = 0.19 (1512.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:55:45.483018: step 383740, loss = 0.15 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:55:46.325421: step 383750, loss = 0.11 (1519.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:55:47.229174: step 383760, loss = 0.23 (1416.3 examples/sec; 0.090 sec/batch)
2017-06-02 11:55:48.094466: step 383770, loss = 0.12 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:55:48.960220: step 383780, loss = 0.14 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:55:49.793232: step 383790, loss = 0.14 (1536.6 examples/sec; 0.083 sec/batch)
2017-06-02 11:55:50.772314: step 383800, loss = 0.17 (1307.3 examples/sec; 0.098 sec/batch)
2017-06-02 11:55:51.537864: step 383810, loss = 0.13 (1672.0 examples/sec; 0.077 sec/batch)
2017-06-02 11:55:52.413009: step 383820, loss = 0.15 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:55:53.283418: step 383830, loss = 0.16 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:55:54.125091: step 383840, loss = 0.12 (1520.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:55:54.957954: step 383850, loss = 0.16 (1536.9 examples/sec; 0.083 sec/batch)
2017-06-02 11:55:55.817441: step 383860, loss = 0.16 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:55:56.655392: step 383870, loss = 0.11 (1527.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:55:57.511366: step 383880, loss = 0.16 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:55:58.373480: step 383890, loss = 0.16 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:55:59.337752: step 383900, loss = 0.12 (1327.4 examples/sec; 0.096 sec/batch)
2017-06-02 11:56:00.123762: step 383910, loss = 0.18 (1628.5 examples/sec; 0.079 sec/batch)
2017-06-02 11:56:00.997802: step 383920, loss = 0.16 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:56:01.864867: step 383930, loss = 0.11 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:56:02.691843: step 383940, loss = 0.14 (1547.8 examples/sec; 0.083 sec/batch)
2017-06-02 11:56:03.565420: step 383950, loss = 0.18 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:56:04.423778: step 383960, loss = 0.14 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:56:05.289607: step 383970, loss = 0.13 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:56:06.156880: step 383980, loss = 0.13 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:56:07.045413: step 383990, loss = 0.18 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:56:08.016665: step 384000, loss = 0.18 (1317.9 examples/sec; 0.097 sec/batch)
2017-06-02 11:56:08.768686: step 384010, loss = 0.14 (1702.1 examples/sec; 0.075 sec/batch)
2017-06-02 11:56:09.620924: step 384020, loss = 0.17 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:56:10.502872: step 384030, loss = 0.16 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:56:11.349901: step 384040, loss = 0.11 (1511.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:56:12.202268: step 384050, loss = 0.14 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:56:13.047215: step 384060, loss = 0.22 (1514.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:56:13.912708: step 384070, loss = 0.14 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:56:14.770240: step 384080, loss = 0.10 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:56:15.668753: step 384090, loss = 0.16 (1424.6 examples/sec; 0.090 sec/batch)
2017-06-02 11:56:16.649818: step 384100, loss = 0.14 (1304.7 examples/sec; 0.098 sec/batch)
2017-06-02 11:56:17.410721: step 384110, loss = 0.12 (1682.2 examples/sec; 0.076 sec/batch)
2017-06-02 11:56:18.294329: step 384120, loss = 0.14 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:56:19.138006: step 384130, loss = 0.14 (1517.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:56:19.991046: step 384140, loss = 0.17 (1500.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:56:20.831043: step 384150, loss = 0.16 (1523.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:56:21.704886: step 384160, loss = 0.18 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:56:22.559893: step 384170, loss = 0.14 (1497.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:56:23.401569: step 384180, loss = 0.14 (1520.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:56:24.286140: step 384190, loss = 0.20 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:56:25.268683: step 384200, loss = 0.17 (1302.7 examples/sec; 0.098 sec/batch)
2017-06-02 11:56:26.053519: step 384210, loss = 0.13 (1631.0 examples/sec; 0.078 sec/batch)
2017-06-02 11:56:26.929929: step 384220, loss = 0.18 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:56:27.802955: step 384230, loss = 0.15 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:56:28.684644: step 384240, loss = 0.16 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 11:56:29.566469: step 384250, loss = 0.16 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:56:30.447105: step 384260, loss = 0.13 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 11:56:31.345627: step 384270, loss = 0.15 (1424.6 examples/sec; 0.090 sec/batch)
2017-06-02 11:56:32.240835: step 384280, loss = 0.13 (1429.8 examples/sec; 0.090 sec/batch)
2017-06-02 11:56:33.102134: step 384290, loss = 0.23 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:56:34.066567: step 384300, loss = 0.24 (1327.2 examples/sec; 0.096 sec/batch)
2017-06-02 11:56:34.859082: step 384310, loss = 0.14 (1615.1 examples/sec; 0.079 sec/batch)
2017-06-02 11:56:35.727290: step 384320, loss = 0.15 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:56:36.610624: step 384330, loss = 0.15 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:56:37.478672: step 384340, loss = 0.12 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:56:38.339838: step 384350, loss = 0.22 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:56:39.217363: step 384360, loss = 0.14 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:56:40.068033: step 384370, loss = 0.17 (1504.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:56:40.936645: step 384380, loss = 0.13 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:56:41.797104: step 384390, loss = 0.14 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:56:42.821140: step 384400, loss = 0.14 (1249.9 examples/sec; 0.102 sec/batch)
2017-06-02 11:56:43.546282: step 384410, loss = 0.19 (1765.2 examples/sec; 0.073 sec/batch)
2017-06-02 11:56:44.416848: step 384420, loss = 0.18 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:56:45.287384: step 384430, loss = 0.11 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:56:46.166122: step 384440, loss = 0.15 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:56:47.040368: step 384450, loss = 0.13 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:56:47.906320: step 384460, loss = 0.12 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:56:48.762098: step 384470, loss = 0.13 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:56:49.641018: step 384480, loss = 0.13 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 11:56:50.483592: step 384490, loss = 0.12 (1519.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:56:51.449894: step 384500, loss = 0.12 (1324.6 examples/sec; 0.097 sec/batch)
2017-06-02 11:56:52.213168: step 384510, loss = 0.17 (1677.0 examples/sec; 0.076 sec/batch)
2017-06-02 11:56:53.064919: step 384520, loss = 0.16 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:56:53.924992: step 384530, loss = 0.15 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:56:54.799199: step 384540, loss = 0.14 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:56:55.683870: step 384550, loss = 0.16 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:56:56.531747: step 384560, loss = 0.14 (1509.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:56:57.378938: step 384570, loss = 0.16 (1510.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:56:58.220813: step 384580, loss = 0.14 (1520.4 examples/sec; 0.084 sec/batch)
2017-06-02 11:56:59.119742: step 384590, loss = 0.16 (1423.9 examples/sec; 0.090 sec/batch)
2017-06-02 11:57:00.114236: step 384600, loss = 0.15 (1287.1 examples/sec; 0.099 sec/batch)
2017-06-02 11:57:00.876321: step 384610, loss = 0.14 (1679.6 examples/sec; 0.076 sec/batch)
2017-06-02 11:57:01.747017: step 384620, loss = 0.15 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:57:02.629357: step 384630, loss = 0.14 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:57:03.484977: step 384640, loss = 0.20 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:57:04.374800: step 384650, loss = 0.15 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 11:57:05.236010: step 384660, loss = 0.14 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:57:06.104392: step 384670, loss = 0.16 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 11:57:07.004371: step 384680, loss = 0.15 (1422.3 examples/sec; 0.090 sec/batch)
2017-06-02 11:57:07.870879: step 384690, loss = 0.13 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:57:08.814422: step 384700, loss = 0.15 (1356.6 examples/sec; 0.094 sec/batch)
2017-06-02 11:57:09.574354: step 384710, loss = 0.14 (1684.4 examples/sec; 0.076 sec/batch)
2017-06-02 11:57:10.443759: step 384720, loss = 0.18 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:57:11.299120: step 384730, loss = 0.14 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:57:12.159523: step 384740, loss = 0.21 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:57:13.022542: step 384750, loss = 0.15 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:57:13.876045: step 384760, loss = 0.18 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:57:14.724206: step 384770, loss = 0.16 (1509.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:57:15.569191: step 384780, loss = 0.17 (1514.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:57:16.399349: step 384790, loss = 0.14 (1541.9 examples/sec; 0.083 sec/batch)
2017-06-02 11:57:17.372438: step 384800, loss = 0.15 (1315.4 examples/sec; 0.097 sec/batch)
2017-06-02 11:57:18.157141: step 384810, loss = 0.14 (1631.2 examples/sec; 0.078 sec/batch)
2017-06-02 11:57:19.043538: step 384820, loss = 0.12 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 11:57:19.890955: step 384830, loss = 0.16 (1510.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:57:20.732595: step 384840, loss = 0.12 (1520.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:57:21.575318: step 384850, loss = 0.14 (1518.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:57:22.419791: step 384860, loss = 0.18 (1515.7 examples/sec; 0.084 sec/batch)
2017-06-02 11:57:23.281207: step 384870, loss = 0.11 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:57:24.148860: step 384880, loss = 0.15 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:57:24.989850: step 384890, loss = 0.12 (1522.0 examples/sec; 0.084 sec/batch)
2017-06-02 11:57:25.963038: step 384900, loss = 0.15 (1315.3 examples/sec; 0.097 sec/batch)
2017-06-02 11:57:26.734629: step 384910, loss = 0.13 (1658.9 examples/sec; 0.077 sec/batch)
2017-06-02 11:57:27.581760: step 384920, loss = 0.15 (1511.0 examples/sec; 0.085 sec/batch)
2017-06-02 11:57:28.432027: step 384930, loss = 0.14 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:57:29.297719: step 384940, loss = 0.11 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:57:30.158286: step 384950, loss = 0.12 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:57:31.019173: step 384960, loss = 0.14 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:57:31.877431: step 384970, loss = 0.14 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:57:32.735449: step 384980, loss = 0.11 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:57:33.615179: step 384990, loss = 0.14 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:57:34.653610: step 385000, loss = 0.19 (1232.6 examples/sec; 0.104 sec/batch)
2017-06-02 11:57:35.412122: step 385010, loss = 0.17 (1687.5 examples/sec; 0.076 sec/batch)
2017-06-02 11:57:36.301743: step 385020, loss = 0.14 (1438.8 examples/sec; 0.089 sec/batch)
2017-06-02 11:57:37.168128: step 385030, loss = 0.17 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:57:38.027969: step 385040, loss = 0.15 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:57:38.908199: step 385050, loss = 0.11 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:57:39.769367: step 385060, loss = 0.12 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:57:40.665477: step 385070, loss = 0.18 (1428.4 examples/sec; 0.090 sec/batch)
2017-06-02 11:57:41.520569: step 385080, loss = 0.15 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:57:42.409860: step 385090, loss = 0.13 (1439.4 examples/sec; 0.089 sec/batch)
2017-06-02 11:57:43.393601: step 385100, loss = 0.13 (1301.1 examples/sec; 0.098 sec/batch)
2017-06-02 11:57:44.152918: step 385110, loss = 0.15 (1685.7 examples/sec; 0.076 sec/batch)
2017-06-02 11:57:45.019660: step 385120, loss = 0.15 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:57:45.876398: step 385130, loss = 0.14 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:57:46.711621: step 385140, loss = 0.15 (1532.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:57:47.563386: step 385150, loss = 0.12 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 11:57:48.431829: step 385160, loss = 0.12 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 11:57:49.291966: step 385170, loss = 0.14 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:57:50.151132: step 385180, loss = 0.18 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:57:51.014445: step 385190, loss = 0.14 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:57:52.025769: step 385200, loss = 0.16 (1265.7 examples/sec; 0.101 sec/batch)
2017-06-02 11:57:52.769635: step 385210, loss = 0.18 (1720.7 examples/sec; 0.074 sec/batch)
2017-06-02 11:57:53.620865: step 385220, loss = 0.21 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:57:54.486803: step 385230, loss = 0.14 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:57:55.349316: step 385240, loss = 0.13 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:57:56.206832: step 385250, loss = 0.13 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:57:57.058077: step 385260, loss = 0.20 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:57:57.907676: step 385270, loss = 0.13 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:57:58.780144: step 385280, loss = 0.13 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:57:59.665645: step 385290, loss = 0.20 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 11:58:00.659706: step 385300, loss = 0.13 (1287.6 examples/sec; 0.099 sec/batch)
2017-06-02 11:58:01.390889: step 385310, loss = 0.13 (1750.6 examples/sec; 0.073 sec/batch)
2017-06-02 11:58:02.246999: step 385320, loss = 0.13 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:58:03.116701: step 385330, loss = 0.13 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:58:03.969896: step 385340, loss = 0.20 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:58:04.866595: step 385350, loss = 0.13 (1427.5 examples/sec; 0.090 sec/batch)
2017-06-02 11:58:05.722436: step 385360, loss = 0.18 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:58:06.583887: step 385370, loss = 0.13 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:58:07.430642: step 385380, loss = 0.14 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:58:08.306788: step 385390, loss = 0.12 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 11:58:09.254613: step 385400, loss = 0.19 (1350.5 examples/sec; 0.095 sec/batch)
2017-06-02 11:58:10.011674: step 385410, loss = 0.14 (1690.7 examples/sec; 0.076 sec/batch)
2017-06-02 11:58:10.853768: step 385420, loss = 0.19 (1520.0 examples/sec; 0.084 sec/batch)
2017-06-02 11:58:11.717758: step 385430, loss = 0.16 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:58:12.579221: step 385440, loss = 0.13 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:58:13.437876: step 385450, loss = 0.13 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:58:14.296858: step 385460, loss = 0.15 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:58:15.163930: step 385470, loss = 0.13 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:58:16.021113: step 385480, loss = 0.18 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:58:16.881183: step 385490, loss = 0.14 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:58:17.849002: step 385500, loss = 0.15 (1322.6 examples/sec; 0.097 sec/batch)
2017-06-02 11:58:18.614047: step 385510, loss = 0.13 (1673.1 examples/sec; 0.077 sec/batch)
2017-06-02 11:58:19.489140: step 385520, loss = 0.17 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:58:20.340286: step 385530, loss = 0.12 (1503.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:58:21.191222: step 385540, loss = 0.15 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:58:22.033267: step 385550, loss = 0.15 (1520.1 examples/sec; 0.084 sec/batch)
2017-06-02 11:58:22.876600: step 385560, loss = 0.23 (1517.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:58:23.765768: step 385570, loss = 0.13 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:58:24.633504: step 385580, loss = 0.15 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:58:25.508358: step 385590, loss = 0.14 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:58:26.472091: step 385600, loss = 0.12 (1328.2 examples/sec; 0.096 sec/batch)
2017-06-02 11:58:27.238444: step 385610, loss = 0.14 (1670.3 examples/sec; 0.077 sec/batch)
2017-06-02 11:58:28.135835: step 385620, loss = 0.14 (1426.3 examples/sec; 0.090 sec/batch)
2017-06-02 11:58:29.017857: step 385630, loss = 0.17 (1451.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:58:29.882576: step 385640, loss = 0.15 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 11:58:30.732745: step 385650, loss = 0.12 (1505.6 examples/sec; 0.085 sec/batch)
2017-06-02 11:58:31.596678: step 385660, loss = 0.14 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:58:32.469374: step 385670, loss = 0.15 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 11:58:33.338096: step 385680, loss = 0.12 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:58:34.199983: step 385690, loss = 0.18 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:58:35.135579: step 385700, loss = 0.17 (1368.1 examples/sec; 0.094 sec/batch)
2017-06-02 11:58:35.914792: step 385710, loss = 0.19 (1642.7 examples/sec; 0.078 sec/batch)
2017-06-02 11:58:36.754041: step 385720, loss = 0.12 (1525.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:58:37.616074: step 385730, loss = 0.15 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 11:58:38.479154: step 385740, loss = 0.15 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:58:39.355432: step 385750, loss = 0.14 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:58:40.248763: step 385760, loss = 0.16 (1432.9 examples/sec; 0.089 sec/batch)
2017-06-02 11:58:41.071996: step 385770, loss = 0.18 (1554.8 examples/sec; 0.082 sec/batch)
2017-06-02 11:58:41.925105: step 385780, loss = 0.14 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:58:42.821515: step 385790, loss = 0.12 (1427.9 examples/sec; 0.090 sec/batch)
2017-06-02 11:58:43.785578: step 385800, loss = 0.11 (1327.7 examples/sec; 0.096 sec/batch)
2017-06-02 11:58:44.569346: step 385810, loss = 0.15 (1633.1 examples/sec; 0.078 sec/batch)
2017-06-02 11:58:45.447478: step 385820, loss = 0.19 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 11:58:46.297752: step 385830, loss = 0.15 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:58:47.137131: step 385840, loss = 0.18 (1524.9 examples/sec; 0.084 sec/batch)
2017-06-02 11:58:48.018839: step 385850, loss = 0.19 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 11:58:48.898993: step 385860, loss = 0.20 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:58:49.761208: step 385870, loss = 0.15 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:58:50.623711: step 385880, loss = 0.14 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:58:51.463867: step 385890, loss = 0.18 (1523.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:58:52.462660: step 385900, loss = 0.17 (1281.6 examples/sec; 0.100 sec/batch)
2017-06-02 11:58:53.205670: step 385910, loss = 0.13 (1722.7 examples/sec; 0.074 sec/batch)
2017-06-02 11:58:54.066815: step 385920, loss = 0.12 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:58:54.926362: step 385930, loss = 0.13 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:58:55.804747: step 385940, loss = 0.16 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 11:58:56.677197: step 385950, loss = 0.14 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 11:58:57.533014: step 385960, loss = 0.14 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:58:58.406995: step 385970, loss = 0.14 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:58:59.267146: step 385980, loss = 0.16 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:59:00.153248: step 385990, loss = 0.19 (1444.5 examples/sec; 0.089 sec/batch)
2017-06-02 11:59:01.108223: step 386000, loss = 0.14 (1340.3 examples/sec; 0.095 sec/batch)
2017-06-02 11:59:01.861036: step 386010, loss = 0.10 (1700.3 examples/sec; 0.075 sec/batch)
2017-06-02 11:59:02.728728: step 386020, loss = 0.17 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:59:03.599793: step 386030, loss = 0.13 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:59:04.467963: step 386040, loss = 0.15 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:59:05.327744: step 386050, loss = 0.15 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:59:06.187979: step 386060, loss = 0.16 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:59:07.044924: step 386070, loss = 0.13 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:59:07.898768: step 386080, loss = 0.13 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:59:08.748093: step 386090, loss = 0.16 (1507.1 examples/sec; 0.085 sec/batch)
2017-06-02 11:59:09.690147: step 386100, loss = 0.14 (1358.7 examples/sec; 0.094 sec/batch)
2017-06-02 11:59:10.466787: step 386110, loss = 0.14 (1648.1 examples/sec; 0.078 sec/batch)
2017-06-02 11:59:11.323079: step 386120, loss = 0.14 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:59:12.175600: step 386130, loss = 0.13 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 11:59:13.050216: step 386140, loss = 0.14 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:59:13.903168: step 386150, loss = 0.18 (1500.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:59:14.755656: step 386160, loss = 0.14 (1501.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:59:15.604176: step 386170, loss = 0.15 (1508.5 examples/sec; 0.085 sec/batch)
2017-06-02 11:59:16.448074: step 386180, loss = 0.14 (1516.8 examples/sec; 0.084 sec/batch)
2017-06-02 11:59:17.299863: step 386190, loss = 0.12 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:59:18.263511: step 386200, loss = 0.15 (1328.3 examples/sec; 0.096 sec/batch)
2017-06-02 11:59:18.997180: step 386210, loss = 0.19 (1744.6 examples/sec; 0.073 sec/batch)
2017-06-02 11:59:19.852968: step 386220, loss = 0.13 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 11:59:20.704447: step 386230, loss = 0.11 (1503.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:59:21.554232: step 386240, loss = 0.16 (1506.3 examples/sec; 0.085 sec/batch)
2017-06-02 11:59:22.430952: step 386250, loss = 0.13 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:59:23.291622: step 386260, loss = 0.12 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 11:59:24.174397: step 386270, loss = 0.16 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:59:25.035140: step 386280, loss = 0.20 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 11:59:25.902678: step 386290, loss = 0.14 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:59:26.874521: step 386300, loss = 0.14 (1317.1 examples/sec; 0.097 sec/batch)
2017-06-02 11:59:27.640124: step 386310, loss = 0.12 (1671.9 examples/sec; 0.077 sec/batch)
2017-06-02 11:59:28.502457: step 386320, loss = 0.17 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:59:29.342634: step 386330, loss = 0.13 (1523.5 examples/sec; 0.084 sec/batch)
2017-06-02 11:59:30.210658: step 386340, loss = 0.17 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:59:31.080080: step 386350, loss = 0.14 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:59:31.939432: step 386360, loss = 0.18 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 11:59:32.812684: step 386370, loss = 0.11 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:59:33.681887: step 386380, loss = 0.17 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 11:59:34.546782: step 386390, loss = 0.16 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 11:59:35.529468: step 386400, loss = 0.14 (1302.5 examples/sec; 0.098 sec/batch)
2017-06-02 11:59:36.301182: step 386410, loss = 0.13 (1658.7 examples/sec; 0.077 sec/batch)
2017-06-02 11:59:37.185678: step 386420, loss = 0.19 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 11:59:38.021639: step 386430, loss = 0.15 (1531.2 examples/sec; 0.084 sec/batch)
2017-06-02 11:59:38.871154: step 386440, loss = 0.18 (1506.7 examples/sec; 0.085 sec/batch)
2017-06-02 11:59:39.725533: step 386450, loss = 0.14 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:59:40.587023: step 386460, loss = 0.11 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 11:59:41.440984: step 386470, loss = 0.15 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 11:59:42.281200: step 386480, loss = 0.18 (1523.4 examples/sec; 0.084 sec/batch)
2017-06-02 11:59:43.150298: step 386490, loss = 0.15 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:59:44.082797: step 386500, loss = 0.16 (1372.7 examples/sec; 0.093 sec/batch)
2017-06-02 11:59:44.835928: step 386510, loss = 0.16 (1699.6 examples/sec; 0.075 sec/batch)
2017-06-02 11:59:45.699954: step 386520, loss = 0.17 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 11:59:46.570430: step 386530, loss = 0.13 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 11:59:47.403918: step 386540, loss = 0.14 (1535.7 examples/sec; 0.083 sec/batch)
2017-06-02 11:59:48.269486: step 386550, loss = 0.16 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:59:49.146841: step 386560, loss = 0.19 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 11:59:50.018280: step 386570, loss = 0.11 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 11:59:50.888819: step 386580, loss = 0.13 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 11:59:51.755277: step 386590, loss = 0.13 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 11:59:52.708924: step 386600, loss = 0.18 (1342.2 examples/sec; 0.095 sec/batch)
2017-06-02 11:59:53.558750: step 386610, loss = 0.14 (1506.2 examples/sec; 0.085 sec/batch)
2017-06-02 11:59:54.457196: step 386620, loss = 0.14 (1424.6 examples/sec; 0.090 sec/batch)
2017-06-02 11:59:55.334329: step 386630, loss = 0.11 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 11:59:56.204370: step 386640, loss = 0.13 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 11:59:57.068338: step 386650, loss = 0.17 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 11:59:57.961802: step 386660, loss = 0.16 (1432.6 examples/sec; 0.089 sec/batch)
2017-06-02 11:59:58.868504: step 386670, loss = 0.16 (1411.7 examples/sec; 0.091 sec/batch)
2017-06-02 11:59:59.722922: step 386680, loss = 0.16 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:00:00.585963: step 386690, loss = 0.15 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:00:01.525420: step 386700, loss = 0.16 (1362.5 examples/sec; 0.094 sec/batch)
2017-06-02 12:00:02.283319: step 386710, loss = 0.15 (1688.9 examples/sec; 0.076 sec/batch)
2017-06-02 12:00:03.162419: step 386720, loss = 0.17 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:00:04.025344: step 386730, loss = 0.12 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:00:04.877556: step 386740, loss = 0.21 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:00:05.740236: step 386750, loss = 0.16 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:00:06.606916: step 386760, loss = 0.14 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:00:07.456557: step 386770, loss = 0.13 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:00:08.304959: step 386780, loss = 0.13 (1508.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:00:09.186302: step 386790, loss = 0.21 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:00:10.151020: step 386800, loss = 0.14 (1326.8 examples/sec; 0.096 sec/batch)
2017-06-02 12:00:10.921501: step 386810, loss = 0.16 (1661.3 examples/sec; 0.077 sec/batch)
2017-06-02 12:00:11.781949: step 386820, loss = 0.16 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:00:12.643223: step 386830, loss = 0.11 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:00:13.489903: step 386840, loss = 0.15 (1511.8 examples/sec; 0.085 sec/batch)
2017-06-02 12:00:14.360606: step 386850, loss = 0.17 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:00:15.208266: step 386860, loss = 0.16 (1510.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:00:16.056784: step 386870, loss = 0.18 (1508.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:00:16.929432: step 386880, loss = 0.17 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:00:17.801948: step 386890, loss = 0.20 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:00:18.770739: step 386900, loss = 0.17 (1321.2 examples/sec; 0.097 sec/batch)
2017-06-02 12:00:19.535855: step 386910, loss = 0.15 (1672.9 examples/sec; 0.077 sec/batch)
2017-06-02 12:00:20.364101: step 386920, loss = 0.13 (1545.4 examples/sec; 0.083 sec/batch)
2017-06-02 12:00:21.211130: step 386930, loss = 0.11 (1511.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:00:22.036690: step 386940, loss = 0.17 (1550.5 examples/sec; 0.083 sec/batch)
2017-06-02 12:00:22.891446: step 386950, loss = 0.14 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:00:23.740875: step 386960, loss = 0.16 (1506.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:00:24.604996: step 386970, loss = 0.15 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:00:25.485534: step 386980, loss = 0.12 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:00:26.358038: step 386990, loss = 0.11 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:00:27.312196: step 387000, loss = 0.18 (1341.5 examples/sec; 0.095 sec/batch)
2017-06-02 12:00:28.064293: step 387010, loss = 0.19 (1701.9 examples/sec; 0.075 sec/batch)
2017-06-02 12:00:28.915100: step 387020, loss = 0.15 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:00:29.789354: step 387030, loss = 0.14 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:00:30.634566: step 387040, loss = 0.13 (1514.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:00:31.469324: step 387050, loss = 0.13 (1533.3 examples/sec; 0.083 sec/batch)
2017-06-02 12:00:32.313463: step 387060, loss = 0.13 (1516.3 examples/sec; 0.084 sec/batch)
2017-06-02 12:00:33.181034: step 387070, loss = 0.13 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:00:34.058100: step 387080, loss = 0.17 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:00:34.915574: step 387090, loss = 0.15 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:00:35.897027: step 387100, loss = 0.13 (1304.2 examples/sec; 0.098 sec/batch)
2017-06-02 12:00:36.639491: step 387110, loss = 0.17 (1724.0 examples/sec; 0.074 sec/batch)
2017-06-02 12:00:37.527755: step 387120, loss = 0.16 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:00:38.408822: step 387130, loss = 0.22 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:00:39.247992: step 387140, loss = 0.22 (1525.3 examples/sec; 0.084 sec/batch)
2017-06-02 12:00:40.112215: step 387150, loss = 0.14 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:00:41.005206: step 387160, loss = 0.14 (1433.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:00:41.879379: step 387170, loss = 0.10 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:00:42.747387: step 387180, loss = 0.16 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:00:43.621588: step 387190, loss = 0.12 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:00:44.630627: step 387200, loss = 0.17 (1268.5 examples/sec; 0.101 sec/batch)
2017-06-02 12:00:45.326446: step 387210, loss = 0.20 (1839.6 examples/sec; 0.070 sec/batch)
2017-06-02 12:00:46.195582: step 387220, loss = 0.15 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:00:47.038725: step 387230, loss = 0.14 (1518.1 examples/sec; 0.084 sec/batch)
2017-06-02 12:00:47.903616: step 387240, loss = 0.13 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:00:48.770288: step 387250, loss = 0.13 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:00:49.643859: step 387260, loss = 0.17 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:00:50.503288: step 387270, loss = 0.15 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:00:51.335329: step 387280, loss = 0.18 (1538.4 examples/sec; 0.083 sec/batch)
2017-06-02 12:00:52.203663: step 387290, loss = 0.13 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:00:53.192115: step 387300, loss = 0.11 (1295.0 examples/sec; 0.099 sec/batch)
2017-06-02 12:00:53.961645: step 387310, loss = 0.14 (1663.3 examples/sec; 0.077 sec/batch)
2017-06-02 12:00:54.839521: step 387320, loss = 0.16 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:00:55.721189: step 387330, loss = 0.19 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:00:56.592906: step 387340, loss = 0.16 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:00:57.453287: step 387350, loss = 0.15 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:00:58.311274: step 387360, loss = 0.13 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:00:59.187567: step 387370, loss = 0.12 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:01:00.044351: step 387380, loss = 0.12 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:01:00.921593: step 387390, loss = 0.13 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:01:01.885173: step 387400, loss = 0.15 (1328.4 examples/sec; 0.096 sec/batch)
2017-06-02 12:01:02.640098: step 387410, loss = 0.16 (1695.6 examples/sec; 0.075 sec/batch)
2017-06-02 12:01:03.514046: step 387420, loss = 0.21 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:01:04.386661: step 387430, loss = 0.15 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:01:05.275278: step 387440, loss = 0.13 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:01:06.169244: step 387450, loss = 0.13 (1431.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:01:07.057927: step 387460, loss = 0.16 (1440.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:01:07.953783: step 387470, loss = 0.13 (1428.8 examples/sec; 0.090 sec/batch)
2017-06-02 12:01:08.847008: step 387480, loss = 0.15 (1433.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:01:09.714496: step 387490, loss = 0.12 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:01:10.676487: step 387500, loss = 0.21 (1330.6 examples/sec; 0.096 sec/batch)
2017-06-02 12:01:11.484308: step 387510, loss = 0.18 (1584.5 examples/sec; 0.081 sec/batch)
2017-06-02 12:01:12.361151: step 387520, loss = 0.20 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:01:13.269871: step 387530, loss = 0.15 (1408.6 examples/sec; 0.091 sec/batch)
2017-06-02 12:01:14.163849: step 387540, loss = 0.16 (1431.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:01:15.043835: step 387550, loss = 0.14 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:01:15.927973: step 387560, loss = 0.20 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:01:16.805826: step 387570, loss = 0.13 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:01:17.687798: step 387580, loss = 0.13 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:01:18.533875: step 387590, loss = 0.12 (1512.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:01:19.523930: step 387600, loss = 0.18 (1292.8 examples/sec; 0.099 sec/batch)
2017-06-02 12:01:20.297074: step 387610, loss = 0.17 (1655.6 examples/sec; 0.077 sec/batch)
2017-06-02 12:01:21.180855: step 387620, loss = 0.17 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:01:22.068799: step 387630, loss = 0.13 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 12:01:22.939190: step 387640, loss = 0.19 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:01:23.807027: step 387650, loss = 0.15 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:01:24.659813: step 387660, loss = 0.15 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:01:25.523866: step 387670, loss = 0.14 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:01:26.405089: step 387680, loss = 0.13 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:01:27.293182: step 387690, loss = 0.16 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:01:28.286281: step 387700, loss = 0.17 (1288.9 examples/sec; 0.099 sec/batch)
2017-06-02 12:01:29.056522: step 387710, loss = 0.13 (1661.8 examples/sec; 0.077 sec/batch)
2017-06-02 12:01:29.916181: step 387720, loss = 0.14 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:01:30.770430: step 387730, loss = 0.16 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:01:31.664444: step 387740, loss = 0.18 (1431.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:01:32.519912: step 387750, loss = 0.19 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:01:33.388870: step 387760, loss = 0.19 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:01:34.279328: step 387770, loss = 0.12 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 12:01:35.163349: step 387780, loss = 0.14 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:01:36.034291: step 387790, loss = 0.14 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:01:37.017333: step 387800, loss = 0.14 (1302.1 examples/sec; 0.098 sec/batch)
2017-06-02 12:01:37.771539: step 387810, loss = 0.12 (1697.2 examples/sec; 0.075 sec/batch)
2017-06-02 12:01:38.623439: step 387820, loss = 0.12 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:01:39.470785: step 387830, loss = 0.13 (1510.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:01:40.304537: step 387840, loss = 0.12 (1535.2 examples/sec; 0.083 sec/batch)
2017-06-02 12:01:41.158065: step 387850, loss = 0.16 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:01:42.005276: step 387860, loss = 0.13 (1510.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:01:42.848621: step 387870, loss = 0.12 (1517.8 examples/sec; 0.084 sec/batch)
2017-06-02 12:01:43.711140: step 387880, loss = 0.15 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:01:44.569185: step 387890, loss = 0.17 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:01:45.545978: step 387900, loss = 0.16 (1310.4 examples/sec; 0.098 sec/batch)
2017-06-02 12:01:46.309336: step 387910, loss = 0.14 (1676.8 examples/sec; 0.076 sec/batch)
2017-06-02 12:01:47.141809: step 387920, loss = 0.15 (1537.6 examples/sec; 0.083 sec/batch)
2017-06-02 12:01:48.005891: step 387930, loss = 0.14 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:01:48.898685: step 387940, loss = 0.17 (1433.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:01:49.764553: step 387950, loss = 0.12 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:01:50.622600: step 387960, loss = 0.14 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:01:51.507378: step 387970, loss = 0.19 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:01:52.371006: step 387980, loss = 0.19 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:01:53.238056: step 387990, loss = 0.14 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:01:54.214033: step 388000, loss = 0.18 (1311.5 examples/sec; 0.098 sec/batch)
2017-06-02 12:01:54.984842: step 388010, loss = 0.16 (1660.6 examples/sec; 0.077 sec/batch)
2017-06-02 12:01:55.843445: step 388020, loss = 0.16 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:01:56.691183: step 388030, loss = 0.14 (1509.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:01:57.568173: step 388040, loss = 0.11 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:01:58.422287: step 388050, loss = 0.12 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:01:59.287373: step 388060, loss = 0.13 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:02:00.188377: step 388070, loss = 0.15 (1420.6 examples/sec; 0.090 sec/batch)
2017-06-02 12:02:01.072553: step 388080, loss = 0.15 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:02:01.915351: step 388090, loss = 0.16 (1518.7 examples/sec; 0.084 sec/batch)
2017-06-02 12:02:02.870757: step 388100, loss = 0.15 (1339.7 examples/sec; 0.096 sec/batch)
2017-06-02 12:02:03.630755: step 388110, loss = 0.17 (1684.2 examples/sec; 0.076 sec/batch)
2017-06-02 12:02:04.520388: step 388120, loss = 0.16 (1438.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:02:05.366236: step 388130, loss = 0.14 (1513.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:02:06.207489: step 388140, loss = 0.17 (1521.6 examples/sec; 0.084 sec/batch)
2017-06-02 12:02:07.050047: step 388150, loss = 0.20 (1519.2 examples/sec; 0.084 sec/batch)
2017-06-02 12:02:07.923955: step 388160, loss = 0.19 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:02:08.789419: step 388170, loss = 0.13 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:02:09.650301: step 388180, loss = 0.13 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:02:10.506064: step 388190, loss = 0.16 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:02:11.477861: step 388200, loss = 0.14 (1317.1 examples/sec; 0.097 sec/batch)
2017-06-02 12:02:12.254260: step 388210, loss = 0.17 (1648.6 examples/sec; 0.078 sec/batch)
2017-06-02 12:02:13.145051: step 388220, loss = 0.15 (1436.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:02:14.023706: step 388230, loss = 0.15 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:02:14.869622: step 388240, loss = 0.12 (1513.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:02:15.737289: step 388250, loss = 0.13 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:02:16.594891: step 388260, loss = 0.15 (1492.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:02:17.435354: step 388270, loss = 0.15 (1523.0 examples/sec; 0.084 sec/batch)
2017-06-02 12:02:18.291364: step 388280, loss = 0.12 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:02:19.156473: step 388290, loss = 0.12 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:02:20.142893: step 388300, loss = 0.15 (1297.6 examples/sec; 0.099 sec/batch)
2017-06-02 12:02:20.914371: step 388310, loss = 0.14 (1659.1 examples/sec; 0.077 sec/batch)
2017-06-02 12:02:21.771490: step 388320, loss = 0.22 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:02:22.635165: step 388330, loss = 0.10 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:02:23.498418: step 388340, loss = 0.17 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:02:24.363312: step 388350, loss = 0.17 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:02:25.232519: step 388360, loss = 0.13 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:02:26.114961: step 388370, loss = 0.18 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:02:26.976321: step 388380, loss = 0.15 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:02:27.839756: step 388390, loss = 0.16 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:02:28.830967: step 388400, loss = 0.20 (1291.3 examples/sec; 0.099 sec/batch)
2017-06-02 12:02:29.600423: step 388410, loss = 0.16 (1663.5 examples/sec; 0.077 sec/batch)
2017-06-02 12:02:30.472196: step 388420, loss = 0.15 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:02:31.348696: step 388430, loss = 0.20 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:02:32.216602: step 388440, loss = 0.14 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:02:33.098055: step 388450, loss = 0.14 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:02:33.954890: step 388460, loss = 0.17 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:02:34.839499: step 388470, loss = 0.16 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:02:35.712306: step 388480, loss = 0.12 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:02:36.600192: step 388490, loss = 0.14 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 12:02:37.583600: step 388500, loss = 0.11 (1301.6 examples/sec; 0.098 sec/batch)
2017-06-02 12:02:38.348338: step 388510, loss = 0.16 (1673.8 examples/sec; 0.076 sec/batch)
2017-06-02 12:02:39.197940: step 388520, loss = 0.16 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:02:40.069250: step 388530, loss = 0.17 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:02:40.952321: step 388540, loss = 0.20 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:02:41.831724: step 388550, loss = 0.11 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:02:42.669355: step 388560, loss = 0.15 (1528.1 examples/sec; 0.084 sec/batch)
2017-06-02 12:02:43.537774: step 388570, loss = 0.13 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:02:44.419567: step 388580, loss = 0.15 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:02:45.317485: step 388590, loss = 0.13 (1425.5 examples/sec; 0.090 sec/batch)
2017-06-02 12:02:46.306908: step 388600, loss = 0.15 (1293.7 examples/sec; 0.099 sec/batch)
2017-06-02 12:02:47.075907: step 388610, loss = 0.18 (1664.5 examples/sec; 0.077 sec/batch)
2017-06-02 12:02:47.949207: step 388620, loss = 0.13 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:02:48.833023: step 388630, loss = 0.14 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:02:49.721191: step 388640, loss = 0.13 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 12:02:50.590888: step 388650, loss = 0.16 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:02:51.457954: step 388660, loss = 0.13 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:02:52.328315: step 388670, loss = 0.15 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:02:53.174625: step 388680, loss = 0.12 (1512.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:02:54.021090: step 388690, loss = 0.18 (1512.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:02:55.033466: step 388700, loss = 0.13 (1264.3 examples/sec; 0.101 sec/batch)
2017-06-02 12:02:55.764573: step 388710, loss = 0.13 (1750.8 examples/sec; 0.073 sec/batch)
2017-06-02 12:02:56.627359: step 388720, loss = 0.14 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:02:57.478457: step 388730, loss = 0.19 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:02:58.365604: step 388740, loss = 0.14 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:02:59.245530: step 388750, loss = 0.14 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:03:00.134675: step 388760, loss = 0.16 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 12:03:00.996320: step 388770, loss = 0.13 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:03:01.874015: step 388780, loss = 0.15 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:03:02.746887: step 388790, loss = 0.22 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:03:03.720678: step 388800, loss = 0.13 (1314.4 examples/sec; 0.097 sec/batch)
2017-06-02 12:03:04.489930: step 388810, loss = 0.15 (1664.0 examples/sec; 0.077 sec/batch)
2017-06-02 12:03:05.357250: step 388820, loss = 0.14 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:03:06.192434: step 388830, loss = 0.20 (1532.6 examples/sec; 0.084 sec/batch)
2017-06-02 12:03:07.079221: step 388840, loss = 0.16 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:03:07.952425: step 388850, loss = 0.13 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:03:08.809252: step 388860, loss = 0.19 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:03:09.682574: step 388870, loss = 0.17 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:03:10.529481: step 388880, loss = 0.16 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:03:11.409118: step 388890, loss = 0.17 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:03:12.371954: step 388900, loss = 0.11 (1329.4 examples/sec; 0.096 sec/batch)
2017-06-02 12:03:13.154139: step 388910, loss = 0.12 (1636.4 examples/sec; 0.078 sec/batch)
2017-06-02 12:03:14.009167: step 388920, loss = 0.20 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:03:14.868086: step 388930, loss = 0.15 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:03:15.745228: step 388940, loss = 0.14 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:03:16.586610: step 388950, loss = 0.13 (1521.3 examples/sec; 0.084 sec/batch)
2017-06-02 12:03:17.434847: step 388960, loss = 0.11 (1509.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:03:18.299707: step 388970, loss = 0.14 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:03:19.167631: step 388980, loss = 0.18 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:03:20.054211: step 388990, loss = 0.13 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:03:21.039433: step 389000, loss = 0.13 (1299.2 examples/sec; 0.099 sec/batch)
2017-06-02 12:03:21.816162: step 389010, loss = 0.13 (1647.9 examples/sec; 0.078 sec/batch)
2017-06-02 12:03:22.686978: step 389020, loss = 0.20 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:03:23.560040: step 389030, loss = 0.18 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:03:24.432719: step 389040, loss = 0.18 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:03:25.310841: step 389050, loss = 0.15 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:03:26.188688: step 389060, loss = 0.16 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:03:27.034182: step 389070, loss = 0.14 (1513.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:03:27.890141: step 389080, loss = 0.12 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:03:28.746992: step 389090, loss = 0.19 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:03:29.756997: step 389100, loss = 0.15 (1267.3 examples/sec; 0.101 sec/batch)
2017-06-02 12:03:30.528888: step 389110, loss = 0.15 (1658.3 examples/sec; 0.077 sec/batch)
2017-06-02 12:03:31.382970: step 389120, loss = 0.18 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:03:32.284742: step 389130, loss = 0.17 (1419.4 examples/sec; 0.090 sec/batch)
2017-06-02 12:03:33.175749: step 389140, loss = 0.15 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 12:03:34.049326: step 389150, loss = 0.13 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:03:34.897889: step 389160, loss = 0.15 (1508.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:03:35.771965: step 389170, loss = 0.15 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:03:36.629607: step 389180, loss = 0.13 (1492.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:03:37.472113: step 389190, loss = 0.19 (1519.3 examples/sec; 0.084 sec/batch)
2017-06-02 12:03:38.433052: step 389200, loss = 0.14 (1332.0 examples/sec; 0.096 sec/batch)
2017-06-02 12:03:39.206978: step 389210, loss = 0.13 (1653.9 examples/sec; 0.077 sec/batch)
2017-06-02 12:03:40.055691: step 389220, loss = 0.20 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:03:40.946592: step 389230, loss = 0.14 (1436.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:03:41.808423: step 389240, loss = 0.23 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:03:42.681259: step 389250, loss = 0.21 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:03:43.538651: step 389260, loss = 0.17 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:03:44.395706: step 389270, loss = 0.24 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:03:45.284477: step 389280, loss = 0.13 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 12:03:46.154574: step 389290, loss = 0.15 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:03:47.118794: step 389300, loss = 0.15 (1327.5 examples/sec; 0.096 sec/batch)
2017-06-02 12:03:47.867023: step 389310, loss = 0.13 (1710.7 examples/sec; 0.075 sec/batch)
2017-06-02 12:03:48.718094: step 389320, loss = 0.16 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:03:49.617614: step 389330, loss = 0.14 (1423.0 examples/sec; 0.090 sec/batch)
2017-06-02 12:03:50.497862: step 389340, loss = 0.13 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:03:51.380550: step 389350, loss = 0.20 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:03:52.209605: step 389360, loss = 0.14 (1543.9 examples/sec; 0.083 sec/batch)
2017-06-02 12:03:53.070988: step 389370, loss = 0.14 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:03:53.939714: step 389380, loss = 0.11 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:03:54.807440: step 389390, loss = 0.14 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:03:55.769829: step 389400, loss = 0.14 (1330.0 examples/sec; 0.096 sec/batch)
2017-06-02 12:03:56.543728: step 389410, loss = 0.13 (1654.0 examples/sec; 0.077 sec/batch)
2017-06-02 12:03:57.395054: step 389420, loss = 0.15 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:03:58.246924: step 389430, loss = 0.13 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:03:59.106978: step 389440, loss = 0.13 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:03:59.988046: step 389450, loss = 0.17 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:04:00.850032: step 389460, loss = 0.15 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:04:01.708791: step 389470, loss = 0.10 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:04:02.576448: step 389480, loss = 0.17 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:04:03.417682: step 389490, loss = 0.16 (1521.6 examples/sec; 0.084 sec/batch)
2017-06-02 12:04:04.361862: step 389500, loss = 0.17 (1355.7 examples/sec; 0.094 sec/batch)
2017-06-02 12:04:05.124785: step 389510, loss = 0.19 (1677.8 examples/sec; 0.076 sec/batch)
2017-06-02 12:04:06.002868: step 389520, loss = 0.17 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:04:06.869991: step 389530, loss = 0.12 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:04:07.728000: step 389540, loss = 0.15 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:04:08.600256: step 389550, loss = 0.11 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:04:09.486341: step 389560, loss = 0.17 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 12:04:10.332647: step 389570, loss = 0.16 (1512.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:04:11.203155: step 389580, loss = 0.17 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:04:12.084015: step 389590, loss = 0.17 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:04:13.050387: step 389600, loss = 0.14 (1324.5 examples/sec; 0.097 sec/batch)
2017-06-02 12:04:13.824664: step 389610, loss = 0.17 (1653.2 examples/sec; 0.077 sec/batch)
2017-06-02 12:04:14.698370: step 389620, loss = 0.18 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:04:15.566611: step 389630, loss = 0.19 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:04:16.447646: step 389640, loss = 0.15 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:04:17.284042: step 389650, loss = 0.19 (1530.4 examples/sec; 0.084 sec/batch)
2017-06-02 12:04:18.158373: step 389660, loss = 0.15 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:04:19.030105: step 389670, loss = 0.14 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:04:19.883839: step 389680, loss = 0.15 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:04:20.730712: step 389690, loss = 0.13 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:04:21.699564: step 389700, loss = 0.13 (1321.2 examples/sec; 0.097 sec/batch)
2017-06-02 12:04:22.478048: step 389710, loss = 0.13 (1644.2 examples/sec; 0.078 sec/batch)
2017-06-02 12:04:23.342744: step 389720, loss = 0.16 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:04:24.197575: step 389730, loss = 0.13 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:04:25.064000: step 389740, loss = 0.17 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:04:25.942860: step 389750, loss = 0.14 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:04:26.818075: step 389760, loss = 0.14 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:04:27.674060: step 389770, loss = 0.12 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:04:28.542303: step 389780, loss = 0.17 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:04:29.426432: step 389790, loss = 0.10 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:04:30.398631: step 389800, loss = 0.16 (1316.6 examples/sec; 0.097 sec/batch)
2017-06-02 12:04:31.157104: step 389810, loss = 0.14 (1687.6 examples/sec; 0.076 sec/batch)
2017-06-02 12:04:32.025501: step 389820, loss = 0.16 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:04:32.931609: step 389830, loss = 0.16 (1412.6 examples/sec; 0.091 sec/batch)
2017-06-02 12:04:33.827597: step 389840, loss = 0.14 (1428.6 examples/sec; 0.090 sec/batch)
2017-06-02 12:04:34.689504: step 389850, loss = 0.14 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:04:35.575446: step 389860, loss = 0.11 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:04:36.470022: step 389870, loss = 0.16 (1430.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:04:37.340667: step 389880, loss = 0.16 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:04:38.226699: step 389890, loss = 0.15 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:04:39.193638: step 389900, loss = 0.15 (1323.8 examples/sec; 0.097 sec/batch)
2017-06-02 12:04:39.988122: step 389910, loss = 0.14 (1611.1 examples/sec; 0.079 sec/batch)
2017-06-02 12:04:40.844411: step 389920, loss = 0.15 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:04:41.720274: step 389930, loss = 0.14 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:04:42.586057: step 389940, loss = 0.16 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:04:43.463298: step 389950, loss = 0.16 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:04:44.355116: step 389960, loss = 0.15 (1435.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:04:45.222151: step 389970, loss = 0.18 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:04:46.096283: step 389980, loss = 0.16 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:04:46.968533: step 389990, loss = 0.17 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:04:47.944290: step 390000, loss = 0.19 (1311.8 examples/sec; 0.098 sec/batch)
2017-06-02 12:04:48.720320: step 390010, loss = 0.11 (1649.4 examples/sec; 0.078 sec/batch)
2017-06-02 12:04:49.594108: step 390020, loss = 0.15 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:04:50.461287: step 390030, loss = 0.13 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:04:51.338702: step 390040, loss = 0.16 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:04:52.228275: step 390050, loss = 0.14 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:04:53.098169: step 390060, loss = 0.15 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:04:53.944591: step 390070, loss = 0.11 (1512.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:04:54.793062: step 390080, loss = 0.13 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:04:55.631049: step 390090, loss = 0.14 (1527.5 examples/sec; 0.084 sec/batch)
2017-06-02 12:04:56.609196: step 390100, loss = 0.16 (1308.6 examples/sec; 0.098 sec/batch)
2017-06-02 12:04:57.376866: step 390110, loss = 0.16 (1667.4 examples/sec; 0.077 sec/batch)
2017-06-02 12:04:58.257878: step 390120, loss = 0.12 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:04:59.132804: step 390130, loss = 0.15 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:05:00.005894: step 390140, loss = 0.12 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:05:00.901474: step 390150, loss = 0.13 (1429.2 examples/sec; 0.090 sec/batch)
2017-06-02 12:05:01.773890: step 390160, loss = 0.18 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:05:02.638535: step 390170, loss = 0.12 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:05:03.494236: step 390180, loss = 0.16 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:05:04.331097: step 390190, loss = 0.14 (1529.5 examples/sec; 0.084 sec/batch)
2017-06-02 12:05:05.294419: step 390200, loss = 0.13 (1328.7 examples/sec; 0.096 sec/batch)
2017-06-02 12:05:06.052427: step 390210, loss = 0.13 (1688.7 examples/sec; 0.076 sec/batch)
2017-06-02 12:05:06.922148: step 390220, loss = 0.15 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:05:07.760503: step 390230, loss = 0.14 (1526.8 examples/sec; 0.084 sec/batch)
2017-06-02 12:05:08.632726: step 390240, loss = 0.14 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:05:09.508612: step 390250, loss = 0.15 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:05:10.379963: step 390260, loss = 0.14 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:05:11.236542: step 390270, loss = 0.19 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:05:12.087315: step 390280, loss = 0.15 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:05:12.932346: step 390290, loss = 0.18 (1514.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:05:13.905311: step 390300, loss = 0.13 (1315.6 examples/sec; 0.097 sec/batch)
2017-06-02 12:05:14.671762: step 390310, loss = 0.15 (1670.0 examples/sec; 0.077 sec/batch)
2017-06-02 12:05:15.530119: step 390320, loss = 0.19 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:05:16.415236: step 390330, loss = 0.20 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 12:05:17.281051: step 390340, loss = 0.17 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:05:18.136267: step 390350, loss = 0.11 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:05:19.014815: step 390360, loss = 0.17 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:05:19.896901: step 390370, loss = 0.12 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:05:20.758544: step 390380, loss = 0.16 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:05:21.629056: step 390390, loss = 0.18 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:05:22.601940: step 390400, loss = 0.14 (1315.7 examples/sec; 0.097 sec/batch)
2017-06-02 12:05:23.364426: step 390410, loss = 0.18 (1678.7 examples/sec; 0.076 sec/batch)
2017-06-02 12:05:24.260619: step 390420, loss = 0.15 (1428.2 examples/sec; 0.090 sec/batch)
2017-06-02 12:05:25.146177: step 390430, loss = 0.13 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:05:26.013212: step 390440, loss = 0.15 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:05:26.872188: step 390450, loss = 0.18 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:05:27.767922: step 390460, loss = 0.14 (1429.0 examples/sec; 0.090 sec/batch)
2017-06-02 12:05:28.636196: step 390470, loss = 0.19 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:05:29.496506: step 390480, loss = 0.15 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:05:30.367555: step 390490, loss = 0.12 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:05:31.319450: step 390500, loss = 0.14 (1344.7 examples/sec; 0.095 sec/batch)
2017-06-02 12:05:32.110339: step 390510, loss = 0.16 (1618.5 examples/sec; 0.079 sec/batch)
2017-06-02 12:05:33.003327: step 390520, loss = 0.15 (1433.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:05:33.885313: step 390530, loss = 0.15 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:05:34.766713: step 390540, loss = 0.16 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:05:35.647902: step 390550, loss = 0.18 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:05:36.543588: step 390560, loss = 0.14 (1429.1 examples/sec; 0.090 sec/batch)
2017-06-02 12:05:37.424525: step 390570, loss = 0.15 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:05:38.300994: step 390580, loss = 0.12 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:05:39.189095: step 390590, loss = 0.13 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:05:40.139451: step 390600, loss = 0.12 (1346.9 examples/sec; 0.095 sec/batch)
2017-06-02 12:05:40.888719: step 390610, loss = 0.13 (1708.3 examples/sec; 0.075 sec/batch)
2017-06-02 12:05:41.740538: step 390620, loss = 0.15 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:05:42.601442: step 390630, loss = 0.14 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:05:43.421768: step 390640, loss = 0.15 (1560.4 examples/sec; 0.082 sec/batch)
2017-06-02 12:05:44.276713: step 390650, loss = 0.13 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:05:45.141040: step 390660, loss = 0.18 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:05:46.011699: step 390670, loss = 0.14 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:05:46.880627: step 390680, loss = 0.14 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:05:47.749647: step 390690, loss = 0.14 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:05:48.744212: step 390700, loss = 0.12 (1287.0 examples/sec; 0.099 sec/batch)
2017-06-02 12:05:49.519247: step 390710, loss = 0.11 (1651.5 examples/sec; 0.078 sec/batch)
2017-06-02 12:05:50.379647: step 390720, loss = 0.13 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:05:51.243533: step 390730, loss = 0.14 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:05:52.108719: step 390740, loss = 0.15 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:05:52.985220: step 390750, loss = 0.15 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:05:53.862992: step 390760, loss = 0.13 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:05:54.727298: step 390770, loss = 0.15 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:05:55.621005: step 390780, loss = 0.19 (1432.2 examples/sec; 0.089 sec/batch)
2017-06-02 12:05:56.477871: step 390790, loss = 0.15 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:05:57.443806: step 390800, loss = 0.14 (1325.1 examples/sec; 0.097 sec/batch)
2017-06-02 12:05:58.218147: step 390810, loss = 0.14 (1653.0 examples/sec; 0.077 sec/batch)
2017-06-02 12:05:59.067039: step 390820, loss = 0.14 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:05:59.935316: step 390830, loss = 0.14 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:06:00.791427: step 390840, loss = 0.17 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:06:01.669598: step 390850, loss = 0.14 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:06:02.522652: step 390860, loss = 0.17 (1500.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:06:03.371371: step 390870, loss = 0.15 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:06:04.211169: step 390880, loss = 0.12 (1524.2 examples/sec; 0.084 sec/batch)
2017-06-02 12:06:05.108254: step 390890, loss = 0.17 (1426.8 examples/sec; 0.090 sec/batch)
2017-06-02 12:06:06.108088: step 390900, loss = 0.15 (1280.2 examples/sec; 0.100 sec/batch)
2017-06-02 12:06:06.866308: step 390910, loss = 0.16 (1688.2 examples/sec; 0.076 sec/batch)
2017-06-02 12:06:07.718988: step 390920, loss = 0.14 (1501.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:06:08.590023: step 390930, loss = 0.15 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:06:09.414721: step 390940, loss = 0.19 (1552.1 examples/sec; 0.082 sec/batch)
2017-06-02 12:06:10.299215: step 390950, loss = 0.12 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:06:11.196813: step 390960, loss = 0.15 (1426.0 examples/sec; 0.090 sec/batch)
2017-06-02 12:06:12.055748: step 390970, loss = 0.19 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:06:12.897571: step 390980, loss = 0.16 (1520.5 examples/sec; 0.084 sec/batch)
2017-06-02 12:06:13.763054: step 390990, loss = 0.14 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:06:14.753515: step 391000, loss = 0.16 (1292.3 examples/sec; 0.099 sec/batch)
2017-06-02 12:06:15.500672: step 391010, loss = 0.15 (1713.1 examples/sec; 0.075 sec/batch)
2017-06-02 12:06:16.363147: step 391020, loss = 0.17 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:06:17.214970: step 391030, loss = 0.14 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:06:18.083690: step 391040, loss = 0.15 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:06:18.939831: step 391050, loss = 0.12 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:06:19.771929: step 391060, loss = 0.18 (1538.3 examples/sec; 0.083 sec/batch)
2017-06-02 12:06:20.623766: step 391070, loss = 0.12 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:06:21.483871: step 391080, loss = 0.15 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:06:22.348173: step 391090, loss = 0.15 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:06:23.306771: step 391100, loss = 0.13 (1335.3 examples/sec; 0.096 sec/batch)
2017-06-02 12:06:24.083777: step 391110, loss = 0.23 (1647.3 examples/sec; 0.078 sec/batch)
2017-06-02 12:06:24.947669: step 391120, loss = 0.12 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:06:25.808678: step 391130, loss = 0.12 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:06:26.681027: step 391140, loss = 0.14 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:06:27.526970: step 391150, loss = 0.20 (1513.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:06:28.373702: step 391160, loss = 0.12 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:06:29.215312: step 391170, loss = 0.16 (1520.9 examples/sec; 0.084 sec/batch)
2017-06-02 12:06:30.090804: step 391180, loss = 0.14 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:06:30.962053: step 391190, loss = 0.15 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:06:31.935962: step 391200, loss = 0.16 (1314.3 examples/sec; 0.097 sec/batch)
2017-06-02 12:06:32.697931: step 391210, loss = 0.13 (1679.9 examples/sec; 0.076 sec/batch)
2017-06-02 12:06:33.536939: step 391220, loss = 0.12 (1525.6 examples/sec; 0.084 sec/batch)
2017-06-02 12:06:34.407459: step 391230, loss = 0.19 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:06:35.257106: step 391240, loss = 0.21 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:06:36.144803: step 391250, loss = 0.12 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:06:36.991722: step 391260, loss = 0.15 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:06:37.857987: step 391270, loss = 0.13 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:06:38.713905: step 391280, loss = 0.10 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:06:39.574075: step 391290, loss = 0.15 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:06:40.540356: step 391300, loss = 0.18 (1324.7 examples/sec; 0.097 sec/batch)
2017-06-02 12:06:41.309882: step 391310, loss = 0.15 (1663.4 examples/sec; 0.077 sec/batch)
2017-06-02 12:06:42.153305: step 391320, loss = 0.16 (1517.6 examples/sec; 0.084 sec/batch)
2017-06-02 12:06:43.030715: step 391330, loss = 0.13 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:06:43.886539: step 391340, loss = 0.13 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:06:44.759201: step 391350, loss = 0.15 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:06:45.633860: step 391360, loss = 0.13 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:06:46.508499: step 391370, loss = 0.14 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:06:47.382510: step 391380, loss = 0.12 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:06:48.246888: step 391390, loss = 0.16 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:06:49.206767: step 391400, loss = 0.15 (1333.5 examples/sec; 0.096 sec/batch)
2017-06-02 12:06:49.960680: step 391410, loss = 0.17 (1697.8 examples/sec; 0.075 sec/batch)
2017-06-02 12:06:50.845143: step 391420, loss = 0.13 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:06:51.698716: step 391430, loss = 0.17 (1499.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:06:52.567438: step 391440, loss = 0.12 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:06:53.406499: step 391450, loss = 0.19 (1525.5 examples/sec; 0.084 sec/batch)
2017-06-02 12:06:54.266436: step 391460, loss = 0.13 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:06:55.097191: step 391470, loss = 0.13 (1540.8 examples/sec; 0.083 sec/batch)
2017-06-02 12:06:55.938581: step 391480, loss = 0.15 (1521.3 examples/sec; 0.084 sec/batch)
2017-06-02 12:06:56.814710: step 391490, loss = 0.18 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:06:57.827165: step 391500, loss = 0.13 (1264.3 examples/sec; 0.101 sec/batch)
2017-06-02 12:06:58.550534: step 391510, loss = 0.14 (1769.5 examples/sec; 0.072 sec/batch)
2017-06-02 12:06:59.405910: step 391520, loss = 0.15 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:07:00.256192: step 391530, loss = 0.12 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:07:01.111743: step 391540, loss = 0.12 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:07:01.989332: step 391550, loss = 0.23 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:07:02.869283: step 391560, loss = 0.17 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:07:03.718277: step 391570, loss = 0.13 (1507.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:07:04.587789: step 391580, loss = 0.12 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:07:05.469272: step 391590, loss = 0.12 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:07:06.435632: step 391600, loss = 0.19 (1324.6 examples/sec; 0.097 sec/batch)
2017-06-02 12:07:07.183160: step 391610, loss = 0.16 (1712.4 examples/sec; 0.075 sec/batch)
2017-06-02 12:07:08.034053: step 391620, loss = 0.14 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:07:08.902506: step 391630, loss = 0.16 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:07:09.792168: step 391640, loss = 0.16 (1438.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:07:10.671395: step 391650, loss = 0.13 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:07:11.532918: step 391660, loss = 0.16 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:07:12.378630: step 391670, loss = 0.17 (1513.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:07:13.240878: step 391680, loss = 0.17 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:07:14.115862: step 391690, loss = 0.16 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:07:15.120690: step 391700, loss = 0.14 (1273.8 examples/sec; 0.100 sec/batch)
2017-06-02 12:07:15.837061: step 391710, loss = 0.13 (1786.8 examples/sec; 0.072 sec/batch)
2017-06-02 12:07:16.698311: step 391720, loss = 0.14 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:07:17.556667: step 391730, loss = 0.15 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:07:18.412588: step 391740, loss = 0.15 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:07:19.278494: step 391750, loss = 0.13 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:07:20.151498: step 391760, loss = 0.13 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:07:21.021624: step 391770, loss = 0.12 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:07:21.896089: step 391780, loss = 0.15 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:07:22.738541: step 391790, loss = 0.17 (1519.4 examples/sec; 0.084 sec/batch)
2017-06-02 12:07:23.776069: step 391800, loss = 0.15 (1233.7 examples/sec; 0.104 sec/batch)
2017-06-02 12:07:24.473646: step 391810, loss = 0.13 (1834.9 examples/sec; 0.070 sec/batch)
2017-06-02 12:07:25.323068: step 391820, loss = 0.13 (1506.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:07:26.209105: step 391830, loss = 0.17 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:07:27.095873: step 391840, loss = 0.16 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:07:27.975712: step 391850, loss = 0.16 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:07:28.834384: step 391860, loss = 0.13 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:07:29.707168: step 391870, loss = 0.14 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:07:30.557445: step 391880, loss = 0.15 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:07:31.420743: step 391890, loss = 0.12 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:07:32.379704: step 391900, loss = 0.17 (1334.8 examples/sec; 0.096 sec/batch)
2017-06-02 12:07:33.137191: step 391910, loss = 0.15 (1689.8 examples/sec; 0.076 sec/batch)
2017-06-02 12:07:34.000649: step 391920, loss = 0.11 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:07:34.873239: step 391930, loss = 0.15 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:07:35.748218: step 391940, loss = 0.13 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:07:36.619138: step 391950, loss = 0.15 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:07:37.477562: step 391960, loss = 0.16 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:07:38.363224: step 391970, loss = 0.19 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 12:07:39.221102: step 391980, loss = 0.13 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:07:40.086887: step 391990, loss = 0.14 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:07:41.068201: step 392000, loss = 0.12 (1304.4 examples/sec; 0.098 sec/batch)
2017-06-02 12:07:41.806835: step 392010, loss = 0.17 (1732.9 examples/sec; 0.074 sec/batch)
2017-06-02 12:07:42.667704: step 392020, loss = 0.13 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:07:43.549476: step 392030, loss = 0.11 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:07:44.412137: step 392040, loss = 0.13 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:07:45.273929: step 392050, loss = 0.16 (1485.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:07:46.126760: step 392060, loss = 0.14 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:07:46.992039: step 392070, loss = 0.14 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:07:47.860828: step 392080, loss = 0.16 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:07:48.716799: step 392090, loss = 0.18 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:07:49.688468: step 392100, loss = 0.12 (1317.3 examples/sec; 0.097 sec/batch)
2017-06-02 12:07:50.451252: step 392110, loss = 0.11 (1678.1 examples/sec; 0.076 sec/batch)
2017-06-02 12:07:51.308990: step 392120, loss = 0.13 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:07:52.176261: step 392130, loss = 0.14 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:07:53.037223: step 392140, loss = 0.13 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:07:53.882889: step 392150, loss = 0.13 (1513.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:07:54.730893: step 392160, loss = 0.19 (1509.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:07:55.603628: step 392170, loss = 0.12 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:07:56.493977: step 392180, loss = 0.16 (1437.6 examples/sec; 0.089 sec/batch)
2017-06-02 12:07:57.376796: step 392190, loss = 0.16 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:07:58.338756: step 392200, loss = 0.13 (1330.6 examples/sec; 0.096 sec/batch)
2017-06-02 12:07:59.105615: step 392210, loss = 0.13 (1669.1 examples/sec; 0.077 sec/batch)
2017-06-02 12:07:59.965292: step 392220, loss = 0.13 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:08:00.829452: step 392230, loss = 0.15 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:08:01.705085: step 392240, loss = 0.11 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:08:02.575765: step 392250, loss = 0.14 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:08:03.441697: step 392260, loss = 0.13 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:08:04.317813: step 392270, loss = 0.16 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:08:05.203300: step 392280, loss = 0.14 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 12:08:06.075627: step 392290, loss = 0.13 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:08:07.046304: step 392300, loss = 0.11 (1318.7 examples/sec; 0.097 sec/batch)
2017-06-02 12:08:07.814806: step 392310, loss = 0.16 (1665.6 examples/sec; 0.077 sec/batch)
2017-06-02 12:08:08.697569: step 392320, loss = 0.23 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:08:09.578553: step 392330, loss = 0.14 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:08:10.443301: step 392340, loss = 0.12 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:08:11.322005: step 392350, loss = 0.19 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:08:12.190294: step 392360, loss = 0.14 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:08:13.039613: step 392370, loss = 0.12 (1507.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:08:13.908393: step 392380, loss = 0.13 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:08:14.752289: step 392390, loss = 0.18 (1516.8 examples/sec; 0.084 sec/batch)
2017-06-02 12:08:15.713936: step 392400, loss = 0.15 (1331.1 examples/sec; 0.096 sec/batch)
2017-06-02 12:08:16.484751: step 392410, loss = 0.16 (1660.6 examples/sec; 0.077 sec/batch)
2017-06-02 12:08:17.343765: step 392420, loss = 0.13 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:08:18.217693: step 392430, loss = 0.17 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:08:19.100256: step 392440, loss = 0.11 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:08:19.981743: step 392450, loss = 0.17 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:08:20.869166: step 392460, loss = 0.12 (1442.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:08:21.759121: step 392470, loss = 0.16 (1438.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:08:22.615173: step 392480, loss = 0.14 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:08:23.473518: step 392490, loss = 0.20 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:08:24.471534: step 392500, loss = 0.16 (1282.5 examples/sec; 0.100 sec/batch)
2017-06-02 12:08:25.243809: step 392510, loss = 0.14 (1657.4 examples/sec; 0.077 sec/batch)
2017-06-02 12:08:26.102501: step 392520, loss = 0.13 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:08:26.985860: step 392530, loss = 0.17 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:08:27.864428: step 392540, loss = 0.13 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:08:28.765601: step 392550, loss = 0.14 (1420.4 examples/sec; 0.090 sec/batch)
2017-06-02 12:08:29.646945: step 392560, loss = 0.15 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:08:30.513924: step 392570, loss = 0.13 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:08:31.331833: step 392580, loss = 0.12 (1565.0 examples/sec; 0.082 sec/batch)
2017-06-02 12:08:32.224017: step 392590, loss = 0.11 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:08:33.214485: step 392600, loss = 0.13 (1292.3 examples/sec; 0.099 sec/batch)
2017-06-02 12:08:33.962271: step 392610, loss = 0.18 (1711.7 examples/sec; 0.075 sec/batch)
2017-06-02 12:08:34.829548: step 392620, loss = 0.17 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:08:35.688415: step 392630, loss = 0.16 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:08:36.527327: step 392640, loss = 0.16 (1525.8 examples/sec; 0.084 sec/batch)
2017-06-02 12:08:37.385038: step 392650, loss = 0.15 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:08:38.258494: step 392660, loss = 0.15 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:08:39.142866: step 392670, loss = 0.16 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:08:40.015245: step 392680, loss = 0.12 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:08:40.878959: step 392690, loss = 0.18 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:08:41.840814: step 392700, loss = 0.16 (1330.8 examples/sec; 0.096 sec/batch)
2017-06-02 12:08:42.611248: step 392710, loss = 0.15 (1661.4 examples/sec; 0.077 sec/batch)
2017-06-02 12:08:43.474376: step 392720, loss = 0.16 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:08:44.325037: step 392730, loss = 0.16 (1504.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:08:45.194567: step 392740, loss = 0.15 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:08:46.050537: step 392750, loss = 0.16 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:08:46.902796: step 392760, loss = 0.15 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:08:47.761682: step 392770, loss = 0.17 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:08:48.631414: step 392780, loss = 0.14 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:08:49.506699: step 392790, loss = 0.20 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:08:50.462300: step 392800, loss = 0.12 (1339.5 examples/sec; 0.096 sec/batch)
2017-06-02 12:08:51.217055: step 392810, loss = 0.15 (1695.9 examples/sec; 0.075 sec/batch)
2017-06-02 12:08:52.067334: step 392820, loss = 0.12 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:08:52.936560: step 392830, loss = 0.14 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:08:53.773355: step 392840, loss = 0.15 (1529.6 examples/sec; 0.084 sec/batch)
2017-06-02 12:08:54.643230: step 392850, loss = 0.13 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:08:55.497163: step 392860, loss = 0.18 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:08:56.364654: step 392870, loss = 0.14 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:08:57.236665: step 392880, loss = 0.14 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:08:58.095385: step 392890, loss = 0.18 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:08:59.066507: step 392900, loss = 0.13 (1318.1 examples/sec; 0.097 sec/batch)
2017-06-02 12:08:59.841155: step 392910, loss = 0.11 (1652.4 examples/sec; 0.077 sec/batch)
2017-06-02 12:09:00.717375: step 392920, loss = 0.17 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:09:01.567650: step 392930, loss = 0.13 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:09:02.431887: step 392940, loss = 0.17 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:09:03.287918: step 392950, loss = 0.12 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:09:04.168691: step 392960, loss = 0.13 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:09:05.048767: step 392970, loss = 0.12 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:09:05.921385: step 392980, loss = 0.13 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:09:06.784633: step 392990, loss = 0.24 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:09:07.734589: step 393000, loss = 0.14 (1347.4 examples/sec; 0.095 sec/batch)
2017-06-02 12:09:08.499073: step 393010, loss = 0.17 (1674.3 examples/sec; 0.076 sec/batch)
2017-06-02 12:09:09.352943: step 393020, loss = 0.11 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:09:10.227545: step 393030, loss = 0.15 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:09:11.120236: step 393040, loss = 0.12 (1433.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:09:12.015725: step 393050, loss = 0.19 (1429.4 examples/sec; 0.090 sec/batch)
2017-06-02 12:09:12.879414: step 393060, loss = 0.14 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:09:13.749350: step 393070, loss = 0.16 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:09:14.617378: step 393080, loss = 0.14 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:09:15.499036: step 393090, loss = 0.13 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:09:16.482721: step 393100, loss = 0.18 (1301.2 examples/sec; 0.098 sec/batch)
2017-06-02 12:09:17.259439: step 393110, loss = 0.13 (1648.0 examples/sec; 0.078 sec/batch)
2017-06-02 12:09:18.126636: step 393120, loss = 0.12 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:09:18.982053: step 393130, loss = 0.12 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:09:19.869860: step 393140, loss = 0.12 (1441.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:09:20.745851: step 393150, loss = 0.13 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:09:21.602121: step 393160, loss = 0.17 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:09:22.472054: step 393170, loss = 0.17 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:09:23.340952: step 393180, loss = 0.12 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:09:24.220586: step 393190, loss = 0.21 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:09:25.258826: step 393200, loss = 0.15 (1232.8 examples/sec; 0.104 sec/batch)
2017-06-02 12:09:25.975806: step 393210, loss = 0.15 (1785.3 examples/sec; 0.072 sec/batch)
2017-06-02 12:09:26.856834: step 393220, loss = 0.17 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:09:27.722318: step 393230, loss = 0.16 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:09:28.603049: step 393240, loss = 0.16 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:09:29.501143: step 393250, loss = 0.15 (1425.2 examples/sec; 0.090 sec/batch)
2017-06-02 12:09:30.373287: step 393260, loss = 0.16 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:09:31.252804: step 393270, loss = 0.12 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:09:32.143222: step 393280, loss = 0.13 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 12:09:33.033031: step 393290, loss = 0.14 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 12:09:34.000139: step 393300, loss = 0.15 (1323.6 examples/sec; 0.097 sec/batch)
2017-06-02 12:09:34.771311: step 393310, loss = 0.12 (1659.8 examples/sec; 0.077 sec/batch)
2017-06-02 12:09:35.650781: step 393320, loss = 0.14 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:09:36.515200: step 393330, loss = 0.15 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:09:37.393568: step 393340, loss = 0.15 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:09:38.238656: step 393350, loss = 0.19 (1514.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:09:39.118961: step 393360, loss = 0.12 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:09:39.992381: step 393370, loss = 0.20 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:09:40.864088: step 393380, loss = 0.11 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:09:41.725918: step 393390, loss = 0.22 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:09:42.670168: step 393400, loss = 0.15 (1355.6 examples/sec; 0.094 sec/batch)
2017-06-02 12:09:43.449553: step 393410, loss = 0.12 (1642.3 examples/sec; 0.078 sec/batch)
2017-06-02 12:09:44.331357: step 393420, loss = 0.14 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:09:45.211609: step 393430, loss = 0.18 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:09:46.059570: step 393440, loss = 0.14 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:09:46.938130: step 393450, loss = 0.14 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:09:47.807438: step 393460, loss = 0.16 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:09:48.650203: step 393470, loss = 0.16 (1518.8 examples/sec; 0.084 sec/batch)
2017-06-02 12:09:49.535024: step 393480, loss = 0.13 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:09:50.397107: step 393490, loss = 0.12 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:09:51.353822: step 393500, loss = 0.13 (1337.9 examples/sec; 0.096 sec/batch)
2017-06-02 12:09:52.105089: step 393510, loss = 0.12 (1703.8 examples/sec; 0.075 sec/batch)
2017-06-02 12:09:53.037431: step 393520, loss = 0.14 (1372.9 examples/sec; 0.093 sec/batch)
2017-06-02 12:09:53.904795: step 393530, loss = 0.13 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:09:54.776202: step 393540, loss = 0.13 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:09:55.679135: step 393550, loss = 0.13 (1417.6 examples/sec; 0.090 sec/batch)
2017-06-02 12:09:56.538485: step 393560, loss = 0.14 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:09:57.417253: step 393570, loss = 0.18 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:09:58.276842: step 393580, loss = 0.13 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:09:59.163375: step 393590, loss = 0.16 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:10:00.140852: step 393600, loss = 0.14 (1309.5 examples/sec; 0.098 sec/batch)
2017-06-02 12:10:00.922812: step 393610, loss = 0.11 (1636.9 examples/sec; 0.078 sec/batch)
2017-06-02 12:10:01.781784: step 393620, loss = 0.16 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:10:02.645049: step 393630, loss = 0.17 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:10:03.515882: step 393640, loss = 0.15 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:10:04.386134: step 393650, loss = 0.19 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:10:05.245137: step 393660, loss = 0.12 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:10:06.092178: step 393670, loss = 0.11 (1511.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:10:06.947801: step 393680, loss = 0.13 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:10:07.821399: step 393690, loss = 0.14 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:10:08.779365: step 393700, loss = 0.18 (1336.2 examples/sec; 0.096 sec/batch)
2017-06-02 12:10:09.534721: step 393710, loss = 0.11 (1694.6 examples/sec; 0.076 sec/batch)
2017-06-02 12:10:10.380282: step 393720, loss = 0.13 (1513.8 examples/sec; 0.085 sec/batch)
2017-06-02 12:10:11.238371: step 393730, loss = 0.15 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:10:12.117562: step 393740, loss = 0.16 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:10:12.986966: step 393750, loss = 0.13 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:10:13.874666: step 393760, loss = 0.11 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:10:14.750464: step 393770, loss = 0.13 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:10:15.623043: step 393780, loss = 0.14 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:10:16.480571: step 393790, loss = 0.17 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:10:17.476582: step 393800, loss = 0.24 (1285.1 examples/sec; 0.100 sec/batch)
2017-06-02 12:10:18.205013: step 393810, loss = 0.12 (1757.2 examples/sec; 0.073 sec/batch)
2017-06-02 12:10:19.070709: step 393820, loss = 0.14 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:10:19.939691: step 393830, loss = 0.16 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:10:20.824818: step 393840, loss = 0.13 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:10:21.688897: step 393850, loss = 0.14 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:10:22.573370: step 393860, loss = 0.17 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:10:23.466377: step 393870, loss = 0.19 (1433.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:10:24.335668: step 393880, loss = 0.13 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:10:25.201615: step 393890, loss = 0.13 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:10:26.175688: step 393900, loss = 0.12 (1314.1 examples/sec; 0.097 sec/batch)
2017-06-02 12:10:26.914922: step 393910, loss = 0.27 (1731.5 examples/sec; 0.074 sec/batch)
2017-06-02 12:10:27.780035: step 393920, loss = 0.13 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:10:28.632564: step 393930, loss = 0.13 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:10:29.505974: step 393940, loss = 0.14 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:10:30.358935: step 393950, loss = 0.14 (1500.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:10:31.195300: step 393960, loss = 0.13 (1530.4 examples/sec; 0.084 sec/batch)
2017-06-02 12:10:32.043269: step 393970, loss = 0.16 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:10:32.919388: step 393980, loss = 0.15 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:10:33.775963: step 393990, loss = 0.17 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:10:34.711571: step 394000, loss = 0.19 (1368.1 examples/sec; 0.094 sec/batch)
2017-06-02 12:10:35.481429: step 394010, loss = 0.12 (1662.7 examples/sec; 0.077 sec/batch)
2017-06-02 12:10:36.362481: step 394020, loss = 0.18 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:10:37.231367: step 394030, loss = 0.12 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:10:38.098853: step 394040, loss = 0.12 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:10:38.953897: step 394050, loss = 0.14 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:10:39.854138: step 394060, loss = 0.15 (1421.8 examples/sec; 0.090 sec/batch)
2017-06-02 12:10:40.706391: step 394070, loss = 0.16 (1501.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:10:41.583322: step 394080, loss = 0.18 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:10:42.454574: step 394090, loss = 0.20 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:10:43.414722: step 394100, loss = 0.14 (1333.1 examples/sec; 0.096 sec/batch)
2017-06-02 12:10:44.182656: step 394110, loss = 0.14 (1666.8 examples/sec; 0.077 sec/batch)
2017-06-02 12:10:45.043834: step 394120, loss = 0.18 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:10:45.919909: step 394130, loss = 0.17 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:10:46.798981: step 394140, loss = 0.14 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:10:47.679784: step 394150, loss = 0.15 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:10:48.549102: step 394160, loss = 0.16 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:10:49.424306: step 394170, loss = 0.12 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:10:50.304954: step 394180, loss = 0.18 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:10:51.174465: step 394190, loss = 0.12 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:10:52.141538: step 394200, loss = 0.12 (1323.6 examples/sec; 0.097 sec/batch)
2017-06-02 12:10:52.908093: step 394210, loss = 0.17 (1669.8 examples/sec; 0.077 sec/batch)
2017-06-02 12:10:53.783689: step 394220, loss = 0.15 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:10:54.640402: step 394230, loss = 0.14 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:10:55.518402: step 394240, loss = 0.14 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:10:56.411488: step 394250, loss = 0.13 (1433.2 examples/sec; 0.089 sec/batch)
2017-06-02 12:10:57.296107: step 394260, loss = 0.16 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:10:58.191247: step 394270, loss = 0.13 (1429.9 examples/sec; 0.090 sec/batch)
2017-06-02 12:10:59.051308: step 394280, loss = 0.13 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:10:59.925463: step 394290, loss = 0.17 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:11:00.900938: step 394300, loss = 0.15 (1312.2 examples/sec; 0.098 sec/batch)
2017-06-02 12:11:01.678094: step 394310, loss = 0.13 (1647.0 examples/sec; 0.078 sec/batch)
2017-06-02 12:11:02.554803: step 394320, loss = 0.17 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:11:03.423966: step 394330, loss = 0.16 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:11:04.271117: step 394340, loss = 0.18 (1511.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:11:05.120844: step 394350, loss = 0.13 (1506.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:11:05.975479: step 394360, loss = 0.15 (1497.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:11:06.877575: step 394370, loss = 0.16 (1418.9 examples/sec; 0.090 sec/batch)
2017-06-02 12:11:07.746146: step 394380, loss = 0.14 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:11:08.621254: step 394390, loss = 0.17 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:11:09.582475: step 394400, loss = 0.13 (1331.6 examples/sec; 0.096 sec/batch)
2017-06-02 12:11:10.359281: step 394410, loss = 0.13 (1647.8 examples/sec; 0.078 sec/batch)
2017-06-02 12:11:11.247616: step 394420, loss = 0.12 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:11:12.089378: step 394430, loss = 0.14 (1520.6 examples/sec; 0.084 sec/batch)
2017-06-02 12:11:12.949228: step 394440, loss = 0.18 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:11:13.842575: step 394450, loss = 0.15 (1432.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:11:14.691036: step 394460, loss = 0.16 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:11:15.551353: step 394470, loss = 0.13 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:11:16.424569: step 394480, loss = 0.21 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:11:17.267051: step 394490, loss = 0.10 (1519.3 examples/sec; 0.084 sec/batch)
2017-06-02 12:11:18.199252: step 394500, loss = 0.12 (1373.1 examples/sec; 0.093 sec/batch)
2017-06-02 12:11:18.951533: step 394510, loss = 0.19 (1701.5 examples/sec; 0.075 sec/batch)
2017-06-02 12:11:19.827251: step 394520, loss = 0.16 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:11:20.679640: step 394530, loss = 0.14 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:11:21.531133: step 394540, loss = 0.12 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:11:22.377575: step 394550, loss = 0.14 (1512.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:11:23.235753: step 394560, loss = 0.16 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:11:24.091914: step 394570, loss = 0.14 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:11:24.940029: step 394580, loss = 0.13 (1509.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:11:25.799936: step 394590, loss = 0.13 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:11:26.767201: step 394600, loss = 0.14 (1323.3 examples/sec; 0.097 sec/batch)
2017-06-02 12:11:27.510653: step 394610, loss = 0.10 (1721.7 examples/sec; 0.074 sec/batch)
2017-06-02 12:11:28.385684: step 394620, loss = 0.13 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:11:29.267546: step 394630, loss = 0.17 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:11:30.110434: step 394640, loss = 0.14 (1518.6 examples/sec; 0.084 sec/batch)
2017-06-02 12:11:30.999618: step 394650, loss = 0.13 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 12:11:31.838253: step 394660, loss = 0.14 (1526.3 examples/sec; 0.084 sec/batch)
2017-06-02 12:11:32.705348: step 394670, loss = 0.14 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:11:33.545037: step 394680, loss = 0.15 (1524.4 examples/sec; 0.084 sec/batch)
2017-06-02 12:11:34.406395: step 394690, loss = 0.14 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:11:35.380091: step 394700, loss = 0.18 (1314.6 examples/sec; 0.097 sec/batch)
2017-06-02 12:11:36.178354: step 394710, loss = 0.18 (1603.5 examples/sec; 0.080 sec/batch)
2017-06-02 12:11:37.052495: step 394720, loss = 0.12 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:11:37.907384: step 394730, loss = 0.17 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:11:38.770290: step 394740, loss = 0.11 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:11:39.649452: step 394750, loss = 0.14 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:11:40.508767: step 394760, loss = 0.15 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:11:41.382412: step 394770, loss = 0.29 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:11:42.261402: step 394780, loss = 0.12 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:11:43.126422: step 394790, loss = 0.15 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:11:44.125468: step 394800, loss = 0.12 (1281.2 examples/sec; 0.100 sec/batch)
2017-06-02 12:11:44.852331: step 394810, loss = 0.13 (1761.0 examples/sec; 0.073 sec/batch)
2017-06-02 12:11:45.727586: step 394820, loss = 0.13 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:11:46.594170: step 394830, loss = 0.12 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:11:47.468410: step 394840, loss = 0.12 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:11:48.340674: step 394850, loss = 0.15 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:11:49.195376: step 394860, loss = 0.13 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:11:50.080684: step 394870, loss = 0.18 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:11:50.937202: step 394880, loss = 0.16 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:11:51.811440: step 394890, loss = 0.12 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:11:52.827341: step 394900, loss = 0.14 (1260.0 examples/sec; 0.102 sec/batch)
2017-06-02 12:11:53.574338: step 394910, loss = 0.15 (1713.5 examples/sec; 0.075 sec/batch)
2017-06-02 12:11:54.439039: step 394920, loss = 0.17 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:11:55.321386: step 394930, loss = 0.15 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:11:56.186043: step 394940, loss = 0.13 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:11:57.090234: step 394950, loss = 0.13 (1415.6 examples/sec; 0.090 sec/batch)
2017-06-02 12:11:57.949614: step 394960, loss = 0.13 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:11:58.792588: step 394970, loss = 0.18 (1518.4 examples/sec; 0.084 sec/batch)
2017-06-02 12:11:59.650082: step 394980, loss = 0.14 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:12:00.509582: step 394990, loss = 0.13 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:12:01.460061: step 395000, loss = 0.14 (1346.7 examples/sec; 0.095 sec/batch)
2017-06-02 12:12:02.233265: step 395010, loss = 0.13 (1655.5 examples/sec; 0.077 sec/batch)
2017-06-02 12:12:03.097972: step 395020, loss = 0.15 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:12:03.950087: step 395030, loss = 0.18 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:12:04.810932: step 395040, loss = 0.12 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:12:05.682408: step 395050, loss = 0.11 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:12:06.536094: step 395060, loss = 0.11 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:12:07.418409: step 395070, loss = 0.14 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:12:08.275023: step 395080, loss = 0.17 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:12:09.125907: step 395090, loss = 0.15 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:12:10.155670: step 395100, loss = 0.14 (1243.0 examples/sec; 0.103 sec/batch)
2017-06-02 12:12:10.894722: step 395110, loss = 0.16 (1731.9 examples/sec; 0.074 sec/batch)
2017-06-02 12:12:11.757011: step 395120, loss = 0.13 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:12:12.628007: step 395130, loss = 0.13 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:12:13.481874: step 395140, loss = 0.14 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:12:14.337752: step 395150, loss = 0.12 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:12:15.222463: step 395160, loss = 0.17 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:12:16.097441: step 395170, loss = 0.15 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:12:16.946419: step 395180, loss = 0.17 (1507.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:12:17.792276: step 395190, loss = 0.12 (1513.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:12:18.785166: step 395200, loss = 0.14 (1289.2 examples/sec; 0.099 sec/batch)
2017-06-02 12:12:19.544395: step 395210, loss = 0.16 (1685.9 examples/sec; 0.076 sec/batch)
2017-06-02 12:12:20.418256: step 395220, loss = 0.12 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:12:21.278965: step 395230, loss = 0.15 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:12:22.131749: step 395240, loss = 0.13 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:12:22.995176: step 395250, loss = 0.13 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:12:23.874743: step 395260, loss = 0.12 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:12:24.763449: step 395270, loss = 0.15 (1440.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:12:25.650342: step 395280, loss = 0.16 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 12:12:26.527071: step 395290, loss = 0.15 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:12:27.508209: step 395300, loss = 0.16 (1304.6 examples/sec; 0.098 sec/batch)
2017-06-02 12:12:28.278374: step 395310, loss = 0.15 (1662.0 examples/sec; 0.077 sec/batch)
2017-06-02 12:12:29.143588: step 395320, loss = 0.15 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:12:30.029375: step 395330, loss = 0.14 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:12:30.906775: step 395340, loss = 0.11 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:12:31.781833: step 395350, loss = 0.16 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:12:32.654032: step 395360, loss = 0.11 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:12:33.540279: step 395370, loss = 0.12 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:12:34.428575: step 395380, loss = 0.15 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:12:35.302164: step 395390, loss = 0.18 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:12:36.335668: step 395400, loss = 0.13 (1238.5 examples/sec; 0.103 sec/batch)
2017-06-02 12:12:37.075597: step 395410, loss = 0.15 (1729.9 examples/sec; 0.074 sec/batch)
2017-06-02 12:12:37.968127: step 395420, loss = 0.13 (1434.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:12:38.817830: step 395430, loss = 0.14 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:12:39.694490: step 395440, loss = 0.13 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:12:40.576892: step 395450, loss = 0.11 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:12:41.453972: step 395460, loss = 0.20 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:12:42.337661: step 395470, loss = 0.11 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:12:43.195411: step 395480, loss = 0.18 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:12:44.075298: step 395490, loss = 0.18 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:12:45.042464: step 395500, loss = 0.14 (1323.5 examples/sec; 0.097 sec/batch)
2017-06-02 12:12:45.818890: step 395510, loss = 0.15 (1648.6 examples/sec; 0.078 sec/batch)
2017-06-02 12:12:46.698944: step 395520, loss = 0.14 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:12:47.569376: step 395530, loss = 0.14 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:12:48.417873: step 395540, loss = 0.17 (1508.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:12:49.313085: step 395550, loss = 0.14 (1429.8 examples/sec; 0.090 sec/batch)
2017-06-02 12:12:50.195662: step 395560, loss = 0.18 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:12:51.075086: step 395570, loss = 0.11 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:12:51.952929: step 395580, loss = 0.14 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:12:52.806149: step 395590, loss = 0.14 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:12:53.847834: step 395600, loss = 0.17 (1228.8 examples/sec; 0.104 sec/batch)
2017-06-02 12:12:54.585846: step 395610, loss = 0.14 (1734.4 examples/sec; 0.074 sec/batch)
2017-06-02 12:12:55.450572: step 395620, loss = 0.17 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:12:56.314239: step 395630, loss = 0.16 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:12:57.175140: step 395640, loss = 0.14 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:12:58.054402: step 395650, loss = 0.12 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:12:58.909640: step 395660, loss = 0.14 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:12:59.816460: step 395670, loss = 0.12 (1411.5 examples/sec; 0.091 sec/batch)
2017-06-02 12:13:00.700702: step 395680, loss = 0.15 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:13:01.574330: step 395690, loss = 0.19 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:13:02.522314: step 395700, loss = 0.16 (1350.2 examples/sec; 0.095 sec/batch)
2017-06-02 12:13:03.272680: step 395710, loss = 0.13 (1705.8 examples/sec; 0.075 sec/batch)
2017-06-02 12:13:04.139014: step 395720, loss = 0.13 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:13:04.992115: step 395730, loss = 0.14 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:13:05.864562: step 395740, loss = 0.20 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:13:06.758439: step 395750, loss = 0.16 (1432.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:13:07.653864: step 395760, loss = 0.17 (1429.5 examples/sec; 0.090 sec/batch)
2017-06-02 12:13:08.541334: step 395770, loss = 0.12 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:13:09.409025: step 395780, loss = 0.18 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:13:10.289004: step 395790, loss = 0.13 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:13:11.265339: step 395800, loss = 0.15 (1311.0 examples/sec; 0.098 sec/batch)
2017-06-02 12:13:12.048262: step 395810, loss = 0.12 (1634.9 examples/sec; 0.078 sec/batch)
2017-06-02 12:13:12.917199: step 395820, loss = 0.14 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:13:13.784661: step 395830, loss = 0.12 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:13:14.650427: step 395840, loss = 0.11 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:13:15.519267: step 395850, loss = 0.12 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:13:16.408563: step 395860, loss = 0.14 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:13:17.296323: step 395870, loss = 0.13 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:13:18.178262: step 395880, loss = 0.14 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:13:19.057238: step 395890, loss = 0.15 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:13:20.073053: step 395900, loss = 0.19 (1260.1 examples/sec; 0.102 sec/batch)
2017-06-02 12:13:20.780258: step 395910, loss = 0.15 (1809.9 examples/sec; 0.071 sec/batch)
2017-06-02 12:13:21.647369: step 395920, loss = 0.23 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:13:22.536837: step 395930, loss = 0.13 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:13:23.421386: step 395940, loss = 0.15 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:13:24.285683: step 395950, loss = 0.17 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:13:25.131839: step 395960, loss = 0.14 (1512.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:13:26.004681: step 395970, loss = 0.15 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:13:26.888782: step 395980, loss = 0.12 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:13:27.724285: step 395990, loss = 0.12 (1532.0 examples/sec; 0.084 sec/batch)
2017-06-02 12:13:28.714949: step 396000, loss = 0.16 (1292.1 examples/sec; 0.099 sec/batch)
2017-06-02 12:13:29.467668: step 396010, loss = 0.14 (1700.5 examples/sec; 0.075 sec/batch)
2017-06-02 12:13:30.344606: step 396020, loss = 0.15 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:13:31.218473: step 396030, loss = 0.16 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:13:32.089674: step 396040, loss = 0.16 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:13:32.960781: step 396050, loss = 0.16 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:13:33.829872: step 396060, loss = 0.12 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:13:34.686742: step 396070, loss = 0.11 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:13:35.557140: step 396080, loss = 0.14 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:13:36.448185: step 396090, loss = 0.13 (1436.5 examples/sec; 0.089 sec/batch)
2017-06-02 12:13:37.427321: step 396100, loss = 0.17 (1307.3 examples/sec; 0.098 sec/batch)
2017-06-02 12:13:38.171784: step 396110, loss = 0.14 (1719.4 examples/sec; 0.074 sec/batch)
2017-06-02 12:13:39.015097: step 396120, loss = 0.13 (1517.8 examples/sec; 0.084 sec/batch)
2017-06-02 12:13:39.875011: step 396130, loss = 0.13 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:13:40.757887: step 396140, loss = 0.15 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:13:41.611171: step 396150, loss = 0.16 (1500.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:13:42.478257: step 396160, loss = 0.18 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:13:43.347610: step 396170, loss = 0.12 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:13:44.203498: step 396180, loss = 0.13 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:13:45.099762: step 396190, loss = 0.17 (1428.2 examples/sec; 0.090 sec/batch)
2017-06-02 12:13:46.082109: step 396200, loss = 0.18 (1303.0 examples/sec; 0.098 sec/batch)
2017-06-02 12:13:46.866374: step 396210, loss = 0.15 (1632.1 examples/sec; 0.078 sec/batch)
2017-06-02 12:13:47.732841: step 396220, loss = 0.17 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:13:48.619626: step 396230, loss = 0.12 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:13:49.501769: step 396240, loss = 0.16 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:13:50.362171: step 396250, loss = 0.15 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:13:51.232947: step 396260, loss = 0.16 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:13:52.120537: step 396270, loss = 0.14 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:13:52.974298: step 396280, loss = 0.17 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:13:53.808216: step 396290, loss = 0.15 (1534.9 examples/sec; 0.083 sec/batch)
2017-06-02 12:13:54.759432: step 396300, loss = 0.17 (1345.6 examples/sec; 0.095 sec/batch)
2017-06-02 12:13:55.537563: step 396310, loss = 0.16 (1645.0 examples/sec; 0.078 sec/batch)
2017-06-02 12:13:56.416173: step 396320, loss = 0.24 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:13:57.274875: step 396330, loss = 0.16 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:13:58.136906: step 396340, loss = 0.14 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:13:58.980667: step 396350, loss = 0.13 (1517.0 examples/sec; 0.084 sec/batch)
2017-06-02 12:13:59.855138: step 396360, loss = 0.14 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:14:00.708479: step 396370, loss = 0.11 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:14:01.585997: step 396380, loss = 0.11 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:14:02.444649: step 396390, loss = 0.14 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:14:03.405351: step 396400, loss = 0.16 (1332.4 examples/sec; 0.096 sec/batch)
2017-06-02 12:14:04.166300: step 396410, loss = 0.12 (1682.1 examples/sec; 0.076 sec/batch)
2017-06-02 12:14:05.025906: step 396420, loss = 0.15 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:14:05.901013: step 396430, loss = 0.12 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:14:06.743336: step 396440, loss = 0.12 (1519.6 examples/sec; 0.084 sec/batch)
2017-06-02 12:14:07.599209: step 396450, loss = 0.12 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:14:08.440413: step 396460, loss = 0.14 (1521.6 examples/sec; 0.084 sec/batch)
2017-06-02 12:14:09.324950: step 396470, loss = 0.16 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:14:10.201675: step 396480, loss = 0.15 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:14:11.059426: step 396490, loss = 0.22 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:14:12.038376: step 396500, loss = 0.14 (1307.5 examples/sec; 0.098 sec/batch)
2017-06-02 12:14:12.819582: step 396510, loss = 0.16 (1638.5 examples/sec; 0.078 sec/batch)
2017-06-02 12:14:13.690049: step 396520, loss = 0.13 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:14:14.534597: step 396530, loss = 0.16 (1515.6 examples/sec; 0.084 sec/batch)
2017-06-02 12:14:15.400589: step 396540, loss = 0.17 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:14:16.275832: step 396550, loss = 0.14 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:14:17.156203: step 396560, loss = 0.17 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:14:17.996547: step 396570, loss = 0.15 (1523.2 examples/sec; 0.084 sec/batch)
2017-06-02 12:14:18.859436: step 396580, loss = 0.17 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:14:19.710290: step 396590, loss = 0.17 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:14:20.653002: step 396600, loss = 0.20 (1357.8 examples/sec; 0.094 sec/batch)
2017-06-02 12:14:21.427558: step 396610, loss = 0.18 (1652.6 examples/sec; 0.077 sec/batch)
2017-06-02 12:14:22.302893: step 396620, loss = 0.13 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:14:23.165413: step 396630, loss = 0.13 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:14:24.049094: step 396640, loss = 0.12 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:14:24.933924: step 396650, loss = 0.12 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:14:25.827662: step 396660, loss = 0.15 (1432.2 examples/sec; 0.089 sec/batch)
2017-06-02 12:14:26.702396: step 396670, loss = 0.16 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:14:27.592969: step 396680, loss = 0.15 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:14:28.489972: step 396690, loss = 0.16 (1427.0 examples/sec; 0.090 sec/batch)
2017-06-02 12:14:29.479934: step 396700, loss = 0.14 (1293.0 examples/sec; 0.099 sec/batch)
2017-06-02 12:14:30.230361: step 396710, loss = 0.13 (1705.8 examples/sec; 0.075 sec/batch)
2017-06-02 12:14:31.131364: step 396720, loss = 0.16 (1420.6 examples/sec; 0.090 sec/batch)
2017-06-02 12:14:32.031153: step 396730, loss = 0.15 (1422.6 examples/sec; 0.090 sec/batch)
2017-06-02 12:14:32.882604: step 396740, loss = 0.14 (1503.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:14:33.733425: step 396750, loss = 0.15 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:14:34.618642: step 396760, loss = 0.17 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:14:35.477378: step 396770, loss = 0.12 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:14:36.344524: step 396780, loss = 0.13 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:14:37.215715: step 396790, loss = 0.16 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:14:38.175123: step 396800, loss = 0.15 (1334.1 examples/sec; 0.096 sec/batch)
2017-06-02 12:14:38.940663: step 396810, loss = 0.14 (1672.0 examples/sec; 0.077 sec/batch)
2017-06-02 12:14:39.835642: step 396820, loss = 0.17 (1430.2 examples/sec; 0.089 sec/batch)
2017-06-02 12:14:40.673727: step 396830, loss = 0.19 (1527.3 examples/sec; 0.084 sec/batch)
2017-06-02 12:14:41.547870: step 396840, loss = 0.17 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:14:42.403871: step 396850, loss = 0.14 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:14:43.294408: step 396860, loss = 0.15 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:14:44.176791: step 396870, loss = 0.12 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:14:45.052558: step 396880, loss = 0.14 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:14:45.932042: step 396890, loss = 0.15 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:14:46.924110: step 396900, loss = 0.18 (1290.2 examples/sec; 0.099 sec/batch)
2017-06-02 12:14:47.674908: step 396910, loss = 0.14 (1704.9 examples/sec; 0.075 sec/batch)
2017-06-02 12:14:48.548855: step 396920, loss = 0.11 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:14:49.414063: step 396930, loss = 0.13 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:14:50.290291: step 396940, loss = 0.12 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:14:51.160113: step 396950, loss = 0.13 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:14:52.014376: step 396960, loss = 0.20 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:14:52.883355: step 396970, loss = 0.19 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:14:53.746156: step 396980, loss = 0.19 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:14:54.617135: step 396990, loss = 0.16 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:14:55.629838: step 397000, loss = 0.15 (1263.9 examples/sec; 0.101 sec/batch)
2017-06-02 12:14:56.380157: step 397010, loss = 0.16 (1706.0 examples/sec; 0.075 sec/batch)
2017-06-02 12:14:57.235649: step 397020, loss = 0.17 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:14:58.110643: step 397030, loss = 0.11 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:14:58.992944: step 397040, loss = 0.12 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:14:59.855342: step 397050, loss = 0.21 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:15:00.743190: step 397060, loss = 0.14 (1441.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:15:01.630950: step 397070, loss = 0.13 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:15:02.482332: step 397080, loss = 0.16 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:15:03.367328: step 397090, loss = 0.13 (1446.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:15:04.372058: step 397100, loss = 0.14 (1274.0 examples/sec; 0.100 sec/batch)
2017-06-02 12:15:05.155878: step 397110, loss = 0.14 (1633.0 examples/sec; 0.078 sec/batch)
2017-06-02 12:15:06.003172: step 397120, loss = 0.14 (1510.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:15:06.867487: step 397130, loss = 0.19 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:15:07.770420: step 397140, loss = 0.16 (1417.6 examples/sec; 0.090 sec/batch)
2017-06-02 12:15:08.648579: step 397150, loss = 0.16 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:15:09.533617: step 397160, loss = 0.14 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:15:10.407387: step 397170, loss = 0.14 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:15:11.276905: step 397180, loss = 0.14 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:15:12.162921: step 397190, loss = 0.11 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:15:13.148607: step 397200, loss = 0.13 (1298.6 examples/sec; 0.099 sec/batch)
2017-06-02 12:15:13.912925: step 397210, loss = 0.16 (1674.7 examples/sec; 0.076 sec/batch)
2017-06-02 12:15:14.790902: step 397220, loss = 0.14 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:15:15.682942: step 397230, loss = 0.17 (1434.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:15:16.545889: step 397240, loss = 0.12 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:15:17.413425: step 397250, loss = 0.17 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:15:18.292348: step 397260, loss = 0.13 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:15:19.176247: step 397270, loss = 0.12 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:15:20.042745: step 397280, loss = 0.11 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:15:20.894341: step 397290, loss = 0.16 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:15:21.866210: step 397300, loss = 0.20 (1317.1 examples/sec; 0.097 sec/batch)
2017-06-02 12:15:22.643751: step 397310, loss = 0.15 (1646.2 examples/sec; 0.078 sec/batch)
2017-06-02 12:15:23.512688: step 397320, loss = 0.14 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:15:24.390199: step 397330, loss = 0.17 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:15:25.254226: step 397340, loss = 0.19 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:15:26.099686: step 397350, loss = 0.13 (1514.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:15:26.983215: step 397360, loss = 0.15 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:15:27.864556: step 397370, loss = 0.12 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:15:28.734315: step 397380, loss = 0.15 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:15:29.631292: step 397390, loss = 0.15 (1427.0 examples/sec; 0.090 sec/batch)
2017-06-02 12:15:30.641535: step 397400, loss = 0.14 (1267.0 examples/sec; 0.101 sec/batch)
2017-06-02 12:15:31.383394: step 397410, loss = 0.14 (1725.4 examples/sec; 0.074 sec/batch)
2017-06-02 12:15:32.259709: step 397420, loss = 0.13 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:15:33.148363: step 397430, loss = 0.15 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:15:34.018156: step 397440, loss = 0.18 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:15:34.875363: step 397450, loss = 0.16 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:15:35.744692: step 397460, loss = 0.13 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:15:36.606699: step 397470, loss = 0.21 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:15:37.463475: step 397480, loss = 0.11 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:15:38.332879: step 397490, loss = 0.13 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:15:39.324990: step 397500, loss = 0.19 (1290.2 examples/sec; 0.099 sec/batch)
2017-06-02 12:15:40.077523: step 397510, loss = 0.15 (1700.9 examples/sec; 0.075 sec/batch)
2017-06-02 12:15:40.947958: step 397520, loss = 0.19 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:15:41.803954: step 397530, loss = 0.19 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:15:42.660674: step 397540, loss = 0.16 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:15:43.539255: step 397550, loss = 0.13 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:15:44.393126: step 397560, loss = 0.15 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:15:45.272111: step 397570, loss = 0.20 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:15:46.136540: step 397580, loss = 0.13 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:15:47.007541: step 397590, loss = 0.15 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:15:47.942701: step 397600, loss = 0.14 (1368.7 examples/sec; 0.094 sec/batch)
2017-06-02 12:15:48.697596: step 397610, loss = 0.14 (1695.6 examples/sec; 0.075 sec/batch)
2017-06-02 12:15:49.535982: step 397620, loss = 0.17 (1526.7 examples/sec; 0.084 sec/batch)
2017-06-02 12:15:50.406621: step 397630, loss = 0.13 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:15:51.290358: step 397640, loss = 0.14 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:15:52.155837: step 397650, loss = 0.15 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:15:53.023553: step 397660, loss = 0.13 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:15:53.918651: step 397670, loss = 0.15 (1430.0 examples/sec; 0.090 sec/batch)
2017-06-02 12:15:54.760722: step 397680, loss = 0.15 (1520.0 examples/sec; 0.084 sec/batch)
2017-06-02 12:15:55.621789: step 397690, loss = 0.16 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:15:56.580955: step 397700, loss = 0.15 (1334.5 examples/sec; 0.096 sec/batch)
2017-06-02 12:15:57.382783: step 397710, loss = 0.22 (1596.4 examples/sec; 0.080 sec/batch)
2017-06-02 12:15:58.254377: step 397720, loss = 0.18 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:15:59.126476: step 397730, loss = 0.14 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:15:59.988578: step 397740, loss = 0.19 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:16:00.833226: step 397750, loss = 0.14 (1515.4 examples/sec; 0.084 sec/batch)
2017-06-02 12:16:01.679624: step 397760, loss = 0.14 (1512.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:16:02.563588: step 397770, loss = 0.13 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:16:03.439298: step 397780, loss = 0.14 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:16:04.322634: step 397790, loss = 0.14 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:16:05.338981: step 397800, loss = 0.14 (1259.4 examples/sec; 0.102 sec/batch)
2017-06-02 12:16:06.096717: step 397810, loss = 0.15 (1689.2 examples/sec; 0.076 sec/batch)
2017-06-02 12:16:06.978566: step 397820, loss = 0.14 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:16:07.840102: step 397830, loss = 0.14 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:16:08.708393: step 397840, loss = 0.12 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:16:09.590270: step 397850, loss = 0.14 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:16:10.500178: step 397860, loss = 0.11 (1406.7 examples/sec; 0.091 sec/batch)
2017-06-02 12:16:11.371781: step 397870, loss = 0.14 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:16:12.243261: step 397880, loss = 0.22 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:16:13.125612: step 397890, loss = 0.12 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:16:14.115206: step 397900, loss = 0.13 (1293.5 examples/sec; 0.099 sec/batch)
2017-06-02 12:16:14.880866: step 397910, loss = 0.17 (1671.7 examples/sec; 0.077 sec/batch)
2017-06-02 12:16:15.777521: step 397920, loss = 0.13 (1427.5 examples/sec; 0.090 sec/batch)
2017-06-02 12:16:16.663453: step 397930, loss = 0.17 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:16:17.565793: step 397940, loss = 0.14 (1418.5 examples/sec; 0.090 sec/batch)
2017-06-02 12:16:18.454769: step 397950, loss = 0.12 (1439.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:16:19.311951: step 397960, loss = 0.17 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:16:20.210654: step 397970, loss = 0.16 (1424.3 examples/sec; 0.090 sec/batch)
2017-06-02 12:16:21.119898: step 397980, loss = 0.16 (1407.8 examples/sec; 0.091 sec/batch)
2017-06-02 12:16:22.000772: step 397990, loss = 0.18 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:16:22.987646: step 398000, loss = 0.16 (1297.0 examples/sec; 0.099 sec/batch)
2017-06-02 12:16:23.778842: step 398010, loss = 0.15 (1617.8 examples/sec; 0.079 sec/batch)
2017-06-02 12:16:24.653486: step 398020, loss = 0.13 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:16:25.544657: step 398030, loss = 0.14 (1436.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:16:26.410808: step 398040, loss = 0.15 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:16:27.270825: step 398050, loss = 0.16 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:16:28.127853: step 398060, loss = 0.17 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:16:28.977461: step 398070, loss = 0.16 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:16:29.846076: step 398080, loss = 0.17 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:16:30.721455: step 398090, loss = 0.14 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:16:31.710692: step 398100, loss = 0.14 (1293.9 examples/sec; 0.099 sec/batch)
2017-06-02 12:16:32.456127: step 398110, loss = 0.15 (1717.1 examples/sec; 0.075 sec/batch)
2017-06-02 12:16:33.322999: step 398120, loss = 0.14 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:16:34.163723: step 398130, loss = 0.19 (1522.5 examples/sec; 0.084 sec/batch)
2017-06-02 12:16:35.017946: step 398140, loss = 0.11 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:16:35.893564: step 398150, loss = 0.14 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:16:36.762685: step 398160, loss = 0.16 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:16:37.631649: step 398170, loss = 0.17 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:16:38.493227: step 398180, loss = 0.19 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:16:39.380295: step 398190, loss = 0.12 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:16:40.345870: step 398200, loss = 0.16 (1325.6 examples/sec; 0.097 sec/batch)
2017-06-02 12:16:41.114256: step 398210, loss = 0.16 (1665.9 examples/sec; 0.077 sec/batch)
2017-06-02 12:16:42.002811: step 398220, loss = 0.16 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 12:16:42.891709: step 398230, loss = 0.14 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:16:43.773081: step 398240, loss = 0.13 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:16:44.662761: step 398250, loss = 0.13 (1438.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:16:45.573104: step 398260, loss = 0.13 (1406.1 examples/sec; 0.091 sec/batch)
2017-06-02 12:16:46.432372: step 398270, loss = 0.12 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:16:47.310136: step 398280, loss = 0.22 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:16:48.201996: step 398290, loss = 0.14 (1435.2 examples/sec; 0.089 sec/batch)
2017-06-02 12:16:49.135912: step 398300, loss = 0.12 (1370.6 examples/sec; 0.093 sec/batch)
2017-06-02 12:16:49.922496: step 398310, loss = 0.16 (1627.3 examples/sec; 0.079 sec/batch)
2017-06-02 12:16:50.827498: step 398320, loss = 0.20 (1414.3 examples/sec; 0.091 sec/batch)
2017-06-02 12:16:51.696527: step 398330, loss = 0.19 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:16:52.581091: step 398340, loss = 0.14 (1447.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:16:53.430085: step 398350, loss = 0.15 (1507.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:16:54.272917: step 398360, loss = 0.13 (1518.8 examples/sec; 0.084 sec/batch)
2017-06-02 12:16:55.148369: step 398370, loss = 0.14 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:16:56.003123: step 398380, loss = 0.12 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:16:56.895531: step 398390, loss = 0.16 (1434.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:16:57.871927: step 398400, loss = 0.12 (1310.9 examples/sec; 0.098 sec/batch)
2017-06-02 12:16:58.638796: step 398410, loss = 0.13 (1669.1 examples/sec; 0.077 sec/batch)
2017-06-02 12:16:59.491921: step 398420, loss = 0.17 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:17:00.379274: step 398430, loss = 0.11 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 12:17:01.238994: step 398440, loss = 0.13 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:17:02.116243: step 398450, loss = 0.16 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:17:02.991238: step 398460, loss = 0.14 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:17:03.862059: step 398470, loss = 0.14 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:17:04.703709: step 398480, loss = 0.16 (1520.8 examples/sec; 0.084 sec/batch)
2017-06-02 12:17:05.564480: step 398490, loss = 0.14 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:17:06.551740: step 398500, loss = 0.14 (1296.5 examples/sec; 0.099 sec/batch)
2017-06-02 12:17:07.332598: step 398510, loss = 0.15 (1639.2 examples/sec; 0.078 sec/batch)
2017-06-02 12:17:08.186920: step 398520, loss = 0.10 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:17:09.062722: step 398530, loss = 0.14 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:17:09.941864: step 398540, loss = 0.14 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:17:10.794782: step 398550, loss = 0.17 (1500.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:17:11.658649: step 398560, loss = 0.16 (1481.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:17:12.507262: step 398570, loss = 0.12 (1508.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:17:13.361641: step 398580, loss = 0.14 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:17:14.222044: step 398590, loss = 0.14 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:17:15.215795: step 398600, loss = 0.17 (1288.1 examples/sec; 0.099 sec/batch)
2017-06-02 12:17:15.981553: step 398610, loss = 0.13 (1671.6 examples/sec; 0.077 sec/batch)
2017-06-02 12:17:16.836621: step 398620, loss = 0.14 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:17:17.696744: step 398630, loss = 0.13 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:17:18.584011: step 398640, loss = 0.14 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 12:17:19.466829: step 398650, loss = 0.14 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:17:20.330057: step 398660, loss = 0.15 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:17:21.183268: step 398670, loss = 0.14 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:17:22.068474: step 398680, loss = 0.19 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:17:22.935024: step 398690, loss = 0.23 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:17:23.929860: step 398700, loss = 0.15 (1286.6 examples/sec; 0.099 sec/batch)
2017-06-02 12:17:24.667979: step 398710, loss = 0.12 (1734.2 examples/sec; 0.074 sec/batch)
2017-06-02 12:17:25.531575: step 398720, loss = 0.15 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:17:26.391308: step 398730, loss = 0.19 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:17:27.255823: step 398740, loss = 0.15 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:17:28.115076: step 398750, loss = 0.17 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:17:28.974800: step 398760, loss = 0.18 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:17:29.854767: step 398770, loss = 0.10 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:17:30.739612: step 398780, loss = 0.13 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:17:31.624832: step 398790, loss = 0.12 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:17:32.610094: step 398800, loss = 0.13 (1299.1 examples/sec; 0.099 sec/batch)
2017-06-02 12:17:33.360313: step 398810, loss = 0.13 (1706.2 examples/sec; 0.075 sec/batch)
2017-06-02 12:17:34.232795: step 398820, loss = 0.13 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:17:35.116019: step 398830, loss = 0.14 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:17:36.012003: step 398840, loss = 0.13 (1428.6 examples/sec; 0.090 sec/batch)
2017-06-02 12:17:36.905765: step 398850, loss = 0.16 (1432.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:17:37.809472: step 398860, loss = 0.14 (1416.4 examples/sec; 0.090 sec/batch)
2017-06-02 12:17:38.658328: step 398870, loss = 0.15 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:17:39.557099: step 398880, loss = 0.16 (1424.2 examples/sec; 0.090 sec/batch)
2017-06-02 12:17:40.439028: step 398890, loss = 0.14 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:17:41.449885: step 398900, loss = 0.11 (1266.2 examples/sec; 0.101 sec/batch)
2017-06-02 12:17:42.215520: step 398910, loss = 0.15 (1671.8 examples/sec; 0.077 sec/batch)
2017-06-02 12:17:43.108434: step 398920, loss = 0.14 (1433.5 examples/sec; 0.089 sec/batch)
2017-06-02 12:17:43.973252: step 398930, loss = 0.13 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:17:44.855920: step 398940, loss = 0.14 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:17:45.744020: step 398950, loss = 0.13 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:17:46.618745: step 398960, loss = 0.16 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:17:47.504663: step 398970, loss = 0.13 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:17:48.368710: step 398980, loss = 0.12 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:17:49.252940: step 398990, loss = 0.12 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:17:50.241038: step 399000, loss = 0.18 (1295.4 examples/sec; 0.099 sec/batch)
2017-06-02 12:17:51.010432: step 399010, loss = 0.13 (1663.6 examples/sec; 0.077 sec/batch)
2017-06-02 12:17:51.888227: step 399020, loss = 0.14 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:17:52.761655: step 399030, loss = 0.15 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:17:53.630362: step 399040, loss = 0.13 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:17:54.497519: step 399050, loss = 0.17 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:17:55.387701: step 399060, loss = 0.14 (1437.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:17:56.272435: step 399070, loss = 0.19 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:17:57.149032: step 399080, loss = 0.14 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:17:58.011494: step 399090, loss = 0.19 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:17:59.016370: step 399100, loss = 0.18 (1273.8 examples/sec; 0.100 sec/batch)
2017-06-02 12:17:59.736978: step 399110, loss = 0.16 (1776.3 examples/sec; 0.072 sec/batch)
2017-06-02 12:18:00.614745: step 399120, loss = 0.14 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:18:01.468987: step 399130, loss = 0.11 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:18:02.355429: step 399140, loss = 0.17 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:18:03.233910: step 399150, loss = 0.16 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:18:04.100619: step 399160, loss = 0.12 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:18:04.994781: step 399170, loss = 0.13 (1431.5 examples/sec; 0.089 sec/batch)
2017-06-02 12:18:05.871296: step 399180, loss = 0.13 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:18:06.738589: step 399190, loss = 0.14 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:18:07.724089: step 399200, loss = 0.13 (1298.8 examples/sec; 0.099 sec/batch)
2017-06-02 12:18:08.519517: step 399210, loss = 0.17 (1609.2 examples/sec; 0.080 sec/batch)
2017-06-02 12:18:09.399372: step 399220, loss = 0.15 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:18:10.290447: step 399230, loss = 0.12 (1436.5 examples/sec; 0.089 sec/batch)
2017-06-02 12:18:11.167312: step 399240, loss = 0.15 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:18:12.034720: step 399250, loss = 0.18 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:18:12.919189: step 399260, loss = 0.14 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:18:13.825522: step 399270, loss = 0.12 (1412.3 examples/sec; 0.091 sec/batch)
2017-06-02 12:18:14.708641: step 399280, loss = 0.15 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:18:15.586104: step 399290, loss = 0.13 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:18:16.557015: step 399300, loss = 0.16 (1318.3 examples/sec; 0.097 sec/batch)
2017-06-02 12:18:17.322117: step 399310, loss = 0.12 (1673.0 examples/sec; 0.077 sec/batch)
2017-06-02 12:18:18.202209: step 399320, loss = 0.16 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:18:19.079424: step 399330, loss = 0.18 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:18:19.942058: step 399340, loss = 0.15 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:18:20.824839: step 399350, loss = 0.13 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:18:21.707334: step 399360, loss = 0.13 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:18:22.571165: step 399370, loss = 0.16 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:18:23.454038: step 399380, loss = 0.12 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:18:24.306866: step 399390, loss = 0.11 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:18:25.256341: step 399400, loss = 0.12 (1348.1 examples/sec; 0.095 sec/batch)
2017-06-02 12:18:26.034855: step 399410, loss = 0.17 (1644.2 examples/sec; 0.078 sec/batch)
2017-06-02 12:18:26.916760: step 399420, loss = 0.15 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:18:27.767696: step 399430, loss = 0.11 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:18:28.634670: step 399440, loss = 0.15 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:18:29.521918: step 399450, loss = 0.17 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:18:30.375211: step 399460, loss = 0.12 (1500.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:18:31.241750: step 399470, loss = 0.14 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:18:32.108507: step 399480, loss = 0.15 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:18:32.985229: step 399490, loss = 0.13 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:18:33.950431: step 399500, loss = 0.14 (1326.1 examples/sec; 0.097 sec/batch)
2017-06-02 12:18:34.707774: step 399510, loss = 0.13 (1690.1 examples/sec; 0.076 sec/batch)
2017-06-02 12:18:35.600318: step 399520, loss = 0.15 (1434.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:18:36.471355: step 399530, loss = 0.19 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:18:37.355600: step 399540, loss = 0.13 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:18:38.220837: step 399550, loss = 0.11 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:18:39.110906: step 399560, loss = 0.11 (1438.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:18:39.996505: step 399570, loss = 0.16 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:18:40.891612: step 399580, loss = 0.13 (1430.0 examples/sec; 0.090 sec/batch)
2017-06-02 12:18:41.789057: step 399590, loss = 0.15 (1426.3 examples/sec; 0.090 sec/batch)
2017-06-02 12:18:42.822050: step 399600, loss = 0.20 (1239.1 examples/sec; 0.103 sec/batch)
2017-06-02 12:18:43.554553: step 399610, loss = 0.19 (1747.4 examples/sec; 0.073 sec/batch)
2017-06-02 12:18:44.405861: step 399620, loss = 0.20 (1503.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:18:45.257995: step 399630, loss = 0.17 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:18:46.136981: step 399640, loss = 0.15 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:18:47.015679: step 399650, loss = 0.16 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:18:47.832665: step 399660, loss = 0.12 (1566.7 examples/sec; 0.082 sec/batch)
2017-06-02 12:18:48.708952: step 399670, loss = 0.14 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:18:49.552980: step 399680, loss = 0.12 (1516.6 examples/sec; 0.084 sec/batch)
2017-06-02 12:18:50.413468: step 399690, loss = 0.11 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:18:51.393471: step 399700, loss = 0.16 (1306.1 examples/sec; 0.098 sec/batch)
2017-06-02 12:18:52.200904: step 399710, loss = 0.17 (1585.3 examples/sec; 0.081 sec/batch)
2017-06-02 12:18:53.092760: step 399720, loss = 0.15 (1435.2 examples/sec; 0.089 sec/batch)
2017-06-02 12:18:53.973438: step 399730, loss = 0.18 (1453.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:18:54.871807: step 399740, loss = 0.19 (1424.8 examples/sec; 0.090 sec/batch)
2017-06-02 12:18:55.743951: step 399750, loss = 0.13 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:18:56.596297: step 399760, loss = 0.20 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:18:57.479930: step 399770, loss = 0.14 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:18:58.341422: step 399780, loss = 0.18 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:18:59.212594: step 399790, loss = 0.17 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:19:00.199300: step 399800, loss = 0.18 (1297.2 examples/sec; 0.099 sec/batch)
2017-06-02 12:19:00.982241: step 399810, loss = 0.13 (1634.8 examples/sec; 0.078 sec/batch)
2017-06-02 12:19:01.840492: step 399820, loss = 0.16 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:19:02.740547: step 399830, loss = 0.15 (1422.1 examples/sec; 0.090 sec/batch)
2017-06-02 12:19:03.614189: step 399840, loss = 0.13 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:19:04.476663: step 399850, loss = 0.14 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:19:05.334868: step 399860, loss = 0.14 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:19:06.167074: step 399870, loss = 0.16 (1538.1 examples/sec; 0.083 sec/batch)
2017-06-02 12:19:07.045799: step 399880, loss = 0.13 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:19:07.924007: step 399890, loss = 0.15 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:19:08.866926: step 399900, loss = 0.15 (1357.5 examples/sec; 0.094 sec/batch)
2017-06-02 12:19:09.626677: step 399910, loss = 0.18 (1684.8 examples/sec; 0.076 sec/batch)
2017-06-02 12:19:10.484143: step 399920, loss = 0.13 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:19:11.368978: step 399930, loss = 0.17 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:19:12.252206: step 399940, loss = 0.17 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:19:13.142430: step 399950, loss = 0.15 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:19:14.011041: step 399960, loss = 0.16 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:19:14.890329: step 399970, loss = 0.15 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:19:15.767638: step 399980, loss = 0.14 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:19:16.638568: step 399990, loss = 0.17 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:19:17.636290: step 400000, loss = 0.14 (1282.9 examples/sec; 0.100 sec/batch)
2017-06-02 12:19:18.353261: step 400010, loss = 0.12 (1785.3 examples/sec; 0.072 sec/batch)
2017-06-02 12:19:19.253635: step 400020, loss = 0.15 (1421.6 examples/sec; 0.090 sec/batch)
2017-06-02 12:19:20.132044: step 400030, loss = 0.12 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:19:21.023572: step 400040, loss = 0.15 (1435.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:19:21.853230: step 400050, loss = 0.11 (1542.8 examples/sec; 0.083 sec/batch)
2017-06-02 12:19:22.706773: step 400060, loss = 0.15 (1499.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:19:23.558320: step 400070, loss = 0.18 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:19:24.426394: step 400080, loss = 0.15 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:19:25.323134: step 400090, loss = 0.13 (1427.4 examples/sec; 0.090 sec/batch)
2017-06-02 12:19:26.308260: step 400100, loss = 0.16 (1299.3 examples/sec; 0.099 sec/batch)
2017-06-02 12:19:27.075948: step 400110, loss = 0.14 (1667.4 examples/sec; 0.077 sec/batch)
2017-06-02 12:19:27.953376: step 400120, loss = 0.15 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:19:28.830789: step 400130, loss = 0.13 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:19:29.712439: step 400140, loss = 0.10 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:19:30.580467: step 400150, loss = 0.18 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:19:31.467546: step 400160, loss = 0.12 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:19:32.341220: step 400170, loss = 0.15 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:19:33.201100: step 400180, loss = 0.13 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:19:34.061004: step 400190, loss = 0.16 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:19:35.103018: step 400200, loss = 0.19 (1228.4 examples/sec; 0.104 sec/batch)
2017-06-02 12:19:35.827445: step 400210, loss = 0.14 (1766.9 examples/sec; 0.072 sec/batch)
2017-06-02 12:19:36.704648: step 400220, loss = 0.15 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:19:37.572603: step 400230, loss = 0.14 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:19:38.450977: step 400240, loss = 0.18 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:19:39.350488: step 400250, loss = 0.14 (1423.0 examples/sec; 0.090 sec/batch)
2017-06-02 12:19:40.229331: step 400260, loss = 0.14 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:19:41.105348: step 400270, loss = 0.16 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:19:41.990476: step 400280, loss = 0.13 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:19:42.881189: step 400290, loss = 0.15 (1437.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:19:43.908699: step 400300, loss = 0.20 (1245.7 examples/sec; 0.103 sec/batch)
2017-06-02 12:19:44.663667: step 400310, loss = 0.20 (1695.5 examples/sec; 0.075 sec/batch)
2017-06-02 12:19:45.553655: step 400320, loss = 0.10 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 12:19:46.433112: step 400330, loss = 0.13 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:19:47.317277: step 400340, loss = 0.11 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:19:48.197185: step 400350, loss = 0.19 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:19:49.043486: step 400360, loss = 0.13 (1512.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:19:49.880145: step 400370, loss = 0.14 (1529.9 examples/sec; 0.084 sec/batch)
2017-06-02 12:19:50.737197: step 400380, loss = 0.15 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:19:51.624307: step 400390, loss = 0.18 (1442.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:19:52.631620: step 400400, loss = 0.14 (1270.7 examples/sec; 0.101 sec/batch)
2017-06-02 12:19:53.496435: step 400410, loss = 0.19 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:19:54.374606: step 400420, loss = 0.15 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:19:55.244607: step 400430, loss = 0.14 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:19:56.131273: step 400440, loss = 0.15 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 12:19:56.993484: step 400450, loss = 0.12 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:19:57.847414: step 400460, loss = 0.19 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:19:58.716585: step 400470, loss = 0.16 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:19:59.596213: step 400480, loss = 0.15 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:20:00.478832: step 400490, loss = 0.16 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:20:01.521907: step 400500, loss = 0.16 (1227.1 examples/sec; 0.104 sec/batch)
2017-06-02 12:20:02.216923: step 400510, loss = 0.13 (1841.7 examples/sec; 0.070 sec/batch)
2017-06-02 12:20:03.073292: step 400520, loss = 0.12 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:20:03.942814: step 400530, loss = 0.13 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:20:04.819153: step 400540, loss = 0.18 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:20:05.669692: step 400550, loss = 0.15 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:20:06.544186: step 400560, loss = 0.15 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:20:07.416024: step 400570, loss = 0.14 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:20:08.274317: step 400580, loss = 0.13 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:20:09.127521: step 400590, loss = 0.11 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:20:10.091778: step 400600, loss = 0.11 (1327.4 examples/sec; 0.096 sec/batch)
2017-06-02 12:20:10.876859: step 400610, loss = 0.16 (1630.4 examples/sec; 0.079 sec/batch)
2017-06-02 12:20:11.747892: step 400620, loss = 0.19 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:20:12.594084: step 400630, loss = 0.12 (1512.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:20:13.459980: step 400640, loss = 0.15 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:20:14.314652: step 400650, loss = 0.12 (1497.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:20:15.188829: step 400660, loss = 0.14 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:20:16.035554: step 400670, loss = 0.13 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:20:16.895376: step 400680, loss = 0.15 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:20:17.772918: step 400690, loss = 0.12 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:20:18.755968: step 400700, loss = 0.18 (1302.1 examples/sec; 0.098 sec/batch)
2017-06-02 12:20:19.519536: step 400710, loss = 0.14 (1676.3 examples/sec; 0.076 sec/batch)
2017-06-02 12:20:20.394101: step 400720, loss = 0.16 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:20:21.253395: step 400730, loss = 0.12 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:20:22.085272: step 400740, loss = 0.18 (1538.7 examples/sec; 0.083 sec/batch)
2017-06-02 12:20:22.946414: step 400750, loss = 0.13 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:20:23.848296: step 400760, loss = 0.12 (1419.3 examples/sec; 0.090 sec/batch)
2017-06-02 12:20:24.719458: step 400770, loss = 0.14 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:20:25.621442: step 400780, loss = 0.15 (1419.1 examples/sec; 0.090 sec/batch)
2017-06-02 12:20:26.498402: step 400790, loss = 0.14 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:20:27.428678: step 400800, loss = 0.14 (1375.9 examples/sec; 0.093 sec/batch)
2017-06-02 12:20:28.203372: step 400810, loss = 0.16 (1652.3 examples/sec; 0.077 sec/batch)
2017-06-02 12:20:29.049863: step 400820, loss = 0.13 (1512.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:20:29.920068: step 400830, loss = 0.14 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:20:30.817125: step 400840, loss = 0.15 (1426.9 examples/sec; 0.090 sec/batch)
2017-06-02 12:20:31.688829: step 400850, loss = 0.15 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:20:32.540667: step 400860, loss = 0.15 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:20:33.400321: step 400870, loss = 0.13 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:20:34.274247: step 400880, loss = 0.13 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:20:35.137917: step 400890, loss = 0.16 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:20:36.122708: step 400900, loss = 0.18 (1299.7 examples/sec; 0.098 sec/batch)
2017-06-02 12:20:36.899559: step 400910, loss = 0.12 (1647.7 examples/sec; 0.078 sec/batch)
2017-06-02 12:20:37.766003: step 400920, loss = 0.17 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:20:38.636889: step 400930, loss = 0.12 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:20:39.513167: step 400940, loss = 0.16 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:20:40.372958: step 400950, loss = 0.13 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:20:41.269370: step 400960, loss = 0.14 (1427.9 examples/sec; 0.090 sec/batch)
2017-06-02 12:20:42.138824: step 400970, loss = 0.13 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:20:43.024357: step 400980, loss = 0.13 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 12:20:43.909066: step 400990, loss = 0.13 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:20:44.877595: step 401000, loss = 0.16 (1321.6 examples/sec; 0.097 sec/batch)
2017-06-02 12:20:45.642891: step 401010, loss = 0.12 (1672.6 examples/sec; 0.077 sec/batch)
2017-06-02 12:20:46.506033: step 401020, loss = 0.19 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:20:47.383750: step 401030, loss = 0.11 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:20:48.267146: step 401040, loss = 0.14 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:20:49.137126: step 401050, loss = 0.18 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:20:50.019110: step 401060, loss = 0.17 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:20:50.888956: step 401070, loss = 0.12 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:20:51.767031: step 401080, loss = 0.15 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:20:52.631880: step 401090, loss = 0.12 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:20:53.591675: step 401100, loss = 0.19 (1333.6 examples/sec; 0.096 sec/batch)
2017-06-02 12:20:54.357275: step 401110, loss = 0.17 (1671.9 examples/sec; 0.077 sec/batch)
2017-06-02 12:20:55.254344: step 401120, loss = 0.15 (1426.9 examples/sec; 0.090 sec/batch)
2017-06-02 12:20:56.124076: step 401130, loss = 0.17 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:20:57.016438: step 401140, loss = 0.16 (1434.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:20:57.880610: step 401150, loss = 0.12 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:20:58.736982: step 401160, loss = 0.13 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:20:59.597108: step 401170, loss = 0.17 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:21:00.457260: step 401180, loss = 0.11 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:21:01.290961: step 401190, loss = 0.19 (1535.3 examples/sec; 0.083 sec/batch)
2017-06-02 12:21:02.224154: step 401200, loss = 0.14 (1371.6 examples/sec; 0.093 sec/batch)
2017-06-02 12:21:03.004446: step 401210, loss = 0.19 (1640.4 examples/sec; 0.078 sec/batch)
2017-06-02 12:21:03.904552: step 401220, loss = 0.12 (1422.0 examples/sec; 0.090 sec/batch)
2017-06-02 12:21:04.790756: step 401230, loss = 0.15 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:21:05.668390: step 401240, loss = 0.14 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:21:06.560331: step 401250, loss = 0.16 (1435.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:21:07.409193: step 401260, loss = 0.17 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:21:08.309247: step 401270, loss = 0.13 (1422.1 examples/sec; 0.090 sec/batch)
2017-06-02 12:21:09.188722: step 401280, loss = 0.16 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:21:10.066316: step 401290, loss = 0.16 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:21:11.045162: step 401300, loss = 0.17 (1307.7 examples/sec; 0.098 sec/batch)
2017-06-02 12:21:11.821627: step 401310, loss = 0.13 (1648.5 examples/sec; 0.078 sec/batch)
2017-06-02 12:21:12.695576: step 401320, loss = 0.15 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:21:13.547915: step 401330, loss = 0.16 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:21:14.415146: step 401340, loss = 0.16 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:21:15.288050: step 401350, loss = 0.12 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:21:16.143379: step 401360, loss = 0.15 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:21:17.021698: step 401370, loss = 0.14 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:21:17.908831: step 401380, loss = 0.11 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:21:18.768054: step 401390, loss = 0.18 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:21:19.751040: step 401400, loss = 0.14 (1302.2 examples/sec; 0.098 sec/batch)
2017-06-02 12:21:20.510085: step 401410, loss = 0.11 (1686.3 examples/sec; 0.076 sec/batch)
2017-06-02 12:21:21.381499: step 401420, loss = 0.11 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:21:22.288392: step 401430, loss = 0.14 (1411.4 examples/sec; 0.091 sec/batch)
2017-06-02 12:21:23.144505: step 401440, loss = 0.12 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:21:24.037942: step 401450, loss = 0.13 (1432.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:21:24.896888: step 401460, loss = 0.16 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:21:25.752752: step 401470, loss = 0.14 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:21:26.602787: step 401480, loss = 0.15 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 12:21:27.461803: step 401490, loss = 0.16 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:21:28.437451: step 401500, loss = 0.12 (1311.9 examples/sec; 0.098 sec/batch)
2017-06-02 12:21:29.213082: step 401510, loss = 0.16 (1650.3 examples/sec; 0.078 sec/batch)
2017-06-02 12:21:30.094629: step 401520, loss = 0.14 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:21:30.955916: step 401530, loss = 0.12 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:21:31.830829: step 401540, loss = 0.16 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:21:32.734307: step 401550, loss = 0.15 (1416.7 examples/sec; 0.090 sec/batch)
2017-06-02 12:21:33.636803: step 401560, loss = 0.16 (1418.3 examples/sec; 0.090 sec/batch)
2017-06-02 12:21:34.516113: step 401570, loss = 0.11 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:21:35.409332: step 401580, loss = 0.17 (1433.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:21:36.287955: step 401590, loss = 0.20 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:21:37.272206: step 401600, loss = 0.12 (1300.5 examples/sec; 0.098 sec/batch)
2017-06-02 12:21:38.046006: step 401610, loss = 0.13 (1654.2 examples/sec; 0.077 sec/batch)
2017-06-02 12:21:38.934014: step 401620, loss = 0.14 (1441.5 examples/sec; 0.089 sec/batch)
2017-06-02 12:21:39.821641: step 401630, loss = 0.12 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:21:40.707489: step 401640, loss = 0.16 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:21:41.558033: step 401650, loss = 0.14 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:21:42.427896: step 401660, loss = 0.15 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:21:43.302640: step 401670, loss = 0.14 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:21:44.174392: step 401680, loss = 0.11 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:21:45.034810: step 401690, loss = 0.12 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:21:46.016630: step 401700, loss = 0.15 (1303.7 examples/sec; 0.098 sec/batch)
2017-06-02 12:21:46.763270: step 401710, loss = 0.14 (1714.3 examples/sec; 0.075 sec/batch)
2017-06-02 12:21:47.611673: step 401720, loss = 0.16 (1508.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:21:48.476513: step 401730, loss = 0.16 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:21:49.316785: step 401740, loss = 0.15 (1523.3 examples/sec; 0.084 sec/batch)
2017-06-02 12:21:50.217237: step 401750, loss = 0.12 (1421.5 examples/sec; 0.090 sec/batch)
2017-06-02 12:21:51.119351: step 401760, loss = 0.13 (1418.9 examples/sec; 0.090 sec/batch)
2017-06-02 12:21:51.979355: step 401770, loss = 0.15 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:21:52.879283: step 401780, loss = 0.13 (1422.3 examples/sec; 0.090 sec/batch)
2017-06-02 12:21:53.738623: step 401790, loss = 0.13 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:21:54.735793: step 401800, loss = 0.14 (1283.6 examples/sec; 0.100 sec/batch)
2017-06-02 12:21:55.479004: step 401810, loss = 0.13 (1722.3 examples/sec; 0.074 sec/batch)
2017-06-02 12:21:56.339104: step 401820, loss = 0.13 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:21:57.221261: step 401830, loss = 0.13 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:21:58.082150: step 401840, loss = 0.11 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:21:58.928687: step 401850, loss = 0.15 (1512.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:21:59.791964: step 401860, loss = 0.13 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:22:00.684154: step 401870, loss = 0.12 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:22:01.555467: step 401880, loss = 0.17 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:22:02.428163: step 401890, loss = 0.15 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:22:03.432936: step 401900, loss = 0.12 (1273.9 examples/sec; 0.100 sec/batch)
2017-06-02 12:22:04.191185: step 401910, loss = 0.12 (1688.1 examples/sec; 0.076 sec/batch)
2017-06-02 12:22:05.074518: step 401920, loss = 0.20 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:22:05.961073: step 401930, loss = 0.14 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:22:06.859500: step 401940, loss = 0.15 (1424.7 examples/sec; 0.090 sec/batch)
2017-06-02 12:22:07.727158: step 401950, loss = 0.25 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:22:08.573510: step 401960, loss = 0.14 (1512.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:22:09.444130: step 401970, loss = 0.16 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:22:10.302380: step 401980, loss = 0.11 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:22:11.150145: step 401990, loss = 0.14 (1509.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:22:12.151317: step 402000, loss = 0.12 (1278.5 examples/sec; 0.100 sec/batch)
2017-06-02 12:22:12.885294: step 402010, loss = 0.20 (1744.0 examples/sec; 0.073 sec/batch)
2017-06-02 12:22:13.750717: step 402020, loss = 0.12 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:22:14.599490: step 402030, loss = 0.18 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:22:15.474329: step 402040, loss = 0.14 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:22:16.340758: step 402050, loss = 0.11 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:22:17.192908: step 402060, loss = 0.13 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:22:18.054913: step 402070, loss = 0.15 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:22:18.901668: step 402080, loss = 0.16 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:22:19.750066: step 402090, loss = 0.13 (1508.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:22:20.700898: step 402100, loss = 0.13 (1346.2 examples/sec; 0.095 sec/batch)
2017-06-02 12:22:21.468576: step 402110, loss = 0.13 (1667.4 examples/sec; 0.077 sec/batch)
2017-06-02 12:22:22.342233: step 402120, loss = 0.16 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:22:23.208663: step 402130, loss = 0.12 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:22:24.060399: step 402140, loss = 0.16 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 12:22:24.928128: step 402150, loss = 0.15 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:22:25.794441: step 402160, loss = 0.13 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:22:26.669696: step 402170, loss = 0.13 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:22:27.542510: step 402180, loss = 0.13 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:22:28.434288: step 402190, loss = 0.21 (1435.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:22:29.407415: step 402200, loss = 0.15 (1315.3 examples/sec; 0.097 sec/batch)
2017-06-02 12:22:30.183615: step 402210, loss = 0.13 (1649.1 examples/sec; 0.078 sec/batch)
2017-06-02 12:22:31.048763: step 402220, loss = 0.15 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:22:31.910476: step 402230, loss = 0.14 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:22:32.800844: step 402240, loss = 0.15 (1437.6 examples/sec; 0.089 sec/batch)
2017-06-02 12:22:33.671601: step 402250, loss = 0.13 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:22:34.505677: step 402260, loss = 0.13 (1534.6 examples/sec; 0.083 sec/batch)
2017-06-02 12:22:35.362307: step 402270, loss = 0.17 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:22:36.220472: step 402280, loss = 0.21 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:22:37.103863: step 402290, loss = 0.15 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:22:38.084000: step 402300, loss = 0.18 (1305.9 examples/sec; 0.098 sec/batch)
2017-06-02 12:22:38.834543: step 402310, loss = 0.17 (1705.4 examples/sec; 0.075 sec/batch)
2017-06-02 12:22:39.716485: step 402320, loss = 0.13 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:22:40.583138: step 402330, loss = 0.13 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:22:41.434953: step 402340, loss = 0.11 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:22:42.299218: step 402350, loss = 0.17 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:22:43.162331: step 402360, loss = 0.17 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:22:44.051234: step 402370, loss = 0.17 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:22:44.939684: step 402380, loss = 0.12 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:22:45.821784: step 402390, loss = 0.13 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:22:46.793457: step 402400, loss = 0.12 (1317.3 examples/sec; 0.097 sec/batch)
2017-06-02 12:22:47.566055: step 402410, loss = 0.15 (1656.8 examples/sec; 0.077 sec/batch)
2017-06-02 12:22:48.446006: step 402420, loss = 0.14 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:22:49.299775: step 402430, loss = 0.13 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:22:50.133870: step 402440, loss = 0.16 (1534.6 examples/sec; 0.083 sec/batch)
2017-06-02 12:22:50.998567: step 402450, loss = 0.14 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:22:51.856278: step 402460, loss = 0.14 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:22:52.718615: step 402470, loss = 0.16 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:22:53.593591: step 402480, loss = 0.12 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:22:54.482598: step 402490, loss = 0.15 (1439.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:22:55.460331: step 402500, loss = 0.16 (1309.1 examples/sec; 0.098 sec/batch)
2017-06-02 12:22:56.260147: step 402510, loss = 0.14 (1600.4 examples/sec; 0.080 sec/batch)
2017-06-02 12:22:57.175907: step 402520, loss = 0.17 (1397.8 examples/sec; 0.092 sec/batch)
2017-06-02 12:22:58.056371: step 402530, loss = 0.15 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:22:58.938488: step 402540, loss = 0.13 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:22:59.816907: step 402550, loss = 0.14 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:23:00.696932: step 402560, loss = 0.18 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:23:01.573821: step 402570, loss = 0.15 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:23:02.451770: step 402580, loss = 0.14 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:23:03.335002: step 402590, loss = 0.19 (1449.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:23:04.334976: step 402600, loss = 0.11 (1280.0 examples/sec; 0.100 sec/batch)
2017-06-02 12:23:05.101813: step 402610, loss = 0.13 (1669.2 examples/sec; 0.077 sec/batch)
2017-06-02 12:23:05.980442: step 402620, loss = 0.14 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:23:06.864232: step 402630, loss = 0.14 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:23:07.756595: step 402640, loss = 0.17 (1434.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:23:08.640611: step 402650, loss = 0.12 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:23:09.513195: step 402660, loss = 0.13 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:23:10.414290: step 402670, loss = 0.12 (1420.5 examples/sec; 0.090 sec/batch)
2017-06-02 12:23:11.262020: step 402680, loss = 0.13 (1509.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:23:12.153837: step 402690, loss = 0.17 (1435.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:23:13.112283: step 402700, loss = 0.14 (1335.5 examples/sec; 0.096 sec/batch)
2017-06-02 12:23:13.878208: step 402710, loss = 0.12 (1671.2 examples/sec; 0.077 sec/batch)
2017-06-02 12:23:14.737916: step 402720, loss = 0.17 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:23:15.612889: step 402730, loss = 0.14 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:23:16.473150: step 402740, loss = 0.20 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:23:17.350700: step 402750, loss = 0.12 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:23:18.235555: step 402760, loss = 0.12 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:23:19.085710: step 402770, loss = 0.13 (1505.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:23:19.968019: step 402780, loss = 0.11 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:23:20.824068: step 402790, loss = 0.15 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:23:21.828937: step 402800, loss = 0.15 (1273.8 examples/sec; 0.100 sec/batch)
2017-06-02 12:23:22.552054: step 402810, loss = 0.21 (1770.1 examples/sec; 0.072 sec/batch)
2017-06-02 12:23:23.434252: step 402820, loss = 0.13 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:23:24.321265: step 402830, loss = 0.13 (1443.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:23:25.226487: step 402840, loss = 0.10 (1414.0 examples/sec; 0.091 sec/batch)
2017-06-02 12:23:26.094448: step 402850, loss = 0.12 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:23:26.965903: step 402860, loss = 0.15 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:23:27.842744: step 402870, loss = 0.15 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:23:28.704655: step 402880, loss = 0.17 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:23:29.555960: step 402890, loss = 0.11 (1503.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:23:30.535139: step 402900, loss = 0.14 (1307.2 examples/sec; 0.098 sec/batch)
2017-06-02 12:23:31.321370: step 402910, loss = 0.17 (1628.0 examples/sec; 0.079 sec/batch)
2017-06-02 12:23:32.177097: step 402920, loss = 0.15 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:23:33.044932: step 402930, loss = 0.18 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:23:33.889880: step 402940, loss = 0.16 (1514.9 examples/sec; 0.084 sec/batch)
2017-06-02 12:23:34.760051: step 402950, loss = 0.11 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:23:35.630153: step 402960, loss = 0.15 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:23:36.478472: step 402970, loss = 0.14 (1508.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:23:37.334282: step 402980, loss = 0.16 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:23:38.215151: step 402990, loss = 0.13 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:23:39.187446: step 403000, loss = 0.13 (1316.5 examples/sec; 0.097 sec/batch)
2017-06-02 12:23:39.950074: step 403010, loss = 0.16 (1678.4 examples/sec; 0.076 sec/batch)
2017-06-02 12:23:40.821458: step 403020, loss = 0.13 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:23:41.704766: step 403030, loss = 0.13 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:23:42.547774: step 403040, loss = 0.16 (1518.4 examples/sec; 0.084 sec/batch)
2017-06-02 12:23:43.407847: step 403050, loss = 0.14 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:23:44.268128: step 403060, loss = 0.17 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:23:45.139848: step 403070, loss = 0.17 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:23:46.029963: step 403080, loss = 0.14 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:23:46.872108: step 403090, loss = 0.15 (1519.9 examples/sec; 0.084 sec/batch)
2017-06-02 12:23:47.806892: step 403100, loss = 0.15 (1369.3 examples/sec; 0.093 sec/batch)
2017-06-02 12:23:48.592572: step 403110, loss = 0.16 (1629.2 examples/sec; 0.079 sec/batch)
2017-06-02 12:23:49.466130: step 403120, loss = 0.14 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:23:50.336761: step 403130, loss = 0.15 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:23:51.209334: step 403140, loss = 0.16 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:23:52.075926: step 403150, loss = 0.12 (1477.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:23:52.945813: step 403160, loss = 0.11 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:23:53.797260: step 403170, loss = 0.14 (1503.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:23:54.643649: step 403180, loss = 0.15 (1512.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:23:55.491728: step 403190, loss = 0.16 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:23:56.484855: step 403200, loss = 0.15 (1288.9 examples/sec; 0.099 sec/batch)
2017-06-02 12:23:57.271831: step 403210, loss = 0.13 (1626.5 examples/sec; 0.079 sec/batch)
2017-06-02 12:23:58.146398: step 403220, loss = 0.12 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:23:59.020243: step 403230, loss = 0.13 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:23:59.886107: step 403240, loss = 0.12 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:24:00.766596: step 403250, loss = 0.16 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:24:01.621797: step 403260, loss = 0.12 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:24:02.484781: step 403270, loss = 0.15 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:24:03.332184: step 403280, loss = 0.14 (1510.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:24:04.204543: step 403290, loss = 0.13 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:24:05.152157: step 403300, loss = 0.11 (1350.8 examples/sec; 0.095 sec/batch)
2017-06-02 12:24:05.919871: step 403310, loss = 0.16 (1667.3 examples/sec; 0.077 sec/batch)
2017-06-02 12:24:06.800383: step 403320, loss = 0.12 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:24:07.665799: step 403330, loss = 0.14 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:24:08.544005: step 403340, loss = 0.19 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:24:09.391852: step 403350, loss = 0.16 (1509.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:24:10.286607: step 403360, loss = 0.13 (1430.5 examples/sec; 0.089 sec/batch)
2017-06-02 12:24:11.165991: step 403370, loss = 0.14 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:24:12.037635: step 403380, loss = 0.18 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:24:12.892549: step 403390, loss = 0.13 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:24:13.857100: step 403400, loss = 0.13 (1327.0 examples/sec; 0.096 sec/batch)
2017-06-02 12:24:14.626847: step 403410, loss = 0.13 (1662.9 examples/sec; 0.077 sec/batch)
2017-06-02 12:24:15.492612: step 403420, loss = 0.12 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:24:16.354110: step 403430, loss = 0.12 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:24:17.223382: step 403440, loss = 0.16 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:24:18.103872: step 403450, loss = 0.14 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:24:18.975965: step 403460, loss = 0.13 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:24:19.844959: step 403470, loss = 0.15 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:24:20.749083: step 403480, loss = 0.15 (1415.7 examples/sec; 0.090 sec/batch)
2017-06-02 12:24:21.669311: step 403490, loss = 0.20 (1391.0 examples/sec; 0.092 sec/batch)
2017-06-02 12:24:22.652019: step 403500, loss = 0.12 (1302.5 examples/sec; 0.098 sec/batch)
2017-06-02 12:24:23.455137: step 403510, loss = 0.14 (1593.8 examples/sec; 0.080 sec/batch)
2017-06-02 12:24:24.326688: step 403520, loss = 0.16 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:24:25.172440: step 403530, loss = 0.14 (1513.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:24:26.061846: step 403540, loss = 0.15 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 12:24:26.931684: step 403550, loss = 0.16 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:24:27.791441: step 403560, loss = 0.11 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:24:28.657541: step 403570, loss = 0.16 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:24:29.539689: step 403580, loss = 0.11 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:24:30.386310: step 403590, loss = 0.17 (1511.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:24:31.366761: step 403600, loss = 0.14 (1305.5 examples/sec; 0.098 sec/batch)
2017-06-02 12:24:32.130067: step 403610, loss = 0.16 (1676.9 examples/sec; 0.076 sec/batch)
2017-06-02 12:24:33.038293: step 403620, loss = 0.14 (1409.3 examples/sec; 0.091 sec/batch)
2017-06-02 12:24:33.926349: step 403630, loss = 0.12 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:24:34.826359: step 403640, loss = 0.15 (1422.2 examples/sec; 0.090 sec/batch)
2017-06-02 12:24:35.729771: step 403650, loss = 0.12 (1416.9 examples/sec; 0.090 sec/batch)
2017-06-02 12:24:36.619815: step 403660, loss = 0.10 (1438.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:24:37.501683: step 403670, loss = 0.14 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:24:38.378067: step 403680, loss = 0.14 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:24:39.234693: step 403690, loss = 0.15 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:24:40.237331: step 403700, loss = 0.13 (1276.6 examples/sec; 0.100 sec/batch)
2017-06-02 12:24:40.979932: step 403710, loss = 0.19 (1723.7 examples/sec; 0.074 sec/batch)
2017-06-02 12:24:41.878887: step 403720, loss = 0.14 (1423.9 examples/sec; 0.090 sec/batch)
2017-06-02 12:24:42.753543: step 403730, loss = 0.13 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:24:43.618757: step 403740, loss = 0.16 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:24:44.483971: step 403750, loss = 0.14 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:24:45.352057: step 403760, loss = 0.18 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:24:46.221635: step 403770, loss = 0.13 (1472.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:24:47.053476: step 403780, loss = 0.13 (1538.8 examples/sec; 0.083 sec/batch)
2017-06-02 12:24:47.904397: step 403790, loss = 0.16 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:24:48.863242: step 403800, loss = 0.17 (1334.9 examples/sec; 0.096 sec/batch)
2017-06-02 12:24:49.647891: step 403810, loss = 0.14 (1631.4 examples/sec; 0.078 sec/batch)
2017-06-02 12:24:50.526469: step 403820, loss = 0.13 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:24:51.400685: step 403830, loss = 0.15 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:24:52.313038: step 403840, loss = 0.18 (1403.0 examples/sec; 0.091 sec/batch)
2017-06-02 12:24:53.162557: step 403850, loss = 0.14 (1506.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:24:54.000299: step 403860, loss = 0.14 (1527.9 examples/sec; 0.084 sec/batch)
2017-06-02 12:24:54.876203: step 403870, loss = 0.13 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:24:55.776168: step 403880, loss = 0.15 (1422.3 examples/sec; 0.090 sec/batch)
2017-06-02 12:24:56.658453: step 403890, loss = 0.15 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:24:57.662214: step 403900, loss = 0.14 (1275.2 examples/sec; 0.100 sec/batch)
2017-06-02 12:24:58.435800: step 403910, loss = 0.13 (1654.6 examples/sec; 0.077 sec/batch)
2017-06-02 12:24:59.305632: step 403920, loss = 0.13 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:25:00.202148: step 403930, loss = 0.16 (1427.7 examples/sec; 0.090 sec/batch)
2017-06-02 12:25:01.088567: step 403940, loss = 0.18 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:25:01.973971: step 403950, loss = 0.14 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:25:02.861202: step 403960, loss = 0.12 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:25:03.748415: step 403970, loss = 0.12 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:25:04.647969: step 403980, loss = 0.13 (1422.9 examples/sec; 0.090 sec/batch)
2017-06-02 12:25:05.511670: step 403990, loss = 0.16 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:25:06.484351: step 404000, loss = 0.15 (1315.9 examples/sec; 0.097 sec/batch)
2017-06-02 12:25:07.259666: step 404010, loss = 0.17 (1650.9 examples/sec; 0.078 sec/batch)
2017-06-02 12:25:08.134084: step 404020, loss = 0.12 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:25:09.008208: step 404030, loss = 0.17 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:25:09.876702: step 404040, loss = 0.14 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:25:10.753664: step 404050, loss = 0.14 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:25:11.626343: step 404060, loss = 0.14 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:25:12.489694: step 404070, loss = 0.15 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:25:13.334800: step 404080, loss = 0.14 (1514.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:25:14.199857: step 404090, loss = 0.14 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:25:15.190526: step 404100, loss = 0.16 (1292.1 examples/sec; 0.099 sec/batch)
2017-06-02 12:25:15.999580: step 404110, loss = 0.11 (1582.1 examples/sec; 0.081 sec/batch)
2017-06-02 12:25:16.900058: step 404120, loss = 0.14 (1421.5 examples/sec; 0.090 sec/batch)
2017-06-02 12:25:17.794064: step 404130, loss = 0.16 (1431.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:25:18.662618: step 404140, loss = 0.16 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:25:19.536742: step 404150, loss = 0.13 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:25:20.415118: step 404160, loss = 0.20 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:25:21.299667: step 404170, loss = 0.13 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:25:22.162206: step 404180, loss = 0.13 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:25:23.060592: step 404190, loss = 0.12 (1424.8 examples/sec; 0.090 sec/batch)
2017-06-02 12:25:24.059032: step 404200, loss = 0.18 (1282.0 examples/sec; 0.100 sec/batch)
2017-06-02 12:25:24.828248: step 404210, loss = 0.13 (1664.1 examples/sec; 0.077 sec/batch)
2017-06-02 12:25:25.714474: step 404220, loss = 0.16 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:25:26.563379: step 404230, loss = 0.18 (1507.8 examples/sec; 0.085 sec/batch)
2017-06-02 12:25:27.484512: step 404240, loss = 0.20 (1389.6 examples/sec; 0.092 sec/batch)
2017-06-02 12:25:28.372538: step 404250, loss = 0.18 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:25:29.254779: step 404260, loss = 0.13 (1450.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:25:30.125921: step 404270, loss = 0.14 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:25:30.994127: step 404280, loss = 0.14 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:25:31.842226: step 404290, loss = 0.13 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:25:32.800069: step 404300, loss = 0.12 (1336.3 examples/sec; 0.096 sec/batch)
2017-06-02 12:25:33.561837: step 404310, loss = 0.15 (1680.3 examples/sec; 0.076 sec/batch)
2017-06-02 12:25:34.417333: step 404320, loss = 0.13 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:25:35.275741: step 404330, loss = 0.12 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:25:36.140238: step 404340, loss = 0.11 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:25:37.010006: step 404350, loss = 0.18 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:25:37.831667: step 404360, loss = 0.13 (1557.8 examples/sec; 0.082 sec/batch)
2017-06-02 12:25:38.712985: step 404370, loss = 0.14 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:25:39.549801: step 404380, loss = 0.21 (1529.6 examples/sec; 0.084 sec/batch)
2017-06-02 12:25:40.421352: step 404390, loss = 0.14 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:25:41.450550: step 404400, loss = 0.13 (1243.7 examples/sec; 0.103 sec/batch)
2017-06-02 12:25:42.164672: step 404410, loss = 0.19 (1792.5 examples/sec; 0.071 sec/batch)
2017-06-02 12:25:43.024697: step 404420, loss = 0.12 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:25:43.903714: step 404430, loss = 0.13 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:25:44.764393: step 404440, loss = 0.14 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:25:45.614072: step 404450, loss = 0.17 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:25:46.503041: step 404460, loss = 0.14 (1439.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:25:47.364938: step 404470, loss = 0.18 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:25:48.235126: step 404480, loss = 0.16 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:25:49.116981: step 404490, loss = 0.11 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:25:50.083024: step 404500, loss = 0.13 (1325.0 examples/sec; 0.097 sec/batch)
2017-06-02 12:25:50.847774: step 404510, loss = 0.14 (1673.7 examples/sec; 0.076 sec/batch)
2017-06-02 12:25:51.710963: step 404520, loss = 0.13 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:25:52.599333: step 404530, loss = 0.13 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:25:53.449358: step 404540, loss = 0.13 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 12:25:54.321420: step 404550, loss = 0.15 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:25:55.204462: step 404560, loss = 0.14 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:25:56.077033: step 404570, loss = 0.17 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:25:56.941001: step 404580, loss = 0.16 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:25:57.829445: step 404590, loss = 0.11 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:25:58.793691: step 404600, loss = 0.14 (1327.5 examples/sec; 0.096 sec/batch)
2017-06-02 12:25:59.574308: step 404610, loss = 0.14 (1639.7 examples/sec; 0.078 sec/batch)
2017-06-02 12:26:00.454901: step 404620, loss = 0.16 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:26:01.333485: step 404630, loss = 0.15 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:26:02.205959: step 404640, loss = 0.13 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:26:03.084996: step 404650, loss = 0.15 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:26:03.955764: step 404660, loss = 0.18 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:26:04.839848: step 404670, loss = 0.16 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:26:05.742852: step 404680, loss = 0.16 (1417.5 examples/sec; 0.090 sec/batch)
2017-06-02 12:26:06.634529: step 404690, loss = 0.17 (1435.5 examples/sec; 0.089 sec/batch)
2017-06-02 12:26:07.597223: step 404700, loss = 0.13 (1329.6 examples/sec; 0.096 sec/batch)
2017-06-02 12:26:08.346000: step 404710, loss = 0.17 (1709.5 examples/sec; 0.075 sec/batch)
2017-06-02 12:26:09.164805: step 404720, loss = 0.15 (1563.3 examples/sec; 0.082 sec/batch)
2017-06-02 12:26:10.035589: step 404730, loss = 0.18 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:26:10.928973: step 404740, loss = 0.15 (1432.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:26:11.801322: step 404750, loss = 0.13 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:26:12.702623: step 404760, loss = 0.11 (1420.2 examples/sec; 0.090 sec/batch)
2017-06-02 12:26:13.604670: step 404770, loss = 0.19 (1419.0 examples/sec; 0.090 sec/batch)
2017-06-02 12:26:14.467275: step 404780, loss = 0.17 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:26:15.348156: step 404790, loss = 0.21 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:26:16.309297: step 404800, loss = 0.14 (1331.8 examples/sec; 0.096 sec/batch)
2017-06-02 12:26:17.100769: step 404810, loss = 0.17 (1617.2 examples/sec; 0.079 sec/batch)
2017-06-02 12:26:17.976431: step 404820, loss = 0.14 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:26:18.808895: step 404830, loss = 0.15 (1537.6 examples/sec; 0.083 sec/batch)
2017-06-02 12:26:19.679486: step 404840, loss = 0.16 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:26:20.551870: step 404850, loss = 0.12 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:26:21.436554: step 404860, loss = 0.15 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:26:22.279063: step 404870, loss = 0.14 (1519.3 examples/sec; 0.084 sec/batch)
2017-06-02 12:26:23.160469: step 404880, loss = 0.15 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:26:24.019775: step 404890, loss = 0.13 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:26:24.970709: step 404900, loss = 0.15 (1346.0 examples/sec; 0.095 sec/batch)
2017-06-02 12:26:25.739715: step 404910, loss = 0.16 (1664.5 examples/sec; 0.077 sec/batch)
2017-06-02 12:26:26.591035: step 404920, loss = 0.15 (1503.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:26:27.481699: step 404930, loss = 0.16 (1437.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:26:28.359486: step 404940, loss = 0.14 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:26:29.231007: step 404950, loss = 0.14 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:26:30.113920: step 404960, loss = 0.13 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:26:31.015369: step 404970, loss = 0.17 (1419.9 examples/sec; 0.090 sec/batch)
2017-06-02 12:26:31.882984: step 404980, loss = 0.11 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:26:32.737609: step 404990, loss = 0.15 (1497.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:26:33.708483: step 405000, loss = 0.18 (1318.4 examples/sec; 0.097 sec/batch)
2017-06-02 12:26:34.475098: step 405010, loss = 0.11 (1669.7 examples/sec; 0.077 sec/batch)
2017-06-02 12:26:35.351610: step 405020, loss = 0.12 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:26:36.228058: step 405030, loss = 0.16 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:26:37.090225: step 405040, loss = 0.12 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:26:37.972203: step 405050, loss = 0.17 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:26:38.840876: step 405060, loss = 0.15 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:26:39.682957: step 405070, loss = 0.13 (1520.0 examples/sec; 0.084 sec/batch)
2017-06-02 12:26:40.540077: step 405080, loss = 0.16 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:26:41.401930: step 405090, loss = 0.13 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:26:42.376229: step 405100, loss = 0.18 (1313.8 examples/sec; 0.097 sec/batch)
2017-06-02 12:26:43.130839: step 405110, loss = 0.13 (1696.3 examples/sec; 0.075 sec/batch)
2017-06-02 12:26:43.968407: step 405120, loss = 0.15 (1528.2 examples/sec; 0.084 sec/batch)
2017-06-02 12:26:44.834604: step 405130, loss = 0.14 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:26:45.731266: step 405140, loss = 0.17 (1427.5 examples/sec; 0.090 sec/batch)
2017-06-02 12:26:46.602604: step 405150, loss = 0.12 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:26:47.487416: step 405160, loss = 0.12 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:26:48.373214: step 405170, loss = 0.12 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:26:49.245405: step 405180, loss = 0.12 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:26:50.109383: step 405190, loss = 0.20 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:26:51.091672: step 405200, loss = 0.16 (1303.1 examples/sec; 0.098 sec/batch)
2017-06-02 12:26:51.875249: step 405210, loss = 0.13 (1633.5 examples/sec; 0.078 sec/batch)
2017-06-02 12:26:52.774797: step 405220, loss = 0.13 (1422.9 examples/sec; 0.090 sec/batch)
2017-06-02 12:26:53.659999: step 405230, loss = 0.16 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:26:54.539810: step 405240, loss = 0.16 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:26:55.417467: step 405250, loss = 0.13 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:26:56.298468: step 405260, loss = 0.17 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:26:57.151858: step 405270, loss = 0.15 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:26:58.020041: step 405280, loss = 0.15 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:26:58.894656: step 405290, loss = 0.12 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:26:59.905039: step 405300, loss = 0.11 (1266.8 examples/sec; 0.101 sec/batch)
2017-06-02 12:27:00.648080: step 405310, loss = 0.12 (1722.6 examples/sec; 0.074 sec/batch)
2017-06-02 12:27:01.514494: step 405320, loss = 0.12 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:27:02.407542: step 405330, loss = 0.14 (1433.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:27:03.314357: step 405340, loss = 0.12 (1411.5 examples/sec; 0.091 sec/batch)
2017-06-02 12:27:04.195103: step 405350, loss = 0.16 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:27:05.079004: step 405360, loss = 0.14 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:27:05.964973: step 405370, loss = 0.13 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:27:06.816571: step 405380, loss = 0.15 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:27:07.695447: step 405390, loss = 0.14 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:27:08.660719: step 405400, loss = 0.18 (1326.1 examples/sec; 0.097 sec/batch)
2017-06-02 12:27:09.456200: step 405410, loss = 0.16 (1609.1 examples/sec; 0.080 sec/batch)
2017-06-02 12:27:10.343489: step 405420, loss = 0.15 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 12:27:11.228712: step 405430, loss = 0.15 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:27:12.090320: step 405440, loss = 0.20 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:27:12.950521: step 405450, loss = 0.14 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:27:13.799464: step 405460, loss = 0.15 (1507.8 examples/sec; 0.085 sec/batch)
2017-06-02 12:27:14.673012: step 405470, loss = 0.14 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:27:15.562015: step 405480, loss = 0.15 (1439.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:27:16.424912: step 405490, loss = 0.15 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:27:17.386490: step 405500, loss = 0.12 (1331.2 examples/sec; 0.096 sec/batch)
2017-06-02 12:27:18.136572: step 405510, loss = 0.15 (1706.5 examples/sec; 0.075 sec/batch)
2017-06-02 12:27:19.018364: step 405520, loss = 0.15 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:27:19.884452: step 405530, loss = 0.14 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:27:20.742507: step 405540, loss = 0.16 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:27:21.585051: step 405550, loss = 0.15 (1519.2 examples/sec; 0.084 sec/batch)
2017-06-02 12:27:22.456761: step 405560, loss = 0.15 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:27:23.312278: step 405570, loss = 0.12 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:27:24.194221: step 405580, loss = 0.12 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:27:25.069073: step 405590, loss = 0.12 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:27:26.040332: step 405600, loss = 0.16 (1317.9 examples/sec; 0.097 sec/batch)
2017-06-02 12:27:26.818929: step 405610, loss = 0.13 (1644.0 examples/sec; 0.078 sec/batch)
2017-06-02 12:27:27.666667: step 405620, loss = 0.12 (1509.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:27:28.556899: step 405630, loss = 0.15 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:27:29.424516: step 405640, loss = 0.12 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:27:30.292007: step 405650, loss = 0.14 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:27:31.177597: step 405660, loss = 0.15 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:27:32.035347: step 405670, loss = 0.13 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:27:32.924447: step 405680, loss = 0.14 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:27:33.809688: step 405690, loss = 0.15 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:27:34.781497: step 405700, loss = 0.13 (1317.1 examples/sec; 0.097 sec/batch)
2017-06-02 12:27:35.550885: step 405710, loss = 0.15 (1663.6 examples/sec; 0.077 sec/batch)
2017-06-02 12:27:36.442004: step 405720, loss = 0.14 (1436.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:27:37.319831: step 405730, loss = 0.14 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:27:38.210458: step 405740, loss = 0.16 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 12:27:39.110211: step 405750, loss = 0.16 (1422.6 examples/sec; 0.090 sec/batch)
2017-06-02 12:27:39.991804: step 405760, loss = 0.12 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:27:40.861940: step 405770, loss = 0.13 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:27:41.751509: step 405780, loss = 0.16 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:27:42.641348: step 405790, loss = 0.16 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:27:43.638647: step 405800, loss = 0.13 (1283.5 examples/sec; 0.100 sec/batch)
2017-06-02 12:27:44.402708: step 405810, loss = 0.13 (1675.3 examples/sec; 0.076 sec/batch)
2017-06-02 12:27:45.299177: step 405820, loss = 0.12 (1427.8 examples/sec; 0.090 sec/batch)
2017-06-02 12:27:46.176331: step 405830, loss = 0.13 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:27:47.014426: step 405840, loss = 0.14 (1527.3 examples/sec; 0.084 sec/batch)
2017-06-02 12:27:47.864807: step 405850, loss = 0.21 (1505.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:27:48.727917: step 405860, loss = 0.12 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:27:49.574789: step 405870, loss = 0.12 (1511.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:27:50.433995: step 405880, loss = 0.15 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:27:51.278818: step 405890, loss = 0.16 (1515.1 examples/sec; 0.084 sec/batch)
2017-06-02 12:27:52.234790: step 405900, loss = 0.15 (1338.9 examples/sec; 0.096 sec/batch)
2017-06-02 12:27:53.014970: step 405910, loss = 0.15 (1640.7 examples/sec; 0.078 sec/batch)
2017-06-02 12:27:53.862026: step 405920, loss = 0.13 (1511.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:27:54.739569: step 405930, loss = 0.12 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:27:55.596365: step 405940, loss = 0.14 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:27:56.480850: step 405950, loss = 0.14 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:27:57.337776: step 405960, loss = 0.17 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:27:58.186520: step 405970, loss = 0.11 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:27:59.034465: step 405980, loss = 0.13 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:27:59.922337: step 405990, loss = 0.12 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 12:28:00.881453: step 406000, loss = 0.15 (1334.6 examples/sec; 0.096 sec/batch)
2017-06-02 12:28:01.621141: step 406010, loss = 0.15 (1730.4 examples/sec; 0.074 sec/batch)
2017-06-02 12:28:02.483460: step 406020, loss = 0.16 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:28:03.369333: step 406030, loss = 0.15 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:28:04.237646: step 406040, loss = 0.12 (1474.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:28:05.108842: step 406050, loss = 0.13 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:28:05.977430: step 406060, loss = 0.15 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:28:06.855027: step 406070, loss = 0.12 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:28:07.727972: step 406080, loss = 0.19 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:28:08.577561: step 406090, loss = 0.13 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:28:09.575856: step 406100, loss = 0.13 (1282.2 examples/sec; 0.100 sec/batch)
2017-06-02 12:28:10.310695: step 406110, loss = 0.14 (1741.9 examples/sec; 0.073 sec/batch)
2017-06-02 12:28:11.183097: step 406120, loss = 0.11 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:28:12.051172: step 406130, loss = 0.17 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:28:12.895282: step 406140, loss = 0.11 (1516.4 examples/sec; 0.084 sec/batch)
2017-06-02 12:28:13.756227: step 406150, loss = 0.16 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:28:14.623851: step 406160, loss = 0.17 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:28:15.497693: step 406170, loss = 0.13 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:28:16.353662: step 406180, loss = 0.17 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:28:17.200103: step 406190, loss = 0.12 (1512.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:28:18.173018: step 406200, loss = 0.12 (1315.6 examples/sec; 0.097 sec/batch)
2017-06-02 12:28:18.945445: step 406210, loss = 0.22 (1657.1 examples/sec; 0.077 sec/batch)
2017-06-02 12:28:19.839647: step 406220, loss = 0.16 (1431.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:28:20.720217: step 406230, loss = 0.11 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:28:21.614767: step 406240, loss = 0.12 (1430.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:28:22.461853: step 406250, loss = 0.16 (1511.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:28:23.339922: step 406260, loss = 0.16 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:28:24.209763: step 406270, loss = 0.14 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:28:25.100202: step 406280, loss = 0.16 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 12:28:25.971870: step 406290, loss = 0.14 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:28:26.959576: step 406300, loss = 0.13 (1295.9 examples/sec; 0.099 sec/batch)
2017-06-02 12:28:27.763265: step 406310, loss = 0.14 (1592.7 examples/sec; 0.080 sec/batch)
2017-06-02 12:28:28.631383: step 406320, loss = 0.13 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:28:29.512048: step 406330, loss = 0.18 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:28:30.411200: step 406340, loss = 0.14 (1423.5 examples/sec; 0.090 sec/batch)
2017-06-02 12:28:31.293634: step 406350, loss = 0.15 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:28:32.185617: step 406360, loss = 0.14 (1435.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:28:33.059891: step 406370, loss = 0.17 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:28:33.944871: step 406380, loss = 0.15 (1446.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:28:34.784863: step 406390, loss = 0.14 (1523.8 examples/sec; 0.084 sec/batch)
2017-06-02 12:28:35.745029: step 406400, loss = 0.11 (1333.1 examples/sec; 0.096 sec/batch)
2017-06-02 12:28:36.500126: step 406410, loss = 0.12 (1695.1 examples/sec; 0.076 sec/batch)
2017-06-02 12:28:37.407979: step 406420, loss = 0.15 (1409.9 examples/sec; 0.091 sec/batch)
2017-06-02 12:28:38.299225: step 406430, loss = 0.12 (1436.2 examples/sec; 0.089 sec/batch)
2017-06-02 12:28:39.193794: step 406440, loss = 0.13 (1430.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:28:40.073927: step 406450, loss = 0.14 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:28:40.946707: step 406460, loss = 0.14 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:28:41.809688: step 406470, loss = 0.15 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:28:42.684085: step 406480, loss = 0.18 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:28:43.525680: step 406490, loss = 0.12 (1520.9 examples/sec; 0.084 sec/batch)
2017-06-02 12:28:44.475754: step 406500, loss = 0.17 (1347.3 examples/sec; 0.095 sec/batch)
2017-06-02 12:28:45.245117: step 406510, loss = 0.15 (1663.7 examples/sec; 0.077 sec/batch)
2017-06-02 12:28:46.093945: step 406520, loss = 0.13 (1508.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:28:46.982006: step 406530, loss = 0.15 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:28:47.842369: step 406540, loss = 0.13 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:28:48.734647: step 406550, loss = 0.11 (1434.5 examples/sec; 0.089 sec/batch)
2017-06-02 12:28:49.553660: step 406560, loss = 0.14 (1562.9 examples/sec; 0.082 sec/batch)
2017-06-02 12:28:50.428827: step 406570, loss = 0.16 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:28:51.300650: step 406580, loss = 0.16 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:28:52.204541: step 406590, loss = 0.16 (1416.1 examples/sec; 0.090 sec/batch)
2017-06-02 12:28:53.184390: step 406600, loss = 0.15 (1306.3 examples/sec; 0.098 sec/batch)
2017-06-02 12:28:53.949476: step 406610, loss = 0.18 (1673.0 examples/sec; 0.077 sec/batch)
2017-06-02 12:28:54.830651: step 406620, loss = 0.16 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:28:55.748025: step 406630, loss = 0.13 (1395.3 examples/sec; 0.092 sec/batch)
2017-06-02 12:28:56.617989: step 406640, loss = 0.12 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:28:57.493673: step 406650, loss = 0.12 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:28:58.384943: step 406660, loss = 0.14 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:28:59.234316: step 406670, loss = 0.13 (1507.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:29:00.113781: step 406680, loss = 0.15 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:29:00.975357: step 406690, loss = 0.14 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:29:01.949185: step 406700, loss = 0.12 (1314.4 examples/sec; 0.097 sec/batch)
2017-06-02 12:29:02.730363: step 406710, loss = 0.16 (1638.6 examples/sec; 0.078 sec/batch)
2017-06-02 12:29:03.619008: step 406720, loss = 0.12 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:29:04.511081: step 406730, loss = 0.15 (1434.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:29:05.400488: step 406740, loss = 0.12 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 12:29:06.284767: step 406750, loss = 0.12 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:29:07.170664: step 406760, loss = 0.11 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:29:08.066397: step 406770, loss = 0.16 (1429.0 examples/sec; 0.090 sec/batch)
2017-06-02 12:29:08.949873: step 406780, loss = 0.14 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:29:09.837470: step 406790, loss = 0.13 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:29:10.809695: step 406800, loss = 0.19 (1316.6 examples/sec; 0.097 sec/batch)
2017-06-02 12:29:11.594298: step 406810, loss = 0.14 (1631.4 examples/sec; 0.078 sec/batch)
2017-06-02 12:29:12.478667: step 406820, loss = 0.15 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:29:13.368343: step 406830, loss = 0.15 (1438.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:29:14.277588: step 406840, loss = 0.13 (1407.8 examples/sec; 0.091 sec/batch)
2017-06-02 12:29:15.138011: step 406850, loss = 0.14 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:29:16.048884: step 406860, loss = 0.12 (1405.3 examples/sec; 0.091 sec/batch)
2017-06-02 12:29:16.939095: step 406870, loss = 0.15 (1437.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:29:17.839581: step 406880, loss = 0.20 (1421.5 examples/sec; 0.090 sec/batch)
2017-06-02 12:29:18.714147: step 406890, loss = 0.11 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:29:19.686311: step 406900, loss = 0.13 (1316.7 examples/sec; 0.097 sec/batch)
2017-06-02 12:29:20.461620: step 406910, loss = 0.11 (1651.0 examples/sec; 0.078 sec/batch)
2017-06-02 12:29:21.323874: step 406920, loss = 0.13 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:29:22.214298: step 406930, loss = 0.16 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 12:29:23.090814: step 406940, loss = 0.14 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:29:23.994330: step 406950, loss = 0.17 (1416.7 examples/sec; 0.090 sec/batch)
2017-06-02 12:29:24.869943: step 406960, loss = 0.13 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:29:25.770096: step 406970, loss = 0.11 (1422.0 examples/sec; 0.090 sec/batch)
2017-06-02 12:29:26.629097: step 406980, loss = 0.17 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:29:27.516758: step 406990, loss = 0.15 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:29:28.486716: step 407000, loss = 0.15 (1319.7 examples/sec; 0.097 sec/batch)
2017-06-02 12:29:29.257032: step 407010, loss = 0.11 (1661.7 examples/sec; 0.077 sec/batch)
2017-06-02 12:29:30.137645: step 407020, loss = 0.12 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:29:31.023381: step 407030, loss = 0.16 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:29:31.907505: step 407040, loss = 0.15 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:29:32.782518: step 407050, loss = 0.18 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:29:33.670414: step 407060, loss = 0.11 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 12:29:34.556361: step 407070, loss = 0.11 (1444.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:29:35.433506: step 407080, loss = 0.17 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:29:36.320710: step 407090, loss = 0.13 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:29:37.273589: step 407100, loss = 0.14 (1343.3 examples/sec; 0.095 sec/batch)
2017-06-02 12:29:38.039012: step 407110, loss = 0.14 (1672.3 examples/sec; 0.077 sec/batch)
2017-06-02 12:29:38.937577: step 407120, loss = 0.13 (1424.5 examples/sec; 0.090 sec/batch)
2017-06-02 12:29:39.827725: step 407130, loss = 0.13 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:29:40.707175: step 407140, loss = 0.15 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:29:41.583688: step 407150, loss = 0.16 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:29:42.465405: step 407160, loss = 0.14 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:29:43.361064: step 407170, loss = 0.18 (1429.1 examples/sec; 0.090 sec/batch)
2017-06-02 12:29:44.195620: step 407180, loss = 0.10 (1533.7 examples/sec; 0.083 sec/batch)
2017-06-02 12:29:45.064033: step 407190, loss = 0.15 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:29:46.065175: step 407200, loss = 0.14 (1278.6 examples/sec; 0.100 sec/batch)
2017-06-02 12:29:46.842235: step 407210, loss = 0.13 (1647.3 examples/sec; 0.078 sec/batch)
2017-06-02 12:29:47.741997: step 407220, loss = 0.10 (1422.6 examples/sec; 0.090 sec/batch)
2017-06-02 12:29:48.616423: step 407230, loss = 0.14 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:29:49.479575: step 407240, loss = 0.17 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:29:50.371374: step 407250, loss = 0.12 (1435.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:29:51.251787: step 407260, loss = 0.15 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:29:52.098495: step 407270, loss = 0.13 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:29:53.135833: step 407280, loss = 0.12 (1233.9 examples/sec; 0.104 sec/batch)
2017-06-02 12:29:53.961407: step 407290, loss = 0.12 (1550.4 examples/sec; 0.083 sec/batch)
2017-06-02 12:29:54.948772: step 407300, loss = 0.12 (1296.4 examples/sec; 0.099 sec/batch)
2017-06-02 12:29:55.670405: step 407310, loss = 0.14 (1773.8 examples/sec; 0.072 sec/batch)
2017-06-02 12:29:56.545563: step 407320, loss = 0.18 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:29:57.420054: step 407330, loss = 0.13 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:29:58.285240: step 407340, loss = 0.16 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:29:59.156135: step 407350, loss = 0.15 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:30:00.038467: step 407360, loss = 0.16 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:30:00.905979: step 407370, loss = 0.19 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:30:01.781362: step 407380, loss = 0.13 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:30:02.662184: step 407390, loss = 0.16 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:30:03.629221: step 407400, loss = 0.19 (1323.6 examples/sec; 0.097 sec/batch)
2017-06-02 12:30:04.400593: step 407410, loss = 0.13 (1659.4 examples/sec; 0.077 sec/batch)
2017-06-02 12:30:05.295881: step 407420, loss = 0.12 (1429.7 examples/sec; 0.090 sec/batch)
2017-06-02 12:30:06.172464: step 407430, loss = 0.15 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:30:07.044452: step 407440, loss = 0.16 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:30:07.907744: step 407450, loss = 0.16 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:30:08.792284: step 407460, loss = 0.12 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:30:09.685049: step 407470, loss = 0.15 (1433.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:30:10.549295: step 407480, loss = 0.19 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:30:11.437921: step 407490, loss = 0.11 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:30:12.412747: step 407500, loss = 0.13 (1313.1 examples/sec; 0.097 sec/batch)
2017-06-02 12:30:13.182386: step 407510, loss = 0.16 (1663.1 examples/sec; 0.077 sec/batch)
2017-06-02 12:30:14.059379: step 407520, loss = 0.16 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:30:14.965324: step 407530, loss = 0.13 (1412.9 examples/sec; 0.091 sec/batch)
2017-06-02 12:30:15.858655: step 407540, loss = 0.13 (1432.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:30:16.742101: step 407550, loss = 0.14 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:30:17.656436: step 407560, loss = 0.13 (1399.9 examples/sec; 0.091 sec/batch)
2017-06-02 12:30:18.545194: step 407570, loss = 0.13 (1440.2 examples/sec; 0.089 sec/batch)
2017-06-02 12:30:19.423271: step 407580, loss = 0.12 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:30:20.300419: step 407590, loss = 0.13 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:30:21.266951: step 407600, loss = 0.14 (1324.3 examples/sec; 0.097 sec/batch)
2017-06-02 12:30:22.050489: step 407610, loss = 0.19 (1633.6 examples/sec; 0.078 sec/batch)
2017-06-02 12:30:22.902033: step 407620, loss = 0.15 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:30:23.775711: step 407630, loss = 0.15 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:30:24.669742: step 407640, loss = 0.17 (1431.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:30:25.549354: step 407650, loss = 0.12 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:30:26.417710: step 407660, loss = 0.15 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:30:27.309893: step 407670, loss = 0.15 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:30:28.199623: step 407680, loss = 0.12 (1438.6 examples/sec; 0.089 sec/batch)
2017-06-02 12:30:29.078070: step 407690, loss = 0.14 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:30:30.062056: step 407700, loss = 0.11 (1300.8 examples/sec; 0.098 sec/batch)
2017-06-02 12:30:30.809507: step 407710, loss = 0.16 (1712.5 examples/sec; 0.075 sec/batch)
2017-06-02 12:30:31.675215: step 407720, loss = 0.13 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:30:32.526528: step 407730, loss = 0.18 (1503.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:30:33.385155: step 407740, loss = 0.13 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:30:34.253822: step 407750, loss = 0.13 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:30:35.127975: step 407760, loss = 0.13 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:30:35.997835: step 407770, loss = 0.16 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:30:36.860343: step 407780, loss = 0.17 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:30:37.724907: step 407790, loss = 0.13 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:30:38.674564: step 407800, loss = 0.11 (1347.8 examples/sec; 0.095 sec/batch)
2017-06-02 12:30:39.461642: step 407810, loss = 0.14 (1626.3 examples/sec; 0.079 sec/batch)
2017-06-02 12:30:40.336835: step 407820, loss = 0.17 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:30:41.201940: step 407830, loss = 0.17 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:30:42.075696: step 407840, loss = 0.22 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:30:42.937745: step 407850, loss = 0.18 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:30:43.813359: step 407860, loss = 0.15 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:30:44.688067: step 407870, loss = 0.12 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:30:45.536906: step 407880, loss = 0.14 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:30:46.386981: step 407890, loss = 0.16 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 12:30:47.398144: step 407900, loss = 0.12 (1265.9 examples/sec; 0.101 sec/batch)
2017-06-02 12:30:48.127901: step 407910, loss = 0.12 (1754.0 examples/sec; 0.073 sec/batch)
2017-06-02 12:30:49.016330: step 407920, loss = 0.12 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:30:49.871141: step 407930, loss = 0.15 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:30:50.739971: step 407940, loss = 0.16 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:30:51.594215: step 407950, loss = 0.11 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:30:52.451662: step 407960, loss = 0.12 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:30:53.324515: step 407970, loss = 0.14 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:30:54.193992: step 407980, loss = 0.23 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:30:55.070424: step 407990, loss = 0.14 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:30:56.020359: step 408000, loss = 0.13 (1347.5 examples/sec; 0.095 sec/batch)
2017-06-02 12:30:56.786306: step 408010, loss = 0.13 (1671.1 examples/sec; 0.077 sec/batch)
2017-06-02 12:30:57.641724: step 408020, loss = 0.13 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:30:58.517820: step 408030, loss = 0.11 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:30:59.413152: step 408040, loss = 0.15 (1429.6 examples/sec; 0.090 sec/batch)
2017-06-02 12:31:00.269905: step 408050, loss = 0.13 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:31:01.135807: step 408060, loss = 0.16 (1478.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:31:02.005091: step 408070, loss = 0.17 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:31:02.886125: step 408080, loss = 0.13 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:31:03.760403: step 408090, loss = 0.14 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:31:04.803127: step 408100, loss = 0.14 (1227.6 examples/sec; 0.104 sec/batch)
2017-06-02 12:31:05.517562: step 408110, loss = 0.17 (1791.7 examples/sec; 0.071 sec/batch)
2017-06-02 12:31:06.355437: step 408120, loss = 0.14 (1527.6 examples/sec; 0.084 sec/batch)
2017-06-02 12:31:07.225128: step 408130, loss = 0.12 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:31:08.080678: step 408140, loss = 0.13 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:31:08.975992: step 408150, loss = 0.15 (1429.7 examples/sec; 0.090 sec/batch)
2017-06-02 12:31:09.846004: step 408160, loss = 0.16 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:31:10.712716: step 408170, loss = 0.15 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:31:11.563744: step 408180, loss = 0.14 (1504.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:31:12.435622: step 408190, loss = 0.15 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:31:13.394942: step 408200, loss = 0.11 (1334.3 examples/sec; 0.096 sec/batch)
2017-06-02 12:31:14.128146: step 408210, loss = 0.13 (1745.8 examples/sec; 0.073 sec/batch)
2017-06-02 12:31:14.980795: step 408220, loss = 0.12 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:31:15.843402: step 408230, loss = 0.10 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:31:16.719772: step 408240, loss = 0.18 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:31:17.550454: step 408250, loss = 0.14 (1540.9 examples/sec; 0.083 sec/batch)
2017-06-02 12:31:18.406909: step 408260, loss = 0.13 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:31:19.254911: step 408270, loss = 0.14 (1509.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:31:20.113336: step 408280, loss = 0.14 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:31:20.945437: step 408290, loss = 0.14 (1538.3 examples/sec; 0.083 sec/batch)
2017-06-02 12:31:21.907171: step 408300, loss = 0.13 (1330.9 examples/sec; 0.096 sec/batch)
2017-06-02 12:31:22.695116: step 408310, loss = 0.16 (1624.5 examples/sec; 0.079 sec/batch)
2017-06-02 12:31:23.566083: step 408320, loss = 0.18 (1469.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:31:24.442525: step 408330, loss = 0.14 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:31:25.287219: step 408340, loss = 0.14 (1515.3 examples/sec; 0.084 sec/batch)
2017-06-02 12:31:26.151445: step 408350, loss = 0.13 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:31:27.012113: step 408360, loss = 0.17 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:31:27.888486: step 408370, loss = 0.15 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:31:28.761579: step 408380, loss = 0.16 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:31:29.615147: step 408390, loss = 0.12 (1499.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:31:30.617000: step 408400, loss = 0.15 (1277.6 examples/sec; 0.100 sec/batch)
2017-06-02 12:31:31.368566: step 408410, loss = 0.13 (1703.1 examples/sec; 0.075 sec/batch)
2017-06-02 12:31:32.255520: step 408420, loss = 0.11 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:31:33.090961: step 408430, loss = 0.11 (1532.1 examples/sec; 0.084 sec/batch)
2017-06-02 12:31:33.952787: step 408440, loss = 0.18 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:31:34.825390: step 408450, loss = 0.15 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:31:35.684397: step 408460, loss = 0.14 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:31:36.575434: step 408470, loss = 0.14 (1436.5 examples/sec; 0.089 sec/batch)
2017-06-02 12:31:37.438145: step 408480, loss = 0.11 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:31:38.327056: step 408490, loss = 0.14 (1439.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:31:39.303233: step 408500, loss = 0.12 (1311.2 examples/sec; 0.098 sec/batch)
2017-06-02 12:31:40.062500: step 408510, loss = 0.14 (1685.8 examples/sec; 0.076 sec/batch)
2017-06-02 12:31:40.947581: step 408520, loss = 0.15 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 12:31:41.862965: step 408530, loss = 0.15 (1398.3 examples/sec; 0.092 sec/batch)
2017-06-02 12:31:42.727207: step 408540, loss = 0.12 (1481.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:31:43.607458: step 408550, loss = 0.14 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:31:44.485888: step 408560, loss = 0.14 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:31:45.350865: step 408570, loss = 0.21 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:31:46.227805: step 408580, loss = 0.12 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:31:47.093479: step 408590, loss = 0.17 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:31:48.117903: step 408600, loss = 0.15 (1249.5 examples/sec; 0.102 sec/batch)
2017-06-02 12:31:48.843791: step 408610, loss = 0.15 (1763.3 examples/sec; 0.073 sec/batch)
2017-06-02 12:31:49.699441: step 408620, loss = 0.13 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:31:50.549121: step 408630, loss = 0.14 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:31:51.402784: step 408640, loss = 0.16 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:31:52.261161: step 408650, loss = 0.19 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:31:53.132670: step 408660, loss = 0.14 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:31:54.005020: step 408670, loss = 0.17 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:31:54.858939: step 408680, loss = 0.10 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:31:55.676139: step 408690, loss = 0.16 (1566.3 examples/sec; 0.082 sec/batch)
2017-06-02 12:31:56.653821: step 408700, loss = 0.21 (1309.2 examples/sec; 0.098 sec/batch)
2017-06-02 12:31:57.411383: step 408710, loss = 0.13 (1689.6 examples/sec; 0.076 sec/batch)
2017-06-02 12:31:58.261986: step 408720, loss = 0.12 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:31:59.133360: step 408730, loss = 0.12 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:31:59.991999: step 408740, loss = 0.15 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:32:00.858288: step 408750, loss = 0.12 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:32:01.721022: step 408760, loss = 0.11 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:32:02.565660: step 408770, loss = 0.14 (1515.4 examples/sec; 0.084 sec/batch)
2017-06-02 12:32:03.403605: step 408780, loss = 0.12 (1527.5 examples/sec; 0.084 sec/batch)
2017-06-02 12:32:04.267343: step 408790, loss = 0.13 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:32:05.218245: step 408800, loss = 0.13 (1346.1 examples/sec; 0.095 sec/batch)
2017-06-02 12:32:05.981151: step 408810, loss = 0.13 (1677.8 examples/sec; 0.076 sec/batch)
2017-06-02 12:32:06.857801: step 408820, loss = 0.20 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:32:07.725426: step 408830, loss = 0.16 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:32:08.616927: step 408840, loss = 0.18 (1435.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:32:09.450403: step 408850, loss = 0.15 (1535.7 examples/sec; 0.083 sec/batch)
2017-06-02 12:32:10.309921: step 408860, loss = 0.17 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:32:11.191662: step 408870, loss = 0.14 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:32:12.046786: step 408880, loss = 0.14 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:32:12.917844: step 408890, loss = 0.14 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:32:13.961383: step 408900, loss = 0.11 (1226.6 examples/sec; 0.104 sec/batch)
2017-06-02 12:32:14.661512: step 408910, loss = 0.13 (1828.2 examples/sec; 0.070 sec/batch)
2017-06-02 12:32:15.535018: step 408920, loss = 0.12 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:32:16.379268: step 408930, loss = 0.15 (1516.1 examples/sec; 0.084 sec/batch)
2017-06-02 12:32:17.278404: step 408940, loss = 0.13 (1423.6 examples/sec; 0.090 sec/batch)
2017-06-02 12:32:18.160461: step 408950, loss = 0.15 (1451.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:32:19.039143: step 408960, loss = 0.12 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:32:19.925156: step 408970, loss = 0.13 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:32:20.775290: step 408980, loss = 0.12 (1505.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:32:21.660656: step 408990, loss = 0.17 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:32:22.609924: step 409000, loss = 0.13 (1348.4 examples/sec; 0.095 sec/batch)
2017-06-02 12:32:23.385042: step 409010, loss = 0.14 (1651.4 examples/sec; 0.078 sec/batch)
2017-06-02 12:32:24.279279: step 409020, loss = 0.15 (1431.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:32:25.162068: step 409030, loss = 0.14 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:32:26.015492: step 409040, loss = 0.15 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 12:32:26.885116: step 409050, loss = 0.16 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:32:27.752992: step 409060, loss = 0.14 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:32:28.623756: step 409070, loss = 0.17 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:32:29.494618: step 409080, loss = 0.17 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:32:30.349540: step 409090, loss = 0.17 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:32:31.316540: step 409100, loss = 0.11 (1323.7 examples/sec; 0.097 sec/batch)
2017-06-02 12:32:32.111807: step 409110, loss = 0.14 (1609.5 examples/sec; 0.080 sec/batch)
2017-06-02 12:32:32.989405: step 409120, loss = 0.15 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:32:33.845137: step 409130, loss = 0.14 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:32:34.686399: step 409140, loss = 0.14 (1521.5 examples/sec; 0.084 sec/batch)
2017-06-02 12:32:35.544923: step 409150, loss = 0.18 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:32:36.411602: step 409160, loss = 0.16 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:32:37.294726: step 409170, loss = 0.14 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:32:38.173667: step 409180, loss = 0.13 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:32:39.010390: step 409190, loss = 0.15 (1529.8 examples/sec; 0.084 sec/batch)
2017-06-02 12:32:39.986444: step 409200, loss = 0.16 (1311.4 examples/sec; 0.098 sec/batch)
2017-06-02 12:32:40.766198: step 409210, loss = 0.14 (1641.5 examples/sec; 0.078 sec/batch)
2017-06-02 12:32:41.634833: step 409220, loss = 0.15 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:32:42.505938: step 409230, loss = 0.13 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:32:43.371391: step 409240, loss = 0.14 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:32:44.214632: step 409250, loss = 0.11 (1518.0 examples/sec; 0.084 sec/batch)
2017-06-02 12:32:45.073304: step 409260, loss = 0.16 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:32:45.934918: step 409270, loss = 0.17 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:32:46.785037: step 409280, loss = 0.13 (1505.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:32:47.641609: step 409290, loss = 0.16 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:32:48.620000: step 409300, loss = 0.13 (1308.2 examples/sec; 0.098 sec/batch)
2017-06-02 12:32:49.386836: step 409310, loss = 0.12 (1669.2 examples/sec; 0.077 sec/batch)
2017-06-02 12:32:50.258597: step 409320, loss = 0.12 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:32:51.142168: step 409330, loss = 0.17 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:32:52.027927: step 409340, loss = 0.11 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:32:52.877609: step 409350, loss = 0.14 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:32:53.730670: step 409360, loss = 0.12 (1500.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:32:54.573653: step 409370, loss = 0.15 (1518.4 examples/sec; 0.084 sec/batch)
2017-06-02 12:32:55.432747: step 409380, loss = 0.14 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:32:56.292957: step 409390, loss = 0.15 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:32:57.293182: step 409400, loss = 0.13 (1279.7 examples/sec; 0.100 sec/batch)
2017-06-02 12:32:58.016722: step 409410, loss = 0.13 (1769.1 examples/sec; 0.072 sec/batch)
2017-06-02 12:32:58.892039: step 409420, loss = 0.12 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:32:59.774619: step 409430, loss = 0.12 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:33:00.651407: step 409440, loss = 0.13 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:33:01.536650: step 409450, loss = 0.15 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:33:02.402115: step 409460, loss = 0.17 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:33:03.285673: step 409470, loss = 0.14 (1448.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:33:04.147148: step 409480, loss = 0.13 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:33:05.011986: step 409490, loss = 0.16 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:33:06.023997: step 409500, loss = 0.14 (1264.8 examples/sec; 0.101 sec/batch)
2017-06-02 12:33:06.794909: step 409510, loss = 0.15 (1660.4 examples/sec; 0.077 sec/batch)
2017-06-02 12:33:07.671198: step 409520, loss = 0.18 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:33:08.539772: step 409530, loss = 0.15 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:33:09.428928: step 409540, loss = 0.15 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 12:33:10.324812: step 409550, loss = 0.10 (1428.7 examples/sec; 0.090 sec/batch)
2017-06-02 12:33:11.202034: step 409560, loss = 0.16 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:33:12.074723: step 409570, loss = 0.13 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:33:12.938258: step 409580, loss = 0.21 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:33:13.796070: step 409590, loss = 0.12 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:33:14.759136: step 409600, loss = 0.14 (1329.1 examples/sec; 0.096 sec/batch)
2017-06-02 12:33:15.528714: step 409610, loss = 0.12 (1663.3 examples/sec; 0.077 sec/batch)
2017-06-02 12:33:16.403356: step 409620, loss = 0.12 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:33:17.261775: step 409630, loss = 0.12 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:33:18.129098: step 409640, loss = 0.13 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:33:18.991665: step 409650, loss = 0.13 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:33:19.834644: step 409660, loss = 0.11 (1518.4 examples/sec; 0.084 sec/batch)
2017-06-02 12:33:20.690563: step 409670, loss = 0.14 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:33:21.555359: step 409680, loss = 0.12 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:33:22.397564: step 409690, loss = 0.15 (1519.8 examples/sec; 0.084 sec/batch)
2017-06-02 12:33:23.351945: step 409700, loss = 0.13 (1341.2 examples/sec; 0.095 sec/batch)
2017-06-02 12:33:24.129495: step 409710, loss = 0.13 (1646.2 examples/sec; 0.078 sec/batch)
2017-06-02 12:33:24.990346: step 409720, loss = 0.14 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:33:25.842194: step 409730, loss = 0.14 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:33:26.721537: step 409740, loss = 0.17 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:33:27.582279: step 409750, loss = 0.17 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:33:28.462406: step 409760, loss = 0.15 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:33:29.342723: step 409770, loss = 0.15 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:33:30.208479: step 409780, loss = 0.12 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:33:31.105011: step 409790, loss = 0.17 (1427.7 examples/sec; 0.090 sec/batch)
2017-06-02 12:33:32.099864: step 409800, loss = 0.14 (1286.6 examples/sec; 0.099 sec/batch)
2017-06-02 12:33:32.832117: step 409810, loss = 0.12 (1748.0 examples/sec; 0.073 sec/batch)
2017-06-02 12:33:33.689998: step 409820, loss = 0.11 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:33:34.516016: step 409830, loss = 0.11 (1549.6 examples/sec; 0.083 sec/batch)
2017-06-02 12:33:35.358784: step 409840, loss = 0.13 (1518.8 examples/sec; 0.084 sec/batch)
2017-06-02 12:33:36.241409: step 409850, loss = 0.18 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:33:37.096664: step 409860, loss = 0.13 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:33:37.968453: step 409870, loss = 0.18 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:33:38.831820: step 409880, loss = 0.14 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:33:39.696757: step 409890, loss = 0.17 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:33:40.663948: step 409900, loss = 0.15 (1323.4 examples/sec; 0.097 sec/batch)
2017-06-02 12:33:41.450253: step 409910, loss = 0.17 (1627.9 examples/sec; 0.079 sec/batch)
2017-06-02 12:33:42.353705: step 409920, loss = 0.12 (1416.8 examples/sec; 0.090 sec/batch)
2017-06-02 12:33:43.207817: step 409930, loss = 0.13 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:33:44.082577: step 409940, loss = 0.11 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:33:44.958244: step 409950, loss = 0.13 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:33:45.831748: step 409960, loss = 0.13 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:33:46.708512: step 409970, loss = 0.16 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:33:47.603514: step 409980, loss = 0.12 (1430.1 examples/sec; 0.090 sec/batch)
2017-06-02 12:33:48.472187: step 409990, loss = 0.12 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:33:49.459512: step 410000, loss = 0.18 (1296.4 examples/sec; 0.099 sec/batch)
2017-06-02 12:33:50.234287: step 410010, loss = 0.16 (1652.1 examples/sec; 0.077 sec/batch)
2017-06-02 12:33:51.110049: step 410020, loss = 0.13 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:33:51.995699: step 410030, loss = 0.13 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:33:52.870491: step 410040, loss = 0.14 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:33:53.737944: step 410050, loss = 0.17 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:33:54.584843: step 410060, loss = 0.17 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:33:55.422838: step 410070, loss = 0.13 (1527.4 examples/sec; 0.084 sec/batch)
2017-06-02 12:33:56.281180: step 410080, loss = 0.14 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:33:57.130944: step 410090, loss = 0.14 (1506.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:33:58.104673: step 410100, loss = 0.11 (1314.5 examples/sec; 0.097 sec/batch)
2017-06-02 12:33:58.858548: step 410110, loss = 0.14 (1697.9 examples/sec; 0.075 sec/batch)
2017-06-02 12:33:59.701911: step 410120, loss = 0.13 (1517.7 examples/sec; 0.084 sec/batch)
2017-06-02 12:34:00.568994: step 410130, loss = 0.15 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:34:01.435925: step 410140, loss = 0.13 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:34:02.319247: step 410150, loss = 0.12 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:34:03.197403: step 410160, loss = 0.13 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:34:04.091084: step 410170, loss = 0.15 (1432.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:34:04.973551: step 410180, loss = 0.17 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:34:05.851923: step 410190, loss = 0.15 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:34:06.862013: step 410200, loss = 0.11 (1267.2 examples/sec; 0.101 sec/batch)
2017-06-02 12:34:07.614067: step 410210, loss = 0.16 (1702.0 examples/sec; 0.075 sec/batch)
2017-06-02 12:34:08.509919: step 410220, loss = 0.16 (1428.8 examples/sec; 0.090 sec/batch)
2017-06-02 12:34:09.390894: step 410230, loss = 0.16 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:34:10.265611: step 410240, loss = 0.13 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:34:11.115367: step 410250, loss = 0.13 (1506.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:34:11.980397: step 410260, loss = 0.11 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:34:12.831090: step 410270, loss = 0.14 (1504.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:34:13.698898: step 410280, loss = 0.12 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:34:14.571342: step 410290, loss = 0.12 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:34:15.552062: step 410300, loss = 0.14 (1305.2 examples/sec; 0.098 sec/batch)
2017-06-02 12:34:16.316863: step 410310, loss = 0.13 (1673.6 examples/sec; 0.076 sec/batch)
2017-06-02 12:34:17.204251: step 410320, loss = 0.12 (1442.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:34:18.101614: step 410330, loss = 0.13 (1426.4 examples/sec; 0.090 sec/batch)
2017-06-02 12:34:18.948894: step 410340, loss = 0.11 (1510.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:34:19.827686: step 410350, loss = 0.12 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:34:20.692694: step 410360, loss = 0.13 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:34:21.523467: step 410370, loss = 0.14 (1540.7 examples/sec; 0.083 sec/batch)
2017-06-02 12:34:22.405629: step 410380, loss = 0.15 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:34:23.278580: step 410390, loss = 0.12 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:34:24.239959: step 410400, loss = 0.14 (1331.4 examples/sec; 0.096 sec/batch)
2017-06-02 12:34:25.005678: step 410410, loss = 0.11 (1671.6 examples/sec; 0.077 sec/batch)
2017-06-02 12:34:25.870394: step 410420, loss = 0.12 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:34:26.713723: step 410430, loss = 0.17 (1517.8 examples/sec; 0.084 sec/batch)
2017-06-02 12:34:27.581342: step 410440, loss = 0.15 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:34:28.447545: step 410450, loss = 0.14 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:34:29.319920: step 410460, loss = 0.15 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:34:30.167990: step 410470, loss = 0.15 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:34:31.017643: step 410480, loss = 0.13 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:34:31.914796: step 410490, loss = 0.23 (1426.7 examples/sec; 0.090 sec/batch)
2017-06-02 12:34:32.854105: step 410500, loss = 0.14 (1362.7 examples/sec; 0.094 sec/batch)
2017-06-02 12:34:33.624941: step 410510, loss = 0.17 (1660.6 examples/sec; 0.077 sec/batch)
2017-06-02 12:34:34.488777: step 410520, loss = 0.11 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:34:35.345340: step 410530, loss = 0.14 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:34:36.213634: step 410540, loss = 0.13 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:34:37.075809: step 410550, loss = 0.18 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:34:37.927020: step 410560, loss = 0.14 (1503.8 examples/sec; 0.085 sec/batch)
2017-06-02 12:34:38.778421: step 410570, loss = 0.14 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:34:39.660947: step 410580, loss = 0.10 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:34:40.548063: step 410590, loss = 0.12 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:34:41.526295: step 410600, loss = 0.12 (1308.5 examples/sec; 0.098 sec/batch)
2017-06-02 12:34:42.300025: step 410610, loss = 0.15 (1654.3 examples/sec; 0.077 sec/batch)
2017-06-02 12:34:43.173053: step 410620, loss = 0.15 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:34:44.021652: step 410630, loss = 0.13 (1508.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:34:44.896174: step 410640, loss = 0.19 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:34:45.788552: step 410650, loss = 0.14 (1434.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:34:46.691884: step 410660, loss = 0.15 (1417.0 examples/sec; 0.090 sec/batch)
2017-06-02 12:34:47.552621: step 410670, loss = 0.11 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:34:48.451397: step 410680, loss = 0.12 (1424.2 examples/sec; 0.090 sec/batch)
2017-06-02 12:34:49.324951: step 410690, loss = 0.11 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:34:50.335227: step 410700, loss = 0.14 (1267.0 examples/sec; 0.101 sec/batch)
2017-06-02 12:34:51.086168: step 410710, loss = 0.21 (1704.6 examples/sec; 0.075 sec/batch)
2017-06-02 12:34:51.963194: step 410720, loss = 0.15 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:34:52.822612: step 410730, loss = 0.11 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:34:53.709175: step 410740, loss = 0.18 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:34:54.598920: step 410750, loss = 0.15 (1438.6 examples/sec; 0.089 sec/batch)
2017-06-02 12:34:55.453187: step 410760, loss = 0.12 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:34:56.311319: step 410770, loss = 0.13 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:34:57.171282: step 410780, loss = 0.15 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:34:58.046211: step 410790, loss = 0.14 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:34:58.996241: step 410800, loss = 0.17 (1347.3 examples/sec; 0.095 sec/batch)
2017-06-02 12:34:59.753749: step 410810, loss = 0.14 (1689.8 examples/sec; 0.076 sec/batch)
2017-06-02 12:35:00.616114: step 410820, loss = 0.15 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:35:01.490024: step 410830, loss = 0.14 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:35:02.350767: step 410840, loss = 0.14 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:35:03.218911: step 410850, loss = 0.13 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:35:04.082839: step 410860, loss = 0.19 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:35:04.974155: step 410870, loss = 0.13 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:35:05.823922: step 410880, loss = 0.13 (1506.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:35:06.681256: step 410890, loss = 0.14 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:35:07.658326: step 410900, loss = 0.12 (1310.0 examples/sec; 0.098 sec/batch)
2017-06-02 12:35:08.414444: step 410910, loss = 0.14 (1692.9 examples/sec; 0.076 sec/batch)
2017-06-02 12:35:09.261762: step 410920, loss = 0.16 (1510.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:35:10.141572: step 410930, loss = 0.18 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:35:11.028959: step 410940, loss = 0.17 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 12:35:11.871133: step 410950, loss = 0.13 (1519.9 examples/sec; 0.084 sec/batch)
2017-06-02 12:35:12.730714: step 410960, loss = 0.11 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:35:13.582145: step 410970, loss = 0.18 (1503.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:35:14.456862: step 410980, loss = 0.13 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:35:15.311300: step 410990, loss = 0.14 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:35:16.288634: step 411000, loss = 0.13 (1309.7 examples/sec; 0.098 sec/batch)
2017-06-02 12:35:17.061956: step 411010, loss = 0.14 (1655.2 examples/sec; 0.077 sec/batch)
2017-06-02 12:35:17.915324: step 411020, loss = 0.15 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:35:18.778466: step 411030, loss = 0.11 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:35:19.603520: step 411040, loss = 0.12 (1551.4 examples/sec; 0.083 sec/batch)
2017-06-02 12:35:20.460800: step 411050, loss = 0.11 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:35:21.324838: step 411060, loss = 0.13 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:35:22.200124: step 411070, loss = 0.13 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:35:23.060056: step 411080, loss = 0.12 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:35:23.917811: step 411090, loss = 0.13 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:35:24.905815: step 411100, loss = 0.16 (1295.5 examples/sec; 0.099 sec/batch)
2017-06-02 12:35:25.653712: step 411110, loss = 0.13 (1711.5 examples/sec; 0.075 sec/batch)
2017-06-02 12:35:26.495863: step 411120, loss = 0.14 (1519.9 examples/sec; 0.084 sec/batch)
2017-06-02 12:35:27.333680: step 411130, loss = 0.18 (1527.8 examples/sec; 0.084 sec/batch)
2017-06-02 12:35:28.199942: step 411140, loss = 0.15 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:35:29.022586: step 411150, loss = 0.12 (1555.9 examples/sec; 0.082 sec/batch)
2017-06-02 12:35:29.887339: step 411160, loss = 0.13 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:35:30.776000: step 411170, loss = 0.14 (1440.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:35:31.618540: step 411180, loss = 0.18 (1519.2 examples/sec; 0.084 sec/batch)
2017-06-02 12:35:32.504092: step 411190, loss = 0.13 (1445.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:35:33.473080: step 411200, loss = 0.13 (1320.9 examples/sec; 0.097 sec/batch)
2017-06-02 12:35:34.184938: step 411210, loss = 0.11 (1798.1 examples/sec; 0.071 sec/batch)
2017-06-02 12:35:35.035424: step 411220, loss = 0.10 (1505.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:35:35.867106: step 411230, loss = 0.19 (1539.0 examples/sec; 0.083 sec/batch)
2017-06-02 12:35:36.741901: step 411240, loss = 0.16 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:35:37.611488: step 411250, loss = 0.14 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:35:38.458905: step 411260, loss = 0.17 (1510.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:35:39.329756: step 411270, loss = 0.17 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:35:40.208867: step 411280, loss = 0.12 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:35:41.094017: step 411290, loss = 0.14 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:35:42.045502: step 411300, loss = 0.12 (1345.3 examples/sec; 0.095 sec/batch)
2017-06-02 12:35:42.808592: step 411310, loss = 0.14 (1677.4 examples/sec; 0.076 sec/batch)
2017-06-02 12:35:43.703964: step 411320, loss = 0.16 (1429.6 examples/sec; 0.090 sec/batch)
2017-06-02 12:35:44.553774: step 411330, loss = 0.12 (1506.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:35:45.411071: step 411340, loss = 0.17 (1493.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:35:46.263471: step 411350, loss = 0.15 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:35:47.092440: step 411360, loss = 0.13 (1544.1 examples/sec; 0.083 sec/batch)
2017-06-02 12:35:47.935694: step 411370, loss = 0.14 (1517.9 examples/sec; 0.084 sec/batch)
2017-06-02 12:35:48.801888: step 411380, loss = 0.14 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:35:49.653272: step 411390, loss = 0.13 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:35:50.631094: step 411400, loss = 0.17 (1309.0 examples/sec; 0.098 sec/batch)
2017-06-02 12:35:51.372430: step 411410, loss = 0.15 (1726.6 examples/sec; 0.074 sec/batch)
2017-06-02 12:35:52.236930: step 411420, loss = 0.13 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:35:53.093257: step 411430, loss = 0.11 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:35:53.959856: step 411440, loss = 0.13 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:35:54.814863: step 411450, loss = 0.17 (1497.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:35:55.672806: step 411460, loss = 0.16 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:35:56.528167: step 411470, loss = 0.12 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:35:57.407073: step 411480, loss = 0.14 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:35:58.268320: step 411490, loss = 0.13 (1486.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:35:59.295270: step 411500, loss = 0.14 (1246.4 examples/sec; 0.103 sec/batch)
2017-06-02 12:36:00.033018: step 411510, loss = 0.13 (1735.0 examples/sec; 0.074 sec/batch)
2017-06-02 12:36:00.917272: step 411520, loss = 0.13 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:36:01.791004: step 411530, loss = 0.11 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:36:02.646273: step 411540, loss = 0.14 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:36:03.496260: step 411550, loss = 0.12 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:36:04.356328: step 411560, loss = 0.12 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:36:05.221918: step 411570, loss = 0.14 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:36:06.062303: step 411580, loss = 0.14 (1523.1 examples/sec; 0.084 sec/batch)
2017-06-02 12:36:06.905041: step 411590, loss = 0.15 (1518.9 examples/sec; 0.084 sec/batch)
2017-06-02 12:36:07.872133: step 411600, loss = 0.11 (1323.5 examples/sec; 0.097 sec/batch)
2017-06-02 12:36:08.635982: step 411610, loss = 0.14 (1675.8 examples/sec; 0.076 sec/batch)
2017-06-02 12:36:09.528911: step 411620, loss = 0.20 (1433.5 examples/sec; 0.089 sec/batch)
2017-06-02 12:36:10.396228: step 411630, loss = 0.12 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:36:11.270648: step 411640, loss = 0.13 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:36:12.124382: step 411650, loss = 0.13 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:36:12.969614: step 411660, loss = 0.12 (1514.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:36:13.834011: step 411670, loss = 0.16 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:36:14.682442: step 411680, loss = 0.15 (1508.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:36:15.553911: step 411690, loss = 0.13 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:36:16.523317: step 411700, loss = 0.17 (1320.4 examples/sec; 0.097 sec/batch)
2017-06-02 12:36:17.279617: step 411710, loss = 0.16 (1692.5 examples/sec; 0.076 sec/batch)
2017-06-02 12:36:18.108344: step 411720, loss = 0.15 (1544.6 examples/sec; 0.083 sec/batch)
2017-06-02 12:36:18.974646: step 411730, loss = 0.12 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:36:19.856399: step 411740, loss = 0.14 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:36:20.750045: step 411750, loss = 0.15 (1432.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:36:21.601270: step 411760, loss = 0.18 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:36:22.444449: step 411770, loss = 0.22 (1518.1 examples/sec; 0.084 sec/batch)
2017-06-02 12:36:23.304347: step 411780, loss = 0.12 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:36:24.187637: step 411790, loss = 0.19 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:36:25.155486: step 411800, loss = 0.14 (1322.5 examples/sec; 0.097 sec/batch)
2017-06-02 12:36:25.936816: step 411810, loss = 0.11 (1638.2 examples/sec; 0.078 sec/batch)
2017-06-02 12:36:26.796619: step 411820, loss = 0.12 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:36:27.644872: step 411830, loss = 0.16 (1508.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:36:28.526084: step 411840, loss = 0.12 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:36:29.379172: step 411850, loss = 0.13 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:36:30.242725: step 411860, loss = 0.18 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:36:31.104881: step 411870, loss = 0.14 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:36:31.979337: step 411880, loss = 0.14 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:36:32.820133: step 411890, loss = 0.12 (1522.4 examples/sec; 0.084 sec/batch)
2017-06-02 12:36:33.817255: step 411900, loss = 0.12 (1283.7 examples/sec; 0.100 sec/batch)
2017-06-02 12:36:34.567992: step 411910, loss = 0.12 (1705.0 examples/sec; 0.075 sec/batch)
2017-06-02 12:36:35.422942: step 411920, loss = 0.13 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:36:36.279285: step 411930, loss = 0.13 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:36:37.146848: step 411940, loss = 0.12 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:36:38.023915: step 411950, loss = 0.18 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:36:38.915011: step 411960, loss = 0.13 (1436.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:36:39.782852: step 411970, loss = 0.14 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:36:40.652672: step 411980, loss = 0.15 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:36:41.518537: step 411990, loss = 0.14 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:36:42.483062: step 412000, loss = 0.17 (1327.1 examples/sec; 0.096 sec/batch)
2017-06-02 12:36:43.267433: step 412010, loss = 0.12 (1631.9 examples/sec; 0.078 sec/batch)
2017-06-02 12:36:44.127324: step 412020, loss = 0.13 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:36:45.000349: step 412030, loss = 0.14 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:36:45.852541: step 412040, loss = 0.20 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:36:46.735677: step 412050, loss = 0.16 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:36:47.612540: step 412060, loss = 0.14 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:36:48.476012: step 412070, loss = 0.17 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:36:49.347629: step 412080, loss = 0.15 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:36:50.210581: step 412090, loss = 0.16 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:36:51.194971: step 412100, loss = 0.11 (1300.3 examples/sec; 0.098 sec/batch)
2017-06-02 12:36:51.965145: step 412110, loss = 0.15 (1662.0 examples/sec; 0.077 sec/batch)
2017-06-02 12:36:52.847215: step 412120, loss = 0.13 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:36:53.738001: step 412130, loss = 0.16 (1436.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:36:54.584843: step 412140, loss = 0.16 (1511.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:36:55.465190: step 412150, loss = 0.12 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:36:56.334024: step 412160, loss = 0.12 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:36:57.242003: step 412170, loss = 0.17 (1409.8 examples/sec; 0.091 sec/batch)
2017-06-02 12:36:58.125086: step 412180, loss = 0.15 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:36:59.013352: step 412190, loss = 0.18 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:37:00.031402: step 412200, loss = 0.12 (1257.3 examples/sec; 0.102 sec/batch)
2017-06-02 12:37:00.748336: step 412210, loss = 0.11 (1785.3 examples/sec; 0.072 sec/batch)
2017-06-02 12:37:01.640290: step 412220, loss = 0.13 (1435.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:37:02.525097: step 412230, loss = 0.13 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:37:03.413150: step 412240, loss = 0.18 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:37:04.299673: step 412250, loss = 0.16 (1443.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:37:05.161159: step 412260, loss = 0.16 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:37:06.039836: step 412270, loss = 0.14 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:37:06.926101: step 412280, loss = 0.12 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:37:07.794464: step 412290, loss = 0.14 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:37:08.756180: step 412300, loss = 0.17 (1331.0 examples/sec; 0.096 sec/batch)
2017-06-02 12:37:09.546237: step 412310, loss = 0.15 (1620.1 examples/sec; 0.079 sec/batch)
2017-06-02 12:37:10.421822: step 412320, loss = 0.13 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:37:11.276625: step 412330, loss = 0.18 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:37:12.127859: step 412340, loss = 0.17 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:37:12.999213: step 412350, loss = 0.12 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:37:13.846840: step 412360, loss = 0.15 (1510.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:37:14.684178: step 412370, loss = 0.13 (1528.6 examples/sec; 0.084 sec/batch)
2017-06-02 12:37:15.535715: step 412380, loss = 0.12 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:37:16.406291: step 412390, loss = 0.13 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:37:17.386505: step 412400, loss = 0.12 (1305.8 examples/sec; 0.098 sec/batch)
2017-06-02 12:37:18.168454: step 412410, loss = 0.13 (1636.9 examples/sec; 0.078 sec/batch)
2017-06-02 12:37:19.055926: step 412420, loss = 0.13 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:37:19.941128: step 412430, loss = 0.15 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:37:20.816406: step 412440, loss = 0.13 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:37:21.699356: step 412450, loss = 0.18 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:37:22.571840: step 412460, loss = 0.16 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:37:23.458099: step 412470, loss = 0.16 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:37:24.320936: step 412480, loss = 0.13 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:37:25.174697: step 412490, loss = 0.16 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:37:26.163571: step 412500, loss = 0.13 (1294.4 examples/sec; 0.099 sec/batch)
2017-06-02 12:37:26.909860: step 412510, loss = 0.15 (1715.2 examples/sec; 0.075 sec/batch)
2017-06-02 12:37:27.769005: step 412520, loss = 0.17 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:37:28.591251: step 412530, loss = 0.13 (1556.7 examples/sec; 0.082 sec/batch)
2017-06-02 12:37:29.464575: step 412540, loss = 0.17 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:37:30.340500: step 412550, loss = 0.16 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:37:31.194994: step 412560, loss = 0.11 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:37:32.057904: step 412570, loss = 0.14 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:37:32.954512: step 412580, loss = 0.20 (1427.6 examples/sec; 0.090 sec/batch)
2017-06-02 12:37:33.836176: step 412590, loss = 0.16 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:37:34.791031: step 412600, loss = 0.12 (1340.5 examples/sec; 0.095 sec/batch)
2017-06-02 12:37:35.578824: step 412610, loss = 0.13 (1624.8 examples/sec; 0.079 sec/batch)
2017-06-02 12:37:36.455210: step 412620, loss = 0.13 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:37:37.334096: step 412630, loss = 0.18 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:37:38.175353: step 412640, loss = 0.16 (1521.5 examples/sec; 0.084 sec/batch)
2017-06-02 12:37:39.038231: step 412650, loss = 0.16 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:37:39.870780: step 412660, loss = 0.18 (1537.4 examples/sec; 0.083 sec/batch)
2017-06-02 12:37:40.727264: step 412670, loss = 0.16 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:37:41.588286: step 412680, loss = 0.14 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:37:42.467255: step 412690, loss = 0.13 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:37:43.484673: step 412700, loss = 0.14 (1258.1 examples/sec; 0.102 sec/batch)
2017-06-02 12:37:44.222018: step 412710, loss = 0.11 (1736.0 examples/sec; 0.074 sec/batch)
2017-06-02 12:37:45.078259: step 412720, loss = 0.14 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:37:45.940627: step 412730, loss = 0.11 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:37:46.788032: step 412740, loss = 0.12 (1510.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:37:47.676860: step 412750, loss = 0.13 (1440.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:37:48.546085: step 412760, loss = 0.12 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:37:49.404881: step 412770, loss = 0.18 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:37:50.295032: step 412780, loss = 0.16 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:37:51.176796: step 412790, loss = 0.14 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:37:52.171335: step 412800, loss = 0.17 (1287.0 examples/sec; 0.099 sec/batch)
2017-06-02 12:37:52.923517: step 412810, loss = 0.12 (1701.7 examples/sec; 0.075 sec/batch)
2017-06-02 12:37:53.796483: step 412820, loss = 0.16 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:37:54.665092: step 412830, loss = 0.12 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:37:55.541233: step 412840, loss = 0.12 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:37:56.427828: step 412850, loss = 0.13 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:37:57.307029: step 412860, loss = 0.13 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:37:58.183630: step 412870, loss = 0.17 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:37:59.046371: step 412880, loss = 0.13 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:37:59.934858: step 412890, loss = 0.14 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:38:00.888894: step 412900, loss = 0.14 (1341.7 examples/sec; 0.095 sec/batch)
2017-06-02 12:38:01.665008: step 412910, loss = 0.16 (1649.2 examples/sec; 0.078 sec/batch)
2017-06-02 12:38:02.538983: step 412920, loss = 0.15 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:38:03.417853: step 412930, loss = 0.13 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:38:04.285130: step 412940, loss = 0.17 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:38:05.159970: step 412950, loss = 0.12 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:38:06.038209: step 412960, loss = 0.10 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:38:06.887662: step 412970, loss = 0.13 (1506.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:38:07.741599: step 412980, loss = 0.12 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:38:08.615593: step 412990, loss = 0.12 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:38:09.591078: step 413000, loss = 0.16 (1312.2 examples/sec; 0.098 sec/batch)
2017-06-02 12:38:10.361649: step 413010, loss = 0.10 (1661.1 examples/sec; 0.077 sec/batch)
2017-06-02 12:38:11.232956: step 413020, loss = 0.12 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:38:12.122946: step 413030, loss = 0.15 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 12:38:12.994666: step 413040, loss = 0.17 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:38:13.857137: step 413050, loss = 0.17 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:38:14.723927: step 413060, loss = 0.13 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:38:15.631674: step 413070, loss = 0.16 (1410.1 examples/sec; 0.091 sec/batch)
2017-06-02 12:38:16.508694: step 413080, loss = 0.11 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:38:17.396613: step 413090, loss = 0.12 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 12:38:18.394510: step 413100, loss = 0.15 (1282.7 examples/sec; 0.100 sec/batch)
2017-06-02 12:38:19.173241: step 413110, loss = 0.11 (1643.7 examples/sec; 0.078 sec/batch)
2017-06-02 12:38:20.059618: step 413120, loss = 0.18 (1444.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:38:20.961087: step 413130, loss = 0.14 (1419.9 examples/sec; 0.090 sec/batch)
2017-06-02 12:38:21.840404: step 413140, loss = 0.15 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:38:22.701907: step 413150, loss = 0.11 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:38:23.576839: step 413160, loss = 0.15 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:38:24.443885: step 413170, loss = 0.19 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:38:25.310198: step 413180, loss = 0.16 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:38:26.155655: step 413190, loss = 0.13 (1514.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:38:27.110789: step 413200, loss = 0.12 (1340.1 examples/sec; 0.096 sec/batch)
2017-06-02 12:38:27.867334: step 413210, loss = 0.15 (1691.9 examples/sec; 0.076 sec/batch)
2017-06-02 12:38:28.734123: step 413220, loss = 0.15 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:38:29.591201: step 413230, loss = 0.12 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:38:30.454857: step 413240, loss = 0.14 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:38:31.318245: step 413250, loss = 0.13 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:38:32.184271: step 413260, loss = 0.13 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:38:33.027404: step 413270, loss = 0.13 (1518.1 examples/sec; 0.084 sec/batch)
2017-06-02 12:38:33.899163: step 413280, loss = 0.17 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:38:34.764686: step 413290, loss = 0.16 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:38:35.762474: step 413300, loss = 0.14 (1282.8 examples/sec; 0.100 sec/batch)
2017-06-02 12:38:36.549815: step 413310, loss = 0.17 (1625.7 examples/sec; 0.079 sec/batch)
2017-06-02 12:38:37.430223: step 413320, loss = 0.16 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:38:38.301942: step 413330, loss = 0.14 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:38:39.165371: step 413340, loss = 0.18 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:38:40.042324: step 413350, loss = 0.15 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:38:40.883855: step 413360, loss = 0.12 (1521.0 examples/sec; 0.084 sec/batch)
2017-06-02 12:38:41.748692: step 413370, loss = 0.11 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:38:42.611192: step 413380, loss = 0.17 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:38:43.486281: step 413390, loss = 0.13 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:38:44.446611: step 413400, loss = 0.13 (1332.9 examples/sec; 0.096 sec/batch)
2017-06-02 12:38:45.215172: step 413410, loss = 0.15 (1665.5 examples/sec; 0.077 sec/batch)
2017-06-02 12:38:46.081281: step 413420, loss = 0.12 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:38:46.941310: step 413430, loss = 0.15 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:38:47.806502: step 413440, loss = 0.15 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:38:48.664242: step 413450, loss = 0.14 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:38:49.539135: step 413460, loss = 0.14 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:38:50.414481: step 413470, loss = 0.12 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:38:51.266017: step 413480, loss = 0.12 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:38:52.124423: step 413490, loss = 0.14 (1491.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:38:53.081707: step 413500, loss = 0.13 (1337.1 examples/sec; 0.096 sec/batch)
2017-06-02 12:38:53.856081: step 413510, loss = 0.15 (1652.9 examples/sec; 0.077 sec/batch)
2017-06-02 12:38:54.714014: step 413520, loss = 0.15 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:38:55.557171: step 413530, loss = 0.17 (1518.1 examples/sec; 0.084 sec/batch)
2017-06-02 12:38:56.399342: step 413540, loss = 0.10 (1519.9 examples/sec; 0.084 sec/batch)
2017-06-02 12:38:57.249248: step 413550, loss = 0.13 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:38:58.108525: step 413560, loss = 0.16 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:38:58.977956: step 413570, loss = 0.11 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:38:59.856483: step 413580, loss = 0.12 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:39:00.756463: step 413590, loss = 0.14 (1422.3 examples/sec; 0.090 sec/batch)
2017-06-02 12:39:01.744518: step 413600, loss = 0.16 (1295.4 examples/sec; 0.099 sec/batch)
2017-06-02 12:39:02.496939: step 413610, loss = 0.14 (1701.2 examples/sec; 0.075 sec/batch)
2017-06-02 12:39:03.381227: step 413620, loss = 0.16 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:39:04.260417: step 413630, loss = 0.11 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:39:05.096321: step 413640, loss = 0.15 (1531.3 examples/sec; 0.084 sec/batch)
2017-06-02 12:39:05.982815: step 413650, loss = 0.16 (1443.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:39:06.843145: step 413660, loss = 0.11 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:39:07.691456: step 413670, loss = 0.19 (1508.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:39:08.555922: step 413680, loss = 0.15 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:39:09.425453: step 413690, loss = 0.16 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:39:10.388735: step 413700, loss = 0.10 (1328.8 examples/sec; 0.096 sec/batch)
2017-06-02 12:39:11.165052: step 413710, loss = 0.13 (1648.8 examples/sec; 0.078 sec/batch)
2017-06-02 12:39:12.042206: step 413720, loss = 0.13 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:39:12.915799: step 413730, loss = 0.17 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:39:13.807164: step 413740, loss = 0.13 (1436.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:39:14.655952: step 413750, loss = 0.11 (1508.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:39:15.543810: step 413760, loss = 0.14 (1441.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:39:16.423788: step 413770, loss = 0.15 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:39:17.316499: step 413780, loss = 0.12 (1433.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:39:18.182145: step 413790, loss = 0.10 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:39:19.152721: step 413800, loss = 0.16 (1318.8 examples/sec; 0.097 sec/batch)
2017-06-02 12:39:19.929915: step 413810, loss = 0.16 (1647.0 examples/sec; 0.078 sec/batch)
2017-06-02 12:39:20.806251: step 413820, loss = 0.12 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:39:21.680299: step 413830, loss = 0.15 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:39:22.546756: step 413840, loss = 0.12 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:39:23.429587: step 413850, loss = 0.12 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:39:24.305660: step 413860, loss = 0.13 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:39:25.191871: step 413870, loss = 0.15 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:39:26.059274: step 413880, loss = 0.13 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:39:26.927495: step 413890, loss = 0.13 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:39:27.895314: step 413900, loss = 0.12 (1322.6 examples/sec; 0.097 sec/batch)
2017-06-02 12:39:28.677326: step 413910, loss = 0.13 (1636.8 examples/sec; 0.078 sec/batch)
2017-06-02 12:39:29.530403: step 413920, loss = 0.25 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:39:30.390516: step 413930, loss = 0.15 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:39:31.277036: step 413940, loss = 0.14 (1443.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:39:32.156341: step 413950, loss = 0.12 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:39:33.015848: step 413960, loss = 0.11 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:39:33.880092: step 413970, loss = 0.11 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:39:34.754947: step 413980, loss = 0.15 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:39:35.627798: step 413990, loss = 0.14 (1466.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:39:36.610621: step 414000, loss = 0.15 (1302.4 examples/sec; 0.098 sec/batch)
2017-06-02 12:39:37.371167: step 414010, loss = 0.15 (1683.0 examples/sec; 0.076 sec/batch)
2017-06-02 12:39:38.241105: step 414020, loss = 0.17 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:39:39.114915: step 414030, loss = 0.12 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:39:39.977240: step 414040, loss = 0.13 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:39:40.820238: step 414050, loss = 0.14 (1518.4 examples/sec; 0.084 sec/batch)
2017-06-02 12:39:41.678564: step 414060, loss = 0.13 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:39:42.554245: step 414070, loss = 0.13 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:39:43.446830: step 414080, loss = 0.15 (1434.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:39:44.294069: step 414090, loss = 0.15 (1510.8 examples/sec; 0.085 sec/batch)
2017-06-02 12:39:45.259007: step 414100, loss = 0.15 (1326.5 examples/sec; 0.096 sec/batch)
2017-06-02 12:39:46.027479: step 414110, loss = 0.13 (1665.6 examples/sec; 0.077 sec/batch)
2017-06-02 12:39:46.891024: step 414120, loss = 0.12 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:39:47.739757: step 414130, loss = 0.18 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:39:48.601245: step 414140, loss = 0.13 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:39:49.473006: step 414150, loss = 0.13 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:39:50.315270: step 414160, loss = 0.14 (1519.7 examples/sec; 0.084 sec/batch)
2017-06-02 12:39:51.168357: step 414170, loss = 0.16 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:39:52.024177: step 414180, loss = 0.10 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:39:52.879226: step 414190, loss = 0.12 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:39:53.914239: step 414200, loss = 0.21 (1236.7 examples/sec; 0.104 sec/batch)
2017-06-02 12:39:54.704462: step 414210, loss = 0.12 (1619.8 examples/sec; 0.079 sec/batch)
2017-06-02 12:39:55.585759: step 414220, loss = 0.12 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:39:56.453186: step 414230, loss = 0.12 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:39:57.326459: step 414240, loss = 0.10 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:39:58.190294: step 414250, loss = 0.12 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:39:59.062876: step 414260, loss = 0.12 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:39:59.915849: step 414270, loss = 0.11 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:40:00.798499: step 414280, loss = 0.15 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:40:01.676679: step 414290, loss = 0.14 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:40:02.643414: step 414300, loss = 0.16 (1324.0 examples/sec; 0.097 sec/batch)
2017-06-02 12:40:03.411311: step 414310, loss = 0.12 (1666.9 examples/sec; 0.077 sec/batch)
2017-06-02 12:40:04.290506: step 414320, loss = 0.12 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:40:05.168318: step 414330, loss = 0.16 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:40:06.047177: step 414340, loss = 0.13 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:40:06.927600: step 414350, loss = 0.17 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:40:07.799148: step 414360, loss = 0.10 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:40:08.677055: step 414370, loss = 0.12 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:40:09.525582: step 414380, loss = 0.12 (1508.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:40:10.369889: step 414390, loss = 0.17 (1516.0 examples/sec; 0.084 sec/batch)
2017-06-02 12:40:11.340297: step 414400, loss = 0.13 (1319.0 examples/sec; 0.097 sec/batch)
2017-06-02 12:40:12.126207: step 414410, loss = 0.12 (1628.7 examples/sec; 0.079 sec/batch)
2017-06-02 12:40:12.995356: step 414420, loss = 0.17 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:40:13.881150: step 414430, loss = 0.12 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:40:14.741977: step 414440, loss = 0.15 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:40:15.610376: step 414450, loss = 0.13 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:40:16.455904: step 414460, loss = 0.20 (1513.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:40:17.322248: step 414470, loss = 0.13 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:40:18.183909: step 414480, loss = 0.15 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:40:19.051662: step 414490, loss = 0.10 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:40:20.070316: step 414500, loss = 0.13 (1256.6 examples/sec; 0.102 sec/batch)
2017-06-02 12:40:20.805981: step 414510, loss = 0.18 (1739.9 examples/sec; 0.074 sec/batch)
2017-06-02 12:40:21.675251: step 414520, loss = 0.13 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:40:22.558952: step 414530, loss = 0.13 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:40:23.448846: step 414540, loss = 0.16 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:40:24.322756: step 414550, loss = 0.15 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:40:25.184854: step 414560, loss = 0.15 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:40:26.081999: step 414570, loss = 0.12 (1426.7 examples/sec; 0.090 sec/batch)
2017-06-02 12:40:26.956428: step 414580, loss = 0.13 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:40:27.818255: step 414590, loss = 0.14 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:40:28.783110: step 414600, loss = 0.13 (1326.6 examples/sec; 0.096 sec/batch)
2017-06-02 12:40:29.565462: step 414610, loss = 0.13 (1636.1 examples/sec; 0.078 sec/batch)
2017-06-02 12:40:30.427998: step 414620, loss = 0.14 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:40:31.280870: step 414630, loss = 0.14 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 12:40:32.175980: step 414640, loss = 0.12 (1430.0 examples/sec; 0.090 sec/batch)
2017-06-02 12:40:33.050840: step 414650, loss = 0.12 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:40:33.910827: step 414660, loss = 0.11 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:40:34.767268: step 414670, loss = 0.13 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:40:35.654899: step 414680, loss = 0.15 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:40:36.553223: step 414690, loss = 0.14 (1424.9 examples/sec; 0.090 sec/batch)
2017-06-02 12:40:37.507932: step 414700, loss = 0.10 (1340.7 examples/sec; 0.095 sec/batch)
2017-06-02 12:40:38.308594: step 414710, loss = 0.20 (1598.7 examples/sec; 0.080 sec/batch)
2017-06-02 12:40:39.198094: step 414720, loss = 0.13 (1439.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:40:40.074692: step 414730, loss = 0.10 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:40:40.953782: step 414740, loss = 0.14 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:40:41.848947: step 414750, loss = 0.12 (1429.9 examples/sec; 0.090 sec/batch)
2017-06-02 12:40:42.751309: step 414760, loss = 0.12 (1418.5 examples/sec; 0.090 sec/batch)
2017-06-02 12:40:43.616820: step 414770, loss = 0.15 (1478.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:40:44.481419: step 414780, loss = 0.11 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:40:45.352184: step 414790, loss = 0.15 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:40:46.317658: step 414800, loss = 0.15 (1325.8 examples/sec; 0.097 sec/batch)
2017-06-02 12:40:47.072781: step 414810, loss = 0.11 (1695.1 examples/sec; 0.076 sec/batch)
2017-06-02 12:40:47.934096: step 414820, loss = 0.13 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:40:48.786303: step 414830, loss = 0.12 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:40:49.662974: step 414840, loss = 0.10 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:40:50.558480: step 414850, loss = 0.16 (1429.4 examples/sec; 0.090 sec/batch)
2017-06-02 12:40:51.438825: step 414860, loss = 0.13 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:40:52.318389: step 414870, loss = 0.15 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:40:53.184984: step 414880, loss = 0.16 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:40:54.069131: step 414890, loss = 0.13 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:40:55.051437: step 414900, loss = 0.11 (1303.0 examples/sec; 0.098 sec/batch)
2017-06-02 12:40:55.829280: step 414910, loss = 0.14 (1645.6 examples/sec; 0.078 sec/batch)
2017-06-02 12:40:56.712620: step 414920, loss = 0.14 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:40:57.602207: step 414930, loss = 0.10 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:40:58.478793: step 414940, loss = 0.13 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:40:59.381553: step 414950, loss = 0.12 (1417.9 examples/sec; 0.090 sec/batch)
2017-06-02 12:41:00.237313: step 414960, loss = 0.14 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:41:01.110024: step 414970, loss = 0.13 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:41:01.960313: step 414980, loss = 0.12 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:41:02.849283: step 414990, loss = 0.15 (1439.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:41:03.815992: step 415000, loss = 0.13 (1324.1 examples/sec; 0.097 sec/batch)
2017-06-02 12:41:04.612395: step 415010, loss = 0.13 (1607.2 examples/sec; 0.080 sec/batch)
2017-06-02 12:41:05.493902: step 415020, loss = 0.17 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:41:06.366824: step 415030, loss = 0.11 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:41:07.238724: step 415040, loss = 0.13 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:41:08.091400: step 415050, loss = 0.15 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:41:08.963527: step 415060, loss = 0.15 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:41:09.838890: step 415070, loss = 0.15 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:41:10.717319: step 415080, loss = 0.15 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:41:11.582960: step 415090, loss = 0.13 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:41:12.536315: step 415100, loss = 0.15 (1342.6 examples/sec; 0.095 sec/batch)
2017-06-02 12:41:13.324114: step 415110, loss = 0.15 (1624.8 examples/sec; 0.079 sec/batch)
2017-06-02 12:41:14.183352: step 415120, loss = 0.15 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:41:15.052758: step 415130, loss = 0.15 (1472.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:41:15.919051: step 415140, loss = 0.19 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:41:16.794606: step 415150, loss = 0.12 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:41:17.647846: step 415160, loss = 0.15 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:41:18.507016: step 415170, loss = 0.16 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:41:19.360957: step 415180, loss = 0.15 (1499.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:41:20.221643: step 415190, loss = 0.11 (1487.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:41:21.193558: step 415200, loss = 0.13 (1317.0 examples/sec; 0.097 sec/batch)
2017-06-02 12:41:21.964638: step 415210, loss = 0.17 (1660.0 examples/sec; 0.077 sec/batch)
2017-06-02 12:41:22.810876: step 415220, loss = 0.13 (1512.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:41:23.687846: step 415230, loss = 0.14 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:41:24.544589: step 415240, loss = 0.14 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:41:25.402856: step 415250, loss = 0.19 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:41:26.260414: step 415260, loss = 0.13 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:41:27.144576: step 415270, loss = 0.14 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:41:28.025039: step 415280, loss = 0.15 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:41:28.922340: step 415290, loss = 0.21 (1426.5 examples/sec; 0.090 sec/batch)
2017-06-02 12:41:29.906542: step 415300, loss = 0.14 (1300.6 examples/sec; 0.098 sec/batch)
2017-06-02 12:41:30.653959: step 415310, loss = 0.16 (1712.5 examples/sec; 0.075 sec/batch)
2017-06-02 12:41:31.527573: step 415320, loss = 0.12 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:41:32.385104: step 415330, loss = 0.17 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:41:33.241376: step 415340, loss = 0.12 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:41:34.090406: step 415350, loss = 0.11 (1507.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:41:34.994747: step 415360, loss = 0.12 (1415.4 examples/sec; 0.090 sec/batch)
2017-06-02 12:41:35.884664: step 415370, loss = 0.16 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:41:36.727661: step 415380, loss = 0.12 (1518.4 examples/sec; 0.084 sec/batch)
2017-06-02 12:41:37.582070: step 415390, loss = 0.13 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:41:38.598679: step 415400, loss = 0.14 (1259.1 examples/sec; 0.102 sec/batch)
2017-06-02 12:41:39.286369: step 415410, loss = 0.17 (1861.3 examples/sec; 0.069 sec/batch)
2017-06-02 12:41:40.118084: step 415420, loss = 0.14 (1539.0 examples/sec; 0.083 sec/batch)
2017-06-02 12:41:40.985723: step 415430, loss = 0.13 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:41:41.854923: step 415440, loss = 0.15 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:41:42.727645: step 415450, loss = 0.12 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:41:43.590448: step 415460, loss = 0.13 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:41:44.451273: step 415470, loss = 0.16 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:41:45.311642: step 415480, loss = 0.13 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:41:46.170117: step 415490, loss = 0.12 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:41:47.127555: step 415500, loss = 0.15 (1336.9 examples/sec; 0.096 sec/batch)
2017-06-02 12:41:47.904757: step 415510, loss = 0.13 (1646.9 examples/sec; 0.078 sec/batch)
2017-06-02 12:41:48.749048: step 415520, loss = 0.15 (1516.1 examples/sec; 0.084 sec/batch)
2017-06-02 12:41:49.588575: step 415530, loss = 0.18 (1524.7 examples/sec; 0.084 sec/batch)
2017-06-02 12:41:50.446763: step 415540, loss = 0.18 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:41:51.298152: step 415550, loss = 0.15 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:41:52.142098: step 415560, loss = 0.11 (1516.7 examples/sec; 0.084 sec/batch)
2017-06-02 12:41:52.995934: step 415570, loss = 0.14 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:41:53.858463: step 415580, loss = 0.15 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:41:54.721120: step 415590, loss = 0.15 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:41:55.691699: step 415600, loss = 0.13 (1318.8 examples/sec; 0.097 sec/batch)
2017-06-02 12:41:56.461583: step 415610, loss = 0.12 (1662.6 examples/sec; 0.077 sec/batch)
2017-06-02 12:41:57.313328: step 415620, loss = 0.11 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 12:41:58.169486: step 415630, loss = 0.17 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:41:59.032323: step 415640, loss = 0.12 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:41:59.889029: step 415650, loss = 0.13 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:42:00.768262: step 415660, loss = 0.13 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:42:01.649025: step 415670, loss = 0.12 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:42:02.519265: step 415680, loss = 0.13 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:42:03.396091: step 415690, loss = 0.15 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:42:04.394930: step 415700, loss = 0.13 (1281.5 examples/sec; 0.100 sec/batch)
2017-06-02 12:42:05.161562: step 415710, loss = 0.13 (1669.7 examples/sec; 0.077 sec/batch)
2017-06-02 12:42:06.035016: step 415720, loss = 0.13 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:42:06.885589: step 415730, loss = 0.12 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:42:07.759448: step 415740, loss = 0.14 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:42:08.619818: step 415750, loss = 0.12 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:42:09.479053: step 415760, loss = 0.12 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:42:10.337595: step 415770, loss = 0.18 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:42:11.201274: step 415780, loss = 0.15 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:42:12.046198: step 415790, loss = 0.15 (1514.9 examples/sec; 0.084 sec/batch)
2017-06-02 12:42:13.019292: step 415800, loss = 0.13 (1315.4 examples/sec; 0.097 sec/batch)
2017-06-02 12:42:13.781945: step 415810, loss = 0.12 (1678.4 examples/sec; 0.076 sec/batch)
2017-06-02 12:42:14.637395: step 415820, loss = 0.17 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:42:15.495958: step 415830, loss = 0.17 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:42:16.371141: step 415840, loss = 0.12 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:42:17.244997: step 415850, loss = 0.14 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:42:18.118217: step 415860, loss = 0.14 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:42:18.954830: step 415870, loss = 0.13 (1530.0 examples/sec; 0.084 sec/batch)
2017-06-02 12:42:19.829819: step 415880, loss = 0.12 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:42:20.683608: step 415890, loss = 0.11 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:42:21.672449: step 415900, loss = 0.13 (1294.4 examples/sec; 0.099 sec/batch)
2017-06-02 12:42:22.407956: step 415910, loss = 0.15 (1740.3 examples/sec; 0.074 sec/batch)
2017-06-02 12:42:23.275454: step 415920, loss = 0.14 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:42:24.130244: step 415930, loss = 0.15 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:42:24.988512: step 415940, loss = 0.15 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:42:25.868337: step 415950, loss = 0.13 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:42:26.724932: step 415960, loss = 0.15 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:42:27.614923: step 415970, loss = 0.12 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 12:42:28.476505: step 415980, loss = 0.15 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:42:29.348326: step 415990, loss = 0.16 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:42:30.310177: step 416000, loss = 0.12 (1330.8 examples/sec; 0.096 sec/batch)
2017-06-02 12:42:31.070327: step 416010, loss = 0.13 (1683.9 examples/sec; 0.076 sec/batch)
2017-06-02 12:42:31.919773: step 416020, loss = 0.13 (1506.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:42:32.798792: step 416030, loss = 0.15 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:42:33.649684: step 416040, loss = 0.12 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:42:34.522364: step 416050, loss = 0.15 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:42:35.354502: step 416060, loss = 0.15 (1538.2 examples/sec; 0.083 sec/batch)
2017-06-02 12:42:36.237316: step 416070, loss = 0.14 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:42:37.102644: step 416080, loss = 0.14 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:42:37.977234: step 416090, loss = 0.14 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:42:38.969830: step 416100, loss = 0.18 (1289.6 examples/sec; 0.099 sec/batch)
2017-06-02 12:42:39.706061: step 416110, loss = 0.12 (1738.6 examples/sec; 0.074 sec/batch)
2017-06-02 12:42:40.583720: step 416120, loss = 0.18 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:42:41.429484: step 416130, loss = 0.13 (1513.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:42:42.311528: step 416140, loss = 0.12 (1451.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:42:43.197330: step 416150, loss = 0.12 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:42:44.085311: step 416160, loss = 0.10 (1441.5 examples/sec; 0.089 sec/batch)
2017-06-02 12:42:44.959955: step 416170, loss = 0.13 (1463.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:42:45.849782: step 416180, loss = 0.17 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 12:42:46.739396: step 416190, loss = 0.20 (1438.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:42:47.730502: step 416200, loss = 0.16 (1291.5 examples/sec; 0.099 sec/batch)
2017-06-02 12:42:48.495170: step 416210, loss = 0.15 (1673.9 examples/sec; 0.076 sec/batch)
2017-06-02 12:42:49.361486: step 416220, loss = 0.21 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:42:50.225813: step 416230, loss = 0.13 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:42:51.097626: step 416240, loss = 0.13 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:42:51.969357: step 416250, loss = 0.15 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:42:52.843830: step 416260, loss = 0.11 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:42:53.722559: step 416270, loss = 0.16 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:42:54.598281: step 416280, loss = 0.15 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:42:55.469385: step 416290, loss = 0.16 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:42:56.455353: step 416300, loss = 0.15 (1298.2 examples/sec; 0.099 sec/batch)
2017-06-02 12:42:57.209676: step 416310, loss = 0.14 (1696.9 examples/sec; 0.075 sec/batch)
2017-06-02 12:42:58.072975: step 416320, loss = 0.17 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:42:58.949858: step 416330, loss = 0.14 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:42:59.814480: step 416340, loss = 0.15 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:43:00.679582: step 416350, loss = 0.15 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:43:01.571026: step 416360, loss = 0.11 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:43:02.438543: step 416370, loss = 0.11 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:43:03.282195: step 416380, loss = 0.12 (1517.2 examples/sec; 0.084 sec/batch)
2017-06-02 12:43:04.140893: step 416390, loss = 0.16 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:43:05.106360: step 416400, loss = 0.13 (1325.8 examples/sec; 0.097 sec/batch)
2017-06-02 12:43:05.865007: step 416410, loss = 0.12 (1687.2 examples/sec; 0.076 sec/batch)
2017-06-02 12:43:06.725792: step 416420, loss = 0.12 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:43:07.584991: step 416430, loss = 0.13 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:43:08.436763: step 416440, loss = 0.13 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:43:09.334807: step 416450, loss = 0.12 (1425.3 examples/sec; 0.090 sec/batch)
2017-06-02 12:43:10.179673: step 416460, loss = 0.14 (1515.1 examples/sec; 0.084 sec/batch)
2017-06-02 12:43:11.030267: step 416470, loss = 0.14 (1504.8 examples/sec; 0.085 sec/batch)
2017-06-02 12:43:11.898442: step 416480, loss = 0.12 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:43:12.742921: step 416490, loss = 0.15 (1515.7 examples/sec; 0.084 sec/batch)
2017-06-02 12:43:13.706423: step 416500, loss = 0.11 (1328.5 examples/sec; 0.096 sec/batch)
2017-06-02 12:43:14.489567: step 416510, loss = 0.13 (1634.4 examples/sec; 0.078 sec/batch)
2017-06-02 12:43:15.348384: step 416520, loss = 0.15 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:43:16.210541: step 416530, loss = 0.10 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:43:17.083323: step 416540, loss = 0.14 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:43:17.946605: step 416550, loss = 0.13 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:43:18.811062: step 416560, loss = 0.12 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:43:19.662032: step 416570, loss = 0.13 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:43:20.530799: step 416580, loss = 0.11 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:43:21.375822: step 416590, loss = 0.12 (1514.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:43:22.366527: step 416600, loss = 0.16 (1292.0 examples/sec; 0.099 sec/batch)
2017-06-02 12:43:23.103476: step 416610, loss = 0.14 (1736.9 examples/sec; 0.074 sec/batch)
2017-06-02 12:43:23.973997: step 416620, loss = 0.15 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:43:24.800858: step 416630, loss = 0.13 (1548.0 examples/sec; 0.083 sec/batch)
2017-06-02 12:43:25.636387: step 416640, loss = 0.15 (1532.0 examples/sec; 0.084 sec/batch)
2017-06-02 12:43:26.485188: step 416650, loss = 0.12 (1508.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:43:27.366284: step 416660, loss = 0.13 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:43:28.240162: step 416670, loss = 0.12 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:43:29.091028: step 416680, loss = 0.11 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:43:29.963109: step 416690, loss = 0.12 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:43:30.952671: step 416700, loss = 0.14 (1293.5 examples/sec; 0.099 sec/batch)
2017-06-02 12:43:31.723248: step 416710, loss = 0.16 (1661.1 examples/sec; 0.077 sec/batch)
2017-06-02 12:43:32.572697: step 416720, loss = 0.16 (1506.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:43:33.407928: step 416730, loss = 0.16 (1532.5 examples/sec; 0.084 sec/batch)
2017-06-02 12:43:34.253002: step 416740, loss = 0.14 (1514.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:43:35.120340: step 416750, loss = 0.15 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:43:35.959893: step 416760, loss = 0.13 (1524.6 examples/sec; 0.084 sec/batch)
2017-06-02 12:43:36.815025: step 416770, loss = 0.16 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:43:37.700006: step 416780, loss = 0.13 (1446.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:43:38.585828: step 416790, loss = 0.12 (1445.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:43:39.563484: step 416800, loss = 0.18 (1309.2 examples/sec; 0.098 sec/batch)
2017-06-02 12:43:40.324821: step 416810, loss = 0.15 (1681.3 examples/sec; 0.076 sec/batch)
2017-06-02 12:43:41.190078: step 416820, loss = 0.17 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:43:42.041079: step 416830, loss = 0.13 (1504.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:43:42.900246: step 416840, loss = 0.12 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:43:43.752998: step 416850, loss = 0.13 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:43:44.611833: step 416860, loss = 0.14 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:43:45.470398: step 416870, loss = 0.13 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:43:46.343492: step 416880, loss = 0.16 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:43:47.203086: step 416890, loss = 0.13 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:43:48.163390: step 416900, loss = 0.12 (1332.9 examples/sec; 0.096 sec/batch)
2017-06-02 12:43:48.936164: step 416910, loss = 0.19 (1656.4 examples/sec; 0.077 sec/batch)
2017-06-02 12:43:49.797125: step 416920, loss = 0.15 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:43:50.644080: step 416930, loss = 0.13 (1511.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:43:51.522143: step 416940, loss = 0.13 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:43:52.394019: step 416950, loss = 0.12 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:43:53.254795: step 416960, loss = 0.18 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:43:54.122353: step 416970, loss = 0.13 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:43:54.995424: step 416980, loss = 0.13 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:43:55.858395: step 416990, loss = 0.12 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:43:56.821755: step 417000, loss = 0.13 (1328.7 examples/sec; 0.096 sec/batch)
2017-06-02 12:43:57.588842: step 417010, loss = 0.16 (1668.7 examples/sec; 0.077 sec/batch)
2017-06-02 12:43:58.435176: step 417020, loss = 0.12 (1512.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:43:59.278637: step 417030, loss = 0.11 (1517.6 examples/sec; 0.084 sec/batch)
2017-06-02 12:44:00.141705: step 417040, loss = 0.12 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:44:01.007692: step 417050, loss = 0.12 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:44:01.874453: step 417060, loss = 0.19 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:44:02.717601: step 417070, loss = 0.13 (1518.1 examples/sec; 0.084 sec/batch)
2017-06-02 12:44:03.573540: step 417080, loss = 0.14 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:44:04.454973: step 417090, loss = 0.13 (1452.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:44:05.456307: step 417100, loss = 0.14 (1278.3 examples/sec; 0.100 sec/batch)
2017-06-02 12:44:06.192889: step 417110, loss = 0.13 (1737.8 examples/sec; 0.074 sec/batch)
2017-06-02 12:44:07.039517: step 417120, loss = 0.14 (1511.8 examples/sec; 0.085 sec/batch)
2017-06-02 12:44:07.896281: step 417130, loss = 0.12 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:44:08.772131: step 417140, loss = 0.15 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:44:09.630022: step 417150, loss = 0.14 (1492.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:44:10.493481: step 417160, loss = 0.14 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:44:11.369875: step 417170, loss = 0.14 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:44:12.254524: step 417180, loss = 0.15 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:44:13.095230: step 417190, loss = 0.15 (1522.5 examples/sec; 0.084 sec/batch)
2017-06-02 12:44:14.033746: step 417200, loss = 0.12 (1363.9 examples/sec; 0.094 sec/batch)
2017-06-02 12:44:14.807912: step 417210, loss = 0.12 (1653.4 examples/sec; 0.077 sec/batch)
2017-06-02 12:44:15.675628: step 417220, loss = 0.14 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:44:16.522599: step 417230, loss = 0.16 (1511.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:44:17.394207: step 417240, loss = 0.14 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:44:18.240959: step 417250, loss = 0.20 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:44:19.086322: step 417260, loss = 0.13 (1514.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:44:19.932205: step 417270, loss = 0.12 (1513.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:44:20.787860: step 417280, loss = 0.14 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:44:21.624211: step 417290, loss = 0.11 (1530.5 examples/sec; 0.084 sec/batch)
2017-06-02 12:44:22.592526: step 417300, loss = 0.15 (1321.9 examples/sec; 0.097 sec/batch)
2017-06-02 12:44:23.352819: step 417310, loss = 0.12 (1683.6 examples/sec; 0.076 sec/batch)
2017-06-02 12:44:24.230981: step 417320, loss = 0.13 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:44:25.076549: step 417330, loss = 0.18 (1513.8 examples/sec; 0.085 sec/batch)
2017-06-02 12:44:25.951748: step 417340, loss = 0.15 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:44:26.815401: step 417350, loss = 0.14 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:44:27.664321: step 417360, loss = 0.10 (1507.8 examples/sec; 0.085 sec/batch)
2017-06-02 12:44:28.516250: step 417370, loss = 0.13 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:44:29.363359: step 417380, loss = 0.10 (1511.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:44:30.207482: step 417390, loss = 0.12 (1516.4 examples/sec; 0.084 sec/batch)
2017-06-02 12:44:31.177336: step 417400, loss = 0.16 (1319.8 examples/sec; 0.097 sec/batch)
2017-06-02 12:44:31.944896: step 417410, loss = 0.16 (1667.6 examples/sec; 0.077 sec/batch)
2017-06-02 12:44:32.831165: step 417420, loss = 0.17 (1444.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:44:33.710468: step 417430, loss = 0.14 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:44:34.558799: step 417440, loss = 0.14 (1508.8 examples/sec; 0.085 sec/batch)
2017-06-02 12:44:35.428948: step 417450, loss = 0.14 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:44:36.281342: step 417460, loss = 0.12 (1501.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:44:37.149765: step 417470, loss = 0.14 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:44:38.004406: step 417480, loss = 0.14 (1497.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:44:38.850476: step 417490, loss = 0.14 (1512.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:44:39.820229: step 417500, loss = 0.14 (1319.9 examples/sec; 0.097 sec/batch)
2017-06-02 12:44:40.557671: step 417510, loss = 0.13 (1735.8 examples/sec; 0.074 sec/batch)
2017-06-02 12:44:41.434839: step 417520, loss = 0.14 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:44:42.284649: step 417530, loss = 0.12 (1506.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:44:43.143386: step 417540, loss = 0.16 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:44:44.003401: step 417550, loss = 0.13 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:44:44.853789: step 417560, loss = 0.15 (1505.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:44:45.711002: step 417570, loss = 0.11 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:44:46.591217: step 417580, loss = 0.14 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:44:47.458218: step 417590, loss = 0.15 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:44:48.450663: step 417600, loss = 0.14 (1289.8 examples/sec; 0.099 sec/batch)
2017-06-02 12:44:49.194268: step 417610, loss = 0.17 (1721.3 examples/sec; 0.074 sec/batch)
2017-06-02 12:44:50.055991: step 417620, loss = 0.13 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:44:50.911414: step 417630, loss = 0.13 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:44:51.792501: step 417640, loss = 0.14 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:44:52.651845: step 417650, loss = 0.12 (1489.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:44:53.533297: step 417660, loss = 0.14 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:44:54.383958: step 417670, loss = 0.14 (1504.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:44:55.220257: step 417680, loss = 0.18 (1530.6 examples/sec; 0.084 sec/batch)
2017-06-02 12:44:56.075225: step 417690, loss = 0.14 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:44:57.043279: step 417700, loss = 0.12 (1322.2 examples/sec; 0.097 sec/batch)
2017-06-02 12:44:57.810185: step 417710, loss = 0.13 (1669.1 examples/sec; 0.077 sec/batch)
2017-06-02 12:44:58.651965: step 417720, loss = 0.12 (1520.6 examples/sec; 0.084 sec/batch)
2017-06-02 12:44:59.515601: step 417730, loss = 0.14 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:45:00.376251: step 417740, loss = 0.14 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:45:01.225545: step 417750, loss = 0.16 (1507.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:45:02.090274: step 417760, loss = 0.12 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:45:02.938609: step 417770, loss = 0.14 (1508.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:45:03.784404: step 417780, loss = 0.15 (1513.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:45:04.657841: step 417790, loss = 0.15 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:45:05.629106: step 417800, loss = 0.14 (1317.9 examples/sec; 0.097 sec/batch)
2017-06-02 12:45:06.399043: step 417810, loss = 0.12 (1662.5 examples/sec; 0.077 sec/batch)
2017-06-02 12:45:07.267999: step 417820, loss = 0.12 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:45:08.123122: step 417830, loss = 0.13 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:45:08.997162: step 417840, loss = 0.13 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:45:09.850045: step 417850, loss = 0.20 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 12:45:10.713865: step 417860, loss = 0.13 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:45:11.559686: step 417870, loss = 0.23 (1513.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:45:12.419343: step 417880, loss = 0.15 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:45:13.291757: step 417890, loss = 0.16 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:45:14.253814: step 417900, loss = 0.13 (1330.5 examples/sec; 0.096 sec/batch)
2017-06-02 12:45:15.015897: step 417910, loss = 0.16 (1679.6 examples/sec; 0.076 sec/batch)
2017-06-02 12:45:15.861595: step 417920, loss = 0.18 (1513.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:45:16.733936: step 417930, loss = 0.13 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:45:17.596253: step 417940, loss = 0.13 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:45:18.455335: step 417950, loss = 0.13 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:45:19.334684: step 417960, loss = 0.14 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:45:20.182508: step 417970, loss = 0.12 (1509.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:45:21.055086: step 417980, loss = 0.14 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:45:21.897547: step 417990, loss = 0.15 (1519.4 examples/sec; 0.084 sec/batch)
2017-06-02 12:45:22.821952: step 418000, loss = 0.14 (1384.7 examples/sec; 0.092 sec/batch)
2017-06-02 12:45:23.580216: step 418010, loss = 0.15 (1688.1 examples/sec; 0.076 sec/batch)
2017-06-02 12:45:24.419261: step 418020, loss = 0.11 (1525.5 examples/sec; 0.084 sec/batch)
2017-06-02 12:45:25.266951: step 418030, loss = 0.17 (1510.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:45:26.125873: step 418040, loss = 0.11 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:45:26.986186: step 418050, loss = 0.11 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:45:27.846163: step 418060, loss = 0.12 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:45:28.682901: step 418070, loss = 0.15 (1529.7 examples/sec; 0.084 sec/batch)
2017-06-02 12:45:29.540400: step 418080, loss = 0.14 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:45:30.421873: step 418090, loss = 0.17 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:45:31.393240: step 418100, loss = 0.19 (1317.7 examples/sec; 0.097 sec/batch)
2017-06-02 12:45:32.157232: step 418110, loss = 0.12 (1675.4 examples/sec; 0.076 sec/batch)
2017-06-02 12:45:32.991666: step 418120, loss = 0.10 (1534.0 examples/sec; 0.083 sec/batch)
2017-06-02 12:45:33.835213: step 418130, loss = 0.13 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 12:45:34.704383: step 418140, loss = 0.12 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:45:35.566488: step 418150, loss = 0.14 (1484.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:45:36.441200: step 418160, loss = 0.12 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:45:37.294392: step 418170, loss = 0.18 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:45:38.139138: step 418180, loss = 0.16 (1515.3 examples/sec; 0.084 sec/batch)
2017-06-02 12:45:39.016302: step 418190, loss = 0.14 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:45:39.988568: step 418200, loss = 0.15 (1316.5 examples/sec; 0.097 sec/batch)
2017-06-02 12:45:40.764207: step 418210, loss = 0.14 (1650.2 examples/sec; 0.078 sec/batch)
2017-06-02 12:45:41.616071: step 418220, loss = 0.12 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:45:42.505882: step 418230, loss = 0.14 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 12:45:43.379147: step 418240, loss = 0.12 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:45:44.241064: step 418250, loss = 0.14 (1485.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:45:45.105063: step 418260, loss = 0.14 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:45:45.980629: step 418270, loss = 0.13 (1461.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:45:46.839551: step 418280, loss = 0.18 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:45:47.719784: step 418290, loss = 0.11 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:45:48.704015: step 418300, loss = 0.12 (1300.5 examples/sec; 0.098 sec/batch)
2017-06-02 12:45:49.476516: step 418310, loss = 0.10 (1657.0 examples/sec; 0.077 sec/batch)
2017-06-02 12:45:50.356646: step 418320, loss = 0.12 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:45:51.191266: step 418330, loss = 0.11 (1533.6 examples/sec; 0.083 sec/batch)
2017-06-02 12:45:52.074315: step 418340, loss = 0.14 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:45:52.933840: step 418350, loss = 0.17 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:45:53.804677: step 418360, loss = 0.20 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:45:54.676544: step 418370, loss = 0.21 (1468.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:45:55.518589: step 418380, loss = 0.17 (1520.1 examples/sec; 0.084 sec/batch)
2017-06-02 12:45:56.357479: step 418390, loss = 0.13 (1525.8 examples/sec; 0.084 sec/batch)
2017-06-02 12:45:57.343208: step 418400, loss = 0.14 (1298.5 examples/sec; 0.099 sec/batch)
2017-06-02 12:45:58.119013: step 418410, loss = 0.14 (1649.9 examples/sec; 0.078 sec/batch)
2017-06-02 12:45:58.994880: step 418420, loss = 0.12 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:45:59.868575: step 418430, loss = 0.15 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:46:00.743997: step 418440, loss = 0.15 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:46:01.632211: step 418450, loss = 0.14 (1441.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:46:02.480190: step 418460, loss = 0.12 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:46:03.326953: step 418470, loss = 0.17 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:46:04.193162: step 418480, loss = 0.14 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:46:05.062419: step 418490, loss = 0.12 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:46:06.005825: step 418500, loss = 0.18 (1356.8 examples/sec; 0.094 sec/batch)
2017-06-02 12:46:06.777048: step 418510, loss = 0.12 (1659.7 examples/sec; 0.077 sec/batch)
2017-06-02 12:46:07.635650: step 418520, loss = 0.17 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:46:08.490192: step 418530, loss = 0.10 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:46:09.339575: step 418540, loss = 0.15 (1507.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:46:10.226362: step 418550, loss = 0.13 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:46:11.105496: step 418560, loss = 0.12 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:46:11.978717: step 418570, loss = 0.12 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:46:12.883003: step 418580, loss = 0.13 (1415.5 examples/sec; 0.090 sec/batch)
2017-06-02 12:46:13.754593: step 418590, loss = 0.13 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:46:14.732773: step 418600, loss = 0.14 (1308.6 examples/sec; 0.098 sec/batch)
2017-06-02 12:46:15.512776: step 418610, loss = 0.11 (1641.1 examples/sec; 0.078 sec/batch)
2017-06-02 12:46:16.384475: step 418620, loss = 0.11 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:46:17.267662: step 418630, loss = 0.14 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:46:18.142668: step 418640, loss = 0.17 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:46:19.021753: step 418650, loss = 0.15 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:46:19.897499: step 418660, loss = 0.10 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:46:20.764302: step 418670, loss = 0.15 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:46:21.656332: step 418680, loss = 0.12 (1434.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:46:22.544782: step 418690, loss = 0.12 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:46:23.535984: step 418700, loss = 0.11 (1291.4 examples/sec; 0.099 sec/batch)
2017-06-02 12:46:24.324750: step 418710, loss = 0.14 (1622.8 examples/sec; 0.079 sec/batch)
2017-06-02 12:46:25.213155: step 418720, loss = 0.13 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:46:26.096539: step 418730, loss = 0.11 (1449.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:46:26.978115: step 418740, loss = 0.14 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:46:27.863479: step 418750, loss = 0.14 (1445.8 examples/sec; 0.089 sec/batch)
2017-06-02 12:46:28.751594: step 418760, loss = 0.12 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 12:46:29.606370: step 418770, loss = 0.13 (1497.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:46:30.472033: step 418780, loss = 0.15 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:46:31.340892: step 418790, loss = 0.18 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:46:32.342447: step 418800, loss = 0.13 (1278.0 examples/sec; 0.100 sec/batch)
2017-06-02 12:46:33.113790: step 418810, loss = 0.13 (1659.4 examples/sec; 0.077 sec/batch)
2017-06-02 12:46:33.990259: step 418820, loss = 0.14 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:46:34.843361: step 418830, loss = 0.16 (1500.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:46:35.703811: step 418840, loss = 0.16 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:46:36.585414: step 418850, loss = 0.11 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:46:37.480806: step 418860, loss = 0.15 (1429.5 examples/sec; 0.090 sec/batch)
2017-06-02 12:46:38.359304: step 418870, loss = 0.12 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:46:39.255162: step 418880, loss = 0.15 (1428.8 examples/sec; 0.090 sec/batch)
2017-06-02 12:46:40.139156: step 418890, loss = 0.16 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:46:41.104174: step 418900, loss = 0.14 (1326.4 examples/sec; 0.097 sec/batch)
2017-06-02 12:46:41.903872: step 418910, loss = 0.12 (1600.6 examples/sec; 0.080 sec/batch)
2017-06-02 12:46:42.791778: step 418920, loss = 0.12 (1441.6 examples/sec; 0.089 sec/batch)
2017-06-02 12:46:43.654885: step 418930, loss = 0.14 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:46:44.531692: step 418940, loss = 0.14 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:46:45.395810: step 418950, loss = 0.12 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:46:46.260466: step 418960, loss = 0.13 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:46:47.144396: step 418970, loss = 0.14 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:46:48.031305: step 418980, loss = 0.15 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 12:46:48.897746: step 418990, loss = 0.12 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:46:49.895045: step 419000, loss = 0.13 (1283.5 examples/sec; 0.100 sec/batch)
2017-06-02 12:46:50.663130: step 419010, loss = 0.11 (1666.5 examples/sec; 0.077 sec/batch)
2017-06-02 12:46:51.525796: step 419020, loss = 0.11 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:46:52.422468: step 419030, loss = 0.13 (1427.5 examples/sec; 0.090 sec/batch)
2017-06-02 12:46:53.298120: step 419040, loss = 0.15 (1461.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:46:54.176923: step 419050, loss = 0.14 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:46:55.061029: step 419060, loss = 0.17 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:46:55.937476: step 419070, loss = 0.13 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:46:56.805314: step 419080, loss = 0.12 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:46:57.654442: step 419090, loss = 0.12 (1507.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:46:58.616969: step 419100, loss = 0.12 (1329.8 examples/sec; 0.096 sec/batch)
2017-06-02 12:46:59.387765: step 419110, loss = 0.14 (1660.6 examples/sec; 0.077 sec/batch)
2017-06-02 12:47:00.257622: step 419120, loss = 0.19 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:47:01.118820: step 419130, loss = 0.12 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:47:01.956227: step 419140, loss = 0.11 (1528.5 examples/sec; 0.084 sec/batch)
2017-06-02 12:47:02.826342: step 419150, loss = 0.11 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:47:03.691026: step 419160, loss = 0.15 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:47:04.548266: step 419170, loss = 0.13 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:47:05.423428: step 419180, loss = 0.14 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:47:06.291865: step 419190, loss = 0.14 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:47:07.303390: step 419200, loss = 0.16 (1265.4 examples/sec; 0.101 sec/batch)
2017-06-02 12:47:08.026339: step 419210, loss = 0.16 (1770.5 examples/sec; 0.072 sec/batch)
2017-06-02 12:47:08.896227: step 419220, loss = 0.16 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:47:09.736525: step 419230, loss = 0.11 (1523.3 examples/sec; 0.084 sec/batch)
2017-06-02 12:47:10.610936: step 419240, loss = 0.12 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:47:11.518037: step 419250, loss = 0.11 (1411.1 examples/sec; 0.091 sec/batch)
2017-06-02 12:47:12.395602: step 419260, loss = 0.11 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:47:13.281344: step 419270, loss = 0.13 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:47:14.168820: step 419280, loss = 0.16 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:47:15.015305: step 419290, loss = 0.18 (1512.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:47:16.010832: step 419300, loss = 0.11 (1285.7 examples/sec; 0.100 sec/batch)
2017-06-02 12:47:16.742660: step 419310, loss = 0.13 (1749.0 examples/sec; 0.073 sec/batch)
2017-06-02 12:47:17.583130: step 419320, loss = 0.13 (1523.0 examples/sec; 0.084 sec/batch)
2017-06-02 12:47:18.446550: step 419330, loss = 0.12 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:47:19.300912: step 419340, loss = 0.12 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:47:20.141183: step 419350, loss = 0.12 (1523.3 examples/sec; 0.084 sec/batch)
2017-06-02 12:47:21.004300: step 419360, loss = 0.13 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:47:21.857897: step 419370, loss = 0.10 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:47:22.702997: step 419380, loss = 0.10 (1514.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:47:23.531438: step 419390, loss = 0.15 (1545.1 examples/sec; 0.083 sec/batch)
2017-06-02 12:47:24.513582: step 419400, loss = 0.12 (1303.3 examples/sec; 0.098 sec/batch)
2017-06-02 12:47:25.254344: step 419410, loss = 0.13 (1728.0 examples/sec; 0.074 sec/batch)
2017-06-02 12:47:26.122820: step 419420, loss = 0.12 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:47:26.997088: step 419430, loss = 0.14 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:47:27.852278: step 419440, loss = 0.12 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:47:28.734018: step 419450, loss = 0.12 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:47:29.595875: step 419460, loss = 0.17 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:47:30.456233: step 419470, loss = 0.12 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:47:31.325492: step 419480, loss = 0.14 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:47:32.192240: step 419490, loss = 0.12 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:47:33.166635: step 419500, loss = 0.14 (1313.6 examples/sec; 0.097 sec/batch)
2017-06-02 12:47:33.938916: step 419510, loss = 0.13 (1657.4 examples/sec; 0.077 sec/batch)
2017-06-02 12:47:34.793947: step 419520, loss = 0.16 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:47:35.662691: step 419530, loss = 0.15 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:47:36.533726: step 419540, loss = 0.13 (1469.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:47:37.381798: step 419550, loss = 0.15 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:47:38.255928: step 419560, loss = 0.12 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:47:39.114515: step 419570, loss = 0.12 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:47:39.997040: step 419580, loss = 0.14 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:47:40.871135: step 419590, loss = 0.19 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:47:41.836705: step 419600, loss = 0.15 (1325.6 examples/sec; 0.097 sec/batch)
2017-06-02 12:47:42.610742: step 419610, loss = 0.16 (1653.7 examples/sec; 0.077 sec/batch)
2017-06-02 12:47:43.461582: step 419620, loss = 0.15 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:47:44.316843: step 419630, loss = 0.13 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:47:45.200723: step 419640, loss = 0.11 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:47:46.046856: step 419650, loss = 0.15 (1512.8 examples/sec; 0.085 sec/batch)
2017-06-02 12:47:46.887850: step 419660, loss = 0.14 (1522.0 examples/sec; 0.084 sec/batch)
2017-06-02 12:47:47.728393: step 419670, loss = 0.12 (1522.8 examples/sec; 0.084 sec/batch)
2017-06-02 12:47:48.595290: step 419680, loss = 0.14 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:47:49.447485: step 419690, loss = 0.15 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:47:50.403108: step 419700, loss = 0.15 (1339.4 examples/sec; 0.096 sec/batch)
2017-06-02 12:47:51.169241: step 419710, loss = 0.14 (1670.7 examples/sec; 0.077 sec/batch)
2017-06-02 12:47:52.030439: step 419720, loss = 0.10 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:47:52.893275: step 419730, loss = 0.13 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:47:53.745888: step 419740, loss = 0.17 (1501.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:47:54.585156: step 419750, loss = 0.13 (1525.1 examples/sec; 0.084 sec/batch)
2017-06-02 12:47:55.436064: step 419760, loss = 0.12 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:47:56.281900: step 419770, loss = 0.17 (1513.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:47:57.145337: step 419780, loss = 0.16 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:47:58.011298: step 419790, loss = 0.12 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:47:58.967434: step 419800, loss = 0.13 (1338.7 examples/sec; 0.096 sec/batch)
2017-06-02 12:47:59.750367: step 419810, loss = 0.14 (1634.9 examples/sec; 0.078 sec/batch)
2017-06-02 12:48:00.612712: step 419820, loss = 0.18 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:48:01.500075: step 419830, loss = 0.13 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 12:48:02.367055: step 419840, loss = 0.16 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:48:03.230811: step 419850, loss = 0.12 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:48:04.080370: step 419860, loss = 0.15 (1506.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:48:04.934034: step 419870, loss = 0.11 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:48:05.791410: step 419880, loss = 0.21 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:48:06.647331: step 419890, loss = 0.14 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:48:07.649995: step 419900, loss = 0.16 (1276.6 examples/sec; 0.100 sec/batch)
2017-06-02 12:48:08.351675: step 419910, loss = 0.11 (1824.2 examples/sec; 0.070 sec/batch)
2017-06-02 12:48:09.201027: step 419920, loss = 0.13 (1507.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:48:10.052923: step 419930, loss = 0.12 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:48:10.892314: step 419940, loss = 0.12 (1524.9 examples/sec; 0.084 sec/batch)
2017-06-02 12:48:11.746631: step 419950, loss = 0.16 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:48:12.594683: step 419960, loss = 0.22 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:48:13.470523: step 419970, loss = 0.14 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:48:14.314026: step 419980, loss = 0.16 (1517.5 examples/sec; 0.084 sec/batch)
2017-06-02 12:48:15.157888: step 419990, loss = 0.12 (1516.9 examples/sec; 0.084 sec/batch)
2017-06-02 12:48:16.135695: step 420000, loss = 0.14 (1309.0 examples/sec; 0.098 sec/batch)
2017-06-02 12:48:16.895901: step 420010, loss = 0.14 (1683.8 examples/sec; 0.076 sec/batch)
2017-06-02 12:48:17.752949: step 420020, loss = 0.12 (1493.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:48:18.599416: step 420030, loss = 0.15 (1512.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:48:19.462969: step 420040, loss = 0.11 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:48:20.306240: step 420050, loss = 0.17 (1517.9 examples/sec; 0.084 sec/batch)
2017-06-02 12:48:21.172631: step 420060, loss = 0.13 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:48:22.003795: step 420070, loss = 0.11 (1540.0 examples/sec; 0.083 sec/batch)
2017-06-02 12:48:22.857968: step 420080, loss = 0.12 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:48:23.705934: step 420090, loss = 0.14 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:48:24.664799: step 420100, loss = 0.16 (1334.9 examples/sec; 0.096 sec/batch)
2017-06-02 12:48:25.449335: step 420110, loss = 0.17 (1631.5 examples/sec; 0.078 sec/batch)
2017-06-02 12:48:26.315314: step 420120, loss = 0.11 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:48:27.182146: step 420130, loss = 0.14 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:48:28.070961: step 420140, loss = 0.12 (1440.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:48:28.944020: step 420150, loss = 0.11 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:48:29.822592: step 420160, loss = 0.13 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:48:30.687717: step 420170, loss = 0.15 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:48:31.591935: step 420180, loss = 0.12 (1415.6 examples/sec; 0.090 sec/batch)
2017-06-02 12:48:32.490633: step 420190, loss = 0.13 (1424.3 examples/sec; 0.090 sec/batch)
2017-06-02 12:48:33.474378: step 420200, loss = 0.10 (1301.1 examples/sec; 0.098 sec/batch)
2017-06-02 12:48:34.248312: step 420210, loss = 0.15 (1653.9 examples/sec; 0.077 sec/batch)
2017-06-02 12:48:35.117358: step 420220, loss = 0.11 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:48:35.991902: step 420230, loss = 0.15 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:48:36.859101: step 420240, loss = 0.13 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:48:37.746692: step 420250, loss = 0.13 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:48:38.627166: step 420260, loss = 0.13 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:48:39.476365: step 420270, loss = 0.13 (1507.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:48:40.343121: step 420280, loss = 0.12 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:48:41.190828: step 420290, loss = 0.14 (1510.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:48:42.155913: step 420300, loss = 0.16 (1326.3 examples/sec; 0.097 sec/batch)
2017-06-02 12:48:42.908312: step 420310, loss = 0.15 (1701.2 examples/sec; 0.075 sec/batch)
2017-06-02 12:48:43.789141: step 420320, loss = 0.14 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:48:44.668679: step 420330, loss = 0.18 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:48:45.549692: step 420340, loss = 0.15 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:48:46.422223: step 420350, loss = 0.18 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:48:47.272577: step 420360, loss = 0.11 (1505.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:48:48.127887: step 420370, loss = 0.15 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:48:49.005270: step 420380, loss = 0.12 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:48:49.870105: step 420390, loss = 0.17 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:48:50.872258: step 420400, loss = 0.11 (1277.3 examples/sec; 0.100 sec/batch)
2017-06-02 12:48:51.610825: step 420410, loss = 0.17 (1733.1 examples/sec; 0.074 sec/batch)
2017-06-02 12:48:52.470516: step 420420, loss = 0.12 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:48:53.303084: step 420430, loss = 0.13 (1537.4 examples/sec; 0.083 sec/batch)
2017-06-02 12:48:54.167448: step 420440, loss = 0.12 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:48:55.034622: step 420450, loss = 0.14 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:48:55.883745: step 420460, loss = 0.14 (1507.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:48:56.733506: step 420470, loss = 0.14 (1506.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:48:57.617293: step 420480, loss = 0.13 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:48:58.489491: step 420490, loss = 0.13 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:48:59.457650: step 420500, loss = 0.19 (1322.1 examples/sec; 0.097 sec/batch)
2017-06-02 12:49:00.215308: step 420510, loss = 0.15 (1689.4 examples/sec; 0.076 sec/batch)
2017-06-02 12:49:01.097751: step 420520, loss = 0.18 (1450.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:49:01.960696: step 420530, loss = 0.15 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:49:02.818521: step 420540, loss = 0.15 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:49:03.700403: step 420550, loss = 0.13 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:49:04.583147: step 420560, loss = 0.15 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:49:05.425351: step 420570, loss = 0.14 (1519.8 examples/sec; 0.084 sec/batch)
2017-06-02 12:49:06.255470: step 420580, loss = 0.11 (1541.9 examples/sec; 0.083 sec/batch)
2017-06-02 12:49:07.138592: step 420590, loss = 0.13 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:49:08.086368: step 420600, loss = 0.15 (1350.5 examples/sec; 0.095 sec/batch)
2017-06-02 12:49:08.834442: step 420610, loss = 0.15 (1711.1 examples/sec; 0.075 sec/batch)
2017-06-02 12:49:09.727859: step 420620, loss = 0.22 (1432.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:49:10.573683: step 420630, loss = 0.14 (1513.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:49:11.431963: step 420640, loss = 0.12 (1491.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:49:12.291129: step 420650, loss = 0.15 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:49:13.141250: step 420660, loss = 0.14 (1505.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:49:13.997220: step 420670, loss = 0.11 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:49:14.838705: step 420680, loss = 0.15 (1521.1 examples/sec; 0.084 sec/batch)
2017-06-02 12:49:15.714576: step 420690, loss = 0.14 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:49:16.662497: step 420700, loss = 0.15 (1350.3 examples/sec; 0.095 sec/batch)
2017-06-02 12:49:17.411131: step 420710, loss = 0.13 (1709.8 examples/sec; 0.075 sec/batch)
2017-06-02 12:49:18.238861: step 420720, loss = 0.13 (1546.4 examples/sec; 0.083 sec/batch)
2017-06-02 12:49:19.091679: step 420730, loss = 0.13 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:49:19.944817: step 420740, loss = 0.13 (1500.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:49:20.802007: step 420750, loss = 0.17 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:49:21.667103: step 420760, loss = 0.11 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:49:22.505302: step 420770, loss = 0.12 (1527.1 examples/sec; 0.084 sec/batch)
2017-06-02 12:49:23.346686: step 420780, loss = 0.11 (1521.3 examples/sec; 0.084 sec/batch)
2017-06-02 12:49:24.226004: step 420790, loss = 0.15 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:49:25.202438: step 420800, loss = 0.16 (1310.9 examples/sec; 0.098 sec/batch)
2017-06-02 12:49:25.978125: step 420810, loss = 0.13 (1650.1 examples/sec; 0.078 sec/batch)
2017-06-02 12:49:26.852105: step 420820, loss = 0.22 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:49:27.713848: step 420830, loss = 0.14 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:49:28.563983: step 420840, loss = 0.10 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:49:29.429511: step 420850, loss = 0.15 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:49:30.274861: step 420860, loss = 0.12 (1514.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:49:31.122692: step 420870, loss = 0.12 (1509.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:49:31.974265: step 420880, loss = 0.14 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:49:32.829593: step 420890, loss = 0.13 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:49:33.792497: step 420900, loss = 0.12 (1329.3 examples/sec; 0.096 sec/batch)
2017-06-02 12:49:34.546274: step 420910, loss = 0.14 (1698.1 examples/sec; 0.075 sec/batch)
2017-06-02 12:49:35.409473: step 420920, loss = 0.16 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:49:36.290404: step 420930, loss = 0.14 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:49:37.139071: step 420940, loss = 0.13 (1508.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:49:37.997478: step 420950, loss = 0.13 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:49:38.832573: step 420960, loss = 0.13 (1532.7 examples/sec; 0.084 sec/batch)
2017-06-02 12:49:39.690797: step 420970, loss = 0.12 (1491.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:49:40.556813: step 420980, loss = 0.15 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:49:41.429778: step 420990, loss = 0.13 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:49:42.378336: step 421000, loss = 0.12 (1349.4 examples/sec; 0.095 sec/batch)
2017-06-02 12:49:43.147278: step 421010, loss = 0.11 (1664.6 examples/sec; 0.077 sec/batch)
2017-06-02 12:49:44.015429: step 421020, loss = 0.13 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:49:44.890024: step 421030, loss = 0.14 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:49:45.753513: step 421040, loss = 0.13 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:49:46.617176: step 421050, loss = 0.12 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:49:47.497149: step 421060, loss = 0.13 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:49:48.346807: step 421070, loss = 0.11 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:49:49.247557: step 421080, loss = 0.13 (1421.0 examples/sec; 0.090 sec/batch)
2017-06-02 12:49:50.127318: step 421090, loss = 0.12 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:49:51.111539: step 421100, loss = 0.13 (1300.5 examples/sec; 0.098 sec/batch)
2017-06-02 12:49:51.873095: step 421110, loss = 0.15 (1680.8 examples/sec; 0.076 sec/batch)
2017-06-02 12:49:52.732397: step 421120, loss = 0.15 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:49:53.693430: step 421130, loss = 0.23 (1331.9 examples/sec; 0.096 sec/batch)
2017-06-02 12:49:54.572219: step 421140, loss = 0.15 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:49:55.463618: step 421150, loss = 0.13 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:49:56.347677: step 421160, loss = 0.15 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:49:57.214063: step 421170, loss = 0.13 (1477.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:49:58.091108: step 421180, loss = 0.17 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:49:58.973872: step 421190, loss = 0.15 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:49:59.956120: step 421200, loss = 0.17 (1303.1 examples/sec; 0.098 sec/batch)
2017-06-02 12:50:00.745803: step 421210, loss = 0.17 (1620.9 examples/sec; 0.079 sec/batch)
2017-06-02 12:50:01.625935: step 421220, loss = 0.15 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:50:02.495687: step 421230, loss = 0.13 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:50:03.362620: step 421240, loss = 0.16 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:50:04.226227: step 421250, loss = 0.13 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:50:05.079584: step 421260, loss = 0.16 (1500.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:50:05.940089: step 421270, loss = 0.12 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:50:06.781844: step 421280, loss = 0.12 (1520.6 examples/sec; 0.084 sec/batch)
2017-06-02 12:50:07.659970: step 421290, loss = 0.13 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:50:08.643746: step 421300, loss = 0.15 (1301.1 examples/sec; 0.098 sec/batch)
2017-06-02 12:50:09.430506: step 421310, loss = 0.11 (1627.0 examples/sec; 0.079 sec/batch)
2017-06-02 12:50:10.282689: step 421320, loss = 0.14 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:50:11.178675: step 421330, loss = 0.15 (1428.6 examples/sec; 0.090 sec/batch)
2017-06-02 12:50:12.048284: step 421340, loss = 0.13 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:50:12.932186: step 421350, loss = 0.14 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:50:13.825658: step 421360, loss = 0.13 (1432.6 examples/sec; 0.089 sec/batch)
2017-06-02 12:50:14.689687: step 421370, loss = 0.18 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:50:15.569724: step 421380, loss = 0.16 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:50:16.438688: step 421390, loss = 0.11 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:50:17.413375: step 421400, loss = 0.13 (1313.2 examples/sec; 0.097 sec/batch)
2017-06-02 12:50:18.196410: step 421410, loss = 0.12 (1634.7 examples/sec; 0.078 sec/batch)
2017-06-02 12:50:19.089014: step 421420, loss = 0.12 (1434.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:50:19.973476: step 421430, loss = 0.12 (1447.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:50:20.860979: step 421440, loss = 0.15 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 12:50:21.743847: step 421450, loss = 0.14 (1449.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:50:22.627593: step 421460, loss = 0.15 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:50:23.499819: step 421470, loss = 0.15 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:50:24.380296: step 421480, loss = 0.14 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:50:25.261325: step 421490, loss = 0.13 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:50:26.257150: step 421500, loss = 0.15 (1285.4 examples/sec; 0.100 sec/batch)
2017-06-02 12:50:27.038498: step 421510, loss = 0.15 (1638.2 examples/sec; 0.078 sec/batch)
2017-06-02 12:50:27.919726: step 421520, loss = 0.13 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:50:28.793095: step 421530, loss = 0.11 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:50:29.648463: step 421540, loss = 0.14 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:50:30.512759: step 421550, loss = 0.17 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:50:31.367682: step 421560, loss = 0.16 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:50:32.244160: step 421570, loss = 0.11 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:50:33.099302: step 421580, loss = 0.16 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:50:33.964883: step 421590, loss = 0.17 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:50:34.931402: step 421600, loss = 0.15 (1324.3 examples/sec; 0.097 sec/batch)
2017-06-02 12:50:35.725305: step 421610, loss = 0.12 (1612.3 examples/sec; 0.079 sec/batch)
2017-06-02 12:50:36.602523: step 421620, loss = 0.12 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:50:37.455594: step 421630, loss = 0.09 (1500.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:50:38.297898: step 421640, loss = 0.14 (1519.7 examples/sec; 0.084 sec/batch)
2017-06-02 12:50:39.180925: step 421650, loss = 0.13 (1449.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:50:40.012756: step 421660, loss = 0.13 (1538.8 examples/sec; 0.083 sec/batch)
2017-06-02 12:50:40.869754: step 421670, loss = 0.17 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:50:41.730296: step 421680, loss = 0.14 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:50:42.589009: step 421690, loss = 0.14 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:50:43.589816: step 421700, loss = 0.15 (1279.0 examples/sec; 0.100 sec/batch)
2017-06-02 12:50:44.339590: step 421710, loss = 0.16 (1707.2 examples/sec; 0.075 sec/batch)
2017-06-02 12:50:45.185626: step 421720, loss = 0.12 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:50:46.032769: step 421730, loss = 0.13 (1510.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:50:46.909000: step 421740, loss = 0.15 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:50:47.784841: step 421750, loss = 0.20 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:50:48.642275: step 421760, loss = 0.11 (1492.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:50:49.509044: step 421770, loss = 0.13 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:50:50.376939: step 421780, loss = 0.16 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:50:51.216688: step 421790, loss = 0.16 (1524.3 examples/sec; 0.084 sec/batch)
2017-06-02 12:50:52.182614: step 421800, loss = 0.16 (1325.1 examples/sec; 0.097 sec/batch)
2017-06-02 12:50:52.957396: step 421810, loss = 0.16 (1652.1 examples/sec; 0.077 sec/batch)
2017-06-02 12:50:53.842034: step 421820, loss = 0.15 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:50:54.694142: step 421830, loss = 0.13 (1502.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:50:55.574660: step 421840, loss = 0.14 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:50:56.442898: step 421850, loss = 0.14 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:50:57.333878: step 421860, loss = 0.19 (1436.6 examples/sec; 0.089 sec/batch)
2017-06-02 12:50:58.212673: step 421870, loss = 0.15 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:50:59.065894: step 421880, loss = 0.13 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:50:59.962476: step 421890, loss = 0.14 (1427.7 examples/sec; 0.090 sec/batch)
2017-06-02 12:51:00.922717: step 421900, loss = 0.11 (1333.0 examples/sec; 0.096 sec/batch)
2017-06-02 12:51:01.692517: step 421910, loss = 0.15 (1662.8 examples/sec; 0.077 sec/batch)
2017-06-02 12:51:02.567467: step 421920, loss = 0.14 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:51:03.447031: step 421930, loss = 0.14 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:51:04.283071: step 421940, loss = 0.17 (1531.0 examples/sec; 0.084 sec/batch)
2017-06-02 12:51:05.160202: step 421950, loss = 0.12 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:51:06.029081: step 421960, loss = 0.12 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:51:06.859099: step 421970, loss = 0.18 (1542.1 examples/sec; 0.083 sec/batch)
2017-06-02 12:51:07.709770: step 421980, loss = 0.12 (1504.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:51:08.564703: step 421990, loss = 0.13 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:51:09.530560: step 422000, loss = 0.15 (1325.2 examples/sec; 0.097 sec/batch)
2017-06-02 12:51:10.298863: step 422010, loss = 0.16 (1666.0 examples/sec; 0.077 sec/batch)
2017-06-02 12:51:11.150562: step 422020, loss = 0.12 (1502.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:51:12.015566: step 422030, loss = 0.14 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:51:12.885897: step 422040, loss = 0.14 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:51:13.746283: step 422050, loss = 0.14 (1487.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:51:14.622566: step 422060, loss = 0.14 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:51:15.497188: step 422070, loss = 0.15 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:51:16.379474: step 422080, loss = 0.18 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:51:17.253198: step 422090, loss = 0.13 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:51:18.221846: step 422100, loss = 0.11 (1321.4 examples/sec; 0.097 sec/batch)
2017-06-02 12:51:18.989748: step 422110, loss = 0.18 (1666.9 examples/sec; 0.077 sec/batch)
2017-06-02 12:51:19.845695: step 422120, loss = 0.14 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:51:20.706756: step 422130, loss = 0.15 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:51:21.560894: step 422140, loss = 0.17 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:51:22.427532: step 422150, loss = 0.12 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:51:23.283713: step 422160, loss = 0.21 (1495.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:51:24.135828: step 422170, loss = 0.12 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:51:25.008042: step 422180, loss = 0.13 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:51:25.874400: step 422190, loss = 0.14 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:51:26.855616: step 422200, loss = 0.12 (1304.5 examples/sec; 0.098 sec/batch)
2017-06-02 12:51:27.635892: step 422210, loss = 0.13 (1640.4 examples/sec; 0.078 sec/batch)
2017-06-02 12:51:28.496061: step 422220, loss = 0.17 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:51:29.388212: step 422230, loss = 0.15 (1434.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:51:30.258554: step 422240, loss = 0.11 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:51:31.136074: step 422250, loss = 0.13 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:51:31.989080: step 422260, loss = 0.13 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:51:32.840557: step 422270, loss = 0.13 (1503.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:51:33.712282: step 422280, loss = 0.16 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:51:34.573289: step 422290, loss = 0.14 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:51:35.540667: step 422300, loss = 0.11 (1323.2 examples/sec; 0.097 sec/batch)
2017-06-02 12:51:36.304957: step 422310, loss = 0.15 (1674.8 examples/sec; 0.076 sec/batch)
2017-06-02 12:51:37.186062: step 422320, loss = 0.12 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:51:38.026883: step 422330, loss = 0.18 (1522.3 examples/sec; 0.084 sec/batch)
2017-06-02 12:51:38.896197: step 422340, loss = 0.12 (1472.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:51:39.760497: step 422350, loss = 0.12 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:51:40.629065: step 422360, loss = 0.14 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:51:41.475349: step 422370, loss = 0.15 (1512.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:51:42.311651: step 422380, loss = 0.15 (1530.5 examples/sec; 0.084 sec/batch)
2017-06-02 12:51:43.176814: step 422390, loss = 0.13 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:51:44.147513: step 422400, loss = 0.16 (1318.6 examples/sec; 0.097 sec/batch)
2017-06-02 12:51:44.908094: step 422410, loss = 0.14 (1682.9 examples/sec; 0.076 sec/batch)
2017-06-02 12:51:45.755851: step 422420, loss = 0.13 (1509.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:51:46.612296: step 422430, loss = 0.15 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:51:47.490533: step 422440, loss = 0.15 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:51:48.374968: step 422450, loss = 0.17 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:51:49.247980: step 422460, loss = 0.15 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:51:50.114095: step 422470, loss = 0.12 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:51:50.966097: step 422480, loss = 0.11 (1502.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:51:51.857762: step 422490, loss = 0.14 (1435.5 examples/sec; 0.089 sec/batch)
2017-06-02 12:51:52.816061: step 422500, loss = 0.14 (1335.7 examples/sec; 0.096 sec/batch)
2017-06-02 12:51:53.595907: step 422510, loss = 0.21 (1641.4 examples/sec; 0.078 sec/batch)
2017-06-02 12:51:54.462231: step 422520, loss = 0.19 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:51:55.315576: step 422530, loss = 0.16 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:51:56.171366: step 422540, loss = 0.11 (1495.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:51:57.055335: step 422550, loss = 0.14 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:51:57.924316: step 422560, loss = 0.13 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:51:58.764061: step 422570, loss = 0.16 (1524.3 examples/sec; 0.084 sec/batch)
2017-06-02 12:51:59.618169: step 422580, loss = 0.15 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:52:00.476145: step 422590, loss = 0.13 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:52:01.456848: step 422600, loss = 0.14 (1305.2 examples/sec; 0.098 sec/batch)
2017-06-02 12:52:02.196537: step 422610, loss = 0.11 (1730.5 examples/sec; 0.074 sec/batch)
2017-06-02 12:52:03.045264: step 422620, loss = 0.15 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:52:03.906788: step 422630, loss = 0.14 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:52:04.786956: step 422640, loss = 0.19 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:52:05.675055: step 422650, loss = 0.10 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:52:06.520558: step 422660, loss = 0.13 (1513.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:52:07.375251: step 422670, loss = 0.16 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:52:08.250676: step 422680, loss = 0.14 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:52:09.115944: step 422690, loss = 0.14 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:52:10.156552: step 422700, loss = 0.16 (1230.0 examples/sec; 0.104 sec/batch)
2017-06-02 12:52:10.884678: step 422710, loss = 0.14 (1758.0 examples/sec; 0.073 sec/batch)
2017-06-02 12:52:11.750961: step 422720, loss = 0.13 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:52:12.619042: step 422730, loss = 0.14 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:52:13.495613: step 422740, loss = 0.11 (1460.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:52:14.351050: step 422750, loss = 0.21 (1496.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:52:15.213769: step 422760, loss = 0.12 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:52:16.090380: step 422770, loss = 0.18 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:52:16.964125: step 422780, loss = 0.14 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:52:17.808277: step 422790, loss = 0.13 (1516.3 examples/sec; 0.084 sec/batch)
2017-06-02 12:52:18.774392: step 422800, loss = 0.14 (1324.9 examples/sec; 0.097 sec/batch)
2017-06-02 12:52:19.520714: step 422810, loss = 0.17 (1715.1 examples/sec; 0.075 sec/batch)
2017-06-02 12:52:20.368070: step 422820, loss = 0.11 (1510.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:52:21.235450: step 422830, loss = 0.17 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:52:22.079025: step 422840, loss = 0.14 (1517.3 examples/sec; 0.084 sec/batch)
2017-06-02 12:52:22.950485: step 422850, loss = 0.13 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:52:23.828854: step 422860, loss = 0.16 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:52:24.706023: step 422870, loss = 0.15 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:52:25.592957: step 422880, loss = 0.13 (1443.2 examples/sec; 0.089 sec/batch)
2017-06-02 12:52:26.465526: step 422890, loss = 0.10 (1466.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:52:27.422211: step 422900, loss = 0.13 (1337.9 examples/sec; 0.096 sec/batch)
2017-06-02 12:52:28.191479: step 422910, loss = 0.13 (1663.9 examples/sec; 0.077 sec/batch)
2017-06-02 12:52:29.030364: step 422920, loss = 0.15 (1525.8 examples/sec; 0.084 sec/batch)
2017-06-02 12:52:29.874672: step 422930, loss = 0.12 (1516.0 examples/sec; 0.084 sec/batch)
2017-06-02 12:52:30.743209: step 422940, loss = 0.14 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:52:31.598580: step 422950, loss = 0.14 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:52:32.462512: step 422960, loss = 0.15 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:52:33.314006: step 422970, loss = 0.11 (1503.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:52:34.192039: step 422980, loss = 0.11 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:52:35.060975: step 422990, loss = 0.12 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:52:36.068762: step 423000, loss = 0.12 (1270.1 examples/sec; 0.101 sec/batch)
2017-06-02 12:52:36.813627: step 423010, loss = 0.18 (1718.5 examples/sec; 0.074 sec/batch)
2017-06-02 12:52:37.646049: step 423020, loss = 0.16 (1537.7 examples/sec; 0.083 sec/batch)
2017-06-02 12:52:38.524845: step 423030, loss = 0.19 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:52:39.405814: step 423040, loss = 0.16 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:52:40.288443: step 423050, loss = 0.11 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:52:41.155349: step 423060, loss = 0.13 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:52:42.038015: step 423070, loss = 0.14 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:52:42.922662: step 423080, loss = 0.15 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:52:43.807379: step 423090, loss = 0.16 (1446.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:52:44.796336: step 423100, loss = 0.13 (1294.3 examples/sec; 0.099 sec/batch)
2017-06-02 12:52:45.542545: step 423110, loss = 0.12 (1715.4 examples/sec; 0.075 sec/batch)
2017-06-02 12:52:46.406596: step 423120, loss = 0.12 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:52:47.309832: step 423130, loss = 0.14 (1417.1 examples/sec; 0.090 sec/batch)
2017-06-02 12:52:48.185563: step 423140, loss = 0.16 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:52:49.058535: step 423150, loss = 0.12 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:52:49.931460: step 423160, loss = 0.12 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:52:50.807695: step 423170, loss = 0.19 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:52:51.681517: step 423180, loss = 0.15 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:52:52.563818: step 423190, loss = 0.12 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:52:53.555274: step 423200, loss = 0.12 (1291.0 examples/sec; 0.099 sec/batch)
2017-06-02 12:52:54.325086: step 423210, loss = 0.14 (1662.7 examples/sec; 0.077 sec/batch)
2017-06-02 12:52:55.210375: step 423220, loss = 0.12 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:52:56.081477: step 423230, loss = 0.13 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:52:56.958192: step 423240, loss = 0.13 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:52:57.825347: step 423250, loss = 0.13 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:52:58.683348: step 423260, loss = 0.12 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:52:59.534117: step 423270, loss = 0.13 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:53:00.426692: step 423280, loss = 0.13 (1434.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:53:01.292820: step 423290, loss = 0.12 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:53:02.266386: step 423300, loss = 0.18 (1314.7 examples/sec; 0.097 sec/batch)
2017-06-02 12:53:03.026034: step 423310, loss = 0.11 (1685.0 examples/sec; 0.076 sec/batch)
2017-06-02 12:53:03.866769: step 423320, loss = 0.11 (1522.5 examples/sec; 0.084 sec/batch)
2017-06-02 12:53:04.724737: step 423330, loss = 0.15 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:53:05.593232: step 423340, loss = 0.14 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:53:06.439950: step 423350, loss = 0.18 (1511.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:53:07.282507: step 423360, loss = 0.13 (1519.2 examples/sec; 0.084 sec/batch)
2017-06-02 12:53:08.132452: step 423370, loss = 0.13 (1506.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:53:08.993941: step 423380, loss = 0.13 (1485.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:53:09.822038: step 423390, loss = 0.15 (1545.7 examples/sec; 0.083 sec/batch)
2017-06-02 12:53:10.786329: step 423400, loss = 0.15 (1327.4 examples/sec; 0.096 sec/batch)
2017-06-02 12:53:11.515194: step 423410, loss = 0.13 (1756.2 examples/sec; 0.073 sec/batch)
2017-06-02 12:53:12.375755: step 423420, loss = 0.19 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:53:13.226853: step 423430, loss = 0.18 (1503.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:53:14.077274: step 423440, loss = 0.13 (1505.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:53:14.938820: step 423450, loss = 0.15 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:53:15.794363: step 423460, loss = 0.17 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:53:16.653573: step 423470, loss = 0.12 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:53:17.494141: step 423480, loss = 0.11 (1522.8 examples/sec; 0.084 sec/batch)
2017-06-02 12:53:18.333430: step 423490, loss = 0.13 (1525.1 examples/sec; 0.084 sec/batch)
2017-06-02 12:53:19.283029: step 423500, loss = 0.15 (1347.9 examples/sec; 0.095 sec/batch)
2017-06-02 12:53:20.060701: step 423510, loss = 0.14 (1645.9 examples/sec; 0.078 sec/batch)
2017-06-02 12:53:20.913915: step 423520, loss = 0.12 (1500.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:53:21.782560: step 423530, loss = 0.13 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:53:22.655539: step 423540, loss = 0.12 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:53:23.520174: step 423550, loss = 0.13 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:53:24.389001: step 423560, loss = 0.11 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:53:25.263346: step 423570, loss = 0.13 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:53:26.118439: step 423580, loss = 0.13 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:53:26.979586: step 423590, loss = 0.12 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:53:27.971904: step 423600, loss = 0.14 (1289.9 examples/sec; 0.099 sec/batch)
2017-06-02 12:53:28.728641: step 423610, loss = 0.14 (1691.5 examples/sec; 0.076 sec/batch)
2017-06-02 12:53:29.572863: step 423620, loss = 0.13 (1516.2 examples/sec; 0.084 sec/batch)
2017-06-02 12:53:30.415920: step 423630, loss = 0.16 (1518.3 examples/sec; 0.084 sec/batch)
2017-06-02 12:53:31.286254: step 423640, loss = 0.16 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:53:32.139112: step 423650, loss = 0.13 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 12:53:32.993538: step 423660, loss = 0.12 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:53:33.857030: step 423670, loss = 0.12 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:53:34.710612: step 423680, loss = 0.15 (1499.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:53:35.547251: step 423690, loss = 0.12 (1529.9 examples/sec; 0.084 sec/batch)
2017-06-02 12:53:36.514093: step 423700, loss = 0.13 (1323.9 examples/sec; 0.097 sec/batch)
2017-06-02 12:53:37.284064: step 423710, loss = 0.19 (1662.4 examples/sec; 0.077 sec/batch)
2017-06-02 12:53:38.148126: step 423720, loss = 0.12 (1481.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:53:38.981033: step 423730, loss = 0.12 (1536.8 examples/sec; 0.083 sec/batch)
2017-06-02 12:53:39.828201: step 423740, loss = 0.15 (1510.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:53:40.679143: step 423750, loss = 0.14 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:53:41.553507: step 423760, loss = 0.14 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:53:42.410487: step 423770, loss = 0.13 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:53:43.299410: step 423780, loss = 0.17 (1439.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:53:44.180396: step 423790, loss = 0.13 (1452.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:53:45.145126: step 423800, loss = 0.12 (1326.8 examples/sec; 0.096 sec/batch)
2017-06-02 12:53:45.895429: step 423810, loss = 0.15 (1706.0 examples/sec; 0.075 sec/batch)
2017-06-02 12:53:46.786004: step 423820, loss = 0.13 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:53:47.642790: step 423830, loss = 0.13 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:53:48.475562: step 423840, loss = 0.13 (1537.0 examples/sec; 0.083 sec/batch)
2017-06-02 12:53:49.344710: step 423850, loss = 0.10 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:53:50.221566: step 423860, loss = 0.16 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:53:51.090392: step 423870, loss = 0.13 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:53:51.959071: step 423880, loss = 0.13 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:53:52.782435: step 423890, loss = 0.15 (1554.6 examples/sec; 0.082 sec/batch)
2017-06-02 12:53:53.786735: step 423900, loss = 0.14 (1274.5 examples/sec; 0.100 sec/batch)
2017-06-02 12:53:54.496796: step 423910, loss = 0.13 (1802.7 examples/sec; 0.071 sec/batch)
2017-06-02 12:53:55.343189: step 423920, loss = 0.12 (1512.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:53:56.179123: step 423930, loss = 0.13 (1531.2 examples/sec; 0.084 sec/batch)
2017-06-02 12:53:57.018104: step 423940, loss = 0.15 (1525.6 examples/sec; 0.084 sec/batch)
2017-06-02 12:53:57.890553: step 423950, loss = 0.11 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:53:58.764755: step 423960, loss = 0.10 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:53:59.613832: step 423970, loss = 0.14 (1507.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:54:00.474165: step 423980, loss = 0.16 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:54:01.348397: step 423990, loss = 0.12 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:54:02.325610: step 424000, loss = 0.16 (1309.9 examples/sec; 0.098 sec/batch)
2017-06-02 12:54:03.070176: step 424010, loss = 0.13 (1719.1 examples/sec; 0.074 sec/batch)
2017-06-02 12:54:03.949159: step 424020, loss = 0.14 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:54:04.818806: step 424030, loss = 0.11 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:54:05.687011: step 424040, loss = 0.15 (1474.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:54:06.562113: step 424050, loss = 0.17 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:54:07.400148: step 424060, loss = 0.14 (1527.4 examples/sec; 0.084 sec/batch)
2017-06-02 12:54:08.269603: step 424070, loss = 0.15 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:54:09.128841: step 424080, loss = 0.16 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:54:09.989745: step 424090, loss = 0.12 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:54:10.971235: step 424100, loss = 0.13 (1304.1 examples/sec; 0.098 sec/batch)
2017-06-02 12:54:11.738085: step 424110, loss = 0.12 (1669.2 examples/sec; 0.077 sec/batch)
2017-06-02 12:54:12.593352: step 424120, loss = 0.13 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:54:13.466989: step 424130, loss = 0.16 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:54:14.317561: step 424140, loss = 0.12 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:54:15.185728: step 424150, loss = 0.13 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:54:16.055611: step 424160, loss = 0.13 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:54:16.915161: step 424170, loss = 0.12 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:54:17.784687: step 424180, loss = 0.14 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:54:18.658857: step 424190, loss = 0.13 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:54:19.662552: step 424200, loss = 0.11 (1275.3 examples/sec; 0.100 sec/batch)
2017-06-02 12:54:20.387603: step 424210, loss = 0.16 (1765.5 examples/sec; 0.073 sec/batch)
2017-06-02 12:54:21.275045: step 424220, loss = 0.13 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 12:54:22.162664: step 424230, loss = 0.17 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:54:23.043821: step 424240, loss = 0.12 (1452.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:54:23.916922: step 424250, loss = 0.14 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:54:24.791262: step 424260, loss = 0.13 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:54:25.666068: step 424270, loss = 0.17 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:54:26.510629: step 424280, loss = 0.14 (1515.6 examples/sec; 0.084 sec/batch)
2017-06-02 12:54:27.367347: step 424290, loss = 0.17 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:54:28.392600: step 424300, loss = 0.11 (1248.5 examples/sec; 0.103 sec/batch)
2017-06-02 12:54:29.128416: step 424310, loss = 0.13 (1739.6 examples/sec; 0.074 sec/batch)
2017-06-02 12:54:29.977961: step 424320, loss = 0.15 (1506.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:54:30.830615: step 424330, loss = 0.12 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:54:31.698347: step 424340, loss = 0.14 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:54:32.551634: step 424350, loss = 0.13 (1500.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:54:33.425192: step 424360, loss = 0.14 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:54:34.300304: step 424370, loss = 0.13 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:54:35.181216: step 424380, loss = 0.15 (1453.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:54:36.045600: step 424390, loss = 0.17 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:54:36.999692: step 424400, loss = 0.14 (1341.6 examples/sec; 0.095 sec/batch)
2017-06-02 12:54:37.785465: step 424410, loss = 0.11 (1629.0 examples/sec; 0.079 sec/batch)
2017-06-02 12:54:38.656759: step 424420, loss = 0.12 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:54:39.516194: step 424430, loss = 0.12 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:54:40.363177: step 424440, loss = 0.16 (1511.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:54:41.242302: step 424450, loss = 0.16 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:54:42.091836: step 424460, loss = 0.11 (1506.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:54:42.964283: step 424470, loss = 0.14 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:54:43.825962: step 424480, loss = 0.13 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:54:44.664703: step 424490, loss = 0.14 (1526.1 examples/sec; 0.084 sec/batch)
2017-06-02 12:54:45.647459: step 424500, loss = 0.14 (1302.5 examples/sec; 0.098 sec/batch)
2017-06-02 12:54:46.399311: step 424510, loss = 0.19 (1702.5 examples/sec; 0.075 sec/batch)
2017-06-02 12:54:47.253337: step 424520, loss = 0.16 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 12:54:48.146143: step 424530, loss = 0.14 (1433.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:54:49.002853: step 424540, loss = 0.11 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:54:49.862293: step 424550, loss = 0.14 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:54:50.715701: step 424560, loss = 0.13 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:54:51.559932: step 424570, loss = 0.16 (1516.2 examples/sec; 0.084 sec/batch)
2017-06-02 12:54:52.436113: step 424580, loss = 0.10 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:54:53.298812: step 424590, loss = 0.13 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:54:54.246906: step 424600, loss = 0.11 (1350.1 examples/sec; 0.095 sec/batch)
2017-06-02 12:54:55.019537: step 424610, loss = 0.15 (1656.7 examples/sec; 0.077 sec/batch)
2017-06-02 12:54:55.863189: step 424620, loss = 0.11 (1517.2 examples/sec; 0.084 sec/batch)
2017-06-02 12:54:56.724625: step 424630, loss = 0.15 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:54:57.597715: step 424640, loss = 0.13 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:54:58.462152: step 424650, loss = 0.11 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:54:59.325000: step 424660, loss = 0.18 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:55:00.178543: step 424670, loss = 0.14 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:55:01.050045: step 424680, loss = 0.11 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:55:01.919124: step 424690, loss = 0.13 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:55:02.934173: step 424700, loss = 0.18 (1261.0 examples/sec; 0.102 sec/batch)
2017-06-02 12:55:03.641817: step 424710, loss = 0.11 (1808.8 examples/sec; 0.071 sec/batch)
2017-06-02 12:55:04.487806: step 424720, loss = 0.15 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:55:05.347518: step 424730, loss = 0.13 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:55:06.200267: step 424740, loss = 0.12 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:55:07.049849: step 424750, loss = 0.15 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:55:07.934213: step 424760, loss = 0.13 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:55:08.795853: step 424770, loss = 0.13 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:55:09.673886: step 424780, loss = 0.12 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:55:10.550203: step 424790, loss = 0.15 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:55:11.512271: step 424800, loss = 0.13 (1330.4 examples/sec; 0.096 sec/batch)
2017-06-02 12:55:12.282289: step 424810, loss = 0.14 (1662.3 examples/sec; 0.077 sec/batch)
2017-06-02 12:55:13.159870: step 424820, loss = 0.16 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:55:14.038580: step 424830, loss = 0.11 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:55:14.892122: step 424840, loss = 0.13 (1499.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:55:15.744665: step 424850, loss = 0.18 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:55:16.608011: step 424860, loss = 0.12 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:55:17.455746: step 424870, loss = 0.13 (1509.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:55:18.315204: step 424880, loss = 0.12 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:55:19.151844: step 424890, loss = 0.15 (1529.9 examples/sec; 0.084 sec/batch)
2017-06-02 12:55:20.122082: step 424900, loss = 0.14 (1319.3 examples/sec; 0.097 sec/batch)
2017-06-02 12:55:20.889407: step 424910, loss = 0.11 (1668.1 examples/sec; 0.077 sec/batch)
2017-06-02 12:55:21.736196: step 424920, loss = 0.11 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:55:22.591468: step 424930, loss = 0.12 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:55:23.434175: step 424940, loss = 0.12 (1518.9 examples/sec; 0.084 sec/batch)
2017-06-02 12:55:24.285648: step 424950, loss = 0.14 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:55:25.103885: step 424960, loss = 0.14 (1564.3 examples/sec; 0.082 sec/batch)
2017-06-02 12:55:25.943431: step 424970, loss = 0.20 (1524.6 examples/sec; 0.084 sec/batch)
2017-06-02 12:55:26.773167: step 424980, loss = 0.13 (1542.7 examples/sec; 0.083 sec/batch)
2017-06-02 12:55:27.637881: step 424990, loss = 0.19 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:55:28.591045: step 425000, loss = 0.15 (1342.9 examples/sec; 0.095 sec/batch)
2017-06-02 12:55:29.351030: step 425010, loss = 0.15 (1684.2 examples/sec; 0.076 sec/batch)
2017-06-02 12:55:30.190604: step 425020, loss = 0.13 (1524.6 examples/sec; 0.084 sec/batch)
2017-06-02 12:55:31.045398: step 425030, loss = 0.13 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:55:31.893029: step 425040, loss = 0.11 (1510.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:55:32.733415: step 425050, loss = 0.14 (1523.1 examples/sec; 0.084 sec/batch)
2017-06-02 12:55:33.596811: step 425060, loss = 0.11 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:55:34.439159: step 425070, loss = 0.16 (1519.6 examples/sec; 0.084 sec/batch)
2017-06-02 12:55:35.308936: step 425080, loss = 0.12 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:55:36.173303: step 425090, loss = 0.11 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:55:37.142998: step 425100, loss = 0.14 (1320.0 examples/sec; 0.097 sec/batch)
2017-06-02 12:55:37.911644: step 425110, loss = 0.17 (1665.3 examples/sec; 0.077 sec/batch)
2017-06-02 12:55:38.764629: step 425120, loss = 0.13 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:55:39.611542: step 425130, loss = 0.17 (1511.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:55:40.491566: step 425140, loss = 0.11 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:55:41.359279: step 425150, loss = 0.15 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:55:42.238238: step 425160, loss = 0.19 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 12:55:43.094545: step 425170, loss = 0.13 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:55:43.939019: step 425180, loss = 0.10 (1515.7 examples/sec; 0.084 sec/batch)
2017-06-02 12:55:44.793379: step 425190, loss = 0.13 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:55:45.795633: step 425200, loss = 0.14 (1277.1 examples/sec; 0.100 sec/batch)
2017-06-02 12:55:46.505295: step 425210, loss = 0.16 (1803.7 examples/sec; 0.071 sec/batch)
2017-06-02 12:55:47.385357: step 425220, loss = 0.15 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:55:48.239530: step 425230, loss = 0.12 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:55:49.090757: step 425240, loss = 0.10 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:55:49.944366: step 425250, loss = 0.14 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:55:50.821086: step 425260, loss = 0.17 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:55:51.688669: step 425270, loss = 0.11 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:55:52.526186: step 425280, loss = 0.13 (1528.3 examples/sec; 0.084 sec/batch)
2017-06-02 12:55:53.376172: step 425290, loss = 0.14 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:55:54.353832: step 425300, loss = 0.14 (1309.2 examples/sec; 0.098 sec/batch)
2017-06-02 12:55:55.099832: step 425310, loss = 0.18 (1715.8 examples/sec; 0.075 sec/batch)
2017-06-02 12:55:55.971798: step 425320, loss = 0.12 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:55:56.837233: step 425330, loss = 0.18 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:55:57.702888: step 425340, loss = 0.12 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:55:58.542744: step 425350, loss = 0.13 (1524.1 examples/sec; 0.084 sec/batch)
2017-06-02 12:55:59.399650: step 425360, loss = 0.18 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:56:00.276686: step 425370, loss = 0.15 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:56:01.140035: step 425380, loss = 0.12 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:56:02.009788: step 425390, loss = 0.10 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:56:02.997911: step 425400, loss = 0.16 (1295.4 examples/sec; 0.099 sec/batch)
2017-06-02 12:56:03.753631: step 425410, loss = 0.12 (1693.8 examples/sec; 0.076 sec/batch)
2017-06-02 12:56:04.621111: step 425420, loss = 0.13 (1475.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:56:05.515356: step 425430, loss = 0.13 (1431.4 examples/sec; 0.089 sec/batch)
2017-06-02 12:56:06.361264: step 425440, loss = 0.17 (1513.2 examples/sec; 0.085 sec/batch)
2017-06-02 12:56:07.228945: step 425450, loss = 0.13 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:56:08.109194: step 425460, loss = 0.14 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:56:08.981494: step 425470, loss = 0.14 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:56:09.861007: step 425480, loss = 0.10 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:56:10.737654: step 425490, loss = 0.16 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:56:11.688208: step 425500, loss = 0.12 (1346.6 examples/sec; 0.095 sec/batch)
2017-06-02 12:56:12.469994: step 425510, loss = 0.13 (1637.3 examples/sec; 0.078 sec/batch)
2017-06-02 12:56:13.354799: step 425520, loss = 0.16 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:56:14.218320: step 425530, loss = 0.14 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:56:15.089986: step 425540, loss = 0.13 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:56:15.922907: step 425550, loss = 0.11 (1536.8 examples/sec; 0.083 sec/batch)
2017-06-02 12:56:16.781969: step 425560, loss = 0.14 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:56:17.640755: step 425570, loss = 0.18 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:56:18.517540: step 425580, loss = 0.16 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:56:19.386055: step 425590, loss = 0.11 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:56:20.405603: step 425600, loss = 0.13 (1255.4 examples/sec; 0.102 sec/batch)
2017-06-02 12:56:21.157532: step 425610, loss = 0.11 (1702.3 examples/sec; 0.075 sec/batch)
2017-06-02 12:56:22.028265: step 425620, loss = 0.17 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:56:22.902309: step 425630, loss = 0.11 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:56:23.765063: step 425640, loss = 0.14 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:56:24.644415: step 425650, loss = 0.17 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:56:25.502689: step 425660, loss = 0.13 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:56:26.371410: step 425670, loss = 0.11 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:56:27.244764: step 425680, loss = 0.15 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:56:28.089949: step 425690, loss = 0.12 (1514.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:56:29.046801: step 425700, loss = 0.16 (1337.7 examples/sec; 0.096 sec/batch)
2017-06-02 12:56:29.807720: step 425710, loss = 0.15 (1682.2 examples/sec; 0.076 sec/batch)
2017-06-02 12:56:30.662780: step 425720, loss = 0.11 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:56:31.496863: step 425730, loss = 0.15 (1534.6 examples/sec; 0.083 sec/batch)
2017-06-02 12:56:32.330532: step 425740, loss = 0.13 (1535.4 examples/sec; 0.083 sec/batch)
2017-06-02 12:56:33.186775: step 425750, loss = 0.12 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:56:34.074383: step 425760, loss = 0.13 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:56:34.908296: step 425770, loss = 0.21 (1534.9 examples/sec; 0.083 sec/batch)
2017-06-02 12:56:35.792206: step 425780, loss = 0.13 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:56:36.683752: step 425790, loss = 0.13 (1435.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:56:37.674827: step 425800, loss = 0.12 (1291.5 examples/sec; 0.099 sec/batch)
2017-06-02 12:56:38.435040: step 425810, loss = 0.13 (1683.7 examples/sec; 0.076 sec/batch)
2017-06-02 12:56:39.299758: step 425820, loss = 0.16 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:56:40.179794: step 425830, loss = 0.13 (1454.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:56:41.067483: step 425840, loss = 0.12 (1441.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:56:41.947760: step 425850, loss = 0.12 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:56:42.798027: step 425860, loss = 0.12 (1505.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:56:43.643474: step 425870, loss = 0.12 (1514.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:56:44.502280: step 425880, loss = 0.17 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:56:45.361929: step 425890, loss = 0.11 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:56:46.338642: step 425900, loss = 0.14 (1310.5 examples/sec; 0.098 sec/batch)
2017-06-02 12:56:47.091743: step 425910, loss = 0.12 (1699.6 examples/sec; 0.075 sec/batch)
2017-06-02 12:56:47.977418: step 425920, loss = 0.11 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 12:56:48.835036: step 425930, loss = 0.14 (1492.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:56:49.694884: step 425940, loss = 0.11 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:56:50.576359: step 425950, loss = 0.12 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:56:51.462092: step 425960, loss = 0.15 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:56:52.327112: step 425970, loss = 0.11 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:56:53.202267: step 425980, loss = 0.13 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:56:54.061501: step 425990, loss = 0.11 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:56:55.025329: step 426000, loss = 0.17 (1328.0 examples/sec; 0.096 sec/batch)
2017-06-02 12:56:55.805317: step 426010, loss = 0.14 (1641.1 examples/sec; 0.078 sec/batch)
2017-06-02 12:56:56.644451: step 426020, loss = 0.19 (1525.4 examples/sec; 0.084 sec/batch)
2017-06-02 12:56:57.526106: step 426030, loss = 0.12 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:56:58.367289: step 426040, loss = 0.13 (1521.7 examples/sec; 0.084 sec/batch)
2017-06-02 12:56:59.250997: step 426050, loss = 0.15 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:57:00.107391: step 426060, loss = 0.10 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:57:00.978384: step 426070, loss = 0.12 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:57:01.846560: step 426080, loss = 0.11 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:57:02.731392: step 426090, loss = 0.13 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:57:03.711184: step 426100, loss = 0.12 (1306.4 examples/sec; 0.098 sec/batch)
2017-06-02 12:57:04.488289: step 426110, loss = 0.10 (1647.1 examples/sec; 0.078 sec/batch)
2017-06-02 12:57:05.364302: step 426120, loss = 0.10 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:57:06.221072: step 426130, loss = 0.17 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:57:07.099068: step 426140, loss = 0.13 (1457.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:57:07.950442: step 426150, loss = 0.16 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:57:08.827351: step 426160, loss = 0.14 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:57:09.699377: step 426170, loss = 0.13 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:57:10.548649: step 426180, loss = 0.16 (1507.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:57:11.392041: step 426190, loss = 0.17 (1517.7 examples/sec; 0.084 sec/batch)
2017-06-02 12:57:12.377209: step 426200, loss = 0.13 (1299.3 examples/sec; 0.099 sec/batch)
2017-06-02 12:57:13.122536: step 426210, loss = 0.12 (1717.4 examples/sec; 0.075 sec/batch)
2017-06-02 12:57:13.981531: step 426220, loss = 0.13 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:57:14.845185: step 426230, loss = 0.12 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:57:15.710134: step 426240, loss = 0.13 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:57:16.576202: step 426250, loss = 0.11 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:57:17.440576: step 426260, loss = 0.12 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:57:18.297142: step 426270, loss = 0.10 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:57:19.167839: step 426280, loss = 0.15 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:57:20.043293: step 426290, loss = 0.12 (1462.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:57:21.008667: step 426300, loss = 0.19 (1325.9 examples/sec; 0.097 sec/batch)
2017-06-02 12:57:21.779414: step 426310, loss = 0.11 (1660.7 examples/sec; 0.077 sec/batch)
2017-06-02 12:57:22.676899: step 426320, loss = 0.23 (1426.2 examples/sec; 0.090 sec/batch)
2017-06-02 12:57:23.555614: step 426330, loss = 0.18 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:57:24.427996: step 426340, loss = 0.10 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:57:25.310920: step 426350, loss = 0.11 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:57:26.193053: step 426360, loss = 0.14 (1451.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:57:27.076749: step 426370, loss = 0.16 (1448.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:57:27.944141: step 426380, loss = 0.13 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 12:57:28.813080: step 426390, loss = 0.14 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:57:29.777323: step 426400, loss = 0.13 (1327.5 examples/sec; 0.096 sec/batch)
2017-06-02 12:57:30.530073: step 426410, loss = 0.15 (1700.5 examples/sec; 0.075 sec/batch)
2017-06-02 12:57:31.407886: step 426420, loss = 0.12 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:57:32.258384: step 426430, loss = 0.13 (1505.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:57:33.127097: step 426440, loss = 0.13 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:57:33.990475: step 426450, loss = 0.15 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:57:34.855881: step 426460, loss = 0.12 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:57:35.730277: step 426470, loss = 0.09 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:57:36.583642: step 426480, loss = 0.14 (1499.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:57:37.426942: step 426490, loss = 0.19 (1517.9 examples/sec; 0.084 sec/batch)
2017-06-02 12:57:38.434114: step 426500, loss = 0.16 (1270.9 examples/sec; 0.101 sec/batch)
2017-06-02 12:57:39.155140: step 426510, loss = 0.16 (1775.2 examples/sec; 0.072 sec/batch)
2017-06-02 12:57:40.014802: step 426520, loss = 0.15 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:57:40.881750: step 426530, loss = 0.15 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:57:41.746693: step 426540, loss = 0.10 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:57:42.592951: step 426550, loss = 0.12 (1512.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:57:43.429169: step 426560, loss = 0.19 (1530.7 examples/sec; 0.084 sec/batch)
2017-06-02 12:57:44.299648: step 426570, loss = 0.16 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:57:45.151496: step 426580, loss = 0.13 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 12:57:46.005794: step 426590, loss = 0.17 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:57:46.954410: step 426600, loss = 0.14 (1349.3 examples/sec; 0.095 sec/batch)
2017-06-02 12:57:47.710113: step 426610, loss = 0.14 (1693.8 examples/sec; 0.076 sec/batch)
2017-06-02 12:57:48.565763: step 426620, loss = 0.14 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:57:49.417956: step 426630, loss = 0.15 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:57:50.276876: step 426640, loss = 0.15 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:57:51.132136: step 426650, loss = 0.12 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:57:52.002237: step 426660, loss = 0.13 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:57:52.868246: step 426670, loss = 0.12 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:57:53.707179: step 426680, loss = 0.14 (1525.7 examples/sec; 0.084 sec/batch)
2017-06-02 12:57:54.568010: step 426690, loss = 0.15 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:57:55.506528: step 426700, loss = 0.16 (1363.9 examples/sec; 0.094 sec/batch)
2017-06-02 12:57:56.246389: step 426710, loss = 0.16 (1730.1 examples/sec; 0.074 sec/batch)
2017-06-02 12:57:57.086409: step 426720, loss = 0.12 (1523.8 examples/sec; 0.084 sec/batch)
2017-06-02 12:57:57.927601: step 426730, loss = 0.13 (1521.7 examples/sec; 0.084 sec/batch)
2017-06-02 12:57:58.782804: step 426740, loss = 0.13 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:57:59.647450: step 426750, loss = 0.18 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:58:00.498362: step 426760, loss = 0.11 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:58:01.382286: step 426770, loss = 0.13 (1448.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:58:02.219882: step 426780, loss = 0.13 (1528.2 examples/sec; 0.084 sec/batch)
2017-06-02 12:58:03.088643: step 426790, loss = 0.14 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:58:04.048014: step 426800, loss = 0.14 (1334.2 examples/sec; 0.096 sec/batch)
2017-06-02 12:58:04.831916: step 426810, loss = 0.12 (1632.9 examples/sec; 0.078 sec/batch)
2017-06-02 12:58:05.695461: step 426820, loss = 0.12 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:58:06.522478: step 426830, loss = 0.13 (1547.7 examples/sec; 0.083 sec/batch)
2017-06-02 12:58:07.387639: step 426840, loss = 0.18 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:58:08.234320: step 426850, loss = 0.12 (1511.8 examples/sec; 0.085 sec/batch)
2017-06-02 12:58:09.111748: step 426860, loss = 0.15 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 12:58:09.956514: step 426870, loss = 0.15 (1515.2 examples/sec; 0.084 sec/batch)
2017-06-02 12:58:10.833005: step 426880, loss = 0.16 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:58:11.692522: step 426890, loss = 0.13 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:58:12.670112: step 426900, loss = 0.14 (1309.3 examples/sec; 0.098 sec/batch)
2017-06-02 12:58:13.435522: step 426910, loss = 0.14 (1672.4 examples/sec; 0.077 sec/batch)
2017-06-02 12:58:14.311841: step 426920, loss = 0.15 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:58:15.166037: step 426930, loss = 0.13 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:58:16.048116: step 426940, loss = 0.15 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:58:16.911506: step 426950, loss = 0.11 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:58:17.794125: step 426960, loss = 0.12 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:58:18.653114: step 426970, loss = 0.12 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:58:19.504290: step 426980, loss = 0.12 (1503.8 examples/sec; 0.085 sec/batch)
2017-06-02 12:58:20.342327: step 426990, loss = 0.11 (1527.4 examples/sec; 0.084 sec/batch)
2017-06-02 12:58:21.374210: step 427000, loss = 0.11 (1240.5 examples/sec; 0.103 sec/batch)
2017-06-02 12:58:22.075191: step 427010, loss = 0.14 (1826.0 examples/sec; 0.070 sec/batch)
2017-06-02 12:58:22.937983: step 427020, loss = 0.16 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:58:23.828085: step 427030, loss = 0.15 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 12:58:24.713493: step 427040, loss = 0.19 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 12:58:25.585284: step 427050, loss = 0.15 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:58:26.474836: step 427060, loss = 0.13 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 12:58:27.344486: step 427070, loss = 0.16 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:58:28.241598: step 427080, loss = 0.14 (1426.8 examples/sec; 0.090 sec/batch)
2017-06-02 12:58:29.124385: step 427090, loss = 0.17 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:58:30.109192: step 427100, loss = 0.15 (1299.8 examples/sec; 0.098 sec/batch)
2017-06-02 12:58:30.898585: step 427110, loss = 0.12 (1621.5 examples/sec; 0.079 sec/batch)
2017-06-02 12:58:31.779802: step 427120, loss = 0.11 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:58:32.677461: step 427130, loss = 0.13 (1426.0 examples/sec; 0.090 sec/batch)
2017-06-02 12:58:33.555285: step 427140, loss = 0.12 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:58:34.444479: step 427150, loss = 0.16 (1439.5 examples/sec; 0.089 sec/batch)
2017-06-02 12:58:35.306781: step 427160, loss = 0.13 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:58:36.192801: step 427170, loss = 0.12 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 12:58:37.062680: step 427180, loss = 0.15 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:58:37.938169: step 427190, loss = 0.18 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:58:38.919295: step 427200, loss = 0.14 (1304.6 examples/sec; 0.098 sec/batch)
2017-06-02 12:58:39.690957: step 427210, loss = 0.12 (1658.8 examples/sec; 0.077 sec/batch)
2017-06-02 12:58:40.552560: step 427220, loss = 0.19 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:58:41.451520: step 427230, loss = 0.11 (1423.9 examples/sec; 0.090 sec/batch)
2017-06-02 12:58:42.291601: step 427240, loss = 0.15 (1523.7 examples/sec; 0.084 sec/batch)
2017-06-02 12:58:43.156147: step 427250, loss = 0.14 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:58:44.034090: step 427260, loss = 0.14 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 12:58:44.894412: step 427270, loss = 0.13 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 12:58:45.734033: step 427280, loss = 0.14 (1524.5 examples/sec; 0.084 sec/batch)
2017-06-02 12:58:46.586855: step 427290, loss = 0.13 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:58:47.552991: step 427300, loss = 0.11 (1324.9 examples/sec; 0.097 sec/batch)
2017-06-02 12:58:48.321778: step 427310, loss = 0.11 (1665.0 examples/sec; 0.077 sec/batch)
2017-06-02 12:58:49.201710: step 427320, loss = 0.14 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 12:58:50.066724: step 427330, loss = 0.13 (1479.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:58:50.926382: step 427340, loss = 0.15 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:58:51.801099: step 427350, loss = 0.12 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 12:58:52.662513: step 427360, loss = 0.15 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:58:53.526690: step 427370, loss = 0.13 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:58:54.377928: step 427380, loss = 0.16 (1503.7 examples/sec; 0.085 sec/batch)
2017-06-02 12:58:55.234650: step 427390, loss = 0.17 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:58:56.223285: step 427400, loss = 0.19 (1294.7 examples/sec; 0.099 sec/batch)
2017-06-02 12:58:56.992261: step 427410, loss = 0.13 (1664.6 examples/sec; 0.077 sec/batch)
2017-06-02 12:58:57.867068: step 427420, loss = 0.14 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:58:58.733986: step 427430, loss = 0.12 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:58:59.602859: step 427440, loss = 0.13 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 12:59:00.433762: step 427450, loss = 0.14 (1540.5 examples/sec; 0.083 sec/batch)
2017-06-02 12:59:01.292515: step 427460, loss = 0.14 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:59:02.155440: step 427470, loss = 0.17 (1483.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:59:03.020134: step 427480, loss = 0.12 (1480.3 examples/sec; 0.086 sec/batch)
2017-06-02 12:59:03.895519: step 427490, loss = 0.14 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:59:04.874456: step 427500, loss = 0.12 (1307.6 examples/sec; 0.098 sec/batch)
2017-06-02 12:59:05.634774: step 427510, loss = 0.13 (1683.5 examples/sec; 0.076 sec/batch)
2017-06-02 12:59:06.502344: step 427520, loss = 0.12 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:59:07.365072: step 427530, loss = 0.12 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:59:08.252350: step 427540, loss = 0.11 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 12:59:09.094393: step 427550, loss = 0.14 (1520.1 examples/sec; 0.084 sec/batch)
2017-06-02 12:59:09.953826: step 427560, loss = 0.10 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:59:10.819239: step 427570, loss = 0.13 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 12:59:11.667888: step 427580, loss = 0.13 (1508.3 examples/sec; 0.085 sec/batch)
2017-06-02 12:59:12.519876: step 427590, loss = 0.15 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:59:13.478345: step 427600, loss = 0.11 (1335.4 examples/sec; 0.096 sec/batch)
2017-06-02 12:59:14.253161: step 427610, loss = 0.12 (1652.0 examples/sec; 0.077 sec/batch)
2017-06-02 12:59:15.082223: step 427620, loss = 0.12 (1543.9 examples/sec; 0.083 sec/batch)
2017-06-02 12:59:15.922030: step 427630, loss = 0.10 (1524.2 examples/sec; 0.084 sec/batch)
2017-06-02 12:59:16.803489: step 427640, loss = 0.15 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:59:17.648281: step 427650, loss = 0.11 (1515.2 examples/sec; 0.084 sec/batch)
2017-06-02 12:59:18.503295: step 427660, loss = 0.13 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 12:59:19.343838: step 427670, loss = 0.14 (1522.8 examples/sec; 0.084 sec/batch)
2017-06-02 12:59:20.206899: step 427680, loss = 0.14 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:59:21.089866: step 427690, loss = 0.19 (1449.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:59:22.041971: step 427700, loss = 0.20 (1344.4 examples/sec; 0.095 sec/batch)
2017-06-02 12:59:22.814077: step 427710, loss = 0.15 (1657.8 examples/sec; 0.077 sec/batch)
2017-06-02 12:59:23.677689: step 427720, loss = 0.09 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 12:59:24.533513: step 427730, loss = 0.17 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 12:59:25.412368: step 427740, loss = 0.13 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 12:59:26.298136: step 427750, loss = 0.20 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 12:59:27.155376: step 427760, loss = 0.18 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:59:28.008123: step 427770, loss = 0.15 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 12:59:28.872710: step 427780, loss = 0.15 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 12:59:29.727942: step 427790, loss = 0.14 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:59:30.670519: step 427800, loss = 0.11 (1358.0 examples/sec; 0.094 sec/batch)
2017-06-02 12:59:31.438473: step 427810, loss = 0.10 (1666.8 examples/sec; 0.077 sec/batch)
2017-06-02 12:59:32.299034: step 427820, loss = 0.17 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 12:59:33.160565: step 427830, loss = 0.14 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:59:34.026305: step 427840, loss = 0.12 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 12:59:34.867601: step 427850, loss = 0.11 (1521.5 examples/sec; 0.084 sec/batch)
2017-06-02 12:59:35.749320: step 427860, loss = 0.13 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 12:59:36.645170: step 427870, loss = 0.12 (1428.8 examples/sec; 0.090 sec/batch)
2017-06-02 12:59:37.529138: step 427880, loss = 0.11 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 12:59:38.394595: step 427890, loss = 0.10 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 12:59:39.377946: step 427900, loss = 0.14 (1301.7 examples/sec; 0.098 sec/batch)
2017-06-02 12:59:40.162740: step 427910, loss = 0.11 (1631.0 examples/sec; 0.078 sec/batch)
2017-06-02 12:59:41.020532: step 427920, loss = 0.14 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 12:59:41.885453: step 427930, loss = 0.12 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 12:59:42.757749: step 427940, loss = 0.10 (1467.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:59:43.640377: step 427950, loss = 0.15 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 12:59:44.512084: step 427960, loss = 0.12 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 12:59:45.367300: step 427970, loss = 0.13 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 12:59:46.246943: step 427980, loss = 0.12 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 12:59:47.120331: step 427990, loss = 0.14 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 12:59:48.108767: step 428000, loss = 0.10 (1295.0 examples/sec; 0.099 sec/batch)
2017-06-02 12:59:48.846419: step 428010, loss = 0.15 (1735.3 examples/sec; 0.074 sec/batch)
2017-06-02 12:59:49.699229: step 428020, loss = 0.14 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 12:59:50.554079: step 428030, loss = 0.11 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 12:59:51.421364: step 428040, loss = 0.13 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 12:59:52.294029: step 428050, loss = 0.16 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 12:59:53.243853: step 428060, loss = 0.11 (1347.6 examples/sec; 0.095 sec/batch)
2017-06-02 12:59:54.092925: step 428070, loss = 0.12 (1507.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:59:54.977812: step 428080, loss = 0.12 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 12:59:55.829180: step 428090, loss = 0.13 (1503.5 examples/sec; 0.085 sec/batch)
2017-06-02 12:59:56.791493: step 428100, loss = 0.13 (1330.1 examples/sec; 0.096 sec/batch)
2017-06-02 12:59:57.577846: step 428110, loss = 0.13 (1627.8 examples/sec; 0.079 sec/batch)
2017-06-02 12:59:58.438659: step 428120, loss = 0.12 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 12:59:59.277781: step 428130, loss = 0.15 (1525.4 examples/sec; 0.084 sec/batch)
2017-06-02 13:00:00.155527: step 428140, loss = 0.13 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 13:00:01.005637: step 428150, loss = 0.15 (1505.7 examples/sec; 0.085 sec/batch)
2017-06-02 13:00:01.886174: step 428160, loss = 0.13 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 13:00:02.739713: step 428170, loss = 0.13 (1499.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:00:03.611625: step 428180, loss = 0.16 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 13:00:04.497778: step 428190, loss = 0.13 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 13:00:05.447310: step 428200, loss = 0.14 (1348.0 examples/sec; 0.095 sec/batch)
2017-06-02 13:00:06.215506: step 428210, loss = 0.18 (1666.2 examples/sec; 0.077 sec/batch)
2017-06-02 13:00:07.087661: step 428220, loss = 0.10 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:00:07.922352: step 428230, loss = 0.19 (1533.5 examples/sec; 0.083 sec/batch)
2017-06-02 13:00:08.766704: step 428240, loss = 0.11 (1516.0 examples/sec; 0.084 sec/batch)
2017-06-02 13:00:09.605370: step 428250, loss = 0.16 (1526.2 examples/sec; 0.084 sec/batch)
2017-06-02 13:00:10.464567: step 428260, loss = 0.13 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:00:11.326400: step 428270, loss = 0.11 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:00:12.181044: step 428280, loss = 0.11 (1497.7 examples/sec; 0.085 sec/batch)
2017-06-02 13:00:13.058324: step 428290, loss = 0.13 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 13:00:14.042993: step 428300, loss = 0.12 (1299.9 examples/sec; 0.098 sec/batch)
2017-06-02 13:00:14.786818: step 428310, loss = 0.15 (1720.8 examples/sec; 0.074 sec/batch)
2017-06-02 13:00:15.650064: step 428320, loss = 0.10 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:00:16.508096: step 428330, loss = 0.16 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:00:17.362922: step 428340, loss = 0.12 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 13:00:18.212528: step 428350, loss = 0.10 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:00:19.097534: step 428360, loss = 0.15 (1446.3 examples/sec; 0.088 sec/batch)
2017-06-02 13:00:19.976930: step 428370, loss = 0.12 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 13:00:20.841984: step 428380, loss = 0.14 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 13:00:21.716600: step 428390, loss = 0.13 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:00:22.682563: step 428400, loss = 0.13 (1325.1 examples/sec; 0.097 sec/batch)
2017-06-02 13:00:23.439479: step 428410, loss = 0.17 (1691.1 examples/sec; 0.076 sec/batch)
2017-06-02 13:00:24.309460: step 428420, loss = 0.11 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 13:00:25.176911: step 428430, loss = 0.13 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:00:26.063335: step 428440, loss = 0.14 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 13:00:26.925341: step 428450, loss = 0.14 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 13:00:27.779474: step 428460, loss = 0.18 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:00:28.627039: step 428470, loss = 0.11 (1510.2 examples/sec; 0.085 sec/batch)
2017-06-02 13:00:29.497248: step 428480, loss = 0.10 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:00:30.376051: step 428490, loss = 0.18 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 13:00:31.360288: step 428500, loss = 0.19 (1300.5 examples/sec; 0.098 sec/batch)
2017-06-02 13:00:32.115534: step 428510, loss = 0.13 (1694.8 examples/sec; 0.076 sec/batch)
2017-06-02 13:00:33.005319: step 428520, loss = 0.13 (1438.5 examples/sec; 0.089 sec/batch)
2017-06-02 13:00:33.886502: step 428530, loss = 0.16 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 13:00:34.719768: step 428540, loss = 0.11 (1536.1 examples/sec; 0.083 sec/batch)
2017-06-02 13:00:35.581068: step 428550, loss = 0.15 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 13:00:36.465130: step 428560, loss = 0.16 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 13:00:37.321682: step 428570, loss = 0.12 (1494.4 examples/sec; 0.086 sec/batch)
2017-06-02 13:00:38.174665: step 428580, loss = 0.12 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:00:39.040011: step 428590, loss = 0.12 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:00:40.010950: step 428600, loss = 0.11 (1318.3 examples/sec; 0.097 sec/batch)
2017-06-02 13:00:40.802858: step 428610, loss = 0.12 (1616.4 examples/sec; 0.079 sec/batch)
2017-06-02 13:00:41.685485: step 428620, loss = 0.16 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 13:00:42.548065: step 428630, loss = 0.12 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 13:00:43.438482: step 428640, loss = 0.10 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 13:00:44.290087: step 428650, loss = 0.13 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 13:00:45.179496: step 428660, loss = 0.12 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 13:00:46.065690: step 428670, loss = 0.19 (1444.4 examples/sec; 0.089 sec/batch)
2017-06-02 13:00:46.962081: step 428680, loss = 0.11 (1427.9 examples/sec; 0.090 sec/batch)
2017-06-02 13:00:47.861134: step 428690, loss = 0.13 (1423.7 examples/sec; 0.090 sec/batch)
2017-06-02 13:00:48.854284: step 428700, loss = 0.14 (1288.8 examples/sec; 0.099 sec/batch)
2017-06-02 13:00:49.616877: step 428710, loss = 0.11 (1678.5 examples/sec; 0.076 sec/batch)
2017-06-02 13:00:50.473761: step 428720, loss = 0.11 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 13:00:51.320902: step 428730, loss = 0.13 (1511.0 examples/sec; 0.085 sec/batch)
2017-06-02 13:00:52.154601: step 428740, loss = 0.11 (1535.3 examples/sec; 0.083 sec/batch)
2017-06-02 13:00:53.036694: step 428750, loss = 0.12 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 13:00:53.876058: step 428760, loss = 0.13 (1525.0 examples/sec; 0.084 sec/batch)
2017-06-02 13:00:54.725629: step 428770, loss = 0.15 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:00:55.598593: step 428780, loss = 0.14 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 13:00:56.461373: step 428790, loss = 0.20 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 13:00:57.420337: step 428800, loss = 0.12 (1334.8 examples/sec; 0.096 sec/batch)
2017-06-02 13:00:58.194817: step 428810, loss = 0.12 (1652.8 examples/sec; 0.077 sec/batch)
2017-06-02 13:00:59.063223: step 428820, loss = 0.12 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:00:59.901946: step 428830, loss = 0.19 (1526.1 examples/sec; 0.084 sec/batch)
2017-06-02 13:01:00.767195: step 428840, loss = 0.13 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 13:01:01.645969: step 428850, loss = 0.12 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 13:01:02.516552: step 428860, loss = 0.15 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 13:01:03.361034: step 428870, loss = 0.17 (1515.7 examples/sec; 0.084 sec/batch)
2017-06-02 13:01:04.213017: step 428880, loss = 0.12 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 13:01:05.065563: step 428890, loss = 0.13 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 13:01:06.019337: step 428900, loss = 0.17 (1342.0 examples/sec; 0.095 sec/batch)
2017-06-02 13:01:06.785278: step 428910, loss = 0.14 (1671.2 examples/sec; 0.077 sec/batch)
2017-06-02 13:01:07.646866: step 428920, loss = 0.12 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 13:01:08.489238: step 428930, loss = 0.12 (1519.5 examples/sec; 0.084 sec/batch)
2017-06-02 13:01:09.361327: step 428940, loss = 0.13 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:01:10.228971: step 428950, loss = 0.12 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:01:11.089278: step 428960, loss = 0.14 (1487.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:01:11.930889: step 428970, loss = 0.13 (1520.9 examples/sec; 0.084 sec/batch)
2017-06-02 13:01:12.817570: step 428980, loss = 0.11 (1443.6 examples/sec; 0.089 sec/batch)
2017-06-02 13:01:13.646113: step 428990, loss = 0.13 (1544.9 examples/sec; 0.083 sec/batch)
2017-06-02 13:01:14.627500: step 429000, loss = 0.15 (1304.3 examples/sec; 0.098 sec/batch)
2017-06-02 13:01:15.417485: step 429010, loss = 0.17 (1620.3 examples/sec; 0.079 sec/batch)
2017-06-02 13:01:16.280958: step 429020, loss = 0.11 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 13:01:17.147702: step 429030, loss = 0.13 (1476.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:01:17.999836: step 429040, loss = 0.20 (1502.1 examples/sec; 0.085 sec/batch)
2017-06-02 13:01:18.876681: step 429050, loss = 0.11 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 13:01:19.743934: step 429060, loss = 0.10 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:01:20.602064: step 429070, loss = 0.14 (1491.6 examples/sec; 0.086 sec/batch)
2017-06-02 13:01:21.461638: step 429080, loss = 0.13 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 13:01:22.333634: step 429090, loss = 0.15 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:01:23.297809: step 429100, loss = 0.16 (1327.6 examples/sec; 0.096 sec/batch)
2017-06-02 13:01:24.077135: step 429110, loss = 0.14 (1642.4 examples/sec; 0.078 sec/batch)
2017-06-02 13:01:24.942102: step 429120, loss = 0.19 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:01:25.811800: step 429130, loss = 0.15 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:01:26.659461: step 429140, loss = 0.13 (1510.0 examples/sec; 0.085 sec/batch)
2017-06-02 13:01:27.529196: step 429150, loss = 0.16 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 13:01:28.411767: step 429160, loss = 0.16 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 13:01:29.289514: step 429170, loss = 0.16 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 13:01:30.136334: step 429180, loss = 0.15 (1511.5 examples/sec; 0.085 sec/batch)
2017-06-02 13:01:30.994139: step 429190, loss = 0.14 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:01:32.013472: step 429200, loss = 0.11 (1255.7 examples/sec; 0.102 sec/batch)
2017-06-02 13:01:32.705831: step 429210, loss = 0.13 (1848.8 examples/sec; 0.069 sec/batch)
2017-06-02 13:01:33.585979: step 429220, loss = 0.11 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 13:01:34.450353: step 429230, loss = 0.15 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 13:01:35.339748: step 429240, loss = 0.14 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 13:01:36.222118: step 429250, loss = 0.13 (1450.6 examples/sec; 0.088 sec/batch)
2017-06-02 13:01:37.089340: step 429260, loss = 0.14 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 13:01:37.947152: step 429270, loss = 0.15 (1492.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:01:38.829074: step 429280, loss = 0.15 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 13:01:39.691584: step 429290, loss = 0.14 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 13:01:40.649634: step 429300, loss = 0.13 (1336.1 examples/sec; 0.096 sec/batch)
2017-06-02 13:01:41.403832: step 429310, loss = 0.11 (1697.1 examples/sec; 0.075 sec/batch)
2017-06-02 13:01:42.248346: step 429320, loss = 0.15 (1515.7 examples/sec; 0.084 sec/batch)
2017-06-02 13:01:43.115568: step 429330, loss = 0.15 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 13:01:44.003863: step 429340, loss = 0.13 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 13:01:44.869185: step 429350, loss = 0.14 (1479.3 examples/sec; 0.087 sec/batch)
2017-06-02 13:01:45.753594: step 429360, loss = 0.16 (1447.3 examples/sec; 0.088 sec/batch)
2017-06-02 13:01:46.627179: step 429370, loss = 0.16 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:01:47.510304: step 429380, loss = 0.14 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 13:01:48.361059: step 429390, loss = 0.14 (1504.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:01:49.324922: step 429400, loss = 0.15 (1328.0 examples/sec; 0.096 sec/batch)
2017-06-02 13:01:50.090801: step 429410, loss = 0.12 (1671.3 examples/sec; 0.077 sec/batch)
2017-06-02 13:01:50.953658: step 429420, loss = 0.12 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 13:01:51.831546: step 429430, loss = 0.14 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 13:01:52.704694: step 429440, loss = 0.15 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 13:01:53.556416: step 429450, loss = 0.13 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 13:01:54.411009: step 429460, loss = 0.11 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 13:01:55.278039: step 429470, loss = 0.13 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 13:01:56.159639: step 429480, loss = 0.17 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 13:01:57.044832: step 429490, loss = 0.14 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 13:01:58.013939: step 429500, loss = 0.12 (1320.8 examples/sec; 0.097 sec/batch)
2017-06-02 13:01:58.771387: step 429510, loss = 0.16 (1689.9 examples/sec; 0.076 sec/batch)
2017-06-02 13:01:59.611106: step 429520, loss = 0.13 (1524.3 examples/sec; 0.084 sec/batch)
2017-06-02 13:02:00.463738: step 429530, loss = 0.15 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 13:02:01.350466: step 429540, loss = 0.15 (1443.5 examples/sec; 0.089 sec/batch)
2017-06-02 13:02:02.195011: step 429550, loss = 0.11 (1515.6 examples/sec; 0.084 sec/batch)
2017-06-02 13:02:03.060422: step 429560, loss = 0.12 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 13:02:03.900341: step 429570, loss = 0.12 (1524.0 examples/sec; 0.084 sec/batch)
2017-06-02 13:02:04.765059: step 429580, loss = 0.12 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:02:05.644907: step 429590, loss = 0.15 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 13:02:06.628416: step 429600, loss = 0.12 (1301.4 examples/sec; 0.098 sec/batch)
2017-06-02 13:02:07.408846: step 429610, loss = 0.12 (1640.1 examples/sec; 0.078 sec/batch)
2017-06-02 13:02:08.263512: step 429620, loss = 0.12 (1497.7 examples/sec; 0.085 sec/batch)
2017-06-02 13:02:09.128570: step 429630, loss = 0.12 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 13:02:10.004884: step 429640, loss = 0.14 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 13:02:10.867579: step 429650, loss = 0.15 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 13:02:11.721391: step 429660, loss = 0.12 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 13:02:12.582517: step 429670, loss = 0.15 (1486.4 examples/sec; 0.086 sec/batch)
2017-06-02 13:02:13.446359: step 429680, loss = 0.14 (1481.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:02:14.321712: step 429690, loss = 0.13 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 13:02:15.266115: step 429700, loss = 0.14 (1355.3 examples/sec; 0.094 sec/batch)
2017-06-02 13:02:16.059187: step 429710, loss = 0.15 (1614.0 examples/sec; 0.079 sec/batch)
2017-06-02 13:02:16.927115: step 429720, loss = 0.14 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:02:17.793075: step 429730, loss = 0.14 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 13:02:18.670630: step 429740, loss = 0.15 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 13:02:19.516865: step 429750, loss = 0.11 (1512.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:02:20.404138: step 429760, loss = 0.12 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 13:02:21.286224: step 429770, loss = 0.14 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 13:02:22.165618: step 429780, loss = 0.13 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 13:02:23.022050: step 429790, loss = 0.12 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 13:02:24.007363: step 429800, loss = 0.16 (1299.1 examples/sec; 0.099 sec/batch)
2017-06-02 13:02:24.786772: step 429810, loss = 0.16 (1642.3 examples/sec; 0.078 sec/batch)
2017-06-02 13:02:25.635576: step 429820, loss = 0.14 (1508.0 examples/sec; 0.085 sec/batch)
2017-06-02 13:02:26.506164: step 429830, loss = 0.16 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 13:02:27.368156: step 429840, loss = 0.14 (1484.9 examples/sec; 0.086 sec/batch)
2017-06-02 13:02:28.247510: step 429850, loss = 0.15 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 13:02:29.101641: step 429860, loss = 0.15 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:02:29.967044: step 429870, loss = 0.13 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 13:02:30.814942: step 429880, loss = 0.14 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:02:31.680569: step 429890, loss = 0.12 (1478.7 examples/sec; 0.087 sec/batch)
2017-06-02 13:02:32.655223: step 429900, loss = 0.13 (1313.3 examples/sec; 0.097 sec/batch)
2017-06-02 13:02:33.425364: step 429910, loss = 0.12 (1662.0 examples/sec; 0.077 sec/batch)
2017-06-02 13:02:34.263812: step 429920, loss = 0.13 (1526.6 examples/sec; 0.084 sec/batch)
2017-06-02 13:02:35.095999: step 429930, loss = 0.13 (1538.1 examples/sec; 0.083 sec/batch)
2017-06-02 13:02:35.959358: step 429940, loss = 0.14 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 13:02:36.837291: step 429950, loss = 0.17 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 13:02:37.691708: step 429960, loss = 0.13 (1498.1 examples/sec; 0.085 sec/batch)
2017-06-02 13:02:38.526329: step 429970, loss = 0.14 (1533.6 examples/sec; 0.083 sec/batch)
2017-06-02 13:02:39.402127: step 429980, loss = 0.13 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 13:02:40.262120: step 429990, loss = 0.15 (1488.4 examples/sec; 0.086 sec/batch)
2017-06-02 13:02:41.259421: step 430000, loss = 0.15 (1283.5 examples/sec; 0.100 sec/batch)
2017-06-02 13:02:41.946225: step 430010, loss = 0.15 (1863.7 examples/sec; 0.069 sec/batch)
2017-06-02 13:02:42.799791: step 430020, loss = 0.13 (1499.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:02:43.687063: step 430030, loss = 0.13 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 13:02:44.554376: step 430040, loss = 0.12 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:02:45.396558: step 430050, loss = 0.11 (1519.9 examples/sec; 0.084 sec/batch)
2017-06-02 13:02:46.235687: step 430060, loss = 0.14 (1525.4 examples/sec; 0.084 sec/batch)
2017-06-02 13:02:47.104380: step 430070, loss = 0.14 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:02:47.933764: step 430080, loss = 0.17 (1543.3 examples/sec; 0.083 sec/batch)
2017-06-02 13:02:48.814003: step 430090, loss = 0.15 (1454.2 examples/sec; 0.088 sec/batch)
2017-06-02 13:02:49.852501: step 430100, loss = 0.13 (1232.5 examples/sec; 0.104 sec/batch)
2017-06-02 13:02:50.560058: step 430110, loss = 0.10 (1809.0 examples/sec; 0.071 sec/batch)
2017-06-02 13:02:51.405629: step 430120, loss = 0.13 (1513.8 examples/sec; 0.085 sec/batch)
2017-06-02 13:02:52.261208: step 430130, loss = 0.10 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 13:02:53.118101: step 430140, loss = 0.15 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:02:53.966297: step 430150, loss = 0.10 (1509.1 examples/sec; 0.085 sec/batch)
2017-06-02 13:02:54.831359: step 430160, loss = 0.12 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 13:02:55.704424: step 430170, loss = 0.11 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 13:02:56.570930: step 430180, loss = 0.13 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:02:57.406538: step 430190, loss = 0.15 (1531.8 examples/sec; 0.084 sec/batch)
2017-06-02 13:02:58.372406: step 430200, loss = 0.14 (1325.2 examples/sec; 0.097 sec/batch)
2017-06-02 13:02:59.129521: step 430210, loss = 0.11 (1690.6 examples/sec; 0.076 sec/batch)
2017-06-02 13:02:59.979045: step 430220, loss = 0.10 (1506.7 examples/sec; 0.085 sec/batch)
2017-06-02 13:03:00.847425: step 430230, loss = 0.15 (1474.0 examples/sec; 0.087 sec/batch)
2017-06-02 13:03:01.726034: step 430240, loss = 0.14 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 13:03:02.589632: step 430250, loss = 0.13 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:03:03.429660: step 430260, loss = 0.12 (1523.8 examples/sec; 0.084 sec/batch)
2017-06-02 13:03:04.291046: step 430270, loss = 0.13 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 13:03:05.179096: step 430280, loss = 0.16 (1441.3 examples/sec; 0.089 sec/batch)
2017-06-02 13:03:06.066865: step 430290, loss = 0.13 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 13:03:07.022992: step 430300, loss = 0.12 (1338.7 examples/sec; 0.096 sec/batch)
2017-06-02 13:03:07.814147: step 430310, loss = 0.14 (1617.9 examples/sec; 0.079 sec/batch)
2017-06-02 13:03:08.688362: step 430320, loss = 0.12 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:03:09.553234: step 430330, loss = 0.12 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 13:03:10.418006: step 430340, loss = 0.10 (1480.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:03:11.293245: step 430350, loss = 0.13 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 13:03:12.160173: step 430360, loss = 0.11 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:03:13.035887: step 430370, loss = 0.14 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 13:03:13.901717: step 430380, loss = 0.15 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 13:03:14.778856: step 430390, loss = 0.12 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 13:03:15.725667: step 430400, loss = 0.15 (1351.9 examples/sec; 0.095 sec/batch)
2017-06-02 13:03:16.503167: step 430410, loss = 0.14 (1646.3 examples/sec; 0.078 sec/batch)
2017-06-02 13:03:17.362343: step 430420, loss = 0.13 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:03:18.223974: step 430430, loss = 0.14 (1485.5 examples/sec; 0.086 sec/batch)
2017-06-02 13:03:19.121794: step 430440, loss = 0.11 (1425.7 examples/sec; 0.090 sec/batch)
2017-06-02 13:03:20.021243: step 430450, loss = 0.12 (1423.1 examples/sec; 0.090 sec/batch)
2017-06-02 13:03:20.909283: step 430460, loss = 0.16 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 13:03:21.765877: step 430470, loss = 0.14 (1494.3 examples/sec; 0.086 sec/batch)
2017-06-02 13:03:22.608452: step 430480, loss = 0.13 (1519.1 examples/sec; 0.084 sec/batch)
2017-06-02 13:03:23.460218: step 430490, loss = 0.15 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 13:03:24.435490: step 430500, loss = 0.12 (1312.4 examples/sec; 0.098 sec/batch)
2017-06-02 13:03:25.201344: step 430510, loss = 0.15 (1671.4 examples/sec; 0.077 sec/batch)
2017-06-02 13:03:26.050889: step 430520, loss = 0.14 (1506.7 examples/sec; 0.085 sec/batch)
2017-06-02 13:03:26.938448: step 430530, loss = 0.17 (1442.2 examples/sec; 0.089 sec/batch)
2017-06-02 13:03:27.806052: step 430540, loss = 0.14 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 13:03:28.671006: step 430550, loss = 0.18 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:03:29.554107: step 430560, loss = 0.12 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 13:03:30.420553: step 430570, loss = 0.14 (1477.3 examples/sec; 0.087 sec/batch)
2017-06-02 13:03:31.254611: step 430580, loss = 0.12 (1534.7 examples/sec; 0.083 sec/batch)
2017-06-02 13:03:32.139997: step 430590, loss = 0.13 (1445.7 examples/sec; 0.089 sec/batch)
2017-06-02 13:03:33.134034: step 430600, loss = 0.15 (1287.7 examples/sec; 0.099 sec/batch)
2017-06-02 13:03:33.868018: step 430610, loss = 0.12 (1743.9 examples/sec; 0.073 sec/batch)
2017-06-02 13:03:34.736751: step 430620, loss = 0.17 (1473.4 examples/sec; 0.087 sec/batch)
2017-06-02 13:03:35.592690: step 430630, loss = 0.13 (1495.4 examples/sec; 0.086 sec/batch)
2017-06-02 13:03:36.447293: step 430640, loss = 0.14 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 13:03:37.312363: step 430650, loss = 0.12 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:03:38.168858: step 430660, loss = 0.12 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 13:03:39.051450: step 430670, loss = 0.12 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 13:03:39.907708: step 430680, loss = 0.14 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 13:03:40.734338: step 430690, loss = 0.13 (1548.4 examples/sec; 0.083 sec/batch)
2017-06-02 13:03:41.750278: step 430700, loss = 0.14 (1259.9 examples/sec; 0.102 sec/batch)
2017-06-02 13:03:42.474437: step 430710, loss = 0.11 (1767.6 examples/sec; 0.072 sec/batch)
2017-06-02 13:03:43.334458: step 430720, loss = 0.16 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 13:03:44.232253: step 430730, loss = 0.16 (1425.7 examples/sec; 0.090 sec/batch)
2017-06-02 13:03:45.082140: step 430740, loss = 0.13 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 13:03:45.915872: step 430750, loss = 0.14 (1535.3 examples/sec; 0.083 sec/batch)
2017-06-02 13:03:46.763030: step 430760, loss = 0.14 (1511.0 examples/sec; 0.085 sec/batch)
2017-06-02 13:03:47.647790: step 430770, loss = 0.15 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 13:03:48.502480: step 430780, loss = 0.14 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:03:49.362288: step 430790, loss = 0.15 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 13:03:50.432042: step 430800, loss = 0.20 (1196.5 examples/sec; 0.107 sec/batch)
2017-06-02 13:03:51.275408: step 430810, loss = 0.12 (1517.7 examples/sec; 0.084 sec/batch)
2017-06-02 13:03:52.185660: step 430820, loss = 0.15 (1406.2 examples/sec; 0.091 sec/batch)
2017-06-02 13:03:53.028403: step 430830, loss = 0.12 (1518.8 examples/sec; 0.084 sec/batch)
2017-06-02 13:03:54.065184: step 430840, loss = 0.12 (1234.6 examples/sec; 0.104 sec/batch)
2017-06-02 13:03:57.654683: step 430850, loss = 0.17 (356.6 examples/sec; 0.359 sec/batch)
2017-06-02 13:04:01.415118: step 430860, loss = 0.15 (340.4 examples/sec; 0.376 sec/batch)
2017-06-02 13:04:04.858794: step 430870, loss = 0.17 (371.7 examples/sec; 0.344 sec/batch)
2017-06-02 13:04:06.444410: step 430880, loss = 0.14 (807.3 examples/sec; 0.159 sec/batch)
2017-06-02 13:04:07.286902: step 430890, loss = 0.14 (1519.3 examples/sec; 0.084 sec/batch)
2017-06-02 13:04:08.262610: step 430900, loss = 0.16 (1311.9 examples/sec; 0.098 sec/batch)
2017-06-02 13:04:09.017367: step 430910, loss = 0.14 (1695.9 examples/sec; 0.075 sec/batch)
2017-06-02 13:04:09.869763: step 430920, loss = 0.16 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:04:10.734160: step 430930, loss = 0.11 (1480.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:04:11.596997: step 430940, loss = 0.18 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 13:04:12.451829: step 430950, loss = 0.12 (1497.4 examples/sec; 0.085 sec/batch)
2017-06-02 13:04:13.312100: step 430960, loss = 0.12 (1487.9 examples/sec; 0.086 sec/batch)
2017-06-02 13:04:14.197225: step 430970, loss = 0.12 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 13:04:15.055077: step 430980, loss = 0.14 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 13:04:15.913162: step 430990, loss = 0.12 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 13:04:17.070156: step 431000, loss = 0.19 (1106.3 examples/sec; 0.116 sec/batch)
2017-06-02 13:04:17.845493: step 431010, loss = 0.20 (1650.9 examples/sec; 0.078 sec/batch)
2017-06-02 13:04:18.739347: step 431020, loss = 0.14 (1432.0 examples/sec; 0.089 sec/batch)
2017-06-02 13:04:19.598906: step 431030, loss = 0.12 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 13:04:20.482741: step 431040, loss = 0.10 (1448.3 examples/sec; 0.088 sec/batch)
2017-06-02 13:04:21.372619: step 431050, loss = 0.14 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 13:04:22.215094: step 431060, loss = 0.14 (1519.3 examples/sec; 0.084 sec/batch)
2017-06-02 13:04:23.094448: step 431070, loss = 0.13 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 13:04:23.964377: step 431080, loss = 0.12 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 13:04:24.809286: step 431090, loss = 0.12 (1515.0 examples/sec; 0.084 sec/batch)
2017-06-02 13:04:25.792567: step 431100, loss = 0.17 (1301.8 examples/sec; 0.098 sec/batch)
2017-06-02 13:04:26.549212: step 431110, loss = 0.14 (1691.7 examples/sec; 0.076 sec/batch)
2017-06-02 13:04:27.425873: step 431120, loss = 0.13 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 13:04:28.290749: step 431130, loss = 0.14 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 13:04:29.164432: step 431140, loss = 0.13 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 13:04:30.012238: step 431150, loss = 0.12 (1509.8 examples/sec; 0.085 sec/batch)
2017-06-02 13:04:30.882355: step 431160, loss = 0.18 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 13:04:31.742836: step 431170, loss = 0.14 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 13:04:32.615708: step 431180, loss = 0.11 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 13:04:33.484408: step 431190, loss = 0.13 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:04:34.469105: step 431200, loss = 0.11 (1299.9 examples/sec; 0.098 sec/batch)
2017-06-02 13:04:35.230393: step 431210, loss = 0.14 (1681.3 examples/sec; 0.076 sec/batch)
2017-06-02 13:04:36.120050: step 431220, loss = 0.15 (1438.8 examples/sec; 0.089 sec/batch)
2017-06-02 13:04:37.014057: step 431230, loss = 0.14 (1431.8 examples/sec; 0.089 sec/batch)
2017-06-02 13:04:37.891699: step 431240, loss = 0.12 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 13:04:38.777480: step 431250, loss = 0.15 (1445.1 examples/sec; 0.089 sec/batch)
2017-06-02 13:04:39.652521: step 431260, loss = 0.12 (1462.8 examples/sec; 0.088 sec/batch)
2017-06-02 13:04:40.516241: step 431270, loss = 0.13 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 13:04:41.373357: step 431280, loss = 0.15 (1493.4 examples/sec; 0.086 sec/batch)
2017-06-02 13:04:42.231989: step 431290, loss = 0.13 (1490.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:04:43.245257: step 431300, loss = 0.15 (1263.2 examples/sec; 0.101 sec/batch)
2017-06-02 13:04:43.960763: step 431310, loss = 0.16 (1788.9 examples/sec; 0.072 sec/batch)
2017-06-02 13:04:44.831135: step 431320, loss = 0.14 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:04:45.699107: step 431330, loss = 0.11 (1474.7 examples/sec; 0.087 sec/batch)
2017-06-02 13:04:46.578108: step 431340, loss = 0.14 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 13:04:47.434102: step 431350, loss = 0.16 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 13:04:48.317966: step 431360, loss = 0.14 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 13:04:49.171433: step 431370, loss = 0.11 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 13:04:50.060309: step 431380, loss = 0.17 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 13:04:50.930512: step 431390, loss = 0.13 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:04:51.911709: step 431400, loss = 0.17 (1304.5 examples/sec; 0.098 sec/batch)
2017-06-02 13:04:52.672939: step 431410, loss = 0.11 (1681.5 examples/sec; 0.076 sec/batch)
2017-06-02 13:04:53.545334: step 431420, loss = 0.14 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:04:54.390457: step 431430, loss = 0.15 (1514.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:04:55.244082: step 431440, loss = 0.15 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 13:04:56.102855: step 431450, loss = 0.13 (1490.5 examples/sec; 0.086 sec/batch)
2017-06-02 13:04:56.941193: step 431460, loss = 0.12 (1526.8 examples/sec; 0.084 sec/batch)
2017-06-02 13:04:57.795210: step 431470, loss = 0.14 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 13:04:58.658014: step 431480, loss = 0.19 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 13:04:59.523346: step 431490, loss = 0.11 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:05:00.531687: step 431500, loss = 0.18 (1269.4 examples/sec; 0.101 sec/batch)
2017-06-02 13:05:01.256679: step 431510, loss = 0.11 (1765.7 examples/sec; 0.072 sec/batch)
2017-06-02 13:05:02.114446: step 431520, loss = 0.12 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 13:05:02.969479: step 431530, loss = 0.11 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 13:05:03.862282: step 431540, loss = 0.13 (1433.7 examples/sec; 0.089 sec/batch)
2017-06-02 13:05:04.749056: step 431550, loss = 0.11 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 13:05:05.637336: step 431560, loss = 0.14 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 13:05:06.619258: step 431570, loss = 0.12 (1303.6 examples/sec; 0.098 sec/batch)
2017-06-02 13:05:07.475328: step 431580, loss = 0.16 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:05:08.311578: step 431590, loss = 0.10 (1530.6 examples/sec; 0.084 sec/batch)
2017-06-02 13:05:09.253072: step 431600, loss = 0.12 (1359.5 examples/sec; 0.094 sec/batch)
2017-06-02 13:05:10.036634: step 431610, loss = 0.12 (1633.6 examples/sec; 0.078 sec/batch)
2017-06-02 13:05:13.551128: step 431620, loss = 0.13 (364.2 examples/sec; 0.351 sec/batch)
2017-06-02 13:05:17.642229: step 431630, loss = 0.12 (312.9 examples/sec; 0.409 sec/batch)
2017-06-02 13:05:21.937464: step 431640, loss = 0.10 (298.0 examples/sec; 0.430 sec/batch)
2017-06-02 13:05:23.309309: step 431650, loss = 0.11 (933.0 examples/sec; 0.137 sec/batch)
2017-06-02 13:05:24.179552: step 431660, loss = 0.10 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:05:25.042580: step 431670, loss = 0.12 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:05:25.905269: step 431680, loss = 0.14 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 13:05:26.780231: step 431690, loss = 0.11 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:05:27.781669: step 431700, loss = 0.17 (1278.2 examples/sec; 0.100 sec/batch)
2017-06-02 13:05:28.548345: step 431710, loss = 0.13 (1669.6 examples/sec; 0.077 sec/batch)
2017-06-02 13:05:29.431554: step 431720, loss = 0.17 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 13:05:30.291065: step 431730, loss = 0.14 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:05:31.155381: step 431740, loss = 0.11 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 13:05:32.036442: step 431750, loss = 0.12 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 13:05:32.921957: step 431760, loss = 0.13 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 13:05:33.972458: step 431770, loss = 0.15 (1218.5 examples/sec; 0.105 sec/batch)
2017-06-02 13:05:34.845103: step 431780, loss = 0.24 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:05:35.724964: step 431790, loss = 0.12 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 13:05:36.719966: step 431800, loss = 0.15 (1286.4 examples/sec; 0.099 sec/batch)
2017-06-02 13:05:37.499764: step 431810, loss = 0.18 (1641.5 examples/sec; 0.078 sec/batch)
2017-06-02 13:05:38.377078: step 431820, loss = 0.13 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 13:05:39.285775: step 431830, loss = 0.22 (1408.7 examples/sec; 0.091 sec/batch)
2017-06-02 13:05:40.173454: step 431840, loss = 0.11 (1441.8 examples/sec; 0.089 sec/batch)
2017-06-02 13:05:41.058964: step 431850, loss = 0.14 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 13:05:41.951406: step 431860, loss = 0.15 (1434.3 examples/sec; 0.089 sec/batch)
2017-06-02 13:05:42.825145: step 431870, loss = 0.12 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 13:05:43.697277: step 431880, loss = 0.13 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 13:05:44.565527: step 431890, loss = 0.10 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:05:45.514289: step 431900, loss = 0.15 (1349.1 examples/sec; 0.095 sec/batch)
2017-06-02 13:05:46.283763: step 431910, loss = 0.12 (1663.5 examples/sec; 0.077 sec/batch)
2017-06-02 13:05:47.177830: step 431920, loss = 0.17 (1431.7 examples/sec; 0.089 sec/batch)
2017-06-02 13:05:48.045409: step 431930, loss = 0.15 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 13:05:48.895957: step 431940, loss = 0.13 (1504.9 examples/sec; 0.085 sec/batch)
2017-06-02 13:05:49.759875: step 431950, loss = 0.12 (1481.6 examples/sec; 0.086 sec/batch)
2017-06-02 13:05:50.634447: step 431960, loss = 0.13 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:05:51.531439: step 431970, loss = 0.16 (1427.0 examples/sec; 0.090 sec/batch)
2017-06-02 13:05:52.407958: step 431980, loss = 0.14 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 13:05:53.282767: step 431990, loss = 0.15 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:05:54.260776: step 432000, loss = 0.13 (1308.8 examples/sec; 0.098 sec/batch)
2017-06-02 13:05:55.026244: step 432010, loss = 0.12 (1672.2 examples/sec; 0.077 sec/batch)
2017-06-02 13:05:55.939952: step 432020, loss = 0.12 (1400.9 examples/sec; 0.091 sec/batch)
2017-06-02 13:05:56.837696: step 432030, loss = 0.13 (1425.8 examples/sec; 0.090 sec/batch)
2017-06-02 13:05:57.713984: step 432040, loss = 0.14 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 13:05:58.585759: step 432050, loss = 0.10 (1468.3 examples/sec; 0.087 sec/batch)
2017-06-02 13:05:59.467516: step 432060, loss = 0.12 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 13:06:00.340817: step 432070, loss = 0.10 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:06:01.232235: step 432080, loss = 0.19 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 13:06:02.103656: step 432090, loss = 0.12 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:06:03.092513: step 432100, loss = 0.15 (1294.4 examples/sec; 0.099 sec/batch)
2017-06-02 13:06:03.866422: step 432110, loss = 0.14 (1654.0 examples/sec; 0.077 sec/batch)
2017-06-02 13:06:04.736506: step 432120, loss = 0.14 (1471.1 examples/sec; 0.087 sec/batch)
2017-06-02 13:06:05.606405: step 432130, loss = 0.16 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 13:06:06.458378: step 432140, loss = 0.10 (1502.4 examples/sec; 0.085 sec/batch)
2017-06-02 13:06:07.307935: step 432150, loss = 0.13 (1506.7 examples/sec; 0.085 sec/batch)
2017-06-02 13:06:08.179300: step 432160, loss = 0.11 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 13:06:09.047863: step 432170, loss = 0.13 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 13:06:09.921824: step 432180, loss = 0.11 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:06:10.795844: step 432190, loss = 0.14 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:06:11.779942: step 432200, loss = 0.11 (1300.7 examples/sec; 0.098 sec/batch)
2017-06-02 13:06:12.525132: step 432210, loss = 0.15 (1717.7 examples/sec; 0.075 sec/batch)
2017-06-02 13:06:13.402771: step 432220, loss = 0.15 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 13:06:14.248800: step 432230, loss = 0.16 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 13:06:15.097823: step 432240, loss = 0.13 (1507.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:06:15.982901: step 432250, loss = 0.14 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 13:06:16.861853: step 432260, loss = 0.10 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 13:06:17.707749: step 432270, loss = 0.11 (1513.2 examples/sec; 0.085 sec/batch)
2017-06-02 13:06:18.588321: step 432280, loss = 0.13 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 13:06:19.458291: step 432290, loss = 0.13 (1471.3 examples/sec; 0.087 sec/batch)
2017-06-02 13:06:20.439025: step 432300, loss = 0.12 (1305.1 examples/sec; 0.098 sec/batch)
2017-06-02 13:06:21.189345: step 432310, loss = 0.15 (1705.9 examples/sec; 0.075 sec/batch)
2017-06-02 13:06:22.023020: step 432320, loss = 0.14 (1535.4 examples/sec; 0.083 sec/batch)
2017-06-02 13:06:22.921527: step 432330, loss = 0.15 (1424.6 examples/sec; 0.090 sec/batch)
2017-06-02 13:06:23.802000: step 432340, loss = 0.18 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 13:06:24.663995: step 432350, loss = 0.15 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 13:06:25.537121: step 432360, loss = 0.12 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:06:26.405668: step 432370, loss = 0.16 (1473.7 examples/sec; 0.087 sec/batch)
2017-06-02 13:06:27.239295: step 432380, loss = 0.11 (1535.5 examples/sec; 0.083 sec/batch)
2017-06-02 13:06:28.108765: step 432390, loss = 0.12 (1472.1 examples/sec; 0.087 sec/batch)
2017-06-02 13:06:29.068688: step 432400, loss = 0.12 (1333.4 examples/sec; 0.096 sec/batch)
2017-06-02 13:06:29.821665: step 432410, loss = 0.17 (1699.9 examples/sec; 0.075 sec/batch)
2017-06-02 13:06:30.717147: step 432420, loss = 0.12 (1429.4 examples/sec; 0.090 sec/batch)
2017-06-02 13:06:31.584285: step 432430, loss = 0.13 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 13:06:32.460279: step 432440, loss = 0.13 (1461.3 examples/sec; 0.088 sec/batch)
2017-06-02 13:06:33.302471: step 432450, loss = 0.15 (1519.8 examples/sec; 0.084 sec/batch)
2017-06-02 13:06:34.119897: step 432460, loss = 0.13 (1565.9 examples/sec; 0.082 sec/batch)
2017-06-02 13:06:34.985030: step 432470, loss = 0.10 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:06:35.827127: step 432480, loss = 0.13 (1520.0 examples/sec; 0.084 sec/batch)
2017-06-02 13:06:36.695166: step 432490, loss = 0.11 (1474.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:06:37.641527: step 432500, loss = 0.14 (1352.5 examples/sec; 0.095 sec/batch)
2017-06-02 13:06:38.402792: step 432510, loss = 0.13 (1681.4 examples/sec; 0.076 sec/batch)
2017-06-02 13:06:39.271409: step 432520, loss = 0.13 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:06:40.130832: step 432530, loss = 0.09 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 13:06:40.987458: step 432540, loss = 0.14 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:06:41.860149: step 432550, loss = 0.17 (1466.7 examples/sec; 0.087 sec/batch)
2017-06-02 13:06:42.707951: step 432560, loss = 0.18 (1509.8 examples/sec; 0.085 sec/batch)
2017-06-02 13:06:43.583198: step 432570, loss = 0.15 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 13:06:44.450178: step 432580, loss = 0.13 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 13:06:45.314697: step 432590, loss = 0.15 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 13:06:46.276445: step 432600, loss = 0.15 (1330.9 examples/sec; 0.096 sec/batch)
2017-06-02 13:06:47.008610: step 432610, loss = 0.14 (1748.2 examples/sec; 0.073 sec/batch)
2017-06-02 13:06:47.866348: step 432620, loss = 0.16 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 13:06:48.737858: step 432630, loss = 0.12 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 13:06:49.598807: step 432640, loss = 0.13 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 13:06:50.446854: step 432650, loss = 0.12 (1509.4 examples/sec; 0.085 sec/batch)
2017-06-02 13:06:51.299279: step 432660, loss = 0.12 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:06:52.143139: step 432670, loss = 0.12 (1516.8 examples/sec; 0.084 sec/batch)
2017-06-02 13:06:53.002624: step 432680, loss = 0.13 (1489.3 examples/sec; 0.086 sec/batch)
2017-06-02 13:06:53.854120: step 432690, loss = 0.15 (1503.2 examples/sec; 0.085 sec/batch)
2017-06-02 13:06:54.805893: step 432700, loss = 0.13 (1344.9 examples/sec; 0.095 sec/batch)
2017-06-02 13:06:55.545027: step 432710, loss = 0.12 (1731.8 examples/sec; 0.074 sec/batch)
2017-06-02 13:06:56.405972: step 432720, loss = 0.16 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 13:06:57.289067: step 432730, loss = 0.14 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 13:06:58.161206: step 432740, loss = 0.15 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 13:06:59.031011: step 432750, loss = 0.13 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:06:59.890711: step 432760, loss = 0.13 (1488.9 examples/sec; 0.086 sec/batch)
2017-06-02 13:07:00.730788: step 432770, loss = 0.13 (1523.7 examples/sec; 0.084 sec/batch)
2017-06-02 13:07:01.575242: step 432780, loss = 0.12 (1515.8 examples/sec; 0.084 sec/batch)
2017-06-02 13:07:02.442313: step 432790, loss = 0.18 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:07:03.416286: step 432800, loss = 0.12 (1314.2 examples/sec; 0.097 sec/batch)
2017-06-02 13:07:04.200839: step 432810, loss = 0.12 (1631.5 examples/sec; 0.078 sec/batch)
2017-06-02 13:07:05.072092: step 432820, loss = 0.12 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:07:05.942258: step 432830, loss = 0.15 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 13:07:06.800937: step 432840, loss = 0.15 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 13:07:07.667777: step 432850, loss = 0.15 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:07:08.536987: step 432860, loss = 0.13 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:07:09.393868: step 432870, loss = 0.13 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 13:07:10.276374: step 432880, loss = 0.17 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 13:07:11.169063: step 432890, loss = 0.15 (1433.9 examples/sec; 0.089 sec/batch)
2017-06-02 13:07:12.171019: step 432900, loss = 0.10 (1277.5 examples/sec; 0.100 sec/batch)
2017-06-02 13:07:12.926017: step 432910, loss = 0.15 (1695.4 examples/sec; 0.076 sec/batch)
2017-06-02 13:07:13.801300: step 432920, loss = 0.13 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 13:07:14.686809: step 432930, loss = 0.13 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 13:07:15.543234: step 432940, loss = 0.12 (1494.6 examples/sec; 0.086 sec/batch)
2017-06-02 13:07:16.427091: step 432950, loss = 0.13 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 13:07:17.290478: step 432960, loss = 0.15 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 13:07:18.158326: step 432970, loss = 0.13 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:07:19.025382: step 432980, loss = 0.11 (1476.3 examples/sec; 0.087 sec/batch)
2017-06-02 13:07:19.904632: step 432990, loss = 0.12 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 13:07:20.847290: step 433000, loss = 0.16 (1357.9 examples/sec; 0.094 sec/batch)
2017-06-02 13:07:21.652112: step 433010, loss = 0.16 (1590.4 examples/sec; 0.080 sec/batch)
2017-06-02 13:07:22.512625: step 433020, loss = 0.16 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 13:07:23.368448: step 433030, loss = 0.16 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 13:07:24.246627: step 433040, loss = 0.10 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 13:07:25.096337: step 433050, loss = 0.16 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 13:07:25.949592: step 433060, loss = 0.14 (1500.1 examples/sec; 0.085 sec/batch)
2017-06-02 13:07:26.814581: step 433070, loss = 0.13 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:07:27.689034: step 433080, loss = 0.14 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:07:28.526388: step 433090, loss = 0.15 (1528.6 examples/sec; 0.084 sec/batch)
2017-06-02 13:07:29.493490: step 433100, loss = 0.14 (1323.5 examples/sec; 0.097 sec/batch)
2017-06-02 13:07:30.257056: step 433110, loss = 0.14 (1676.4 examples/sec; 0.076 sec/batch)
2017-06-02 13:07:31.106091: step 433120, loss = 0.11 (1507.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:07:31.979135: step 433130, loss = 0.14 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 13:07:32.865760: step 433140, loss = 0.13 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 13:07:33.740687: step 433150, loss = 0.12 (1463.0 examples/sec; 0.087 sec/batch)
2017-06-02 13:07:34.624841: step 433160, loss = 0.11 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 13:07:35.514682: step 433170, loss = 0.13 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 13:07:36.372314: step 433180, loss = 0.17 (1492.5 examples/sec; 0.086 sec/batch)
2017-06-02 13:07:37.245434: step 433190, loss = 0.14 (1466.0 examples/sec; 0.087 sec/batch)
2017-06-02 13:07:38.193141: step 433200, loss = 0.12 (1350.6 examples/sec; 0.095 sec/batch)
2017-06-02 13:07:38.970657: step 433210, loss = 0.14 (1646.3 examples/sec; 0.078 sec/batch)
2017-06-02 13:07:39.861787: step 433220, loss = 0.10 (1436.4 examples/sec; 0.089 sec/batch)
2017-06-02 13:07:40.741779: step 433230, loss = 0.19 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 13:07:41.592280: step 433240, loss = 0.11 (1505.0 examples/sec; 0.085 sec/batch)
2017-06-02 13:07:42.434389: step 433250, loss = 0.14 (1520.0 examples/sec; 0.084 sec/batch)
2017-06-02 13:07:43.260967: step 433260, loss = 0.11 (1548.6 examples/sec; 0.083 sec/batch)
2017-06-02 13:07:44.133584: step 433270, loss = 0.14 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:07:44.991948: step 433280, loss = 0.12 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:07:45.851044: step 433290, loss = 0.13 (1489.9 examples/sec; 0.086 sec/batch)
2017-06-02 13:07:46.826586: step 433300, loss = 0.14 (1312.1 examples/sec; 0.098 sec/batch)
2017-06-02 13:07:47.592183: step 433310, loss = 0.14 (1671.9 examples/sec; 0.077 sec/batch)
2017-06-02 13:07:48.452005: step 433320, loss = 0.11 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 13:07:49.302880: step 433330, loss = 0.14 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 13:07:50.149006: step 433340, loss = 0.15 (1512.8 examples/sec; 0.085 sec/batch)
2017-06-02 13:07:51.018108: step 433350, loss = 0.12 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:07:51.886186: step 433360, loss = 0.13 (1474.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:07:52.762221: step 433370, loss = 0.15 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 13:07:53.629378: step 433380, loss = 0.13 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 13:07:54.488806: step 433390, loss = 0.11 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 13:07:55.452164: step 433400, loss = 0.10 (1328.7 examples/sec; 0.096 sec/batch)
2017-06-02 13:07:56.225980: step 433410, loss = 0.13 (1654.1 examples/sec; 0.077 sec/batch)
2017-06-02 13:07:57.080308: step 433420, loss = 0.12 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 13:07:57.935016: step 433430, loss = 0.10 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:07:58.774624: step 433440, loss = 0.14 (1524.5 examples/sec; 0.084 sec/batch)
2017-06-02 13:07:59.627166: step 433450, loss = 0.15 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 13:08:00.475334: step 433460, loss = 0.13 (1509.1 examples/sec; 0.085 sec/batch)
2017-06-02 13:08:01.347359: step 433470, loss = 0.14 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:08:02.204292: step 433480, loss = 0.14 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 13:08:03.085550: step 433490, loss = 0.13 (1452.5 examples/sec; 0.088 sec/batch)
2017-06-02 13:08:04.077648: step 433500, loss = 0.14 (1290.2 examples/sec; 0.099 sec/batch)
2017-06-02 13:08:04.786957: step 433510, loss = 0.14 (1804.6 examples/sec; 0.071 sec/batch)
2017-06-02 13:08:05.657534: step 433520, loss = 0.14 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 13:08:06.513588: step 433530, loss = 0.12 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:08:07.367432: step 433540, loss = 0.10 (1499.1 examples/sec; 0.085 sec/batch)
2017-06-02 13:08:08.230687: step 433550, loss = 0.15 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:08:09.130101: step 433560, loss = 0.12 (1423.1 examples/sec; 0.090 sec/batch)
2017-06-02 13:08:09.966408: step 433570, loss = 0.16 (1530.5 examples/sec; 0.084 sec/batch)
2017-06-02 13:08:10.848269: step 433580, loss = 0.12 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 13:08:11.706257: step 433590, loss = 0.16 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:08:12.677602: step 433600, loss = 0.15 (1317.8 examples/sec; 0.097 sec/batch)
2017-06-02 13:08:13.443226: step 433610, loss = 0.10 (1671.8 examples/sec; 0.077 sec/batch)
2017-06-02 13:08:14.289322: step 433620, loss = 0.11 (1512.8 examples/sec; 0.085 sec/batch)
2017-06-02 13:08:15.122837: step 433630, loss = 0.15 (1535.7 examples/sec; 0.083 sec/batch)
2017-06-02 13:08:15.989513: step 433640, loss = 0.14 (1476.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:08:16.868319: step 433650, loss = 0.12 (1456.5 examples/sec; 0.088 sec/batch)
2017-06-02 13:08:17.755987: step 433660, loss = 0.12 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 13:08:18.612890: step 433670, loss = 0.11 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 13:08:19.459328: step 433680, loss = 0.18 (1512.3 examples/sec; 0.085 sec/batch)
2017-06-02 13:08:20.295525: step 433690, loss = 0.11 (1530.7 examples/sec; 0.084 sec/batch)
2017-06-02 13:08:21.255809: step 433700, loss = 0.20 (1332.9 examples/sec; 0.096 sec/batch)
2017-06-02 13:08:22.011886: step 433710, loss = 0.13 (1692.9 examples/sec; 0.076 sec/batch)
2017-06-02 13:08:22.869215: step 433720, loss = 0.12 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 13:08:23.746892: step 433730, loss = 0.12 (1458.4 examples/sec; 0.088 sec/batch)
2017-06-02 13:08:24.625098: step 433740, loss = 0.15 (1457.5 examples/sec; 0.088 sec/batch)
2017-06-02 13:08:25.500818: step 433750, loss = 0.18 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 13:08:26.381229: step 433760, loss = 0.18 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 13:08:27.245668: step 433770, loss = 0.15 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 13:08:28.119497: step 433780, loss = 0.14 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:08:28.973994: step 433790, loss = 0.15 (1497.9 examples/sec; 0.085 sec/batch)
2017-06-02 13:08:29.911127: step 433800, loss = 0.14 (1365.9 examples/sec; 0.094 sec/batch)
2017-06-02 13:08:30.688903: step 433810, loss = 0.13 (1645.7 examples/sec; 0.078 sec/batch)
2017-06-02 13:08:31.544297: step 433820, loss = 0.12 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 13:08:32.386290: step 433830, loss = 0.13 (1520.2 examples/sec; 0.084 sec/batch)
2017-06-02 13:08:33.264836: step 433840, loss = 0.13 (1457.0 examples/sec; 0.088 sec/batch)
2017-06-02 13:08:34.106895: step 433850, loss = 0.11 (1520.1 examples/sec; 0.084 sec/batch)
2017-06-02 13:08:34.980400: step 433860, loss = 0.18 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 13:08:35.839360: step 433870, loss = 0.15 (1490.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:08:36.683105: step 433880, loss = 0.14 (1517.0 examples/sec; 0.084 sec/batch)
2017-06-02 13:08:37.533189: step 433890, loss = 0.17 (1505.7 examples/sec; 0.085 sec/batch)
2017-06-02 13:08:38.507130: step 433900, loss = 0.11 (1314.2 examples/sec; 0.097 sec/batch)
2017-06-02 13:08:39.262919: step 433910, loss = 0.12 (1693.6 examples/sec; 0.076 sec/batch)
2017-06-02 13:08:40.137414: step 433920, loss = 0.12 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 13:08:40.995445: step 433930, loss = 0.11 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:08:41.881970: step 433940, loss = 0.19 (1443.8 examples/sec; 0.089 sec/batch)
2017-06-02 13:08:42.724313: step 433950, loss = 0.11 (1519.6 examples/sec; 0.084 sec/batch)
2017-06-02 13:08:43.581820: step 433960, loss = 0.11 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 13:08:44.454762: step 433970, loss = 0.15 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 13:08:45.343788: step 433980, loss = 0.13 (1439.8 examples/sec; 0.089 sec/batch)
2017-06-02 13:08:46.215371: step 433990, loss = 0.15 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:08:47.218587: step 434000, loss = 0.12 (1275.9 examples/sec; 0.100 sec/batch)
2017-06-02 13:08:47.975270: step 434010, loss = 0.15 (1691.6 examples/sec; 0.076 sec/batch)
2017-06-02 13:08:48.840165: step 434020, loss = 0.13 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 13:08:49.692334: step 434030, loss = 0.13 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 13:08:50.585220: step 434040, loss = 0.11 (1433.6 examples/sec; 0.089 sec/batch)
2017-06-02 13:08:51.462786: step 434050, loss = 0.13 (1458.6 examples/sec; 0.088 sec/batch)
2017-06-02 13:08:52.321942: step 434060, loss = 0.16 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:08:53.181118: step 434070, loss = 0.13 (1489.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:08:54.017131: step 434080, loss = 0.15 (1531.1 examples/sec; 0.084 sec/batch)
2017-06-02 13:08:54.894193: step 434090, loss = 0.17 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 13:08:55.845635: step 434100, loss = 0.10 (1345.3 examples/sec; 0.095 sec/batch)
2017-06-02 13:08:56.601679: step 434110, loss = 0.13 (1693.0 examples/sec; 0.076 sec/batch)
2017-06-02 13:08:57.463267: step 434120, loss = 0.16 (1485.6 examples/sec; 0.086 sec/batch)
2017-06-02 13:08:58.322137: step 434130, loss = 0.13 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 13:08:59.178077: step 434140, loss = 0.12 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 13:09:00.043955: step 434150, loss = 0.14 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 13:09:00.919366: step 434160, loss = 0.12 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 13:09:01.790659: step 434170, loss = 0.15 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 13:09:02.656805: step 434180, loss = 0.12 (1477.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:09:03.529337: step 434190, loss = 0.13 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 13:09:04.483746: step 434200, loss = 0.14 (1341.1 examples/sec; 0.095 sec/batch)
2017-06-02 13:09:05.231631: step 434210, loss = 0.14 (1711.5 examples/sec; 0.075 sec/batch)
2017-06-02 13:09:06.110594: step 434220, loss = 0.13 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 13:09:06.981066: step 434230, loss = 0.12 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:09:07.854920: step 434240, loss = 0.13 (1464.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:09:08.723858: step 434250, loss = 0.16 (1473.1 examples/sec; 0.087 sec/batch)
2017-06-02 13:09:09.583964: step 434260, loss = 0.11 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:09:10.453706: step 434270, loss = 0.11 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 13:09:11.292766: step 434280, loss = 0.15 (1525.5 examples/sec; 0.084 sec/batch)
2017-06-02 13:09:12.149702: step 434290, loss = 0.17 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 13:09:13.127421: step 434300, loss = 0.12 (1309.1 examples/sec; 0.098 sec/batch)
2017-06-02 13:09:13.882351: step 434310, loss = 0.14 (1695.5 examples/sec; 0.075 sec/batch)
2017-06-02 13:09:14.742015: step 434320, loss = 0.13 (1489.0 examples/sec; 0.086 sec/batch)
2017-06-02 13:09:15.598493: step 434330, loss = 0.12 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 13:09:16.455485: step 434340, loss = 0.15 (1493.6 examples/sec; 0.086 sec/batch)
2017-06-02 13:09:17.328981: step 434350, loss = 0.14 (1465.3 examples/sec; 0.087 sec/batch)
2017-06-02 13:09:18.197597: step 434360, loss = 0.17 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:09:19.077084: step 434370, loss = 0.16 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 13:09:19.947265: step 434380, loss = 0.11 (1471.0 examples/sec; 0.087 sec/batch)
2017-06-02 13:09:20.809668: step 434390, loss = 0.12 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:09:21.768642: step 434400, loss = 0.14 (1334.8 examples/sec; 0.096 sec/batch)
2017-06-02 13:09:22.512039: step 434410, loss = 0.17 (1721.8 examples/sec; 0.074 sec/batch)
2017-06-02 13:09:23.390389: step 434420, loss = 0.13 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 13:09:24.251443: step 434430, loss = 0.17 (1486.6 examples/sec; 0.086 sec/batch)
2017-06-02 13:09:25.111218: step 434440, loss = 0.15 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 13:09:25.975414: step 434450, loss = 0.17 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:09:26.838906: step 434460, loss = 0.11 (1482.3 examples/sec; 0.086 sec/batch)
2017-06-02 13:09:27.731990: step 434470, loss = 0.11 (1433.2 examples/sec; 0.089 sec/batch)
2017-06-02 13:09:28.593384: step 434480, loss = 0.14 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 13:09:29.455782: step 434490, loss = 0.13 (1484.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:09:30.425512: step 434500, loss = 0.12 (1319.9 examples/sec; 0.097 sec/batch)
2017-06-02 13:09:31.194082: step 434510, loss = 0.13 (1665.4 examples/sec; 0.077 sec/batch)
2017-06-02 13:09:32.064433: step 434520, loss = 0.15 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:09:32.911205: step 434530, loss = 0.13 (1511.5 examples/sec; 0.085 sec/batch)
2017-06-02 13:09:33.767032: step 434540, loss = 0.13 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 13:09:34.609340: step 434550, loss = 0.11 (1519.6 examples/sec; 0.084 sec/batch)
2017-06-02 13:09:35.482096: step 434560, loss = 0.13 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:09:36.333505: step 434570, loss = 0.13 (1503.4 examples/sec; 0.085 sec/batch)
2017-06-02 13:09:37.171003: step 434580, loss = 0.11 (1528.4 examples/sec; 0.084 sec/batch)
2017-06-02 13:09:38.055222: step 434590, loss = 0.14 (1447.6 examples/sec; 0.088 sec/batch)
2017-06-02 13:09:39.036364: step 434600, loss = 0.15 (1304.6 examples/sec; 0.098 sec/batch)
2017-06-02 13:09:39.806430: step 434610, loss = 0.12 (1662.2 examples/sec; 0.077 sec/batch)
2017-06-02 13:09:40.680449: step 434620, loss = 0.15 (1464.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:09:41.552435: step 434630, loss = 0.14 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:09:42.406159: step 434640, loss = 0.11 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 13:09:43.263018: step 434650, loss = 0.12 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:09:44.157150: step 434660, loss = 0.12 (1431.6 examples/sec; 0.089 sec/batch)
2017-06-02 13:09:45.036123: step 434670, loss = 0.14 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 13:09:45.917300: step 434680, loss = 0.16 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 13:09:46.805173: step 434690, loss = 0.13 (1441.7 examples/sec; 0.089 sec/batch)
2017-06-02 13:09:47.773266: step 434700, loss = 0.13 (1322.2 examples/sec; 0.097 sec/batch)
2017-06-02 13:09:48.518885: step 434710, loss = 0.13 (1716.7 examples/sec; 0.075 sec/batch)
2017-06-02 13:09:49.382470: step 434720, loss = 0.13 (1482.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:09:50.245750: step 434730, loss = 0.14 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 13:09:51.093800: step 434740, loss = 0.15 (1509.4 examples/sec; 0.085 sec/batch)
2017-06-02 13:09:51.960862: step 434750, loss = 0.12 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:09:52.841459: step 434760, loss = 0.17 (1453.6 examples/sec; 0.088 sec/batch)
2017-06-02 13:09:53.770047: step 434770, loss = 0.15 (1378.5 examples/sec; 0.093 sec/batch)
2017-06-02 13:09:54.646435: step 434780, loss = 0.10 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 13:09:55.512467: step 434790, loss = 0.14 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 13:09:56.527571: step 434800, loss = 0.13 (1261.0 examples/sec; 0.102 sec/batch)
2017-06-02 13:09:57.259507: step 434810, loss = 0.14 (1748.8 examples/sec; 0.073 sec/batch)
2017-06-02 13:09:58.112335: step 434820, loss = 0.11 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 13:09:59.009124: step 434830, loss = 0.15 (1427.3 examples/sec; 0.090 sec/batch)
2017-06-02 13:09:59.876793: step 434840, loss = 0.10 (1475.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:10:00.762825: step 434850, loss = 0.17 (1444.6 examples/sec; 0.089 sec/batch)
2017-06-02 13:10:01.621646: step 434860, loss = 0.14 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 13:10:02.470762: step 434870, loss = 0.17 (1507.5 examples/sec; 0.085 sec/batch)
2017-06-02 13:10:03.339268: step 434880, loss = 0.12 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:10:04.201017: step 434890, loss = 0.12 (1485.4 examples/sec; 0.086 sec/batch)
2017-06-02 13:10:05.219368: step 434900, loss = 0.16 (1256.9 examples/sec; 0.102 sec/batch)
2017-06-02 13:10:05.977179: step 434910, loss = 0.14 (1689.1 examples/sec; 0.076 sec/batch)
2017-06-02 13:10:06.865151: step 434920, loss = 0.13 (1441.5 examples/sec; 0.089 sec/batch)
2017-06-02 13:10:07.700888: step 434930, loss = 0.13 (1531.6 examples/sec; 0.084 sec/batch)
2017-06-02 13:10:08.580477: step 434940, loss = 0.13 (1455.2 examples/sec; 0.088 sec/batch)
2017-06-02 13:10:09.432779: step 434950, loss = 0.21 (1501.8 examples/sec; 0.085 sec/batch)
2017-06-02 13:10:10.263312: step 434960, loss = 0.18 (1541.2 examples/sec; 0.083 sec/batch)
2017-06-02 13:10:11.141262: step 434970, loss = 0.13 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 13:10:12.020292: step 434980, loss = 0.12 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 13:10:12.881701: step 434990, loss = 0.14 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 13:10:13.876022: step 435000, loss = 0.14 (1287.3 examples/sec; 0.099 sec/batch)
2017-06-02 13:10:14.647219: step 435010, loss = 0.14 (1659.8 examples/sec; 0.077 sec/batch)
2017-06-02 13:10:15.516322: step 435020, loss = 0.14 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:10:16.399639: step 435030, loss = 0.14 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 13:10:17.269087: step 435040, loss = 0.16 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:10:18.122753: step 435050, loss = 0.16 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 13:10:18.989678: step 435060, loss = 0.14 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:10:19.862470: step 435070, loss = 0.15 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:10:20.705119: step 435080, loss = 0.14 (1519.0 examples/sec; 0.084 sec/batch)
2017-06-02 13:10:21.585604: step 435090, loss = 0.12 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 13:10:22.550916: step 435100, loss = 0.14 (1326.0 examples/sec; 0.097 sec/batch)
2017-06-02 13:10:23.300199: step 435110, loss = 0.17 (1708.3 examples/sec; 0.075 sec/batch)
2017-06-02 13:10:24.144933: step 435120, loss = 0.15 (1515.3 examples/sec; 0.084 sec/batch)
2017-06-02 13:10:25.014104: step 435130, loss = 0.15 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 13:10:25.868347: step 435140, loss = 0.11 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 13:10:26.716693: step 435150, loss = 0.12 (1508.8 examples/sec; 0.085 sec/batch)
2017-06-02 13:10:27.573354: step 435160, loss = 0.12 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 13:10:28.429595: step 435170, loss = 0.12 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 13:10:29.300067: step 435180, loss = 0.14 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:10:30.141162: step 435190, loss = 0.16 (1521.8 examples/sec; 0.084 sec/batch)
2017-06-02 13:10:31.124357: step 435200, loss = 0.14 (1301.9 examples/sec; 0.098 sec/batch)
2017-06-02 13:10:31.895009: step 435210, loss = 0.12 (1660.9 examples/sec; 0.077 sec/batch)
2017-06-02 13:10:32.759488: step 435220, loss = 0.11 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 13:10:33.623471: step 435230, loss = 0.14 (1481.5 examples/sec; 0.086 sec/batch)
2017-06-02 13:10:34.519837: step 435240, loss = 0.13 (1428.0 examples/sec; 0.090 sec/batch)
2017-06-02 13:10:35.393453: step 435250, loss = 0.11 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:10:36.251996: step 435260, loss = 0.14 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 13:10:37.137873: step 435270, loss = 0.14 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 13:10:38.007718: step 435280, loss = 0.13 (1471.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:10:38.896888: step 435290, loss = 0.12 (1439.6 examples/sec; 0.089 sec/batch)
2017-06-02 13:10:39.876218: step 435300, loss = 0.10 (1307.0 examples/sec; 0.098 sec/batch)
2017-06-02 13:10:40.622468: step 435310, loss = 0.17 (1715.2 examples/sec; 0.075 sec/batch)
2017-06-02 13:10:41.492166: step 435320, loss = 0.12 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:10:42.338185: step 435330, loss = 0.13 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 13:10:43.211602: step 435340, loss = 0.12 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:10:44.102760: step 435350, loss = 0.12 (1436.3 examples/sec; 0.089 sec/batch)
2017-06-02 13:10:44.963400: step 435360, loss = 0.13 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 13:10:45.824780: step 435370, loss = 0.13 (1486.0 examples/sec; 0.086 sec/batch)
2017-06-02 13:10:46.704751: step 435380, loss = 0.12 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 13:10:47.567476: step 435390, loss = 0.12 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 13:10:48.554664: step 435400, loss = 0.18 (1296.6 examples/sec; 0.099 sec/batch)
2017-06-02 13:10:49.332907: step 435410, loss = 0.13 (1644.7 examples/sec; 0.078 sec/batch)
2017-06-02 13:10:50.184861: step 435420, loss = 0.13 (1502.5 examples/sec; 0.085 sec/batch)
2017-06-02 13:10:51.067966: step 435430, loss = 0.11 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 13:10:51.919737: step 435440, loss = 0.15 (1502.8 examples/sec; 0.085 sec/batch)
2017-06-02 13:10:52.777442: step 435450, loss = 0.14 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 13:10:53.642991: step 435460, loss = 0.15 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:10:54.517746: step 435470, loss = 0.15 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 13:10:55.411239: step 435480, loss = 0.12 (1432.6 examples/sec; 0.089 sec/batch)
2017-06-02 13:10:56.253955: step 435490, loss = 0.14 (1518.9 examples/sec; 0.084 sec/batch)
2017-06-02 13:10:57.268863: step 435500, loss = 0.16 (1261.2 examples/sec; 0.101 sec/batch)
2017-06-02 13:10:58.000627: step 435510, loss = 0.16 (1749.2 examples/sec; 0.073 sec/batch)
2017-06-02 13:10:58.865583: step 435520, loss = 0.16 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:10:59.722208: step 435530, loss = 0.12 (1494.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:11:00.575181: step 435540, loss = 0.16 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:11:01.443118: step 435550, loss = 0.13 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:11:02.314835: step 435560, loss = 0.17 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 13:11:03.193811: step 435570, loss = 0.12 (1456.3 examples/sec; 0.088 sec/batch)
2017-06-02 13:11:04.060578: step 435580, loss = 0.13 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 13:11:04.944679: step 435590, loss = 0.13 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 13:11:05.927627: step 435600, loss = 0.14 (1302.2 examples/sec; 0.098 sec/batch)
2017-06-02 13:11:06.712770: step 435610, loss = 0.15 (1630.3 examples/sec; 0.079 sec/batch)
2017-06-02 13:11:07.583400: step 435620, loss = 0.12 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:11:08.477333: step 435630, loss = 0.14 (1431.9 examples/sec; 0.089 sec/batch)
2017-06-02 13:11:09.349721: step 435640, loss = 0.11 (1467.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:11:10.260358: step 435650, loss = 0.13 (1405.6 examples/sec; 0.091 sec/batch)
2017-06-02 13:11:11.126039: step 435660, loss = 0.14 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:11:11.984017: step 435670, loss = 0.16 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 13:11:12.856262: step 435680, loss = 0.14 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:11:13.707804: step 435690, loss = 0.13 (1503.1 examples/sec; 0.085 sec/batch)
2017-06-02 13:11:14.699109: step 435700, loss = 0.14 (1291.2 examples/sec; 0.099 sec/batch)
2017-06-02 13:11:15.436219: step 435710, loss = 0.18 (1736.5 examples/sec; 0.074 sec/batch)
2017-06-02 13:11:16.294673: step 435720, loss = 0.16 (1491.0 examples/sec; 0.086 sec/batch)
2017-06-02 13:11:17.124763: step 435730, loss = 0.13 (1542.0 examples/sec; 0.083 sec/batch)
2017-06-02 13:11:17.970744: step 435740, loss = 0.18 (1513.0 examples/sec; 0.085 sec/batch)
2017-06-02 13:11:18.809081: step 435750, loss = 0.12 (1526.8 examples/sec; 0.084 sec/batch)
2017-06-02 13:11:19.653608: step 435760, loss = 0.13 (1515.6 examples/sec; 0.084 sec/batch)
2017-06-02 13:11:20.484071: step 435770, loss = 0.12 (1541.3 examples/sec; 0.083 sec/batch)
2017-06-02 13:11:21.339160: step 435780, loss = 0.19 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 13:11:22.213478: step 435790, loss = 0.13 (1464.0 examples/sec; 0.087 sec/batch)
2017-06-02 13:11:23.207432: step 435800, loss = 0.15 (1287.8 examples/sec; 0.099 sec/batch)
2017-06-02 13:11:23.922963: step 435810, loss = 0.13 (1788.9 examples/sec; 0.072 sec/batch)
2017-06-02 13:11:24.776684: step 435820, loss = 0.12 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 13:11:25.648485: step 435830, loss = 0.16 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:11:26.490601: step 435840, loss = 0.12 (1520.0 examples/sec; 0.084 sec/batch)
2017-06-02 13:11:27.378886: step 435850, loss = 0.13 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 13:11:28.242143: step 435860, loss = 0.18 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:11:29.096746: step 435870, loss = 0.15 (1497.8 examples/sec; 0.085 sec/batch)
2017-06-02 13:11:29.969488: step 435880, loss = 0.11 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:11:30.849204: step 435890, loss = 0.16 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 13:11:31.841307: step 435900, loss = 0.12 (1290.2 examples/sec; 0.099 sec/batch)
2017-06-02 13:11:32.584010: step 435910, loss = 0.14 (1723.4 examples/sec; 0.074 sec/batch)
2017-06-02 13:11:33.426097: step 435920, loss = 0.12 (1520.0 examples/sec; 0.084 sec/batch)
2017-06-02 13:11:34.274008: step 435930, loss = 0.12 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:11:35.116783: step 435940, loss = 0.13 (1518.8 examples/sec; 0.084 sec/batch)
2017-06-02 13:11:35.982247: step 435950, loss = 0.14 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 13:11:36.835700: step 435960, loss = 0.12 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 13:11:37.700578: step 435970, loss = 0.13 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 13:11:38.563481: step 435980, loss = 0.13 (1483.4 examples/sec; 0.086 sec/batch)
2017-06-02 13:11:39.427173: step 435990, loss = 0.13 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 13:11:40.398172: step 436000, loss = 0.15 (1318.2 examples/sec; 0.097 sec/batch)
2017-06-02 13:11:41.144053: step 436010, loss = 0.14 (1716.1 examples/sec; 0.075 sec/batch)
2017-06-02 13:11:42.025763: step 436020, loss = 0.11 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 13:11:42.899050: step 436030, loss = 0.16 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:11:43.769231: step 436040, loss = 0.15 (1470.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:11:44.622746: step 436050, loss = 0.11 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 13:11:45.492669: step 436060, loss = 0.11 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 13:11:46.360311: step 436070, loss = 0.12 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 13:11:47.225171: step 436080, loss = 0.15 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 13:11:48.108312: step 436090, loss = 0.15 (1449.4 examples/sec; 0.088 sec/batch)
2017-06-02 13:11:49.109256: step 436100, loss = 0.13 (1278.8 examples/sec; 0.100 sec/batch)
2017-06-02 13:11:49.875841: step 436110, loss = 0.11 (1669.8 examples/sec; 0.077 sec/batch)
2017-06-02 13:11:50.729637: step 436120, loss = 0.15 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 13:11:51.599247: step 436130, loss = 0.13 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:11:52.476722: step 436140, loss = 0.13 (1458.7 examples/sec; 0.088 sec/batch)
2017-06-02 13:11:53.327424: step 436150, loss = 0.14 (1504.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:11:54.196122: step 436160, loss = 0.14 (1473.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:11:55.062016: step 436170, loss = 0.13 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 13:11:55.914203: step 436180, loss = 0.16 (1502.0 examples/sec; 0.085 sec/batch)
2017-06-02 13:11:56.792573: step 436190, loss = 0.12 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 13:11:57.773326: step 436200, loss = 0.13 (1305.1 examples/sec; 0.098 sec/batch)
2017-06-02 13:11:58.539344: step 436210, loss = 0.15 (1671.0 examples/sec; 0.077 sec/batch)
2017-06-02 13:11:59.404908: step 436220, loss = 0.17 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:12:00.259856: step 436230, loss = 0.14 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 13:12:01.111680: step 436240, loss = 0.14 (1502.7 examples/sec; 0.085 sec/batch)
2017-06-02 13:12:01.985049: step 436250, loss = 0.15 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:12:02.845666: step 436260, loss = 0.15 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 13:12:03.692740: step 436270, loss = 0.10 (1511.1 examples/sec; 0.085 sec/batch)
2017-06-02 13:12:04.533757: step 436280, loss = 0.11 (1522.0 examples/sec; 0.084 sec/batch)
2017-06-02 13:12:05.391498: step 436290, loss = 0.12 (1492.3 examples/sec; 0.086 sec/batch)
2017-06-02 13:12:06.336497: step 436300, loss = 0.12 (1354.5 examples/sec; 0.094 sec/batch)
2017-06-02 13:12:07.091829: step 436310, loss = 0.12 (1694.6 examples/sec; 0.076 sec/batch)
2017-06-02 13:12:07.977334: step 436320, loss = 0.14 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 13:12:08.844498: step 436330, loss = 0.14 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 13:12:09.686670: step 436340, loss = 0.15 (1519.9 examples/sec; 0.084 sec/batch)
2017-06-02 13:12:10.557913: step 436350, loss = 0.17 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 13:12:11.424441: step 436360, loss = 0.12 (1477.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:12:12.287460: step 436370, loss = 0.12 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:12:13.140321: step 436380, loss = 0.15 (1500.9 examples/sec; 0.085 sec/batch)
2017-06-02 13:12:13.999281: step 436390, loss = 0.15 (1490.1 examples/sec; 0.086 sec/batch)
2017-06-02 13:12:14.945767: step 436400, loss = 0.15 (1352.4 examples/sec; 0.095 sec/batch)
2017-06-02 13:12:15.723763: step 436410, loss = 0.12 (1645.3 examples/sec; 0.078 sec/batch)
2017-06-02 13:12:16.598485: step 436420, loss = 0.11 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 13:12:17.489770: step 436430, loss = 0.15 (1436.1 examples/sec; 0.089 sec/batch)
2017-06-02 13:12:18.366910: step 436440, loss = 0.15 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 13:12:19.254515: step 436450, loss = 0.12 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 13:12:20.110025: step 436460, loss = 0.14 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:12:20.951202: step 436470, loss = 0.12 (1521.7 examples/sec; 0.084 sec/batch)
2017-06-02 13:12:21.803294: step 436480, loss = 0.11 (1502.2 examples/sec; 0.085 sec/batch)
2017-06-02 13:12:22.648346: step 436490, loss = 0.18 (1514.7 examples/sec; 0.085 sec/batch)
2017-06-02 13:12:23.624449: step 436500, loss = 0.13 (1311.3 examples/sec; 0.098 sec/batch)
2017-06-02 13:12:24.382806: step 436510, loss = 0.15 (1687.8 examples/sec; 0.076 sec/batch)
2017-06-02 13:12:25.262087: step 436520, loss = 0.10 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 13:12:26.147272: step 436530, loss = 0.13 (1446.0 examples/sec; 0.089 sec/batch)
2017-06-02 13:12:27.034572: step 436540, loss = 0.13 (1442.6 examples/sec; 0.089 sec/batch)
2017-06-02 13:12:27.918318: step 436550, loss = 0.12 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 13:12:28.804936: step 436560, loss = 0.13 (1443.7 examples/sec; 0.089 sec/batch)
2017-06-02 13:12:29.676904: step 436570, loss = 0.13 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:12:30.506039: step 436580, loss = 0.16 (1543.8 examples/sec; 0.083 sec/batch)
2017-06-02 13:12:31.334902: step 436590, loss = 0.12 (1544.3 examples/sec; 0.083 sec/batch)
2017-06-02 13:12:32.287014: step 436600, loss = 0.14 (1344.4 examples/sec; 0.095 sec/batch)
2017-06-02 13:12:33.028518: step 436610, loss = 0.14 (1726.2 examples/sec; 0.074 sec/batch)
2017-06-02 13:12:33.909150: step 436620, loss = 0.14 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 13:12:34.762929: step 436630, loss = 0.13 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 13:12:35.628696: step 436640, loss = 0.13 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 13:12:36.506324: step 436650, loss = 0.12 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 13:12:37.361420: step 436660, loss = 0.12 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 13:12:38.209466: step 436670, loss = 0.13 (1509.3 examples/sec; 0.085 sec/batch)
2017-06-02 13:12:39.047449: step 436680, loss = 0.13 (1527.5 examples/sec; 0.084 sec/batch)
2017-06-02 13:12:39.897024: step 436690, loss = 0.17 (1506.7 examples/sec; 0.085 sec/batch)
2017-06-02 13:12:40.859892: step 436700, loss = 0.13 (1329.3 examples/sec; 0.096 sec/batch)
2017-06-02 13:12:41.651575: step 436710, loss = 0.18 (1616.8 examples/sec; 0.079 sec/batch)
2017-06-02 13:12:42.533644: step 436720, loss = 0.11 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 13:12:43.368709: step 436730, loss = 0.13 (1532.8 examples/sec; 0.084 sec/batch)
2017-06-02 13:12:44.218288: step 436740, loss = 0.13 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:12:45.084401: step 436750, loss = 0.15 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:12:45.966957: step 436760, loss = 0.12 (1450.3 examples/sec; 0.088 sec/batch)
2017-06-02 13:12:46.826899: step 436770, loss = 0.12 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 13:12:47.679659: step 436780, loss = 0.16 (1501.0 examples/sec; 0.085 sec/batch)
2017-06-02 13:12:48.542940: step 436790, loss = 0.13 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 13:12:49.499287: step 436800, loss = 0.14 (1338.4 examples/sec; 0.096 sec/batch)
2017-06-02 13:12:50.257987: step 436810, loss = 0.13 (1687.1 examples/sec; 0.076 sec/batch)
2017-06-02 13:12:51.132382: step 436820, loss = 0.13 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:12:51.967994: step 436830, loss = 0.15 (1531.8 examples/sec; 0.084 sec/batch)
2017-06-02 13:12:52.822344: step 436840, loss = 0.14 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 13:12:53.693758: step 436850, loss = 0.16 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:12:54.568180: step 436860, loss = 0.15 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:12:55.439019: step 436870, loss = 0.12 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:12:56.323075: step 436880, loss = 0.14 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 13:12:57.186748: step 436890, loss = 0.13 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 13:12:58.112086: step 436900, loss = 0.11 (1383.3 examples/sec; 0.093 sec/batch)
2017-06-02 13:12:58.858945: step 436910, loss = 0.10 (1713.9 examples/sec; 0.075 sec/batch)
2017-06-02 13:12:59.727412: step 436920, loss = 0.17 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:13:00.605554: step 436930, loss = 0.13 (1457.6 examples/sec; 0.088 sec/batch)
2017-06-02 13:13:01.455484: step 436940, loss = 0.13 (1506.0 examples/sec; 0.085 sec/batch)
2017-06-02 13:13:02.320876: step 436950, loss = 0.12 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 13:13:03.189347: step 436960, loss = 0.10 (1473.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:13:04.022489: step 436970, loss = 0.12 (1536.4 examples/sec; 0.083 sec/batch)
2017-06-02 13:13:04.891624: step 436980, loss = 0.15 (1472.7 examples/sec; 0.087 sec/batch)
2017-06-02 13:13:05.762167: step 436990, loss = 0.11 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 13:13:06.734602: step 437000, loss = 0.14 (1316.3 examples/sec; 0.097 sec/batch)
2017-06-02 13:13:07.504225: step 437010, loss = 0.15 (1663.2 examples/sec; 0.077 sec/batch)
2017-06-02 13:13:08.362942: step 437020, loss = 0.11 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 13:13:09.216239: step 437030, loss = 0.15 (1500.1 examples/sec; 0.085 sec/batch)
2017-06-02 13:13:10.093184: step 437040, loss = 0.14 (1459.6 examples/sec; 0.088 sec/batch)
2017-06-02 13:13:10.943094: step 437050, loss = 0.16 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 13:13:11.789637: step 437060, loss = 0.13 (1512.0 examples/sec; 0.085 sec/batch)
2017-06-02 13:13:12.655657: step 437070, loss = 0.12 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 13:13:13.517224: step 437080, loss = 0.14 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 13:13:14.386915: step 437090, loss = 0.14 (1471.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:13:15.345543: step 437100, loss = 0.12 (1335.2 examples/sec; 0.096 sec/batch)
2017-06-02 13:13:16.114044: step 437110, loss = 0.10 (1665.6 examples/sec; 0.077 sec/batch)
2017-06-02 13:13:16.959879: step 437120, loss = 0.15 (1513.3 examples/sec; 0.085 sec/batch)
2017-06-02 13:13:17.809533: step 437130, loss = 0.16 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 13:13:18.667229: step 437140, loss = 0.12 (1492.4 examples/sec; 0.086 sec/batch)
2017-06-02 13:13:19.521486: step 437150, loss = 0.15 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 13:13:20.399559: step 437160, loss = 0.13 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 13:13:21.251196: step 437170, loss = 0.11 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 13:13:22.108533: step 437180, loss = 0.13 (1493.0 examples/sec; 0.086 sec/batch)
2017-06-02 13:13:22.953401: step 437190, loss = 0.10 (1515.0 examples/sec; 0.084 sec/batch)
2017-06-02 13:13:23.910373: step 437200, loss = 0.13 (1337.6 examples/sec; 0.096 sec/batch)
2017-06-02 13:13:24.687863: step 437210, loss = 0.13 (1646.3 examples/sec; 0.078 sec/batch)
2017-06-02 13:13:25.555162: step 437220, loss = 0.17 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:13:26.448651: step 437230, loss = 0.13 (1432.6 examples/sec; 0.089 sec/batch)
2017-06-02 13:13:27.299547: step 437240, loss = 0.15 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 13:13:28.168790: step 437250, loss = 0.11 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:13:29.028036: step 437260, loss = 0.13 (1489.7 examples/sec; 0.086 sec/batch)
2017-06-02 13:13:29.899049: step 437270, loss = 0.16 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:13:30.740286: step 437280, loss = 0.13 (1521.6 examples/sec; 0.084 sec/batch)
2017-06-02 13:13:31.589356: step 437290, loss = 0.12 (1507.5 examples/sec; 0.085 sec/batch)
2017-06-02 13:13:32.593041: step 437300, loss = 0.10 (1275.3 examples/sec; 0.100 sec/batch)
2017-06-02 13:13:33.326226: step 437310, loss = 0.14 (1745.8 examples/sec; 0.073 sec/batch)
2017-06-02 13:13:34.191215: step 437320, loss = 0.12 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:13:35.075180: step 437330, loss = 0.12 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 13:13:35.935346: step 437340, loss = 0.16 (1488.1 examples/sec; 0.086 sec/batch)
2017-06-02 13:13:36.798338: step 437350, loss = 0.11 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:13:37.664057: step 437360, loss = 0.11 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:13:38.494920: step 437370, loss = 0.17 (1540.6 examples/sec; 0.083 sec/batch)
2017-06-02 13:13:39.373237: step 437380, loss = 0.12 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 13:13:40.221884: step 437390, loss = 0.13 (1508.2 examples/sec; 0.085 sec/batch)
2017-06-02 13:13:41.188535: step 437400, loss = 0.15 (1324.1 examples/sec; 0.097 sec/batch)
2017-06-02 13:13:41.955142: step 437410, loss = 0.14 (1669.7 examples/sec; 0.077 sec/batch)
2017-06-02 13:13:42.790087: step 437420, loss = 0.12 (1533.0 examples/sec; 0.083 sec/batch)
2017-06-02 13:13:43.676225: step 437430, loss = 0.13 (1444.5 examples/sec; 0.089 sec/batch)
2017-06-02 13:13:44.552253: step 437440, loss = 0.14 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 13:13:45.437007: step 437450, loss = 0.15 (1446.7 examples/sec; 0.088 sec/batch)
2017-06-02 13:13:46.307471: step 437460, loss = 0.12 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:13:47.176721: step 437470, loss = 0.18 (1472.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:13:48.030902: step 437480, loss = 0.17 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 13:13:48.923626: step 437490, loss = 0.15 (1433.8 examples/sec; 0.089 sec/batch)
2017-06-02 13:13:49.905340: step 437500, loss = 0.13 (1303.8 examples/sec; 0.098 sec/batch)
2017-06-02 13:13:50.699474: step 437510, loss = 0.13 (1611.8 examples/sec; 0.079 sec/batch)
2017-06-02 13:13:51.552140: step 437520, loss = 0.13 (1501.2 examples/sec; 0.085 sec/batch)
2017-06-02 13:13:52.455124: step 437530, loss = 0.12 (1417.5 examples/sec; 0.090 sec/batch)
2017-06-02 13:13:53.337016: step 437540, loss = 0.11 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 13:13:54.254551: step 437550, loss = 0.12 (1395.0 examples/sec; 0.092 sec/batch)
2017-06-02 13:13:55.092780: step 437560, loss = 0.13 (1527.0 examples/sec; 0.084 sec/batch)
2017-06-02 13:13:55.999584: step 437570, loss = 0.14 (1411.5 examples/sec; 0.091 sec/batch)
2017-06-02 13:13:56.915834: step 437580, loss = 0.12 (1397.0 examples/sec; 0.092 sec/batch)
2017-06-02 13:13:57.775675: step 437590, loss = 0.13 (1488.7 examples/sec; 0.086 sec/batch)
2017-06-02 13:13:58.782721: step 437600, loss = 0.11 (1271.0 examples/sec; 0.101 sec/batch)
2017-06-02 13:13:59.578394: step 437610, loss = 0.12 (1608.7 examples/sec; 0.080 sec/batch)
2017-06-02 13:14:00.458183: step 437620, loss = 0.17 (1454.9 examples/sec; 0.088 sec/batch)
2017-06-02 13:14:01.317097: step 437630, loss = 0.15 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 13:14:02.181378: step 437640, loss = 0.13 (1481.0 examples/sec; 0.086 sec/batch)
2017-06-02 13:14:03.048715: step 437650, loss = 0.16 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:14:03.922404: step 437660, loss = 0.11 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 13:14:04.799761: step 437670, loss = 0.12 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 13:14:05.639073: step 437680, loss = 0.13 (1525.1 examples/sec; 0.084 sec/batch)
2017-06-02 13:14:06.537834: step 437690, loss = 0.13 (1424.2 examples/sec; 0.090 sec/batch)
2017-06-02 13:14:07.525188: step 437700, loss = 0.14 (1296.4 examples/sec; 0.099 sec/batch)
2017-06-02 13:14:08.300192: step 437710, loss = 0.12 (1651.6 examples/sec; 0.077 sec/batch)
2017-06-02 13:14:09.169396: step 437720, loss = 0.10 (1472.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:14:10.050764: step 437730, loss = 0.12 (1452.3 examples/sec; 0.088 sec/batch)
2017-06-02 13:14:10.905735: step 437740, loss = 0.15 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 13:14:11.757757: step 437750, loss = 0.13 (1502.3 examples/sec; 0.085 sec/batch)
2017-06-02 13:14:12.596813: step 437760, loss = 0.11 (1525.5 examples/sec; 0.084 sec/batch)
2017-06-02 13:14:13.480954: step 437770, loss = 0.10 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 13:14:14.332431: step 437780, loss = 0.11 (1503.3 examples/sec; 0.085 sec/batch)
2017-06-02 13:14:15.223081: step 437790, loss = 0.16 (1437.2 examples/sec; 0.089 sec/batch)
2017-06-02 13:14:16.182208: step 437800, loss = 0.15 (1334.5 examples/sec; 0.096 sec/batch)
2017-06-02 13:14:16.957705: step 437810, loss = 0.14 (1650.6 examples/sec; 0.078 sec/batch)
2017-06-02 13:14:17.799808: step 437820, loss = 0.13 (1520.0 examples/sec; 0.084 sec/batch)
2017-06-02 13:14:18.665543: step 437830, loss = 0.14 (1478.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:14:19.526174: step 437840, loss = 0.13 (1487.3 examples/sec; 0.086 sec/batch)
2017-06-02 13:14:20.394959: step 437850, loss = 0.16 (1473.3 examples/sec; 0.087 sec/batch)
2017-06-02 13:14:21.287904: step 437860, loss = 0.13 (1433.4 examples/sec; 0.089 sec/batch)
2017-06-02 13:14:22.155138: step 437870, loss = 0.12 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 13:14:23.031214: step 437880, loss = 0.12 (1461.0 examples/sec; 0.088 sec/batch)
2017-06-02 13:14:23.910234: step 437890, loss = 0.16 (1456.2 examples/sec; 0.088 sec/batch)
2017-06-02 13:14:24.883280: step 437900, loss = 0.10 (1315.5 examples/sec; 0.097 sec/batch)
2017-06-02 13:14:25.637848: step 437910, loss = 0.14 (1696.3 examples/sec; 0.075 sec/batch)
2017-06-02 13:14:26.513959: step 437920, loss = 0.13 (1461.1 examples/sec; 0.088 sec/batch)
2017-06-02 13:14:27.405452: step 437930, loss = 0.12 (1435.7 examples/sec; 0.089 sec/batch)
2017-06-02 13:14:28.261748: step 437940, loss = 0.13 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:14:29.134918: step 437950, loss = 0.10 (1465.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:14:29.986551: step 437960, loss = 0.14 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 13:14:30.845816: step 437970, loss = 0.16 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 13:14:31.697469: step 437980, loss = 0.14 (1503.0 examples/sec; 0.085 sec/batch)
2017-06-02 13:14:32.557210: step 437990, loss = 0.16 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:14:33.568162: step 438000, loss = 0.12 (1266.1 examples/sec; 0.101 sec/batch)
2017-06-02 13:14:34.316751: step 438010, loss = 0.15 (1709.9 examples/sec; 0.075 sec/batch)
2017-06-02 13:14:35.157566: step 438020, loss = 0.13 (1522.3 examples/sec; 0.084 sec/batch)
2017-06-02 13:14:36.044798: step 438030, loss = 0.11 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 13:14:36.931188: step 438040, loss = 0.12 (1444.0 examples/sec; 0.089 sec/batch)
2017-06-02 13:14:37.779098: step 438050, loss = 0.17 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:14:38.656360: step 438060, loss = 0.11 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 13:14:39.529376: step 438070, loss = 0.14 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:14:40.382958: step 438080, loss = 0.11 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 13:14:41.222210: step 438090, loss = 0.15 (1525.2 examples/sec; 0.084 sec/batch)
2017-06-02 13:14:42.229831: step 438100, loss = 0.12 (1270.3 examples/sec; 0.101 sec/batch)
2017-06-02 13:14:42.972287: step 438110, loss = 0.18 (1724.0 examples/sec; 0.074 sec/batch)
2017-06-02 13:14:43.822505: step 438120, loss = 0.12 (1505.5 examples/sec; 0.085 sec/batch)
2017-06-02 13:14:44.681787: step 438130, loss = 0.10 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 13:14:45.558674: step 438140, loss = 0.14 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 13:14:46.426393: step 438150, loss = 0.14 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 13:14:47.296813: step 438160, loss = 0.10 (1470.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:14:48.146816: step 438170, loss = 0.13 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 13:14:49.021367: step 438180, loss = 0.13 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:14:49.871831: step 438190, loss = 0.16 (1505.1 examples/sec; 0.085 sec/batch)
2017-06-02 13:14:50.827089: step 438200, loss = 0.14 (1339.9 examples/sec; 0.096 sec/batch)
2017-06-02 13:14:51.617927: step 438210, loss = 0.15 (1618.5 examples/sec; 0.079 sec/batch)
2017-06-02 13:14:52.522334: step 438220, loss = 0.14 (1415.4 examples/sec; 0.090 sec/batch)
2017-06-02 13:14:53.415478: step 438230, loss = 0.11 (1433.0 examples/sec; 0.089 sec/batch)
2017-06-02 13:14:54.297172: step 438240, loss = 0.14 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 13:14:55.186685: step 438250, loss = 0.14 (1439.0 examples/sec; 0.089 sec/batch)
2017-06-02 13:14:56.034603: step 438260, loss = 0.11 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:14:56.905940: step 438270, loss = 0.13 (1469.0 examples/sec; 0.087 sec/batch)
2017-06-02 13:14:57.777035: step 438280, loss = 0.09 (1469.4 examples/sec; 0.087 sec/batch)
2017-06-02 13:14:58.650260: step 438290, loss = 0.19 (1465.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:14:59.605555: step 438300, loss = 0.17 (1339.9 examples/sec; 0.096 sec/batch)
2017-06-02 13:15:00.379220: step 438310, loss = 0.15 (1654.5 examples/sec; 0.077 sec/batch)
2017-06-02 13:15:01.238092: step 438320, loss = 0.14 (1490.3 examples/sec; 0.086 sec/batch)
2017-06-02 13:15:02.077914: step 438330, loss = 0.12 (1524.1 examples/sec; 0.084 sec/batch)
2017-06-02 13:15:02.942908: step 438340, loss = 0.12 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:15:03.787246: step 438350, loss = 0.11 (1516.0 examples/sec; 0.084 sec/batch)
2017-06-02 13:15:04.617465: step 438360, loss = 0.13 (1541.8 examples/sec; 0.083 sec/batch)
2017-06-02 13:15:05.506781: step 438370, loss = 0.15 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 13:15:06.383926: step 438380, loss = 0.13 (1459.3 examples/sec; 0.088 sec/batch)
2017-06-02 13:15:07.255354: step 438390, loss = 0.12 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:15:08.267186: step 438400, loss = 0.11 (1265.0 examples/sec; 0.101 sec/batch)
2017-06-02 13:15:08.996185: step 438410, loss = 0.13 (1755.8 examples/sec; 0.073 sec/batch)
2017-06-02 13:15:09.861130: step 438420, loss = 0.12 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 13:15:10.737834: step 438430, loss = 0.13 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 13:15:11.622931: step 438440, loss = 0.13 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 13:15:12.500775: step 438450, loss = 0.13 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 13:15:13.383802: step 438460, loss = 0.15 (1449.6 examples/sec; 0.088 sec/batch)
2017-06-02 13:15:14.241202: step 438470, loss = 0.13 (1492.9 examples/sec; 0.086 sec/batch)
2017-06-02 13:15:15.144025: step 438480, loss = 0.10 (1417.8 examples/sec; 0.090 sec/batch)
2017-06-02 13:15:16.011804: step 438490, loss = 0.11 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 13:15:16.989360: step 438500, loss = 0.14 (1309.4 examples/sec; 0.098 sec/batch)
2017-06-02 13:15:17.761493: step 438510, loss = 0.13 (1657.7 examples/sec; 0.077 sec/batch)
2017-06-02 13:15:18.620529: step 438520, loss = 0.13 (1490.0 examples/sec; 0.086 sec/batch)
2017-06-02 13:15:19.481416: step 438530, loss = 0.13 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:15:20.370476: step 438540, loss = 0.12 (1439.7 examples/sec; 0.089 sec/batch)
2017-06-02 13:15:21.236815: step 438550, loss = 0.15 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:15:22.130243: step 438560, loss = 0.12 (1432.7 examples/sec; 0.089 sec/batch)
2017-06-02 13:15:22.966213: step 438570, loss = 0.12 (1531.2 examples/sec; 0.084 sec/batch)
2017-06-02 13:15:23.839563: step 438580, loss = 0.13 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:15:24.681850: step 438590, loss = 0.11 (1519.7 examples/sec; 0.084 sec/batch)
2017-06-02 13:15:25.676203: step 438600, loss = 0.13 (1287.3 examples/sec; 0.099 sec/batch)
2017-06-02 13:15:26.472684: step 438610, loss = 0.11 (1607.1 examples/sec; 0.080 sec/batch)
2017-06-02 13:15:27.349710: step 438620, loss = 0.12 (1459.5 examples/sec; 0.088 sec/batch)
2017-06-02 13:15:28.251910: step 438630, loss = 0.15 (1418.7 examples/sec; 0.090 sec/batch)
2017-06-02 13:15:29.123285: step 438640, loss = 0.13 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:15:29.995457: step 438650, loss = 0.16 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:15:30.874834: step 438660, loss = 0.13 (1455.6 examples/sec; 0.088 sec/batch)
2017-06-02 13:15:31.750365: step 438670, loss = 0.12 (1462.0 examples/sec; 0.088 sec/batch)
2017-06-02 13:15:32.618151: step 438680, loss = 0.16 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 13:15:33.506618: step 438690, loss = 0.14 (1440.7 examples/sec; 0.089 sec/batch)
2017-06-02 13:15:34.518763: step 438700, loss = 0.12 (1264.6 examples/sec; 0.101 sec/batch)
2017-06-02 13:15:35.257556: step 438710, loss = 0.13 (1732.6 examples/sec; 0.074 sec/batch)
2017-06-02 13:15:36.148764: step 438720, loss = 0.19 (1436.2 examples/sec; 0.089 sec/batch)
2017-06-02 13:15:37.019991: step 438730, loss = 0.19 (1469.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:15:37.881805: step 438740, loss = 0.13 (1485.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:15:38.715357: step 438750, loss = 0.12 (1535.6 examples/sec; 0.083 sec/batch)
2017-06-02 13:15:39.545463: step 438760, loss = 0.14 (1542.0 examples/sec; 0.083 sec/batch)
2017-06-02 13:15:40.387702: step 438770, loss = 0.12 (1519.7 examples/sec; 0.084 sec/batch)
2017-06-02 13:15:41.248171: step 438780, loss = 0.13 (1487.6 examples/sec; 0.086 sec/batch)
2017-06-02 13:15:42.117235: step 438790, loss = 0.14 (1472.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:15:43.121278: step 438800, loss = 0.16 (1274.8 examples/sec; 0.100 sec/batch)
2017-06-02 13:15:43.867291: step 438810, loss = 0.17 (1715.8 examples/sec; 0.075 sec/batch)
2017-06-02 13:15:44.726122: step 438820, loss = 0.12 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 13:15:45.606465: step 438830, loss = 0.13 (1454.0 examples/sec; 0.088 sec/batch)
2017-06-02 13:15:46.461814: step 438840, loss = 0.14 (1496.5 examples/sec; 0.086 sec/batch)
2017-06-02 13:15:47.328586: step 438850, loss = 0.18 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 13:15:48.170065: step 438860, loss = 0.10 (1521.1 examples/sec; 0.084 sec/batch)
2017-06-02 13:15:49.028873: step 438870, loss = 0.13 (1490.4 examples/sec; 0.086 sec/batch)
2017-06-02 13:15:49.911568: step 438880, loss = 0.10 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 13:15:50.777869: step 438890, loss = 0.14 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:15:51.741037: step 438900, loss = 0.11 (1328.9 examples/sec; 0.096 sec/batch)
2017-06-02 13:15:52.510903: step 438910, loss = 0.13 (1662.6 examples/sec; 0.077 sec/batch)
2017-06-02 13:15:53.351645: step 438920, loss = 0.16 (1522.4 examples/sec; 0.084 sec/batch)
2017-06-02 13:15:54.201317: step 438930, loss = 0.12 (1506.5 examples/sec; 0.085 sec/batch)
2017-06-02 13:15:55.035372: step 438940, loss = 0.15 (1534.7 examples/sec; 0.083 sec/batch)
2017-06-02 13:15:55.896585: step 438950, loss = 0.11 (1486.3 examples/sec; 0.086 sec/batch)
2017-06-02 13:15:56.741408: step 438960, loss = 0.14 (1515.1 examples/sec; 0.084 sec/batch)
2017-06-02 13:15:57.595028: step 438970, loss = 0.11 (1499.5 examples/sec; 0.085 sec/batch)
2017-06-02 13:15:58.462143: step 438980, loss = 0.14 (1476.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:15:59.343646: step 438990, loss = 0.11 (1452.0 examples/sec; 0.088 sec/batch)
2017-06-02 13:16:00.316991: step 439000, loss = 0.13 (1315.1 examples/sec; 0.097 sec/batch)
2017-06-02 13:16:01.062261: step 439010, loss = 0.10 (1717.5 examples/sec; 0.075 sec/batch)
2017-06-02 13:16:01.906268: step 439020, loss = 0.19 (1516.6 examples/sec; 0.084 sec/batch)
2017-06-02 13:16:02.752827: step 439030, loss = 0.13 (1512.0 examples/sec; 0.085 sec/batch)
2017-06-02 13:16:03.641208: step 439040, loss = 0.12 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 13:16:04.507265: step 439050, loss = 0.13 (1477.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:16:05.374485: step 439060, loss = 0.12 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 13:16:06.216558: step 439070, loss = 0.11 (1520.1 examples/sec; 0.084 sec/batch)
2017-06-02 13:16:07.061789: step 439080, loss = 0.12 (1514.4 examples/sec; 0.085 sec/batch)
2017-06-02 13:16:07.938214: step 439090, loss = 0.13 (1460.5 examples/sec; 0.088 sec/batch)
2017-06-02 13:16:08.905871: step 439100, loss = 0.12 (1322.8 examples/sec; 0.097 sec/batch)
2017-06-02 13:16:09.680341: step 439110, loss = 0.12 (1652.7 examples/sec; 0.077 sec/batch)
2017-06-02 13:16:10.536170: step 439120, loss = 0.15 (1495.6 examples/sec; 0.086 sec/batch)
2017-06-02 13:16:11.381300: step 439130, loss = 0.11 (1514.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:16:12.235543: step 439140, loss = 0.17 (1498.4 examples/sec; 0.085 sec/batch)
2017-06-02 13:16:13.113186: step 439150, loss = 0.10 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 13:16:13.974101: step 439160, loss = 0.13 (1486.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:16:14.843063: step 439170, loss = 0.13 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 13:16:15.696098: step 439180, loss = 0.10 (1500.5 examples/sec; 0.085 sec/batch)
2017-06-02 13:16:16.567819: step 439190, loss = 0.15 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 13:16:17.513154: step 439200, loss = 0.14 (1354.0 examples/sec; 0.095 sec/batch)
2017-06-02 13:16:18.278300: step 439210, loss = 0.17 (1672.9 examples/sec; 0.077 sec/batch)
2017-06-02 13:16:19.143864: step 439220, loss = 0.15 (1478.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:16:20.009185: step 439230, loss = 0.11 (1479.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:16:20.866151: step 439240, loss = 0.19 (1493.7 examples/sec; 0.086 sec/batch)
2017-06-02 13:16:21.736988: step 439250, loss = 0.12 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:16:22.617063: step 439260, loss = 0.12 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 13:16:23.468028: step 439270, loss = 0.12 (1504.2 examples/sec; 0.085 sec/batch)
2017-06-02 13:16:24.342595: step 439280, loss = 0.15 (1463.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:16:25.198854: step 439290, loss = 0.14 (1494.9 examples/sec; 0.086 sec/batch)
2017-06-02 13:16:26.193467: step 439300, loss = 0.12 (1286.9 examples/sec; 0.099 sec/batch)
2017-06-02 13:16:26.901198: step 439310, loss = 0.13 (1808.7 examples/sec; 0.071 sec/batch)
2017-06-02 13:16:27.773439: step 439320, loss = 0.13 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:16:28.653694: step 439330, loss = 0.16 (1454.1 examples/sec; 0.088 sec/batch)
2017-06-02 13:16:29.487015: step 439340, loss = 0.13 (1536.0 examples/sec; 0.083 sec/batch)
2017-06-02 13:16:30.368488: step 439350, loss = 0.12 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 13:16:31.246889: step 439360, loss = 0.14 (1457.2 examples/sec; 0.088 sec/batch)
2017-06-02 13:16:32.101250: step 439370, loss = 0.11 (1498.2 examples/sec; 0.085 sec/batch)
2017-06-02 13:16:32.957694: step 439380, loss = 0.13 (1494.5 examples/sec; 0.086 sec/batch)
2017-06-02 13:16:33.823549: step 439390, loss = 0.14 (1478.3 examples/sec; 0.087 sec/batch)
2017-06-02 13:16:34.795912: step 439400, loss = 0.10 (1316.4 examples/sec; 0.097 sec/batch)
2017-06-02 13:16:35.571568: step 439410, loss = 0.13 (1650.2 examples/sec; 0.078 sec/batch)
2017-06-02 13:16:36.409625: step 439420, loss = 0.14 (1527.4 examples/sec; 0.084 sec/batch)
2017-06-02 13:16:37.253948: step 439430, loss = 0.15 (1516.0 examples/sec; 0.084 sec/batch)
2017-06-02 13:16:38.099040: step 439440, loss = 0.12 (1514.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:16:38.962192: step 439450, loss = 0.12 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 13:16:39.827022: step 439460, loss = 0.12 (1480.1 examples/sec; 0.086 sec/batch)
2017-06-02 13:16:40.647196: step 439470, loss = 0.13 (1560.6 examples/sec; 0.082 sec/batch)
2017-06-02 13:16:41.499053: step 439480, loss = 0.12 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:16:42.374198: step 439490, loss = 0.15 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 13:16:43.373778: step 439500, loss = 0.13 (1280.5 examples/sec; 0.100 sec/batch)
2017-06-02 13:16:44.140457: step 439510, loss = 0.15 (1669.5 examples/sec; 0.077 sec/batch)
2017-06-02 13:16:45.011353: step 439520, loss = 0.14 (1469.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:16:45.854953: step 439530, loss = 0.13 (1517.3 examples/sec; 0.084 sec/batch)
2017-06-02 13:16:46.694377: step 439540, loss = 0.11 (1524.9 examples/sec; 0.084 sec/batch)
2017-06-02 13:16:47.549280: step 439550, loss = 0.13 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 13:16:48.441377: step 439560, loss = 0.14 (1434.9 examples/sec; 0.089 sec/batch)
2017-06-02 13:16:49.310203: step 439570, loss = 0.12 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:16:50.182745: step 439580, loss = 0.13 (1467.0 examples/sec; 0.087 sec/batch)
2017-06-02 13:16:51.033356: step 439590, loss = 0.13 (1504.8 examples/sec; 0.085 sec/batch)
2017-06-02 13:16:51.985407: step 439600, loss = 0.15 (1344.5 examples/sec; 0.095 sec/batch)
2017-06-02 13:16:52.754210: step 439610, loss = 0.12 (1664.9 examples/sec; 0.077 sec/batch)
2017-06-02 13:16:53.628437: step 439620, loss = 0.14 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 13:16:54.491555: step 439630, loss = 0.14 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 13:16:55.375674: step 439640, loss = 0.16 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 13:16:56.230772: step 439650, loss = 0.17 (1496.9 examples/sec; 0.086 sec/batch)
2017-06-02 13:16:57.090811: step 439660, loss = 0.12 (1488.3 examples/sec; 0.086 sec/batch)
2017-06-02 13:16:57.940398: step 439670, loss = 0.17 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:16:58.796092: step 439680, loss = 0.16 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 13:16:59.634164: step 439690, loss = 0.14 (1527.3 examples/sec; 0.084 sec/batch)
2017-06-02 13:17:00.595654: step 439700, loss = 0.16 (1331.3 examples/sec; 0.096 sec/batch)
2017-06-02 13:17:01.338214: step 439710, loss = 0.13 (1723.8 examples/sec; 0.074 sec/batch)
2017-06-02 13:17:02.188374: step 439720, loss = 0.11 (1505.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:17:03.053541: step 439730, loss = 0.10 (1479.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:17:03.916643: step 439740, loss = 0.12 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 13:17:04.762051: step 439750, loss = 0.10 (1514.1 examples/sec; 0.085 sec/batch)
2017-06-02 13:17:05.608709: step 439760, loss = 0.12 (1511.9 examples/sec; 0.085 sec/batch)
2017-06-02 13:17:06.474484: step 439770, loss = 0.14 (1478.4 examples/sec; 0.087 sec/batch)
2017-06-02 13:17:07.312359: step 439780, loss = 0.11 (1527.7 examples/sec; 0.084 sec/batch)
2017-06-02 13:17:08.183785: step 439790, loss = 0.11 (1468.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:17:09.150933: step 439800, loss = 0.13 (1323.5 examples/sec; 0.097 sec/batch)
2017-06-02 13:17:09.923661: step 439810, loss = 0.15 (1656.5 examples/sec; 0.077 sec/batch)
2017-06-02 13:17:10.801301: step 439820, loss = 0.18 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 13:17:11.684519: step 439830, loss = 0.16 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 13:17:12.547196: step 439840, loss = 0.14 (1483.7 examples/sec; 0.086 sec/batch)
2017-06-02 13:17:13.393521: step 439850, loss = 0.13 (1512.4 examples/sec; 0.085 sec/batch)
2017-06-02 13:17:14.278998: step 439860, loss = 0.15 (1445.5 examples/sec; 0.089 sec/batch)
2017-06-02 13:17:15.148776: step 439870, loss = 0.11 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 13:17:16.022353: step 439880, loss = 0.14 (1465.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:17:16.901261: step 439890, loss = 0.13 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 13:17:17.862930: step 439900, loss = 0.11 (1331.0 examples/sec; 0.096 sec/batch)
2017-06-02 13:17:18.641982: step 439910, loss = 0.13 (1643.0 examples/sec; 0.078 sec/batch)
2017-06-02 13:17:19.506888: step 439920, loss = 0.12 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 13:17:20.383517: step 439930, loss = 0.14 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 13:17:21.261303: step 439940, loss = 0.17 (1458.2 examples/sec; 0.088 sec/batch)
2017-06-02 13:17:22.128631: step 439950, loss = 0.15 (1475.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:17:23.002268: step 439960, loss = 0.13 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 13:17:23.845880: step 439970, loss = 0.15 (1517.3 examples/sec; 0.084 sec/batch)
2017-06-02 13:17:24.710433: step 439980, loss = 0.13 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 13:17:25.580968: step 439990, loss = 0.12 (1470.4 examples/sec; 0.087 sec/batch)
2017-06-02 13:17:26.539347: step 440000, loss = 0.15 (1335.6 examples/sec; 0.096 sec/batch)
2017-06-02 13:17:27.291170: step 440010, loss = 0.11 (1702.6 examples/sec; 0.075 sec/batch)
2017-06-02 13:17:28.178277: step 440020, loss = 0.18 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 13:17:29.054276: step 440030, loss = 0.12 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 13:17:29.901140: step 440040, loss = 0.13 (1511.5 examples/sec; 0.085 sec/batch)
2017-06-02 13:17:30.774623: step 440050, loss = 0.12 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 13:17:31.615766: step 440060, loss = 0.12 (1521.7 examples/sec; 0.084 sec/batch)
2017-06-02 13:17:32.490475: step 440070, loss = 0.18 (1463.3 examples/sec; 0.087 sec/batch)
2017-06-02 13:17:33.377180: step 440080, loss = 0.11 (1443.5 examples/sec; 0.089 sec/batch)
2017-06-02 13:17:34.218669: step 440090, loss = 0.13 (1521.1 examples/sec; 0.084 sec/batch)
2017-06-02 13:17:35.189286: step 440100, loss = 0.17 (1318.7 examples/sec; 0.097 sec/batch)
2017-06-02 13:17:35.955700: step 440110, loss = 0.10 (1670.1 examples/sec; 0.077 sec/batch)
2017-06-02 13:17:36.791925: step 440120, loss = 0.12 (1530.7 examples/sec; 0.084 sec/batch)
2017-06-02 13:17:37.632578: step 440130, loss = 0.14 (1522.6 examples/sec; 0.084 sec/batch)
2017-06-02 13:17:38.512216: step 440140, loss = 0.12 (1455.1 examples/sec; 0.088 sec/batch)
2017-06-02 13:17:39.370572: step 440150, loss = 0.16 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:17:40.230132: step 440160, loss = 0.12 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:17:41.119012: step 440170, loss = 0.14 (1440.0 examples/sec; 0.089 sec/batch)
2017-06-02 13:17:42.000508: step 440180, loss = 0.12 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 13:17:42.868063: step 440190, loss = 0.16 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 13:17:43.867579: step 440200, loss = 0.16 (1280.6 examples/sec; 0.100 sec/batch)
2017-06-02 13:17:44.638171: step 440210, loss = 0.16 (1661.1 examples/sec; 0.077 sec/batch)
2017-06-02 13:17:45.485391: step 440220, loss = 0.12 (1510.8 examples/sec; 0.085 sec/batch)
2017-06-02 13:17:46.347699: step 440230, loss = 0.16 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 13:17:47.231342: step 440240, loss = 0.10 (1448.6 examples/sec; 0.088 sec/batch)
2017-06-02 13:17:48.091574: step 440250, loss = 0.12 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 13:17:48.990033: step 440260, loss = 0.15 (1424.7 examples/sec; 0.090 sec/batch)
2017-06-02 13:17:49.838782: step 440270, loss = 0.13 (1508.0 examples/sec; 0.085 sec/batch)
2017-06-02 13:17:50.695569: step 440280, loss = 0.14 (1494.0 examples/sec; 0.086 sec/batch)
2017-06-02 13:17:51.567229: step 440290, loss = 0.11 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:17:52.562585: step 440300, loss = 0.14 (1286.0 examples/sec; 0.100 sec/batch)
2017-06-02 13:17:53.281487: step 440310, loss = 0.13 (1780.5 examples/sec; 0.072 sec/batch)
2017-06-02 13:17:54.141080: step 440320, loss = 0.12 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 13:17:55.033291: step 440330, loss = 0.13 (1434.6 examples/sec; 0.089 sec/batch)
2017-06-02 13:17:55.901007: step 440340, loss = 0.13 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 13:17:56.761220: step 440350, loss = 0.13 (1488.0 examples/sec; 0.086 sec/batch)
2017-06-02 13:17:57.611186: step 440360, loss = 0.11 (1506.0 examples/sec; 0.085 sec/batch)
2017-06-02 13:17:58.487195: step 440370, loss = 0.10 (1461.2 examples/sec; 0.088 sec/batch)
2017-06-02 13:17:59.330191: step 440380, loss = 0.11 (1518.4 examples/sec; 0.084 sec/batch)
2017-06-02 13:18:00.185949: step 440390, loss = 0.15 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:18:01.157981: step 440400, loss = 0.15 (1316.8 examples/sec; 0.097 sec/batch)
2017-06-02 13:18:01.893242: step 440410, loss = 0.15 (1740.9 examples/sec; 0.074 sec/batch)
2017-06-02 13:18:02.737472: step 440420, loss = 0.15 (1516.2 examples/sec; 0.084 sec/batch)
2017-06-02 13:18:03.603721: step 440430, loss = 0.20 (1477.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:18:04.469947: step 440440, loss = 0.10 (1477.7 examples/sec; 0.087 sec/batch)
2017-06-02 13:18:05.329339: step 440450, loss = 0.10 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 13:18:06.195381: step 440460, loss = 0.16 (1478.0 examples/sec; 0.087 sec/batch)
2017-06-02 13:18:07.045352: step 440470, loss = 0.14 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 13:18:07.904898: step 440480, loss = 0.12 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:18:08.758846: step 440490, loss = 0.14 (1498.9 examples/sec; 0.085 sec/batch)
2017-06-02 13:18:09.718818: step 440500, loss = 0.13 (1333.4 examples/sec; 0.096 sec/batch)
2017-06-02 13:18:10.484571: step 440510, loss = 0.16 (1671.6 examples/sec; 0.077 sec/batch)
2017-06-02 13:18:11.352036: step 440520, loss = 0.13 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:18:12.202809: step 440530, loss = 0.15 (1504.5 examples/sec; 0.085 sec/batch)
2017-06-02 13:18:13.042488: step 440540, loss = 0.13 (1524.4 examples/sec; 0.084 sec/batch)
2017-06-02 13:18:13.906118: step 440550, loss = 0.14 (1482.1 examples/sec; 0.086 sec/batch)
2017-06-02 13:18:14.737344: step 440560, loss = 0.11 (1539.9 examples/sec; 0.083 sec/batch)
2017-06-02 13:18:15.630737: step 440570, loss = 0.17 (1432.7 examples/sec; 0.089 sec/batch)
2017-06-02 13:18:16.500200: step 440580, loss = 0.15 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:18:17.340663: step 440590, loss = 0.13 (1523.0 examples/sec; 0.084 sec/batch)
2017-06-02 13:18:18.290788: step 440600, loss = 0.13 (1347.2 examples/sec; 0.095 sec/batch)
2017-06-02 13:18:19.058508: step 440610, loss = 0.14 (1667.3 examples/sec; 0.077 sec/batch)
2017-06-02 13:18:19.946832: step 440620, loss = 0.17 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 13:18:20.801511: step 440630, loss = 0.10 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:18:21.669054: step 440640, loss = 0.12 (1475.4 examples/sec; 0.087 sec/batch)
2017-06-02 13:18:22.525763: step 440650, loss = 0.17 (1494.1 examples/sec; 0.086 sec/batch)
2017-06-02 13:18:23.388355: step 440660, loss = 0.13 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 13:18:24.229587: step 440670, loss = 0.14 (1521.6 examples/sec; 0.084 sec/batch)
2017-06-02 13:18:25.092837: step 440680, loss = 0.18 (1482.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:18:25.962457: step 440690, loss = 0.13 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:18:26.893007: step 440700, loss = 0.15 (1375.5 examples/sec; 0.093 sec/batch)
2017-06-02 13:18:27.671113: step 440710, loss = 0.12 (1645.0 examples/sec; 0.078 sec/batch)
2017-06-02 13:18:28.512254: step 440720, loss = 0.10 (1521.8 examples/sec; 0.084 sec/batch)
2017-06-02 13:18:29.367804: step 440730, loss = 0.11 (1496.1 examples/sec; 0.086 sec/batch)
2017-06-02 13:18:30.217701: step 440740, loss = 0.13 (1506.1 examples/sec; 0.085 sec/batch)
2017-06-02 13:18:31.096824: step 440750, loss = 0.12 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 13:18:31.947643: step 440760, loss = 0.16 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 13:18:32.815066: step 440770, loss = 0.12 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:18:33.678064: step 440780, loss = 0.13 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:18:34.566632: step 440790, loss = 0.12 (1440.5 examples/sec; 0.089 sec/batch)
2017-06-02 13:18:35.550036: step 440800, loss = 0.12 (1301.6 examples/sec; 0.098 sec/batch)
2017-06-02 13:18:36.315960: step 440810, loss = 0.15 (1671.2 examples/sec; 0.077 sec/batch)
2017-06-02 13:18:37.190032: step 440820, loss = 0.14 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 13:18:38.038353: step 440830, loss = 0.12 (1508.9 examples/sec; 0.085 sec/batch)
2017-06-02 13:18:38.876705: step 440840, loss = 0.13 (1526.8 examples/sec; 0.084 sec/batch)
2017-06-02 13:18:39.735246: step 440850, loss = 0.14 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 13:18:40.607068: step 440860, loss = 0.11 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:18:41.470250: step 440870, loss = 0.16 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 13:18:42.337266: step 440880, loss = 0.16 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 13:18:43.193121: step 440890, loss = 0.13 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 13:18:44.141668: step 440900, loss = 0.14 (1349.4 examples/sec; 0.095 sec/batch)
2017-06-02 13:18:44.916693: step 440910, loss = 0.12 (1651.6 examples/sec; 0.078 sec/batch)
2017-06-02 13:18:45.789330: step 440920, loss = 0.12 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:18:46.653409: step 440930, loss = 0.11 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 13:18:47.507465: step 440940, loss = 0.12 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 13:18:48.391650: step 440950, loss = 0.14 (1447.7 examples/sec; 0.088 sec/batch)
2017-06-02 13:18:49.259769: step 440960, loss = 0.13 (1474.4 examples/sec; 0.087 sec/batch)
2017-06-02 13:18:50.138649: step 440970, loss = 0.13 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 13:18:51.004972: step 440980, loss = 0.11 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:18:51.845158: step 440990, loss = 0.13 (1523.5 examples/sec; 0.084 sec/batch)
2017-06-02 13:18:52.803738: step 441000, loss = 0.16 (1335.3 examples/sec; 0.096 sec/batch)
2017-06-02 13:18:53.574543: step 441010, loss = 0.13 (1660.6 examples/sec; 0.077 sec/batch)
2017-06-02 13:18:54.447406: step 441020, loss = 0.13 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 13:18:55.326006: step 441030, loss = 0.13 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 13:18:56.191188: step 441040, loss = 0.14 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 13:18:57.064635: step 441050, loss = 0.13 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:18:57.915549: step 441060, loss = 0.13 (1504.3 examples/sec; 0.085 sec/batch)
2017-06-02 13:18:58.814471: step 441070, loss = 0.12 (1423.9 examples/sec; 0.090 sec/batch)
2017-06-02 13:18:59.663897: step 441080, loss = 0.14 (1506.9 examples/sec; 0.085 sec/batch)
2017-06-02 13:19:00.500792: step 441090, loss = 0.13 (1529.5 examples/sec; 0.084 sec/batch)
2017-06-02 13:19:01.450671: step 441100, loss = 0.13 (1347.5 examples/sec; 0.095 sec/batch)
2017-06-02 13:19:02.209953: step 441110, loss = 0.13 (1685.8 examples/sec; 0.076 sec/batch)
2017-06-02 13:19:03.074579: step 441120, loss = 0.13 (1480.4 examples/sec; 0.086 sec/batch)
2017-06-02 13:19:03.915649: step 441130, loss = 0.13 (1521.9 examples/sec; 0.084 sec/batch)
2017-06-02 13:19:04.750262: step 441140, loss = 0.12 (1533.6 examples/sec; 0.083 sec/batch)
2017-06-02 13:19:05.618144: step 441150, loss = 0.11 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:19:06.491566: step 441160, loss = 0.14 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:19:07.352057: step 441170, loss = 0.17 (1487.5 examples/sec; 0.086 sec/batch)
2017-06-02 13:19:08.215459: step 441180, loss = 0.10 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 13:19:09.069117: step 441190, loss = 0.12 (1499.4 examples/sec; 0.085 sec/batch)
2017-06-02 13:19:10.043151: step 441200, loss = 0.13 (1314.1 examples/sec; 0.097 sec/batch)
2017-06-02 13:19:10.781988: step 441210, loss = 0.17 (1732.5 examples/sec; 0.074 sec/batch)
2017-06-02 13:19:11.676888: step 441220, loss = 0.15 (1430.3 examples/sec; 0.089 sec/batch)
2017-06-02 13:19:12.557253: step 441230, loss = 0.11 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 13:19:13.425490: step 441240, loss = 0.14 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:19:14.321793: step 441250, loss = 0.15 (1428.1 examples/sec; 0.090 sec/batch)
2017-06-02 13:19:15.190334: step 441260, loss = 0.11 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 13:19:16.054913: step 441270, loss = 0.15 (1480.5 examples/sec; 0.086 sec/batch)
2017-06-02 13:19:16.941990: step 441280, loss = 0.15 (1442.9 examples/sec; 0.089 sec/batch)
2017-06-02 13:19:17.796929: step 441290, loss = 0.15 (1497.2 examples/sec; 0.085 sec/batch)
2017-06-02 13:19:18.779048: step 441300, loss = 0.16 (1303.3 examples/sec; 0.098 sec/batch)
2017-06-02 13:19:19.553553: step 441310, loss = 0.17 (1652.7 examples/sec; 0.077 sec/batch)
2017-06-02 13:19:20.433086: step 441320, loss = 0.12 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 13:19:21.271891: step 441330, loss = 0.13 (1526.0 examples/sec; 0.084 sec/batch)
2017-06-02 13:19:22.149850: step 441340, loss = 0.14 (1457.9 examples/sec; 0.088 sec/batch)
2017-06-02 13:19:22.999621: step 441350, loss = 0.13 (1506.3 examples/sec; 0.085 sec/batch)
2017-06-02 13:19:23.862161: step 441360, loss = 0.15 (1484.0 examples/sec; 0.086 sec/batch)
2017-06-02 13:19:24.746672: step 441370, loss = 0.12 (1447.1 examples/sec; 0.088 sec/batch)
2017-06-02 13:19:25.585574: step 441380, loss = 0.11 (1525.8 examples/sec; 0.084 sec/batch)
2017-06-02 13:19:26.402406: step 441390, loss = 0.13 (1567.0 examples/sec; 0.082 sec/batch)
2017-06-02 13:19:27.429515: step 441400, loss = 0.14 (1246.2 examples/sec; 0.103 sec/batch)
2017-06-02 13:19:28.134111: step 441410, loss = 0.11 (1816.6 examples/sec; 0.070 sec/batch)
2017-06-02 13:19:29.003758: step 441420, loss = 0.13 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:19:29.848065: step 441430, loss = 0.16 (1516.0 examples/sec; 0.084 sec/batch)
2017-06-02 13:19:30.701591: step 441440, loss = 0.12 (1499.7 examples/sec; 0.085 sec/batch)
2017-06-02 13:19:31.547028: step 441450, loss = 0.11 (1514.0 examples/sec; 0.085 sec/batch)
2017-06-02 13:19:32.417623: step 441460, loss = 0.14 (1470.3 examples/sec; 0.087 sec/batch)
2017-06-02 13:19:33.273128: step 441470, loss = 0.12 (1496.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:19:34.145366: step 441480, loss = 0.14 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 13:19:35.023104: step 441490, loss = 0.11 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 13:19:35.995841: step 441500, loss = 0.14 (1315.9 examples/sec; 0.097 sec/batch)
2017-06-02 13:19:36.753789: step 441510, loss = 0.15 (1688.8 examples/sec; 0.076 sec/batch)
2017-06-02 13:19:37.607944: step 441520, loss = 0.10 (1498.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:19:38.460807: step 441530, loss = 0.15 (1500.8 examples/sec; 0.085 sec/batch)
2017-06-02 13:19:39.325772: step 441540, loss = 0.14 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:19:40.171702: step 441550, loss = 0.14 (1513.1 examples/sec; 0.085 sec/batch)
2017-06-02 13:19:41.035180: step 441560, loss = 0.13 (1482.4 examples/sec; 0.086 sec/batch)
2017-06-02 13:19:41.905096: step 441570, loss = 0.09 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 13:19:42.784154: step 441580, loss = 0.14 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 13:19:43.662737: step 441590, loss = 0.11 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 13:19:44.649361: step 441600, loss = 0.12 (1297.3 examples/sec; 0.099 sec/batch)
2017-06-02 13:19:45.400586: step 441610, loss = 0.13 (1703.9 examples/sec; 0.075 sec/batch)
2017-06-02 13:19:46.277810: step 441620, loss = 0.11 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 13:19:47.141590: step 441630, loss = 0.14 (1481.9 examples/sec; 0.086 sec/batch)
2017-06-02 13:19:47.988869: step 441640, loss = 0.18 (1510.7 examples/sec; 0.085 sec/batch)
2017-06-02 13:19:48.819671: step 441650, loss = 0.19 (1540.7 examples/sec; 0.083 sec/batch)
2017-06-02 13:19:49.663602: step 441660, loss = 0.15 (1516.7 examples/sec; 0.084 sec/batch)
2017-06-02 13:19:50.521565: step 441670, loss = 0.14 (1491.9 examples/sec; 0.086 sec/batch)
2017-06-02 13:19:51.377686: step 441680, loss = 0.14 (1495.1 examples/sec; 0.086 sec/batch)
2017-06-02 13:19:52.216770: step 441690, loss = 0.11 (1525.5 examples/sec; 0.084 sec/batch)
2017-06-02 13:19:53.307015: step 441700, loss = 0.14 (1174.1 examples/sec; 0.109 sec/batch)
2017-06-02 13:19:53.995606: step 441710, loss = 0.14 (1858.9 examples/sec; 0.069 sec/batch)
2017-06-02 13:19:54.859782: step 441720, loss = 0.13 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:19:55.722292: step 441730, loss = 0.13 (1484.1 examples/sec; 0.086 sec/batch)
2017-06-02 13:19:56.571992: step 441740, loss = 0.11 (1506.4 examples/sec; 0.085 sec/batch)
2017-06-02 13:19:57.453863: step 441750, loss = 0.13 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 13:19:58.311041: step 441760, loss = 0.13 (1493.3 examples/sec; 0.086 sec/batch)
2017-06-02 13:19:59.181669: step 441770, loss = 0.16 (1470.2 examples/sec; 0.087 sec/batch)
2017-06-02 13:20:00.041578: step 441780, loss = 0.12 (1488.6 examples/sec; 0.086 sec/batch)
2017-06-02 13:20:00.919235: step 441790, loss = 0.14 (1458.3 examples/sec; 0.088 sec/batch)
2017-06-02 13:20:01.884588: step 441800, loss = 0.13 (1325.9 examples/sec; 0.097 sec/batch)
2017-06-02 13:20:02.651194: step 441810, loss = 0.16 (1669.7 examples/sec; 0.077 sec/batch)
2017-06-02 13:20:03.522766: step 441820, loss = 0.14 (1468.6 examples/sec; 0.087 sec/batch)
2017-06-02 13:20:04.390050: step 441830, loss = 0.12 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 13:20:05.260413: step 441840, loss = 0.12 (1470.7 examples/sec; 0.087 sec/batch)
2017-06-02 13:20:06.103323: step 441850, loss = 0.13 (1518.6 examples/sec; 0.084 sec/batch)
2017-06-02 13:20:06.952329: step 441860, loss = 0.13 (1507.6 examples/sec; 0.085 sec/batch)
2017-06-02 13:20:07.823063: step 441870, loss = 0.15 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 13:20:08.698855: step 441880, loss = 0.15 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 13:20:09.571782: step 441890, loss = 0.11 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 13:20:10.540735: step 441900, loss = 0.16 (1321.0 examples/sec; 0.097 sec/batch)
2017-06-02 13:20:11.321958: step 441910, loss = 0.11 (1638.4 examples/sec; 0.078 sec/batch)
2017-06-02 13:20:12.195867: step 441920, loss = 0.14 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 13:20:13.074761: step 441930, loss = 0.12 (1456.4 examples/sec; 0.088 sec/batch)
2017-06-02 13:20:13.970706: step 441940, loss = 0.12 (1428.6 examples/sec; 0.090 sec/batch)
2017-06-02 13:20:14.815283: step 441950, loss = 0.13 (1515.6 examples/sec; 0.084 sec/batch)
2017-06-02 13:20:15.666336: step 441960, loss = 0.11 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 13:20:16.528396: step 441970, loss = 0.16 (1484.8 examples/sec; 0.086 sec/batch)
2017-06-02 13:20:17.377879: step 441980, loss = 0.11 (1506.8 examples/sec; 0.085 sec/batch)
2017-06-02 13:20:18.242074: step 441990, loss = 0.13 (1481.2 examples/sec; 0.086 sec/batch)
2017-06-02 13:20:19.206460: step 442000, loss = 0.16 (1327.3 examples/sec; 0.096 sec/batch)
2017-06-02 13:20:19.968606: step 442010, loss = 0.15 (1679.5 examples/sec; 0.076 sec/batch)
