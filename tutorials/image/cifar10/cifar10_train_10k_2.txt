Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-06-02 16:42:57.749040: step 0, loss = 4.67 (85.5 examples/sec; 1.497 sec/batch)
2017-06-02 16:42:58.527927: step 10, loss = 4.62 (1643.3 examples/sec; 0.078 sec/batch)
2017-06-02 16:42:59.453055: step 20, loss = 4.50 (1383.6 examples/sec; 0.093 sec/batch)
2017-06-02 16:43:00.380521: step 30, loss = 4.51 (1380.1 examples/sec; 0.093 sec/batch)
2017-06-02 16:43:01.313654: step 40, loss = 4.37 (1371.7 examples/sec; 0.093 sec/batch)
2017-06-02 16:43:02.270006: step 50, loss = 4.21 (1338.4 examples/sec; 0.096 sec/batch)
2017-06-02 16:43:03.212328: step 60, loss = 4.24 (1358.4 examples/sec; 0.094 sec/batch)
2017-06-02 16:43:04.140253: step 70, loss = 4.10 (1379.4 examples/sec; 0.093 sec/batch)
2017-06-02 16:43:05.083696: step 80, loss = 4.21 (1356.7 examples/sec; 0.094 sec/batch)
2017-06-02 16:43:06.033435: step 90, loss = 4.20 (1347.7 examples/sec; 0.095 sec/batch)
2017-06-02 16:43:07.113354: step 100, loss = 4.19 (1185.3 examples/sec; 0.108 sec/batch)
2017-06-02 16:43:07.950219: step 110, loss = 4.21 (1529.5 examples/sec; 0.084 sec/batch)
2017-06-02 16:43:08.898652: step 120, loss = 4.09 (1349.6 examples/sec; 0.095 sec/batch)
2017-06-02 16:43:09.776763: step 130, loss = 3.93 (1457.7 examples/sec; 0.088 sec/batch)
2017-06-02 16:43:10.723829: step 140, loss = 4.08 (1351.5 examples/sec; 0.095 sec/batch)
2017-06-02 16:43:11.690288: step 150, loss = 4.00 (1324.4 examples/sec; 0.097 sec/batch)
2017-06-02 16:43:12.623369: step 160, loss = 3.90 (1371.8 examples/sec; 0.093 sec/batch)
2017-06-02 16:43:13.589109: step 170, loss = 3.79 (1325.4 examples/sec; 0.097 sec/batch)
2017-06-02 16:43:14.498050: step 180, loss = 3.86 (1408.2 examples/sec; 0.091 sec/batch)
2017-06-02 16:43:15.438303: step 190, loss = 3.82 (1361.3 examples/sec; 0.094 sec/batch)
2017-06-02 16:43:16.501243: step 200, loss = 3.82 (1204.2 examples/sec; 0.106 sec/batch)
2017-06-02 16:43:17.325967: step 210, loss = 3.84 (1552.0 examples/sec; 0.082 sec/batch)
2017-06-02 16:43:18.236303: step 220, loss = 4.18 (1406.1 examples/sec; 0.091 sec/batch)
2017-06-02 16:43:19.142165: step 230, loss = 3.81 (1413.0 examples/sec; 0.091 sec/batch)
2017-06-02 16:43:20.104975: step 240, loss = 3.76 (1329.5 examples/sec; 0.096 sec/batch)
2017-06-02 16:43:21.061087: step 250, loss = 3.71 (1338.7 examples/sec; 0.096 sec/batch)
2017-06-02 16:43:22.009353: step 260, loss = 3.91 (1349.9 examples/sec; 0.095 sec/batch)
2017-06-02 16:43:22.911929: step 270, loss = 3.91 (1418.2 examples/sec; 0.090 sec/batch)
2017-06-02 16:43:23.835398: step 280, loss = 3.54 (1386.1 examples/sec; 0.092 sec/batch)
2017-06-02 16:43:24.765898: step 290, loss = 3.65 (1375.6 examples/sec; 0.093 sec/batch)
2017-06-02 16:43:25.823759: step 300, loss = 3.49 (1210.0 examples/sec; 0.106 sec/batch)
2017-06-02 16:43:26.659209: step 310, loss = 3.79 (1532.1 examples/sec; 0.084 sec/batch)
2017-06-02 16:43:27.565992: step 320, loss = 3.50 (1411.6 examples/sec; 0.091 sec/batch)
2017-06-02 16:43:28.532082: step 330, loss = 3.34 (1324.9 examples/sec; 0.097 sec/batch)
2017-06-02 16:43:29.475668: step 340, loss = 3.35 (1356.5 examples/sec; 0.094 sec/batch)
2017-06-02 16:43:30.411765: step 350, loss = 3.38 (1367.4 examples/sec; 0.094 sec/batch)
2017-06-02 16:43:31.343315: step 360, loss = 3.31 (1374.1 examples/sec; 0.093 sec/batch)
2017-06-02 16:43:32.238128: step 370, loss = 3.30 (1430.5 examples/sec; 0.089 sec/batch)
2017-06-02 16:43:33.148363: step 380, loss = 3.35 (1406.2 examples/sec; 0.091 sec/batch)
2017-06-02 16:43:34.070739: step 390, loss = 3.38 (1387.7 examples/sec; 0.092 sec/batch)
2017-06-02 16:43:35.124768: step 400, loss = 3.38 (1214.4 examples/sec; 0.105 sec/batch)
2017-06-02 16:43:35.934021: step 410, loss = 3.33 (1581.7 examples/sec; 0.081 sec/batch)
2017-06-02 16:43:36.870019: step 420, loss = 3.23 (1367.5 examples/sec; 0.094 sec/batch)
2017-06-02 16:43:37.793806: step 430, loss = 3.20 (1385.6 examples/sec; 0.092 sec/batch)
2017-06-02 16:43:38.731852: step 440, loss = 3.41 (1364.6 examples/sec; 0.094 sec/batch)
2017-06-02 16:43:39.648993: step 450, loss = 3.23 (1395.6 examples/sec; 0.092 sec/batch)
2017-06-02 16:43:40.567942: step 460, loss = 3.34 (1392.9 examples/sec; 0.092 sec/batch)
2017-06-02 16:43:41.465239: step 470, loss = 3.08 (1426.5 examples/sec; 0.090 sec/batch)
2017-06-02 16:43:42.387680: step 480, loss = 3.17 (1387.6 examples/sec; 0.092 sec/batch)
2017-06-02 16:43:43.294156: step 490, loss = 3.13 (1412.1 examples/sec; 0.091 sec/batch)
2017-06-02 16:43:44.330804: step 500, loss = 3.08 (1234.8 examples/sec; 0.104 sec/batch)
2017-06-02 16:43:45.131111: step 510, loss = 3.12 (1599.4 examples/sec; 0.080 sec/batch)
2017-06-02 16:43:46.073373: step 520, loss = 3.80 (1358.4 examples/sec; 0.094 sec/batch)
2017-06-02 16:43:47.002188: step 530, loss = 3.10 (1378.1 examples/sec; 0.093 sec/batch)
2017-06-02 16:43:47.954585: step 540, loss = 3.06 (1344.0 examples/sec; 0.095 sec/batch)
2017-06-02 16:43:48.836449: step 550, loss = 3.18 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 16:43:49.774163: step 560, loss = 2.95 (1365.0 examples/sec; 0.094 sec/batch)
2017-06-02 16:43:50.690824: step 570, loss = 2.99 (1396.4 examples/sec; 0.092 sec/batch)
2017-06-02 16:43:51.614182: step 580, loss = 3.06 (1386.2 examples/sec; 0.092 sec/batch)
2017-06-02 16:43:52.508945: step 590, loss = 2.90 (1430.5 examples/sec; 0.089 sec/batch)
2017-06-02 16:43:53.550944: step 600, loss = 3.10 (1228.4 examples/sec; 0.104 sec/batch)
2017-06-02 16:43:54.357368: step 610, loss = 3.10 (1587.3 examples/sec; 0.081 sec/batch)
2017-06-02 16:43:55.294793: step 620, loss = 3.12 (1365.4 examples/sec; 0.094 sec/batch)
2017-06-02 16:43:56.262242: step 630, loss = 3.15 (1323.1 examples/sec; 0.097 sec/batch)
2017-06-02 16:43:57.154341: step 640, loss = 3.01 (1434.8 examples/sec; 0.089 sec/batch)
2017-06-02 16:43:58.046088: step 650, loss = 2.87 (1435.4 examples/sec; 0.089 sec/batch)
2017-06-02 16:43:58.936542: step 660, loss = 2.80 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 16:43:59.851020: step 670, loss = 2.76 (1399.7 examples/sec; 0.091 sec/batch)
2017-06-02 16:44:00.725378: step 680, loss = 2.82 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 16:44:01.652640: step 690, loss = 2.91 (1380.4 examples/sec; 0.093 sec/batch)
2017-06-02 16:44:02.688564: step 700, loss = 2.75 (1235.6 examples/sec; 0.104 sec/batch)
2017-06-02 16:44:03.428321: step 710, loss = 2.91 (1730.3 examples/sec; 0.074 sec/batch)
2017-06-02 16:44:04.333638: step 720, loss = 2.89 (1413.9 examples/sec; 0.091 sec/batch)
2017-06-02 16:44:05.239326: step 730, loss = 3.11 (1413.3 examples/sec; 0.091 sec/batch)
2017-06-02 16:44:06.100046: step 740, loss = 2.79 (1487.1 examples/sec; 0.086 sec/batch)
2017-06-02 16:44:06.964371: step 750, loss = 2.78 (1480.9 examples/sec; 0.086 sec/batch)
2017-06-02 16:44:07.810096: step 760, loss = 2.91 (1513.5 examples/sec; 0.085 sec/batch)
2017-06-02 16:44:08.667914: step 770, loss = 2.75 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 16:44:09.506725: step 780, loss = 2.75 (1526.0 examples/sec; 0.084 sec/batch)
2017-06-02 16:44:10.693507: step 790, loss = 2.56 (1078.6 examples/sec; 0.119 sec/batch)
2017-06-02 16:44:12.066694: step 800, loss = 2.65 (932.1 examples/sec; 0.137 sec/batch)
2017-06-02 16:44:13.068333: step 810, loss = 2.83 (1277.9 examples/sec; 0.100 sec/batch)
2017-06-02 16:44:13.927838: step 820, loss = 2.57 (1489.2 examples/sec; 0.086 sec/batch)
2017-06-02 16:44:14.930747: step 830, loss = 2.52 (1276.3 examples/sec; 0.100 sec/batch)
2017-06-02 16:44:16.118060: step 840, loss = 2.50 (1078.1 examples/sec; 0.119 sec/batch)
2017-06-02 16:44:17.272422: step 850, loss = 2.69 (1108.8 examples/sec; 0.115 sec/batch)
2017-06-02 16:44:18.123497: step 860, loss = 2.47 (1504.0 examples/sec; 0.085 sec/batch)
2017-06-02 16:44:19.041292: step 870, loss = 2.60 (1394.7 examples/sec; 0.092 sec/batch)
2017-06-02 16:44:19.938559: step 880, loss = 2.66 (1426.5 examples/sec; 0.090 sec/batch)
2017-06-02 16:44:20.820479: step 890, loss = 2.73 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 16:44:21.805715: step 900, loss = 2.43 (1299.2 examples/sec; 0.099 sec/batch)
2017-06-02 16:44:22.616062: step 910, loss = 2.70 (1579.6 examples/sec; 0.081 sec/batch)
2017-06-02 16:44:23.520183: step 920, loss = 2.45 (1415.7 examples/sec; 0.090 sec/batch)
2017-06-02 16:44:24.425779: step 930, loss = 2.47 (1413.4 examples/sec; 0.091 sec/batch)
2017-06-02 16:44:25.308114: step 940, loss = 2.71 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 16:44:26.227095: step 950, loss = 2.56 (1392.9 examples/sec; 0.092 sec/batch)
2017-06-02 16:44:27.119140: step 960, loss = 2.72 (1434.9 examples/sec; 0.089 sec/batch)
2017-06-02 16:44:27.957654: step 970, loss = 2.37 (1526.5 examples/sec; 0.084 sec/batch)
2017-06-02 16:44:28.843319: step 980, loss = 2.61 (1445.3 examples/sec; 0.089 sec/batch)
2017-06-02 16:44:29.733064: step 990, loss = 2.39 (1438.6 examples/sec; 0.089 sec/batch)
2017-06-02 16:44:30.713158: step 1000, loss = 2.36 (1306.0 examples/sec; 0.098 sec/batch)
2017-06-02 16:44:31.493345: step 1010, loss = 2.19 (1640.6 examples/sec; 0.078 sec/batch)
2017-06-02 16:44:32.384046: step 1020, loss = 2.46 (1437.1 examples/sec; 0.089 sec/batch)
2017-06-02 16:44:33.280085: step 1030, loss = 2.33 (1428.5 examples/sec; 0.090 sec/batch)
2017-06-02 16:44:34.137703: step 1040, loss = 2.49 (1492.5 examples/sec; 0.086 sec/batch)
2017-06-02 16:44:35.036330: step 1050, loss = 2.23 (1424.4 examples/sec; 0.090 sec/batch)
2017-06-02 16:44:35.919815: step 1060, loss = 2.32 (1448.8 examples/sec; 0.088 sec/batch)
2017-06-02 16:44:36.782839: step 1070, loss = 2.37 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 16:44:37.648232: step 1080, loss = 2.10 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 16:44:38.503258: step 1090, loss = 2.32 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 16:44:39.523332: step 1100, loss = 2.29 (1254.8 examples/sec; 0.102 sec/batch)
2017-06-02 16:44:40.271962: step 1110, loss = 2.32 (1709.8 examples/sec; 0.075 sec/batch)
2017-06-02 16:44:41.141886: step 1120, loss = 2.39 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 16:44:42.018750: step 1130, loss = 2.17 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 16:44:42.858119: step 1140, loss = 2.24 (1524.9 examples/sec; 0.084 sec/batch)
2017-06-02 16:44:43.736565: step 1150, loss = 2.06 (1457.1 examples/sec; 0.088 sec/batch)
2017-06-02 16:44:44.639428: step 1160, loss = 2.29 (1417.7 examples/sec; 0.090 sec/batch)
2017-06-02 16:44:45.525432: step 1170, loss = 2.40 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 16:44:46.347815: step 1180, loss = 2.38 (1556.5 examples/sec; 0.082 sec/batch)
2017-06-02 16:44:47.213822: step 1190, loss = 2.16 (1478.1 examples/sec; 0.087 sec/batch)
2017-06-02 16:44:48.209764: step 1200, loss = 2.33 (1285.2 examples/sec; 0.100 sec/batch)
2017-06-02 16:44:48.956282: step 1210, loss = 2.21 (1714.6 examples/sec; 0.075 sec/batch)
2017-06-02 16:44:49.865262: step 1220, loss = 2.27 (1408.2 examples/sec; 0.091 sec/batch)
2017-06-02 16:44:50.744776: step 1230, loss = 2.28 (1455.3 examples/sec; 0.088 sec/batch)
2017-06-02 16:44:51.633281: step 1240, loss = 2.16 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 16:44:52.474806: step 1250, loss = 2.13 (1521.0 examples/sec; 0.084 sec/batch)
2017-06-02 16:44:53.352110: step 1260, loss = 2.04 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 16:44:54.251749: step 1270, loss = 2.17 (1422.8 examples/sec; 0.090 sec/batch)
2017-06-02 16:44:55.092678: step 1280, loss = 2.31 (1522.1 examples/sec; 0.084 sec/batch)
2017-06-02 16:44:55.970507: step 1290, loss = 2.30 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 16:44:56.977876: step 1300, loss = 1.97 (1270.6 examples/sec; 0.101 sec/batch)
2017-06-02 16:44:57.702009: step 1310, loss = 2.40 (1767.7 examples/sec; 0.072 sec/batch)
2017-06-02 16:44:58.594889: step 1320, loss = 2.18 (1433.6 examples/sec; 0.089 sec/batch)
2017-06-02 16:44:59.485121: step 1330, loss = 2.17 (1437.8 examples/sec; 0.089 sec/batch)
2017-06-02 16:45:00.310257: step 1340, loss = 2.11 (1551.3 examples/sec; 0.083 sec/batch)
2017-06-02 16:45:01.194016: step 1350, loss = 2.03 (1448.4 examples/sec; 0.088 sec/batch)
2017-06-02 16:45:02.070702: step 1360, loss = 2.12 (1460.0 examples/sec; 0.088 sec/batch)
2017-06-02 16:45:02.899576: step 1370, loss = 2.14 (1544.3 examples/sec; 0.083 sec/batch)
2017-06-02 16:45:03.803179: step 1380, loss = 1.85 (1416.5 examples/sec; 0.090 sec/batch)
2017-06-02 16:45:04.677607: step 1390, loss = 1.96 (1463.8 examples/sec; 0.087 sec/batch)
2017-06-02 16:45:05.722574: step 1400, loss = 2.06 (1224.9 examples/sec; 0.104 sec/batch)
2017-06-02 16:45:06.450529: step 1410, loss = 1.99 (1758.4 examples/sec; 0.073 sec/batch)
2017-06-02 16:45:07.353818: step 1420, loss = 1.98 (1417.0 examples/sec; 0.090 sec/batch)
2017-06-02 16:45:08.236113: step 1430, loss = 2.02 (1450.8 examples/sec; 0.088 sec/batch)
2017-06-02 16:45:09.072376: step 1440, loss = 2.25 (1530.6 examples/sec; 0.084 sec/batch)
2017-06-02 16:45:09.958861: step 1450, loss = 1.93 (1443.9 examples/sec; 0.089 sec/batch)
2017-06-02 16:45:10.853746: step 1460, loss = 2.26 (1430.3 examples/sec; 0.089 sec/batch)
2017-06-02 16:45:11.673120: step 1470, loss = 1.85 (1562.2 examples/sec; 0.082 sec/batch)
2017-06-02 16:45:12.563846: step 1480, loss = 1.96 (1437.0 examples/sec; 0.089 sec/batch)
2017-06-02 16:45:13.444353: step 1490, loss = 1.96 (1453.7 examples/sec; 0.088 sec/batch)
2017-06-02 16:45:14.407515: step 1500, loss = 1.89 (1329.0 examples/sec; 0.096 sec/batch)
2017-06-02 16:45:15.156636: step 1510, loss = 1.89 (1708.7 examples/sec; 0.075 sec/batch)
2017-06-02 16:45:16.048474: step 1520, loss = 1.97 (1435.2 examples/sec; 0.089 sec/batch)
2017-06-02 16:45:16.926303: step 1530, loss = 2.10 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 16:45:17.755413: step 1540, loss = 1.84 (1543.8 examples/sec; 0.083 sec/batch)
2017-06-02 16:45:18.629397: step 1550, loss = 2.04 (1464.6 examples/sec; 0.087 sec/batch)
2017-06-02 16:45:19.506702: step 1560, loss = 2.07 (1459.0 examples/sec; 0.088 sec/batch)
2017-06-02 16:45:20.387507: step 1570, loss = 2.01 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 16:45:21.205156: step 1580, loss = 1.86 (1565.5 examples/sec; 0.082 sec/batch)
2017-06-02 16:45:22.080948: step 1590, loss = 2.07 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 16:45:23.070861: step 1600, loss = 2.19 (1293.0 examples/sec; 0.099 sec/batch)
2017-06-02 16:45:23.796574: step 1610, loss = 1.85 (1763.8 examples/sec; 0.073 sec/batch)
2017-06-02 16:45:24.673231: step 1620, loss = 1.79 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 16:45:25.540618: step 1630, loss = 1.83 (1475.7 examples/sec; 0.087 sec/batch)
2017-06-02 16:45:26.370945: step 1640, loss = 1.70 (1541.6 examples/sec; 0.083 sec/batch)
2017-06-02 16:45:27.252250: step 1650, loss = 1.84 (1452.4 examples/sec; 0.088 sec/batch)
2017-06-02 16:45:28.133058: step 1660, loss = 1.74 (1453.2 examples/sec; 0.088 sec/batch)
2017-06-02 16:45:28.965417: step 1670, loss = 1.92 (1537.8 examples/sec; 0.083 sec/batch)
2017-06-02 16:45:29.847299: step 1680, loss = 1.79 (1451.5 examples/sec; 0.088 sec/batch)
2017-06-02 16:45:30.731183: step 1690, loss = 1.85 (1448.2 examples/sec; 0.088 sec/batch)
2017-06-02 16:45:31.685663: step 1700, loss = 1.73 (1341.0 examples/sec; 0.095 sec/batch)
2017-06-02 16:45:32.463421: step 1710, loss = 1.86 (1645.7 examples/sec; 0.078 sec/batch)
2017-06-02 16:45:33.351088: step 1720, loss = 1.71 (1442.0 examples/sec; 0.089 sec/batch)
2017-06-02 16:45:34.231490: step 1730, loss = 2.06 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 16:45:35.055904: step 1740, loss = 1.88 (1552.6 examples/sec; 0.082 sec/batch)
2017-06-02 16:45:35.930287: step 1750, loss = 1.73 (1463.9 examples/sec; 0.087 sec/batch)
2017-06-02 16:45:36.787103: step 1760, loss = 1.81 (1493.9 examples/sec; 0.086 sec/batch)
2017-06-02 16:45:37.602852: step 1770, loss = 1.85 (1569.1 examples/sec; 0.082 sec/batch)
2017-06-02 16:45:38.460048: step 1780, loss = 1.90 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 16:45:39.311312: step 1790, loss = 1.84 (1503.6 examples/sec; 0.085 sec/batch)
2017-06-02 16:45:40.295700: step 1800, loss = 1.65 (1300.3 examples/sec; 0.098 sec/batch)
2017-06-02 16:45:41.029002: step 1810, loss = 1.72 (1745.5 examples/sec; 0.073 sec/batch)
2017-06-02 16:45:41.907757: step 1820, loss = 1.80 (1456.6 examples/sec; 0.088 sec/batch)
2017-06-02 16:45:42.787506: step 1830, loss = 1.81 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 16:45:43.620076: step 1840, loss = 1.74 (1537.4 examples/sec; 0.083 sec/batch)
2017-06-02 16:45:44.498374: step 1850, loss = 1.65 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 16:45:45.372960: step 1860, loss = 1.77 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 16:45:46.251611: step 1870, loss = 1.69 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 16:45:47.090296: step 1880, loss = 1.55 (1526.2 examples/sec; 0.084 sec/batch)
2017-06-02 16:45:47.975979: step 1890, loss = 1.54 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 16:45:48.974178: step 1900, loss = 1.54 (1282.3 examples/sec; 0.100 sec/batch)
2017-06-02 16:45:49.673029: step 1910, loss = 1.68 (1831.7 examples/sec; 0.070 sec/batch)
2017-06-02 16:45:50.557908: step 1920, loss = 1.80 (1446.5 examples/sec; 0.088 sec/batch)
2017-06-02 16:45:51.447936: step 1930, loss = 1.73 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 16:45:52.303565: step 1940, loss = 1.69 (1496.0 examples/sec; 0.086 sec/batch)
2017-06-02 16:45:53.198959: step 1950, loss = 1.83 (1429.5 examples/sec; 0.090 sec/batch)
2017-06-02 16:45:54.103714: step 1960, loss = 1.62 (1414.7 examples/sec; 0.090 sec/batch)
2017-06-02 16:45:54.999146: step 1970, loss = 1.83 (1429.5 examples/sec; 0.090 sec/batch)
2017-06-02 16:45:55.823851: step 1980, loss = 1.62 (1552.1 examples/sec; 0.082 sec/batch)
2017-06-02 16:45:56.716302: step 1990, loss = 1.47 (1434.3 examples/sec; 0.089 sec/batch)
2017-06-02 16:45:57.714728: step 2000, loss = 1.60 (1282.0 examples/sec; 0.100 sec/batch)
2017-06-02 16:45:58.449539: step 2010, loss = 1.64 (1741.9 examples/sec; 0.073 sec/batch)
2017-06-02 16:45:59.343280: step 2020, loss = 1.74 (1432.2 examples/sec; 0.089 sec/batch)
2017-06-02 16:46:00.228969: step 2030, loss = 1.35 (1445.2 examples/sec; 0.089 sec/batch)
2017-06-02 16:46:01.072745: step 2040, loss = 1.49 (1517.0 examples/sec; 0.084 sec/batch)
2017-06-02 16:46:01.947727: step 2050, loss = 1.75 (1462.9 examples/sec; 0.087 sec/batch)
2017-06-02 16:46:02.827033: step 2060, loss = 1.73 (1455.7 examples/sec; 0.088 sec/batch)
2017-06-02 16:46:03.708096: step 2070, loss = 1.57 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 16:46:04.538723: step 2080, loss = 1.80 (1541.0 examples/sec; 0.083 sec/batch)
2017-06-02 16:46:05.405627: step 2090, loss = 1.50 (1476.5 examples/sec; 0.087 sec/batch)
2017-06-02 16:46:06.389694: step 2100, loss = 1.67 (1300.7 examples/sec; 0.098 sec/batch)
2017-06-02 16:46:07.119212: step 2110, loss = 1.47 (1754.6 examples/sec; 0.073 sec/batch)
2017-06-02 16:46:07.987132: step 2120, loss = 1.57 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 16:46:08.863776: step 2130, loss = 1.75 (1460.1 examples/sec; 0.088 sec/batch)
2017-06-02 16:46:09.704948: step 2140, loss = 1.66 (1521.7 examples/sec; 0.084 sec/batch)
2017-06-02 16:46:10.609239: step 2150, loss = 1.40 (1415.5 examples/sec; 0.090 sec/batch)
2017-06-02 16:46:11.493557: step 2160, loss = 1.56 (1447.5 examples/sec; 0.088 sec/batch)
2017-06-02 16:46:12.325728: step 2170, loss = 1.68 (1538.1 examples/sec; 0.083 sec/batch)
2017-06-02 16:46:13.206780: step 2180, loss = 1.61 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 16:46:14.089288: step 2190, loss = 1.62 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 16:46:15.095339: step 2200, loss = 1.54 (1272.3 examples/sec; 0.101 sec/batch)
2017-06-02 16:46:15.791618: step 2210, loss = 1.70 (1838.3 examples/sec; 0.070 sec/batch)
2017-06-02 16:46:16.664403: step 2220, loss = 1.38 (1466.6 examples/sec; 0.087 sec/batch)
2017-06-02 16:46:17.544287: step 2230, loss = 1.57 (1454.7 examples/sec; 0.088 sec/batch)
2017-06-02 16:46:18.363048: step 2240, loss = 1.39 (1563.3 examples/sec; 0.082 sec/batch)
2017-06-02 16:46:19.218741: step 2250, loss = 1.63 (1495.9 examples/sec; 0.086 sec/batch)
2017-06-02 16:46:20.095097: step 2260, loss = 1.40 (1460.6 examples/sec; 0.088 sec/batch)
2017-06-02 16:46:20.920843: step 2270, loss = 1.77 (1550.1 examples/sec; 0.083 sec/batch)
2017-06-02 16:46:21.790630: step 2280, loss = 1.70 (1471.6 examples/sec; 0.087 sec/batch)
2017-06-02 16:46:22.663663: step 2290, loss = 1.54 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 16:46:23.624352: step 2300, loss = 1.50 (1332.4 examples/sec; 0.096 sec/batch)
2017-06-02 16:46:24.407967: step 2310, loss = 1.55 (1633.5 examples/sec; 0.078 sec/batch)
2017-06-02 16:46:25.272454: step 2320, loss = 1.55 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 16:46:26.154812: step 2330, loss = 1.91 (1450.7 examples/sec; 0.088 sec/batch)
2017-06-02 16:46:26.995424: step 2340, loss = 1.41 (1522.7 examples/sec; 0.084 sec/batch)
2017-06-02 16:46:27.880468: step 2350, loss = 1.56 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 16:46:28.762442: step 2360, loss = 1.40 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 16:46:29.597170: step 2370, loss = 1.54 (1533.4 examples/sec; 0.083 sec/batch)
2017-06-02 16:46:30.495970: step 2380, loss = 1.61 (1424.1 examples/sec; 0.090 sec/batch)
2017-06-02 16:46:31.364232: step 2390, loss = 1.50 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 16:46:32.340235: step 2400, loss = 1.44 (1311.5 examples/sec; 0.098 sec/batch)
2017-06-02 16:46:33.070307: step 2410, loss = 1.60 (1753.3 examples/sec; 0.073 sec/batch)
2017-06-02 16:46:33.917080: step 2420, loss = 1.36 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 16:46:34.753638: step 2430, loss = 1.52 (1530.1 examples/sec; 0.084 sec/batch)
2017-06-02 16:46:35.570329: step 2440, loss = 1.37 (1567.3 examples/sec; 0.082 sec/batch)
2017-06-02 16:46:36.426285: step 2450, loss = 1.35 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 16:46:37.294089: step 2460, loss = 1.44 (1474.9 examples/sec; 0.087 sec/batch)
2017-06-02 16:46:38.112075: step 2470, loss = 1.28 (1564.8 examples/sec; 0.082 sec/batch)
2017-06-02 16:46:38.999568: step 2480, loss = 1.31 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 16:46:39.889572: step 2490, loss = 1.46 (1438.2 examples/sec; 0.089 sec/batch)
2017-06-02 16:46:40.860215: step 2500, loss = 1.33 (1318.7 examples/sec; 0.097 sec/batch)
2017-06-02 16:46:41.664762: step 2510, loss = 1.33 (1591.0 examples/sec; 0.080 sec/batch)
2017-06-02 16:46:42.570046: step 2520, loss = 1.53 (1413.9 examples/sec; 0.091 sec/batch)
2017-06-02 16:46:43.449466: step 2530, loss = 1.37 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 16:46:44.284641: step 2540, loss = 1.33 (1532.6 examples/sec; 0.084 sec/batch)
2017-06-02 16:46:45.184233: step 2550, loss = 1.34 (1422.9 examples/sec; 0.090 sec/batch)
2017-06-02 16:46:46.060079: step 2560, loss = 1.37 (1461.4 examples/sec; 0.088 sec/batch)
2017-06-02 16:46:46.959335: step 2570, loss = 1.34 (1423.4 examples/sec; 0.090 sec/batch)
2017-06-02 16:46:47.797947: step 2580, loss = 1.34 (1526.3 examples/sec; 0.084 sec/batch)
2017-06-02 16:46:48.680705: step 2590, loss = 1.28 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 16:46:49.658010: step 2600, loss = 1.41 (1309.7 examples/sec; 0.098 sec/batch)
2017-06-02 16:46:50.398660: step 2610, loss = 1.62 (1728.2 examples/sec; 0.074 sec/batch)
2017-06-02 16:46:51.281845: step 2620, loss = 1.33 (1449.3 examples/sec; 0.088 sec/batch)
2017-06-02 16:46:52.159027: step 2630, loss = 1.36 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 16:46:53.006752: step 2640, loss = 1.36 (1509.9 examples/sec; 0.085 sec/batch)
2017-06-02 16:46:53.851461: step 2650, loss = 1.25 (1515.3 examples/sec; 0.084 sec/batch)
2017-06-02 16:46:54.714289: step 2660, loss = 1.20 (1483.5 examples/sec; 0.086 sec/batch)
2017-06-02 16:46:55.584318: step 2670, loss = 1.34 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 16:46:56.410160: step 2680, loss = 1.27 (1549.9 examples/sec; 0.083 sec/batch)
2017-06-02 16:46:57.298500: step 2690, loss = 1.32 (1440.9 examples/sec; 0.089 sec/batch)
2017-06-02 16:46:58.269321: step 2700, loss = 1.43 (1318.5 examples/sec; 0.097 sec/batch)
2017-06-02 16:46:59.002599: step 2710, loss = 1.37 (1745.6 examples/sec; 0.073 sec/batch)
2017-06-02 16:46:59.885288: step 2720, loss = 1.39 (1450.1 examples/sec; 0.088 sec/batch)
2017-06-02 16:47:00.752711: step 2730, loss = 1.36 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 16:47:01.588250: step 2740, loss = 1.20 (1532.0 examples/sec; 0.084 sec/batch)
2017-06-02 16:47:02.479791: step 2750, loss = 1.32 (1435.7 examples/sec; 0.089 sec/batch)
2017-06-02 16:47:03.353904: step 2760, loss = 1.29 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 16:47:04.226044: step 2770, loss = 1.23 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 16:47:05.071461: step 2780, loss = 1.42 (1514.0 examples/sec; 0.085 sec/batch)
2017-06-02 16:47:05.965437: step 2790, loss = 1.35 (1431.8 examples/sec; 0.089 sec/batch)
2017-06-02 16:47:06.965239: step 2800, loss = 1.19 (1280.3 examples/sec; 0.100 sec/batch)
2017-06-02 16:47:07.680124: step 2810, loss = 1.28 (1790.5 examples/sec; 0.071 sec/batch)
2017-06-02 16:47:08.565357: step 2820, loss = 1.21 (1445.9 examples/sec; 0.089 sec/batch)
2017-06-02 16:47:09.438317: step 2830, loss = 1.43 (1466.3 examples/sec; 0.087 sec/batch)
2017-06-02 16:47:10.276867: step 2840, loss = 1.33 (1526.4 examples/sec; 0.084 sec/batch)
2017-06-02 16:47:11.160848: step 2850, loss = 1.45 (1448.0 examples/sec; 0.088 sec/batch)
2017-06-02 16:47:12.045224: step 2860, loss = 1.30 (1447.4 examples/sec; 0.088 sec/batch)
2017-06-02 16:47:12.899913: step 2870, loss = 1.41 (1497.6 examples/sec; 0.085 sec/batch)
2017-06-02 16:47:13.793840: step 2880, loss = 1.29 (1431.9 examples/sec; 0.089 sec/batch)
2017-06-02 16:47:14.665825: step 2890, loss = 1.40 (1467.9 examples/sec; 0.087 sec/batch)
2017-06-02 16:47:15.610255: step 2900, loss = 1.34 (1355.3 examples/sec; 0.094 sec/batch)
2017-06-02 16:47:16.388915: step 2910, loss = 1.50 (1643.8 examples/sec; 0.078 sec/batch)
2017-06-02 16:47:17.272338: step 2920, loss = 1.29 (1448.9 examples/sec; 0.088 sec/batch)
2017-06-02 16:47:18.134981: step 2930, loss = 1.18 (1483.8 examples/sec; 0.086 sec/batch)
2017-06-02 16:47:18.977937: step 2940, loss = 1.16 (1518.5 examples/sec; 0.084 sec/batch)
2017-06-02 16:47:19.855823: step 2950, loss = 1.50 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 16:47:20.732716: step 2960, loss = 1.26 (1459.7 examples/sec; 0.088 sec/batch)
2017-06-02 16:47:21.583353: step 2970, loss = 1.38 (1504.8 examples/sec; 0.085 sec/batch)
2017-06-02 16:47:22.457972: step 2980, loss = 1.48 (1463.5 examples/sec; 0.087 sec/batch)
2017-06-02 16:47:23.347936: step 2990, loss = 1.22 (1438.3 examples/sec; 0.089 sec/batch)
2017-06-02 16:47:24.332001: step 3000, loss = 1.29 (1300.7 examples/sec; 0.098 sec/batch)
2017-06-02 16:47:25.060194: step 3010, loss = 1.25 (1757.8 examples/sec; 0.073 sec/batch)
2017-06-02 16:47:25.923172: step 3020, loss = 1.35 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 16:47:26.794985: step 3030, loss = 1.28 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 16:47:27.616805: step 3040, loss = 1.11 (1557.5 examples/sec; 0.082 sec/batch)
2017-06-02 16:47:28.489429: step 3050, loss = 1.39 (1466.8 examples/sec; 0.087 sec/batch)
2017-06-02 16:47:29.349514: step 3060, loss = 1.33 (1488.2 examples/sec; 0.086 sec/batch)
2017-06-02 16:47:30.198015: step 3070, loss = 1.10 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 16:47:31.085226: step 3080, loss = 1.11 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 16:47:31.947183: step 3090, loss = 1.16 (1485.0 examples/sec; 0.086 sec/batch)
2017-06-02 16:47:32.918188: step 3100, loss = 1.27 (1318.2 examples/sec; 0.097 sec/batch)
2017-06-02 16:47:33.675262: step 3110, loss = 1.15 (1690.7 examples/sec; 0.076 sec/batch)
2017-06-02 16:47:34.532788: step 3120, loss = 1.39 (1492.7 examples/sec; 0.086 sec/batch)
2017-06-02 16:47:35.436611: step 3130, loss = 1.44 (1416.2 examples/sec; 0.090 sec/batch)
2017-06-02 16:47:36.264509: step 3140, loss = 1.21 (1546.1 examples/sec; 0.083 sec/batch)
2017-06-02 16:47:37.124261: step 3150, loss = 1.34 (1488.8 examples/sec; 0.086 sec/batch)
2017-06-02 16:47:38.016977: step 3160, loss = 1.20 (1433.8 examples/sec; 0.089 sec/batch)
2017-06-02 16:47:38.847321: step 3170, loss = 1.31 (1541.5 examples/sec; 0.083 sec/batch)
2017-06-02 16:47:39.739726: step 3180, loss = 1.29 (1434.3 examples/sec; 0.089 sec/batch)
2017-06-02 16:47:40.628143: step 3190, loss = 1.11 (1440.8 examples/sec; 0.089 sec/batch)
2017-06-02 16:47:41.627803: step 3200, loss = 1.19 (1280.4 examples/sec; 0.100 sec/batch)
2017-06-02 16:47:42.352215: step 3210, loss = 1.19 (1766.9 examples/sec; 0.072 sec/batch)
2017-06-02 16:47:43.219669: step 3220, loss = 1.08 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 16:47:44.091887: step 3230, loss = 1.17 (1467.5 examples/sec; 0.087 sec/batch)
2017-06-02 16:47:44.934663: step 3240, loss = 1.40 (1518.8 examples/sec; 0.084 sec/batch)
2017-06-02 16:47:45.810489: step 3250, loss = 1.26 (1461.5 examples/sec; 0.088 sec/batch)
2017-06-02 16:47:46.655956: step 3260, loss = 1.20 (1513.9 examples/sec; 0.085 sec/batch)
2017-06-02 16:47:47.491931: step 3270, loss = 1.03 (1531.2 examples/sec; 0.084 sec/batch)
2017-06-02 16:47:48.375955: step 3280, loss = 1.26 (1447.9 examples/sec; 0.088 sec/batch)
2017-06-02 16:47:49.252193: step 3290, loss = 1.22 (1460.8 examples/sec; 0.088 sec/batch)
2017-06-02 16:47:50.199902: step 3300, loss = 1.33 (1350.6 examples/sec; 0.095 sec/batch)
2017-06-02 16:47:50.959469: step 3310, loss = 1.10 (1685.2 examples/sec; 0.076 sec/batch)
2017-06-02 16:47:51.831598: step 3320, loss = 1.31 (1467.7 examples/sec; 0.087 sec/batch)
2017-06-02 16:47:52.721463: step 3330, loss = 1.31 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 16:47:53.570199: step 3340, loss = 1.12 (1508.1 examples/sec; 0.085 sec/batch)
2017-06-02 16:47:54.454825: step 3350, loss = 1.00 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 16:47:55.341652: step 3360, loss = 1.17 (1443.3 examples/sec; 0.089 sec/batch)
2017-06-02 16:47:56.186543: step 3370, loss = 1.33 (1515.0 examples/sec; 0.084 sec/batch)
2017-06-02 16:47:57.058343: step 3380, loss = 1.38 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 16:47:57.917081: step 3390, loss = 1.15 (1490.6 examples/sec; 0.086 sec/batch)
2017-06-02 16:47:58.876553: step 3400, loss = 1.17 (1334.1 examples/sec; 0.096 sec/batch)
2017-06-02 16:47:59.669707: step 3410, loss = 1.09 (1613.8 examples/sec; 0.079 sec/batch)
2017-06-02 16:48:00.544505: step 3420, loss = 1.13 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 16:48:01.377849: step 3430, loss = 1.14 (1536.0 examples/sec; 0.083 sec/batch)
2017-06-02 16:48:02.268912: step 3440, loss = 1.13 (1436.5 examples/sec; 0.089 sec/batch)
2017-06-02 16:48:03.171496: step 3450, loss = 1.15 (1418.1 examples/sec; 0.090 sec/batch)
2017-06-02 16:48:04.070317: step 3460, loss = 1.19 (1424.1 examples/sec; 0.090 sec/batch)
2017-06-02 16:48:04.916126: step 3470, loss = 1.13 (1513.3 examples/sec; 0.085 sec/batch)
2017-06-02 16:48:05.804141: step 3480, loss = 1.27 (1441.4 examples/sec; 0.089 sec/batch)
2017-06-02 16:48:06.673591: step 3490, loss = 1.16 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 16:48:07.610245: step 3500, loss = 1.15 (1366.6 examples/sec; 0.094 sec/batch)
2017-06-02 16:48:08.389369: step 3510, loss = 1.06 (1642.9 examples/sec; 0.078 sec/batch)
2017-06-02 16:48:09.252489: step 3520, loss = 1.11 (1483.0 examples/sec; 0.086 sec/batch)
2017-06-02 16:48:10.084410: step 3530, loss = 1.11 (1538.6 examples/sec; 0.083 sec/batch)
2017-06-02 16:48:10.949378: step 3540, loss = 1.32 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 16:48:11.836596: step 3550, loss = 1.12 (1442.7 examples/sec; 0.089 sec/batch)
2017-06-02 16:48:12.686609: step 3560, loss = 1.09 (1505.9 examples/sec; 0.085 sec/batch)
2017-06-02 16:48:13.547572: step 3570, loss = 0.95 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 16:48:14.445325: step 3580, loss = 1.09 (1425.8 examples/sec; 0.090 sec/batch)
2017-06-02 16:48:15.347871: step 3590, loss = 1.05 (1418.2 examples/sec; 0.090 sec/batch)
2017-06-02 16:48:16.272349: step 3600, loss = 1.30 (1384.6 examples/sec; 0.092 sec/batch)
2017-06-02 16:48:17.039518: step 3610, loss = 1.17 (1668.5 examples/sec; 0.077 sec/batch)
2017-06-02 16:48:17.916289: step 3620, loss = 1.05 (1459.9 examples/sec; 0.088 sec/batch)
2017-06-02 16:48:18.765124: step 3630, loss = 1.17 (1507.9 examples/sec; 0.085 sec/batch)
2017-06-02 16:48:19.647619: step 3640, loss = 1.07 (1450.4 examples/sec; 0.088 sec/batch)
2017-06-02 16:48:20.544708: step 3650, loss = 1.26 (1426.8 examples/sec; 0.090 sec/batch)
2017-06-02 16:48:21.380938: step 3660, loss = 1.17 (1530.7 examples/sec; 0.084 sec/batch)
2017-06-02 16:48:22.268364: step 3670, loss = 1.10 (1442.4 examples/sec; 0.089 sec/batch)
2017-06-02 16:48:23.169963: step 3680, loss = 1.19 (1419.7 examples/sec; 0.090 sec/batch)
2017-06-02 16:48:24.001837: step 3690, loss = 1.22 (1538.7 examples/sec; 0.083 sec/batch)
2017-06-02 16:48:24.960961: step 3700, loss = 1.35 (1334.6 examples/sec; 0.096 sec/batch)
2017-06-02 16:48:25.748314: step 3710, loss = 1.11 (1625.7 examples/sec; 0.079 sec/batch)
2017-06-02 16:48:26.626988: step 3720, loss = 1.23 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 16:48:27.485345: step 3730, loss = 1.15 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 16:48:28.350537: step 3740, loss = 1.04 (1479.4 examples/sec; 0.087 sec/batch)
2017-06-02 16:48:29.225660: step 3750, loss = 1.19 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 16:48:30.059564: step 3760, loss = 1.07 (1534.9 examples/sec; 0.083 sec/batch)
2017-06-02 16:48:30.941048: step 3770, loss = 1.21 (1452.1 examples/sec; 0.088 sec/batch)
2017-06-02 16:48:31.818229: step 3780, loss = 1.24 (1459.2 examples/sec; 0.088 sec/batch)
2017-06-02 16:48:32.663828: step 3790, loss = 1.01 (1513.7 examples/sec; 0.085 sec/batch)
2017-06-02 16:48:33.672239: step 3800, loss = 1.34 (1269.3 examples/sec; 0.101 sec/batch)
2017-06-02 16:48:34.445992: step 3810, loss = 1.18 (1654.3 examples/sec; 0.077 sec/batch)
2017-06-02 16:48:35.271669: step 3820, loss = 1.09 (1550.2 examples/sec; 0.083 sec/batch)
2017-06-02 16:48:36.147814: step 3830, loss = 1.11 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 16:48:37.033245: step 3840, loss = 0.94 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 16:48:37.874182: step 3850, loss = 1.07 (1522.1 examples/sec; 0.084 sec/batch)
2017-06-02 16:48:38.727613: step 3860, loss = 1.34 (1499.8 examples/sec; 0.085 sec/batch)
2017-06-02 16:48:39.582995: step 3870, loss = 1.13 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 16:48:40.438704: step 3880, loss = 1.17 (1495.8 examples/sec; 0.086 sec/batch)
2017-06-02 16:48:41.262196: step 3890, loss = 1.05 (1554.4 examples/sec; 0.082 sec/batch)
2017-06-02 16:48:42.488344: step 3900, loss = 0.96 (1043.9 examples/sec; 0.123 sec/batch)
2017-06-02 16:48:43.610969: step 3910, loss = 1.16 (1140.2 examples/sec; 0.112 sec/batch)
2017-06-02 16:48:44.865517: step 3920, loss = 1.25 (1020.3 examples/sec; 0.125 sec/batch)
2017-06-02 16:48:46.153433: step 3930, loss = 1.17 (993.9 examples/sec; 0.129 sec/batch)
2017-06-02 16:48:47.060776: step 3940, loss = 1.03 (1410.7 examples/sec; 0.091 sec/batch)
2017-06-02 16:48:47.882414: step 3950, loss = 1.01 (1557.9 examples/sec; 0.082 sec/batch)
2017-06-02 16:48:49.052272: step 3960, loss = 1.04 (1094.2 examples/sec; 0.117 sec/batch)
2017-06-02 16:48:50.288525: step 3970, loss = 0.92 (1035.4 examples/sec; 0.124 sec/batch)
2017-06-02 16:48:51.591441: step 3980, loss = 0.99 (982.4 examples/sec; 0.130 sec/batch)
2017-06-02 16:48:52.550317: step 3990, loss = 1.07 (1334.9 examples/sec; 0.096 sec/batch)
2017-06-02 16:48:53.526456: step 4000, loss = 1.11 (1311.3 examples/sec; 0.098 sec/batch)
2017-06-02 16:48:54.251553: step 4010, loss = 1.20 (1765.3 examples/sec; 0.073 sec/batch)
2017-06-02 16:48:55.117243: step 4020, loss = 0.99 (1478.6 examples/sec; 0.087 sec/batch)
2017-06-02 16:48:55.991291: step 4030, loss = 1.18 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 16:48:56.842131: step 4040, loss = 1.27 (1504.4 examples/sec; 0.085 sec/batch)
2017-06-02 16:48:57.713318: step 4050, loss = 1.17 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 16:48:58.601450: step 4060, loss = 1.13 (1441.2 examples/sec; 0.089 sec/batch)
2017-06-02 16:48:59.493814: step 4070, loss = 1.11 (1434.4 examples/sec; 0.089 sec/batch)
2017-06-02 16:49:00.324941: step 4080, loss = 1.10 (1540.1 examples/sec; 0.083 sec/batch)
2017-06-02 16:49:01.216981: step 4090, loss = 1.02 (1434.9 examples/sec; 0.089 sec/batch)
2017-06-02 16:49:02.183992: step 4100, loss = 1.03 (1323.7 examples/sec; 0.097 sec/batch)
2017-06-02 16:49:02.906738: step 4110, loss = 1.04 (1771.0 examples/sec; 0.072 sec/batch)
2017-06-02 16:49:03.798149: step 4120, loss = 1.03 (1435.9 examples/sec; 0.089 sec/batch)
2017-06-02 16:49:04.675987: step 4130, loss = 1.04 (1458.1 examples/sec; 0.088 sec/batch)
2017-06-02 16:49:05.555827: step 4140, loss = 1.04 (1454.8 examples/sec; 0.088 sec/batch)
2017-06-02 16:49:06.391217: step 4150, loss = 0.96 (1532.2 examples/sec; 0.084 sec/batch)
2017-06-02 16:49:07.259727: step 4160, loss = 1.23 (1473.8 examples/sec; 0.087 sec/batch)
2017-06-02 16:49:08.112875: step 4170, loss = 1.14 (1500.3 examples/sec; 0.085 sec/batch)
2017-06-02 16:49:08.940159: step 4180, loss = 1.12 (1547.2 examples/sec; 0.083 sec/batch)
2017-06-02 16:49:09.813640: step 4190, loss = 0.89 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 16:49:10.801491: step 4200, loss = 1.10 (1295.7 examples/sec; 0.099 sec/batch)
2017-06-02 16:49:11.556342: step 4210, loss = 1.11 (1695.7 examples/sec; 0.075 sec/batch)
2017-06-02 16:49:12.392369: step 4220, loss = 1.10 (1531.1 examples/sec; 0.084 sec/batch)
2017-06-02 16:49:13.288511: step 4230, loss = 1.10 (1428.3 examples/sec; 0.090 sec/batch)
2017-06-02 16:49:14.167235: step 4240, loss = 1.07 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 16:49:14.996920: step 4250, loss = 1.11 (1542.8 examples/sec; 0.083 sec/batch)
2017-06-02 16:49:15.859539: step 4260, loss = 1.16 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 16:49:16.722536: step 4270, loss = 1.17 (1483.2 examples/sec; 0.086 sec/batch)
2017-06-02 16:49:17.553024: step 4280, loss = 1.07 (1541.3 examples/sec; 0.083 sec/batch)
2017-06-02 16:49:18.434619: step 4290, loss = 1.06 (1451.9 examples/sec; 0.088 sec/batch)
2017-06-02 16:49:19.420223: step 4300, loss = 1.14 (1298.7 examples/sec; 0.099 sec/batch)
2017-06-02 16:49:20.200339: step 4310, loss = 1.04 (1640.8 examples/sec; 0.078 sec/batch)
2017-06-02 16:49:21.046959: step 4320, loss = 1.08 (1511.9 examples/sec; 0.085 sec/batch)
2017-06-02 16:49:21.937512: step 4330, loss = 1.08 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 16:49:22.818285: step 4340, loss = 1.01 (1453.3 examples/sec; 0.088 sec/batch)
2017-06-02 16:49:23.653311: step 4350, loss = 1.04 (1532.9 examples/sec; 0.084 sec/batch)
2017-06-02 16:49:24.532763: step 4360, loss = 1.20 (1455.5 examples/sec; 0.088 sec/batch)
2017-06-02 16:49:25.444311: step 4370, loss = 1.22 (1404.2 examples/sec; 0.091 sec/batch)
2017-06-02 16:49:26.274538: step 4380, loss = 1.08 (1541.8 examples/sec; 0.083 sec/batch)
2017-06-02 16:49:27.131426: step 4390, loss = 1.11 (1493.8 examples/sec; 0.086 sec/batch)
2017-06-02 16:49:28.121328: step 4400, loss = 0.99 (1293.1 examples/sec; 0.099 sec/batch)
2017-06-02 16:49:28.904215: step 4410, loss = 1.27 (1635.0 examples/sec; 0.078 sec/batch)
2017-06-02 16:49:29.730677: step 4420, loss = 1.16 (1548.8 examples/sec; 0.083 sec/batch)
2017-06-02 16:49:30.615480: step 4430, loss = 1.24 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 16:49:31.501337: step 4440, loss = 0.93 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 16:49:32.357398: step 4450, loss = 1.16 (1495.2 examples/sec; 0.086 sec/batch)
2017-06-02 16:49:33.236528: step 4460, loss = 1.00 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 16:49:34.114464: step 4470, loss = 1.18 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 16:49:34.940117: step 4480, loss = 1.12 (1550.3 examples/sec; 0.083 sec/batch)
2017-06-02 16:49:35.840975: step 4490, loss = 1.36 (1420.9 examples/sec; 0.090 sec/batch)
2017-06-02 16:49:36.838688: step 4500, loss = 0.95 (1282.9 examples/sec; 0.100 sec/batch)
2017-06-02 16:49:37.560928: step 4510, loss = 1.22 (1772.3 examples/sec; 0.072 sec/batch)
2017-06-02 16:49:38.419444: step 4520, loss = 1.04 (1490.9 examples/sec; 0.086 sec/batch)
2017-06-02 16:49:39.268476: step 4530, loss = 0.98 (1507.6 examples/sec; 0.085 sec/batch)
2017-06-02 16:49:40.120340: step 4540, loss = 1.19 (1502.6 examples/sec; 0.085 sec/batch)
2017-06-02 16:49:40.974398: step 4550, loss = 1.05 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 16:49:41.872771: step 4560, loss = 0.98 (1424.8 examples/sec; 0.090 sec/batch)
2017-06-02 16:49:42.739988: step 4570, loss = 1.06 (1476.0 examples/sec; 0.087 sec/batch)
2017-06-02 16:49:43.552120: step 4580, loss = 1.06 (1576.1 examples/sec; 0.081 sec/batch)
2017-06-02 16:49:44.447551: step 4590, loss = 1.06 (1429.5 examples/sec; 0.090 sec/batch)
2017-06-02 16:49:45.403663: step 4600, loss = 0.86 (1338.8 examples/sec; 0.096 sec/batch)
2017-06-02 16:49:46.134677: step 4610, loss = 1.11 (1751.0 examples/sec; 0.073 sec/batch)
2017-06-02 16:49:46.972392: step 4620, loss = 1.01 (1528.0 examples/sec; 0.084 sec/batch)
2017-06-02 16:49:47.807576: step 4630, loss = 0.96 (1532.6 examples/sec; 0.084 sec/batch)
2017-06-02 16:49:48.633290: step 4640, loss = 0.89 (1550.2 examples/sec; 0.083 sec/batch)
2017-06-02 16:49:49.506691: step 4650, loss = 1.12 (1465.5 examples/sec; 0.087 sec/batch)
2017-06-02 16:49:50.380477: step 4660, loss = 1.27 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 16:49:51.225429: step 4670, loss = 1.16 (1514.9 examples/sec; 0.084 sec/batch)
2017-06-02 16:49:52.066951: step 4680, loss = 1.08 (1521.0 examples/sec; 0.084 sec/batch)
2017-06-02 16:49:52.934085: step 4690, loss = 1.11 (1476.1 examples/sec; 0.087 sec/batch)
2017-06-02 16:49:53.889978: step 4700, loss = 1.01 (1339.1 examples/sec; 0.096 sec/batch)
2017-06-02 16:49:54.619063: step 4710, loss = 1.07 (1755.6 examples/sec; 0.073 sec/batch)
2017-06-02 16:49:55.483932: step 4720, loss = 0.97 (1480.0 examples/sec; 0.086 sec/batch)
2017-06-02 16:49:56.349042: step 4730, loss = 1.14 (1479.6 examples/sec; 0.087 sec/batch)
2017-06-02 16:49:57.179090: step 4740, loss = 1.09 (1542.1 examples/sec; 0.083 sec/batch)
2017-06-02 16:49:58.059238: step 4750, loss = 0.83 (1454.3 examples/sec; 0.088 sec/batch)
2017-06-02 16:49:58.917615: step 4760, loss = 0.92 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 16:49:59.723437: step 4770, loss = 1.00 (1588.4 examples/sec; 0.081 sec/batch)
2017-06-02 16:50:00.570463: step 4780, loss = 0.91 (1511.2 examples/sec; 0.085 sec/batch)
2017-06-02 16:50:01.433642: step 4790, loss = 1.16 (1482.9 examples/sec; 0.086 sec/batch)
2017-06-02 16:50:02.398360: step 4800, loss = 1.01 (1326.8 examples/sec; 0.096 sec/batch)
2017-06-02 16:50:03.141542: step 4810, loss = 0.97 (1722.3 examples/sec; 0.074 sec/batch)
2017-06-02 16:50:04.018606: step 4820, loss = 1.05 (1459.4 examples/sec; 0.088 sec/batch)
2017-06-02 16:50:04.896248: step 4830, loss = 1.05 (1458.5 examples/sec; 0.088 sec/batch)
2017-06-02 16:50:05.727090: step 4840, loss = 0.83 (1540.6 examples/sec; 0.083 sec/batch)
2017-06-02 16:50:06.602464: step 4850, loss = 0.99 (1462.2 examples/sec; 0.088 sec/batch)
2017-06-02 16:50:07.464717: step 4860, loss = 0.94 (1484.5 examples/sec; 0.086 sec/batch)
2017-06-02 16:50:08.286590: step 4870, loss = 0.98 (1557.4 examples/sec; 0.082 sec/batch)
2017-06-02 16:50:09.157051: step 4880, loss = 0.99 (1470.5 examples/sec; 0.087 sec/batch)
2017-06-02 16:50:10.022509: step 4890, loss = 1.05 (1479.0 examples/sec; 0.087 sec/batch)
2017-06-02 16:50:10.964913: step 4900, loss = 1.21 (1358.2 examples/sec; 0.094 sec/batch)
2017-06-02 16:50:11.766692: step 4910, loss = 1.02 (1596.4 examples/sec; 0.080 sec/batch)
2017-06-02 16:50:12.648617: step 4920, loss = 1.05 (1451.4 examples/sec; 0.088 sec/batch)
2017-06-02 16:50:13.504531: step 4930, loss = 1.15 (1495.5 examples/sec; 0.086 sec/batch)
2017-06-02 16:50:14.339780: step 4940, loss = 1.07 (1532.5 examples/sec; 0.084 sec/batch)
2017-06-02 16:50:15.209380: step 4950, loss = 0.95 (1471.9 examples/sec; 0.087 sec/batch)
2017-06-02 16:50:16.088068: step 4960, loss = 1.09 (1456.7 examples/sec; 0.088 sec/batch)
2017-06-02 16:50:16.922455: step 4970, loss = 1.04 (1534.1 examples/sec; 0.083 sec/batch)
2017-06-02 16:50:17.789710: step 4980, loss = 1.09 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 16:50:18.679856: step 4990, loss = 1.11 (1438.0 examples/sec; 0.089 sec/batch)
2017-06-02 16:50:19.636911: step 5000, loss = 0.94 (1337.4 examples/sec; 0.096 sec/batch)
2017-06-02 16:50:20.383067: step 5010, loss = 1.02 (1715.5 examples/sec; 0.075 sec/batch)
2017-06-02 16:50:21.256697: step 5020, loss = 1.01 (1465.1 examples/sec; 0.087 sec/batch)
2017-06-02 16:50:22.093778: step 5030, loss = 1.20 (1529.1 examples/sec; 0.084 sec/batch)
2017-06-02 16:50:22.948826: step 5040, loss = 1.11 (1497.0 examples/sec; 0.086 sec/batch)
2017-06-02 16:50:23.815444: step 5050, loss = 1.07 (1477.0 examples/sec; 0.087 sec/batch)
2017-06-02 16:50:24.687244: step 5060, loss = 1.09 (1468.2 examples/sec; 0.087 sec/batch)
2017-06-02 16:50:25.513901: step 5070, loss = 1.01 (1548.4 examples/sec; 0.083 sec/batch)
2017-06-02 16:50:26.383955: step 5080, loss = 0.88 (1471.2 examples/sec; 0.087 sec/batch)
2017-06-02 16:50:27.271302: step 5090, loss = 1.13 (1442.5 examples/sec; 0.089 sec/batch)
2017-06-02 16:50:28.216183: step 5100, loss = 1.02 (1354.7 examples/sec; 0.094 sec/batch)
2017-06-02 16:50:29.006377: step 5110, loss = 0.98 (1620.1 examples/sec; 0.079 sec/batch)
2017-06-02 16:50:29.886328: step 5120, loss = 1.02 (1454.4 examples/sec; 0.088 sec/batch)
2017-06-02 16:50:30.700863: step 5130, loss = 1.15 (1571.4 examples/sec; 0.081 sec/batch)
2017-06-02 16:50:31.598081: step 5140, loss = 1.23 (1426.6 examples/sec; 0.090 sec/batch)
2017-06-02 16:50:32.509545: step 5150, loss = 0.90 (1404.3 examples/sec; 0.091 sec/batch)
2017-06-02 16:50:33.388743: step 5160, loss = 0.86 (1455.9 examples/sec; 0.088 sec/batch)
2017-06-02 16:50:34.245966: step 5170, loss = 0.86 (1493.2 examples/sec; 0.086 sec/batch)
2017-06-02 16:50:35.127960: step 5180, loss = 1.19 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 16:50:36.014761: step 5190, loss = 0.90 (1443.4 examples/sec; 0.089 sec/batch)
2017-06-02 16:50:36.969778: step 5200, loss = 1.05 (1340.3 examples/sec; 0.096 sec/batch)
2017-06-02 16:50:37.762790: step 5210, loss = 1.14 (1614.1 examples/sec; 0.079 sec/batch)
2017-06-02 16:50:38.651006: step 5220, loss = 1.04 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 16:50:39.475218: step 5230, loss = 0.92 (1553.0 examples/sec; 0.082 sec/batch)
2017-06-02 16:50:40.353835: step 5240, loss = 0.99 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 16:50:41.235945: step 5250, loss = 1.29 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 16:50:42.086539: step 5260, loss = 0.97 (1504.8 examples/sec; 0.085 sec/batch)
2017-06-02 16:50:42.966963: step 5270, loss = 0.84 (1453.9 examples/sec; 0.088 sec/batch)
2017-06-02 16:50:43.843812: step 5280, loss = 0.97 (1459.8 examples/sec; 0.088 sec/batch)
2017-06-02 16:50:44.697986: step 5290, loss = 0.96 (1498.5 examples/sec; 0.085 sec/batch)
2017-06-02 16:50:45.676581: step 5300, loss = 0.93 (1308.0 examples/sec; 0.098 sec/batch)
2017-06-02 16:50:46.434217: step 5310, loss = 1.02 (1689.5 examples/sec; 0.076 sec/batch)
2017-06-02 16:50:47.340436: step 5320, loss = 0.80 (1412.5 examples/sec; 0.091 sec/batch)
2017-06-02 16:50:48.173830: step 5330, loss = 0.80 (1535.9 examples/sec; 0.083 sec/batch)
2017-06-02 16:50:49.034812: step 5340, loss = 1.04 (1486.7 examples/sec; 0.086 sec/batch)
2017-06-02 16:50:49.931216: step 5350, loss = 1.18 (1428.0 examples/sec; 0.090 sec/batch)
2017-06-02 16:50:50.777787: step 5360, loss = 0.87 (1511.9 examples/sec; 0.085 sec/batch)
2017-06-02 16:50:51.670671: step 5370, loss = 1.09 (1433.6 examples/sec; 0.089 sec/batch)
2017-06-02 16:50:52.532965: step 5380, loss = 0.93 (1484.4 examples/sec; 0.086 sec/batch)
2017-06-02 16:50:53.397463: step 5390, loss = 0.97 (1480.6 examples/sec; 0.086 sec/batch)
2017-06-02 16:50:54.357132: step 5400, loss = 1.11 (1333.8 examples/sec; 0.096 sec/batch)
2017-06-02 16:50:55.141709: step 5410, loss = 0.84 (1631.5 examples/sec; 0.078 sec/batch)
2017-06-02 16:50:55.992856: step 5420, loss = 1.04 (1503.9 examples/sec; 0.085 sec/batch)
2017-06-02 16:50:56.882722: step 5430, loss = 0.84 (1438.4 examples/sec; 0.089 sec/batch)
2017-06-02 16:50:57.766033: step 5440, loss = 0.86 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 16:50:58.587161: step 5450, loss = 1.01 (1558.8 examples/sec; 0.082 sec/batch)
2017-06-02 16:50:59.463298: step 5460, loss = 1.10 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 16:51:00.326625: step 5470, loss = 1.01 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 16:51:01.174592: step 5480, loss = 1.10 (1509.5 examples/sec; 0.085 sec/batch)
2017-06-02 16:51:02.028597: step 5490, loss = 0.91 (1498.8 examples/sec; 0.085 sec/batch)
2017-06-02 16:51:03.044653: step 5500, loss = 0.95 (1259.8 examples/sec; 0.102 sec/batch)
2017-06-02 16:51:03.731598: step 5510, loss = 1.08 (1863.3 examples/sec; 0.069 sec/batch)
2017-06-02 16:51:04.599355: step 5520, loss = 0.98 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 16:51:05.473051: step 5530, loss = 0.82 (1465.0 examples/sec; 0.087 sec/batch)
2017-06-02 16:51:06.302970: step 5540, loss = 0.99 (1542.3 examples/sec; 0.083 sec/batch)
2017-06-02 16:51:07.172741: step 5550, loss = 1.11 (1471.7 examples/sec; 0.087 sec/batch)
2017-06-02 16:51:08.069470: step 5560, loss = 1.12 (1427.4 examples/sec; 0.090 sec/batch)
2017-06-02 16:51:08.932831: step 5570, loss = 0.78 (1482.6 examples/sec; 0.086 sec/batch)
2017-06-02 16:51:09.780247: step 5580, loss = 0.97 (1510.5 examples/sec; 0.085 sec/batch)
2017-06-02 16:51:10.661134: step 5590, loss = 0.89 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 16:51:11.611366: step 5600, loss = 0.92 (1347.0 examples/sec; 0.095 sec/batch)
2017-06-02 16:51:12.339008: step 5610, loss = 1.02 (1759.1 examples/sec; 0.073 sec/batch)
2017-06-02 16:51:13.206606: step 5620, loss = 0.95 (1475.3 examples/sec; 0.087 sec/batch)
2017-06-02 16:51:14.083131: step 5630, loss = 0.96 (1460.3 examples/sec; 0.088 sec/batch)
2017-06-02 16:51:14.923000: step 5640, loss = 1.00 (1524.1 examples/sec; 0.084 sec/batch)
2017-06-02 16:51:15.804064: step 5650, loss = 0.96 (1452.8 examples/sec; 0.088 sec/batch)
2017-06-02 16:51:16.686685: step 5660, loss = 0.84 (1450.2 examples/sec; 0.088 sec/batch)
2017-06-02 16:51:17.528424: step 5670, loss = 0.81 (1520.7 examples/sec; 0.084 sec/batch)
2017-06-02 16:51:18.390784: step 5680, loss = 0.99 (1484.3 examples/sec; 0.086 sec/batch)
2017-06-02 16:51:19.253358: step 5690, loss = 0.73 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 16:51:20.217301: step 5700, loss = 0.86 (1327.9 examples/sec; 0.096 sec/batch)
2017-06-02 16:51:20.964492: step 5710, loss = 1.16 (1713.1 examples/sec; 0.075 sec/batch)
2017-06-02 16:51:21.824416: step 5720, loss = 0.92 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 16:51:22.670329: step 5730, loss = 1.05 (1513.1 examples/sec; 0.085 sec/batch)
2017-06-02 16:51:23.550770: step 5740, loss = 0.90 (1453.8 examples/sec; 0.088 sec/batch)
2017-06-02 16:51:24.419613: step 5750, loss = 0.97 (1473.2 examples/sec; 0.087 sec/batch)
2017-06-02 16:51:25.266179: step 5760, loss = 0.84 (1512.0 examples/sec; 0.085 sec/batch)
2017-06-02 16:51:26.133898: step 5770, loss = 1.17 (1475.1 examples/sec; 0.087 sec/batch)
2017-06-02 16:51:27.017994: step 5780, loss = 1.00 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 16:51:27.843881: step 5790, loss = 1.15 (1549.9 examples/sec; 0.083 sec/batch)
2017-06-02 16:51:28.835842: step 5800, loss = 0.95 (1290.4 examples/sec; 0.099 sec/batch)
2017-06-02 16:51:29.633281: step 5810, loss = 0.90 (1605.1 examples/sec; 0.080 sec/batch)
2017-06-02 16:51:30.458715: step 5820, loss = 0.77 (1550.7 examples/sec; 0.083 sec/batch)
2017-06-02 16:51:31.348274: step 5830, loss = 0.98 (1438.9 examples/sec; 0.089 sec/batch)
2017-06-02 16:51:32.204297: step 5840, loss = 1.03 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 16:51:33.055034: step 5850, loss = 0.89 (1504.6 examples/sec; 0.085 sec/batch)
2017-06-02 16:51:33.939858: step 5860, loss = 1.01 (1446.6 examples/sec; 0.088 sec/batch)
2017-06-02 16:51:34.800703: step 5870, loss = 1.03 (1486.9 examples/sec; 0.086 sec/batch)
2017-06-02 16:51:35.612933: step 5880, loss = 1.04 (1575.9 examples/sec; 0.081 sec/batch)
2017-06-02 16:51:36.516700: step 5890, loss = 1.05 (1416.3 examples/sec; 0.090 sec/batch)
2017-06-02 16:51:37.503180: step 5900, loss = 0.92 (1297.6 examples/sec; 0.099 sec/batch)
2017-06-02 16:51:38.238667: step 5910, loss = 0.94 (1740.3 examples/sec; 0.074 sec/batch)
2017-06-02 16:51:39.085424: step 5920, loss = 0.80 (1511.6 examples/sec; 0.085 sec/batch)
2017-06-02 16:51:39.940587: step 5930, loss = 0.93 (1496.8 examples/sec; 0.086 sec/batch)
2017-06-02 16:51:40.809234: step 5940, loss = 0.87 (1473.6 examples/sec; 0.087 sec/batch)
2017-06-02 16:51:41.644867: step 5950, loss = 0.94 (1531.8 examples/sec; 0.084 sec/batch)
2017-06-02 16:51:42.516913: step 5960, loss = 0.90 (1467.8 examples/sec; 0.087 sec/batch)
2017-06-02 16:51:43.396143: step 5970, loss = 1.09 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 16:51:44.215012: step 5980, loss = 0.95 (1563.1 examples/sec; 0.082 sec/batch)
2017-06-02 16:51:45.082454: step 5990, loss = 0.94 (1475.6 examples/sec; 0.087 sec/batch)
2017-06-02 16:51:46.066898: step 6000, loss = 0.98 (1300.2 examples/sec; 0.098 sec/batch)
2017-06-02 16:51:46.784201: step 6010, loss = 0.87 (1784.4 examples/sec; 0.072 sec/batch)
2017-06-02 16:51:47.645269: step 6020, loss = 0.90 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 16:51:48.527039: step 6030, loss = 0.98 (1451.6 examples/sec; 0.088 sec/batch)
2017-06-02 16:51:49.402321: step 6040, loss = 0.91 (1462.4 examples/sec; 0.088 sec/batch)
2017-06-02 16:51:50.257553: step 6050, loss = 0.88 (1496.7 examples/sec; 0.086 sec/batch)
2017-06-02 16:51:51.151522: step 6060, loss = 0.96 (1431.8 examples/sec; 0.089 sec/batch)
2017-06-02 16:51:52.028903: step 6070, loss = 1.03 (1458.9 examples/sec; 0.088 sec/batch)
2017-06-02 16:51:52.865612: step 6080, loss = 0.94 (1529.8 examples/sec; 0.084 sec/batch)
2017-06-02 16:51:53.746268: step 6090, loss = 0.84 (1453.5 examples/sec; 0.088 sec/batch)
2017-06-02 16:51:54.719867: step 6100, loss = 1.05 (1314.7 examples/sec; 0.097 sec/batch)
2017-06-02 16:51:55.437405: step 6110, loss = 0.91 (1783.9 examples/sec; 0.072 sec/batch)
2017-06-02 16:51:56.329039: step 6120, loss = 0.96 (1435.6 examples/sec; 0.089 sec/batch)
2017-06-02 16:51:57.208111: step 6130, loss = 0.95 (1456.1 examples/sec; 0.088 sec/batch)
2017-06-02 16:51:58.040040: step 6140, loss = 0.82 (1538.6 examples/sec; 0.083 sec/batch)
2017-06-02 16:51:58.917947: step 6150, loss = 0.81 (1458.0 examples/sec; 0.088 sec/batch)
2017-06-02 16:51:59.781673: step 6160, loss = 0.85 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 16:52:00.610659: step 6170, loss = 1.00 (1544.1 examples/sec; 0.083 sec/batch)
2017-06-02 16:52:01.482279: step 6180, loss = 0.83 (1468.5 examples/sec; 0.087 sec/batch)
2017-06-02 16:52:02.335251: step 6190, loss = 0.94 (1500.6 examples/sec; 0.085 sec/batch)
2017-06-02 16:52:03.259661: step 6200, loss = 0.89 (1384.7 examples/sec; 0.092 sec/batch)
2017-06-02 16:52:04.021944: step 6210, loss = 0.88 (1679.2 examples/sec; 0.076 sec/batch)
2017-06-02 16:52:04.933295: step 6220, loss = 0.83 (1404.5 examples/sec; 0.091 sec/batch)
2017-06-02 16:52:05.796319: step 6230, loss = 0.87 (1483.1 examples/sec; 0.086 sec/batch)
2017-06-02 16:52:06.681347: step 6240, loss = 0.86 (1446.3 examples/sec; 0.089 sec/batch)
2017-06-02 16:52:07.561085: step 6250, loss = 0.89 (1455.0 examples/sec; 0.088 sec/batch)
2017-06-02 16:52:08.410675: step 6260, loss = 1.02 (1506.6 examples/sec; 0.085 sec/batch)
2017-06-02 16:52:09.286410: step 6270, loss = 0.91 (1461.6 examples/sec; 0.088 sec/batch)
2017-06-02 16:52:10.199607: step 6280, loss = 1.01 (1401.7 examples/sec; 0.091 sec/batch)
2017-06-02 16:52:11.092157: step 6290, loss = 1.06 (1434.1 examples/sec; 0.089 sec/batch)
2017-06-02 16:52:12.039221: step 6300, loss = 0.93 (1351.5 examples/sec; 0.095 sec/batch)
2017-06-02 16:52:12.828193: step 6310, loss = 1.12 (1622.4 examples/sec; 0.079 sec/batch)
2017-06-02 16:52:13.688803: step 6320, loss = 0.98 (1487.4 examples/sec; 0.086 sec/batch)
2017-06-02 16:52:14.524951: step 6330, loss = 0.80 (1530.7 examples/sec; 0.084 sec/batch)
2017-06-02 16:52:15.426392: step 6340, loss = 0.79 (1420.0 examples/sec; 0.090 sec/batch)
2017-06-02 16:52:16.309705: step 6350, loss = 1.10 (1449.1 examples/sec; 0.088 sec/batch)
2017-06-02 16:52:17.157602: step 6360, loss = 0.90 (1509.6 examples/sec; 0.085 sec/batch)
2017-06-02 16:52:18.030985: step 6370, loss = 0.96 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 16:52:18.920405: step 6380, loss = 0.88 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 16:52:19.744038: step 6390, loss = 1.04 (1554.1 examples/sec; 0.082 sec/batch)
2017-06-02 16:52:20.727830: step 6400, loss = 0.75 (1301.1 examples/sec; 0.098 sec/batch)
2017-06-02 16:52:21.548109: step 6410, loss = 1.01 (1560.4 examples/sec; 0.082 sec/batch)
2017-06-02 16:52:22.382105: step 6420, loss = 0.91 (1534.8 examples/sec; 0.083 sec/batch)
2017-06-02 16:52:23.269514: step 6430, loss = 0.91 (1442.4 examples/sec; 0.089 sec/batch)
2017-06-02 16:52:24.154656: step 6440, loss = 0.94 (1446.1 examples/sec; 0.089 sec/batch)
2017-06-02 16:52:24.985349: step 6450, loss = 1.07 (1540.9 examples/sec; 0.083 sec/batch)
2017-06-02 16:52:25.872928: step 6460, loss = 0.84 (1442.1 examples/sec; 0.089 sec/batch)
2017-06-02 16:52:26.766255: step 6470, loss = 1.25 (1432.8 examples/sec; 0.089 sec/batch)
2017-06-02 16:52:27.622278: step 6480, loss = 0.97 (1495.3 examples/sec; 0.086 sec/batch)
2017-06-02 16:52:28.503938: step 6490, loss = 1.12 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 16:52:29.492041: step 6500, loss = 0.95 (1295.4 examples/sec; 0.099 sec/batch)
2017-06-02 16:52:30.279675: step 6510, loss = 0.75 (1625.1 examples/sec; 0.079 sec/batch)
2017-06-02 16:52:31.099429: step 6520, loss = 0.82 (1561.4 examples/sec; 0.082 sec/batch)
2017-06-02 16:52:31.957006: step 6530, loss = 0.96 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 16:52:32.853428: step 6540, loss = 0.79 (1427.9 examples/sec; 0.090 sec/batch)
2017-06-02 16:52:33.677213: step 6550, loss = 0.82 (1553.8 examples/sec; 0.082 sec/batch)
2017-06-02 16:52:34.575479: step 6560, loss = 0.88 (1425.0 examples/sec; 0.090 sec/batch)
2017-06-02 16:52:35.450322: step 6570, loss = 0.96 (1463.1 examples/sec; 0.087 sec/batch)
2017-06-02 16:52:36.302863: step 6580, loss = 0.94 (1501.4 examples/sec; 0.085 sec/batch)
2017-06-02 16:52:37.179347: step 6590, loss = 0.92 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 16:52:38.180007: step 6600, loss = 1.01 (1279.2 examples/sec; 0.100 sec/batch)
2017-06-02 16:52:38.948103: step 6610, loss = 1.01 (1666.5 examples/sec; 0.077 sec/batch)
2017-06-02 16:52:39.835113: step 6620, loss = 1.07 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 16:52:40.735489: step 6630, loss = 1.00 (1421.6 examples/sec; 0.090 sec/batch)
2017-06-02 16:52:41.548235: step 6640, loss = 1.01 (1574.9 examples/sec; 0.081 sec/batch)
2017-06-02 16:52:42.417337: step 6650, loss = 0.87 (1472.8 examples/sec; 0.087 sec/batch)
2017-06-02 16:52:43.291833: step 6660, loss = 0.97 (1463.7 examples/sec; 0.087 sec/batch)
2017-06-02 16:52:44.115963: step 6670, loss = 1.30 (1553.2 examples/sec; 0.082 sec/batch)
2017-06-02 16:52:44.980987: step 6680, loss = 0.91 (1479.7 examples/sec; 0.087 sec/batch)
2017-06-02 16:52:45.799855: step 6690, loss = 0.87 (1563.1 examples/sec; 0.082 sec/batch)
2017-06-02 16:52:46.765054: step 6700, loss = 0.93 (1326.2 examples/sec; 0.097 sec/batch)
2017-06-02 16:52:47.531449: step 6710, loss = 1.06 (1670.1 examples/sec; 0.077 sec/batch)
2017-06-02 16:52:48.365292: step 6720, loss = 1.06 (1535.1 examples/sec; 0.083 sec/batch)
2017-06-02 16:52:49.240995: step 6730, loss = 0.95 (1461.7 examples/sec; 0.088 sec/batch)
2017-06-02 16:52:50.116187: step 6740, loss = 1.00 (1462.5 examples/sec; 0.088 sec/batch)
2017-06-02 16:52:50.933507: step 6750, loss = 0.90 (1566.1 examples/sec; 0.082 sec/batch)
2017-06-02 16:52:51.804472: step 6760, loss = 0.91 (1469.6 examples/sec; 0.087 sec/batch)
2017-06-02 16:52:52.651480: step 6770, loss = 0.96 (1511.2 examples/sec; 0.085 sec/batch)
2017-06-02 16:52:53.468674: step 6780, loss = 0.85 (1566.3 examples/sec; 0.082 sec/batch)
2017-06-02 16:52:54.311654: step 6790, loss = 1.04 (1518.4 examples/sec; 0.084 sec/batch)
2017-06-02 16:52:55.307706: step 6800, loss = 0.98 (1285.1 examples/sec; 0.100 sec/batch)
2017-06-02 16:52:56.015665: step 6810, loss = 0.99 (1808.0 examples/sec; 0.071 sec/batch)
2017-06-02 16:52:56.910263: step 6820, loss = 1.17 (1430.8 examples/sec; 0.089 sec/batch)
2017-06-02 16:52:57.816542: step 6830, loss = 1.07 (1412.4 examples/sec; 0.091 sec/batch)
2017-06-02 16:52:58.629450: step 6840, loss = 0.88 (1574.6 examples/sec; 0.081 sec/batch)
2017-06-02 16:52:59.505715: step 6850, loss = 1.00 (1460.7 examples/sec; 0.088 sec/batch)
2017-06-02 16:53:00.360562: step 6860, loss = 1.00 (1497.3 examples/sec; 0.085 sec/batch)
2017-06-02 16:53:01.185354: step 6870, loss = 0.76 (1551.9 examples/sec; 0.082 sec/batch)
2017-06-02 16:53:02.089414: step 6880, loss = 1.07 (1415.8 examples/sec; 0.090 sec/batch)
2017-06-02 16:53:02.947808: step 6890, loss = 0.93 (1491.2 examples/sec; 0.086 sec/batch)
2017-06-02 16:53:03.892691: step 6900, loss = 0.76 (1354.7 examples/sec; 0.094 sec/batch)
2017-06-02 16:53:04.660685: step 6910, loss = 0.96 (1666.7 examples/sec; 0.077 sec/batch)
2017-06-02 16:53:05.554319: step 6920, loss = 0.90 (1432.4 examples/sec; 0.089 sec/batch)
2017-06-02 16:53:06.401151: step 6930, loss = 1.02 (1511.5 examples/sec; 0.085 sec/batch)
2017-06-02 16:53:07.283098: step 6940, loss = 0.87 (1451.3 examples/sec; 0.088 sec/batch)
2017-06-02 16:53:08.157267: step 6950, loss = 0.91 (1464.2 examples/sec; 0.087 sec/batch)
2017-06-02 16:53:09.004531: step 6960, loss = 1.19 (1510.7 examples/sec; 0.085 sec/batch)
2017-06-02 16:53:09.910667: step 6970, loss = 0.82 (1412.6 examples/sec; 0.091 sec/batch)
2017-06-02 16:53:10.781382: step 6980, loss = 0.87 (1470.1 examples/sec; 0.087 sec/batch)
2017-06-02 16:53:11.597943: step 6990, loss = 0.92 (1567.6 examples/sec; 0.082 sec/batch)
2017-06-02 16:53:12.586462: step 7000, loss = 0.96 (1294.9 examples/sec; 0.099 sec/batch)
2017-06-02 16:53:13.365866: step 7010, loss = 0.90 (1642.3 examples/sec; 0.078 sec/batch)
2017-06-02 16:53:14.202378: step 7020, loss = 0.83 (1530.2 examples/sec; 0.084 sec/batch)
2017-06-02 16:53:15.075355: step 7030, loss = 0.97 (1466.2 examples/sec; 0.087 sec/batch)
2017-06-02 16:53:15.982908: step 7040, loss = 0.97 (1410.4 examples/sec; 0.091 sec/batch)
2017-06-02 16:53:16.842188: step 7050, loss = 0.86 (1489.6 examples/sec; 0.086 sec/batch)
2017-06-02 16:53:17.700435: step 7060, loss = 0.89 (1491.4 examples/sec; 0.086 sec/batch)
2017-06-02 16:53:18.554217: step 7070, loss = 0.95 (1499.3 examples/sec; 0.085 sec/batch)
2017-06-02 16:53:19.421156: step 7080, loss = 0.89 (1476.4 examples/sec; 0.087 sec/batch)
2017-06-02 16:53:20.276410: step 7090, loss = 0.94 (1496.6 examples/sec; 0.086 sec/batch)
2017-06-02 16:53:21.316847: step 7100, loss = 0.72 (1230.2 examples/sec; 0.104 sec/batch)
2017-06-02 16:53:22.413479: step 7110, loss = 0.85 (1167.2 examples/sec; 0.110 sec/batch)
2017-06-02 16:53:23.721529: step 7120, loss = 0.94 (978.6 examples/sec; 0.131 sec/batch)
2017-06-02 16:53:24.898853: step 7130, loss = 1.04 (1087.2 examples/sec; 0.118 sec/batch)
2017-06-02 16:53:25.743619: step 7140, loss = 0.85 (1515.2 examples/sec; 0.084 sec/batch)
2017-06-02 16:53:26.690741: step 7150, loss = 0.87 (1351.5 examples/sec; 0.095 sec/batch)
2017-06-02 16:53:27.860419: step 7160, loss = 0.93 (1094.3 examples/sec; 0.117 sec/batch)
2017-06-02 16:53:29.094754: step 7170, loss = 0.78 (1037.0 examples/sec; 0.123 sec/batch)
2017-06-02 16:53:30.264435: step 7180, loss = 1.15 (1094.3 examples/sec; 0.117 sec/batch)
2017-06-02 16:53:31.094777: step 7190, loss = 1.03 (1541.5 examples/sec; 0.083 sec/batch)
2017-06-02 16:53:32.072700: step 7200, loss = 0.98 (1308.9 examples/sec; 0.098 sec/batch)
2017-06-02 16:53:32.838821: step 7210, loss = 1.06 (1670.8 examples/sec; 0.077 sec/batch)
2017-06-02 16:53:33.683506: step 7220, loss = 0.94 (1515.4 examples/sec; 0.084 sec/batch)
2017-06-02 16:53:34.530094: step 7230, loss = 0.92 (1511.9 examples/sec; 0.085 sec/batch)
2017-06-02 16:53:35.411750: step 7240, loss = 0.83 (1451.8 examples/sec; 0.088 sec/batch)
2017-06-02 16:53:36.289208: step 7250, loss = 0.79 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 16:53:37.123301: step 7260, loss = 0.83 (1534.6 examples/sec; 0.083 sec/batch)
2017-06-02 16:53:38.012293: step 7270, loss = 0.92 (1439.8 examples/sec; 0.089 sec/batch)
2017-06-02 16:53:38.901587: step 7280, loss = 0.87 (1439.3 examples/sec; 0.089 sec/batch)
2017-06-02 16:53:39.732573: step 7290, loss = 0.89 (1540.3 examples/sec; 0.083 sec/batch)
2017-06-02 16:53:40.722798: step 7300, loss = 0.84 (1292.6 examples/sec; 0.099 sec/batch)
2017-06-02 16:53:41.516502: step 7310, loss = 0.85 (1612.7 examples/sec; 0.079 sec/batch)
2017-06-02 16:53:42.343193: step 7320, loss = 0.90 (1548.3 examples/sec; 0.083 sec/batch)
2017-06-02 16:53:43.235079: step 7330, loss = 0.82 (1435.2 examples/sec; 0.089 sec/batch)
2017-06-02 16:53:44.094662: step 7340, loss = 1.00 (1489.1 examples/sec; 0.086 sec/batch)
2017-06-02 16:53:44.978706: step 7350, loss = 0.96 (1447.8 examples/sec; 0.088 sec/batch)
2017-06-02 16:53:45.809841: step 7360, loss = 0.86 (1540.1 examples/sec; 0.083 sec/batch)
2017-06-02 16:53:46.703000: step 7370, loss = 1.03 (1433.1 examples/sec; 0.089 sec/batch)
2017-06-02 16:53:47.609544: step 7380, loss = 0.94 (1412.0 examples/sec; 0.091 sec/batch)
2017-06-02 16:53:48.436302: step 7390, loss = 0.92 (1548.2 examples/sec; 0.083 sec/batch)
2017-06-02 16:53:49.412317: step 7400, loss = 0.95 (1311.4 examples/sec; 0.098 sec/batch)
2017-06-02 16:53:50.203761: step 7410, loss = 0.93 (1617.3 examples/sec; 0.079 sec/batch)
2017-06-02 16:53:51.047311: step 7420, loss = 0.86 (1517.4 examples/sec; 0.084 sec/batch)
2017-06-02 16:53:51.944053: step 7430, loss = 0.90 (1427.4 examples/sec; 0.090 sec/batch)
2017-06-02 16:53:52.851241: step 7440, loss = 0.93 (1411.0 examples/sec; 0.091 sec/batch)
2017-06-02 16:53:53.714633: step 7450, loss = 0.93 (1482.5 examples/sec; 0.086 sec/batch)
2017-06-02 16:53:54.533740: step 7460, loss = 1.05 (1562.7 examples/sec; 0.082 sec/batch)
2017-06-02 16:53:55.420023: step 7470, loss = 0.96 (1444.2 examples/sec; 0.089 sec/batch)
2017-06-02 16:53:56.299481: step 7480, loss = 0.85 (1455.4 examples/sec; 0.088 sec/batch)
2017-06-02 16:53:57.146632: step 7490, loss = 0.85 (1510.9 examples/sec; 0.085 sec/batch)
2017-06-02 16:53:58.111964: step 7500, loss = 0.73 (1326.0 examples/sec; 0.097 sec/batch)
2017-06-02 16:53:58.900570: step 7510, loss = 1.06 (1623.1 examples/sec; 0.079 sec/batch)
2017-06-02 16:53:59.777027: step 7520, loss = 0.96 (1460.4 examples/sec; 0.088 sec/batch)
2017-06-02 16:54:00.602686: step 7530, loss = 0.95 (1550.3 examples/sec; 0.083 sec/batch)
2017-06-02 16:54:01.472950: step 7540, loss = 0.85 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 16:54:02.363357: step 7550, loss = 0.86 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 16:54:03.194719: step 7560, loss = 1.13 (1539.6 examples/sec; 0.083 sec/batch)
2017-06-02 16:54:04.086271: step 7570, loss = 1.09 (1435.7 examples/sec; 0.089 sec/batch)
2017-06-02 16:54:04.951231: step 7580, loss = 0.90 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 16:54:05.824560: step 7590, loss = 0.85 (1465.7 examples/sec; 0.087 sec/batch)
2017-06-02 16:54:06.752716: step 7600, loss = 0.79 (1379.1 examples/sec; 0.093 sec/batch)
2017-06-02 16:54:07.530796: step 7610, loss = 0.96 (1645.1 examples/sec; 0.078 sec/batch)
2017-06-02 16:54:08.403877: step 7620, loss = 0.81 (1466.1 examples/sec; 0.087 sec/batch)
2017-06-02 16:54:09.217039: step 7630, loss = 1.05 (1574.1 examples/sec; 0.081 sec/batch)
2017-06-02 16:54:10.054384: step 7640, loss = 0.99 (1528.6 examples/sec; 0.084 sec/batch)
2017-06-02 16:54:10.929196: step 7650, loss = 0.97 (1463.2 examples/sec; 0.087 sec/batch)
2017-06-02 16:54:11.777434: step 7660, loss = 0.77 (1509.0 examples/sec; 0.085 sec/batch)
2017-06-02 16:54:12.638764: step 7670, loss = 0.90 (1486.1 examples/sec; 0.086 sec/batch)
2017-06-02 16:54:13.491880: step 7680, loss = 1.00 (1500.4 examples/sec; 0.085 sec/batch)
2017-06-02 16:54:14.354088: step 7690, loss = 0.99 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 16:54:15.311663: step 7700, loss = 1.05 (1336.7 examples/sec; 0.096 sec/batch)
2017-06-02 16:54:16.101515: step 7710, loss = 0.96 (1620.5 examples/sec; 0.079 sec/batch)
2017-06-02 16:54:16.969811: step 7720, loss = 0.86 (1474.2 examples/sec; 0.087 sec/batch)
2017-06-02 16:54:17.801456: step 7730, loss = 0.94 (1539.1 examples/sec; 0.083 sec/batch)
2017-06-02 16:54:18.649919: step 7740, loss = 1.03 (1508.6 examples/sec; 0.085 sec/batch)
2017-06-02 16:54:19.523422: step 7750, loss = 0.98 (1465.4 examples/sec; 0.087 sec/batch)
2017-06-02 16:54:20.355350: step 7760, loss = 0.98 (1538.6 examples/sec; 0.083 sec/batch)
2017-06-02 16:54:21.230356: step 7770, loss = 0.96 (1462.9 examples/sec; 0.088 sec/batch)
2017-06-02 16:54:22.099359: step 7780, loss = 0.95 (1473.0 examples/sec; 0.087 sec/batch)
2017-06-02 16:54:22.930244: step 7790, loss = 0.94 (1540.5 examples/sec; 0.083 sec/batch)
2017-06-02 16:54:23.904441: step 7800, loss = 0.79 (1313.9 examples/sec; 0.097 sec/batch)
2017-06-02 16:54:24.699773: step 7810, loss = 0.75 (1609.4 examples/sec; 0.080 sec/batch)
2017-06-02 16:54:25.597714: step 7820, loss = 0.85 (1425.5 examples/sec; 0.090 sec/batch)
2017-06-02 16:54:26.442764: step 7830, loss = 0.83 (1514.7 examples/sec; 0.085 sec/batch)
2017-06-02 16:54:27.333156: step 7840, loss = 0.97 (1437.5 examples/sec; 0.089 sec/batch)
2017-06-02 16:54:28.240803: step 7850, loss = 0.91 (1410.2 examples/sec; 0.091 sec/batch)
2017-06-02 16:54:29.143572: step 7860, loss = 1.24 (1417.9 examples/sec; 0.090 sec/batch)
2017-06-02 16:54:29.961034: step 7870, loss = 0.98 (1565.8 examples/sec; 0.082 sec/batch)
2017-06-02 16:54:30.830956: step 7880, loss = 0.87 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 16:54:31.701724: step 7890, loss = 0.86 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 16:54:32.619995: step 7900, loss = 0.90 (1393.9 examples/sec; 0.092 sec/batch)
2017-06-02 16:54:33.420324: step 7910, loss = 0.89 (1599.3 examples/sec; 0.080 sec/batch)
2017-06-02 16:54:34.307283: step 7920, loss = 0.90 (1443.1 examples/sec; 0.089 sec/batch)
2017-06-02 16:54:35.161083: step 7930, loss = 0.99 (1499.2 examples/sec; 0.085 sec/batch)
2017-06-02 16:54:36.004850: step 7940, loss = 0.96 (1517.0 examples/sec; 0.084 sec/batch)
2017-06-02 16:54:36.859175: step 7950, loss = 1.13 (1498.3 examples/sec; 0.085 sec/batch)
2017-06-02 16:54:37.730483: step 7960, loss = 0.83 (1469.1 examples/sec; 0.087 sec/batch)
2017-06-02 16:54:38.545541: step 7970, loss = 0.90 (1570.4 examples/sec; 0.082 sec/batch)
2017-06-02 16:54:39.417993: step 7980, loss = 0.97 (1467.1 examples/sec; 0.087 sec/batch)
2017-06-02 16:54:40.276630: step 7990, loss = 1.00 (1490.7 examples/sec; 0.086 sec/batch)
2017-06-02 16:54:41.210593: step 8000, loss = 0.92 (1370.5 examples/sec; 0.093 sec/batch)
2017-06-02 16:54:42.001803: step 8010, loss = 0.84 (1617.8 examples/sec; 0.079 sec/batch)
2017-06-02 16:54:42.887798: step 8020, loss = 0.76 (1444.7 examples/sec; 0.089 sec/batch)
2017-06-02 16:54:43.696070: step 8030, loss = 0.90 (1583.6 examples/sec; 0.081 sec/batch)
2017-06-02 16:54:44.577248: step 8040, loss = 0.83 (1452.6 examples/sec; 0.088 sec/batch)
2017-06-02 16:54:45.465747: step 8050, loss = 0.80 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 16:54:46.297414: step 8060, loss = 0.94 (1539.1 examples/sec; 0.083 sec/batch)
2017-06-02 16:54:47.182893: step 8070, loss = 0.86 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 16:54:48.067950: step 8080, loss = 0.89 (1446.2 examples/sec; 0.089 sec/batch)
2017-06-02 16:54:48.926029: step 8090, loss = 0.99 (1491.7 examples/sec; 0.086 sec/batch)
2017-06-02 16:54:49.858473: step 8100, loss = 1.01 (1372.7 examples/sec; 0.093 sec/batch)
2017-06-02 16:54:50.618649: step 8110, loss = 0.99 (1683.8 examples/sec; 0.076 sec/batch)
2017-06-02 16:54:51.490561: step 8120, loss = 0.85 (1468.0 examples/sec; 0.087 sec/batch)
2017-06-02 16:54:52.333710: step 8130, loss = 0.96 (1518.1 examples/sec; 0.084 sec/batch)
2017-06-02 16:54:53.218362: step 8140, loss = 0.81 (1446.9 examples/sec; 0.088 sec/batch)
2017-06-02 16:54:54.108891: step 8150, loss = 0.96 (1437.3 examples/sec; 0.089 sec/batch)
2017-06-02 16:54:54.915859: step 8160, loss = 0.93 (1586.2 examples/sec; 0.081 sec/batch)
2017-06-02 16:54:55.789215: step 8170, loss = 1.03 (1465.6 examples/sec; 0.087 sec/batch)
2017-06-02 16:54:56.659460: step 8180, loss = 0.79 (1470.8 examples/sec; 0.087 sec/batch)
2017-06-02 16:54:57.518852: step 8190, loss = 0.99 (1489.4 examples/sec; 0.086 sec/batch)
2017-06-02 16:54:58.450142: step 8200, loss = 0.95 (1374.4 examples/sec; 0.093 sec/batch)
2017-06-02 16:54:59.265425: step 8210, loss = 0.91 (1570.0 examples/sec; 0.082 sec/batch)
2017-06-02 16:55:00.180723: step 8220, loss = 0.79 (1398.4 examples/sec; 0.092 sec/batch)
2017-06-02 16:55:00.995876: step 8230, loss = 0.96 (1570.3 examples/sec; 0.082 sec/batch)
2017-06-02 16:55:01.853746: step 8240, loss = 0.82 (1492.1 examples/sec; 0.086 sec/batch)
2017-06-02 16:55:02.754017: step 8250, loss = 1.04 (1421.8 examples/sec; 0.090 sec/batch)
2017-06-02 16:55:03.653763: step 8260, loss = 1.03 (1422.6 examples/sec; 0.090 sec/batch)
2017-06-02 16:55:04.492024: step 8270, loss = 0.96 (1527.0 examples/sec; 0.084 sec/batch)
2017-06-02 16:55:05.349570: step 8280, loss = 1.03 (1492.6 examples/sec; 0.086 sec/batch)
2017-06-02 16:55:06.220722: step 8290, loss = 0.97 (1469.3 examples/sec; 0.087 sec/batch)
2017-06-02 16:55:07.171234: step 8300, loss = 0.97 (1346.6 examples/sec; 0.095 sec/batch)
2017-06-02 16:55:07.922135: step 8310, loss = 0.75 (1704.6 examples/sec; 0.075 sec/batch)
2017-06-02 16:55:08.763004: step 8320, loss = 0.86 (1522.2 examples/sec; 0.084 sec/batch)
2017-06-02 16:55:09.617050: step 8330, loss = 0.91 (1498.7 examples/sec; 0.085 sec/batch)
2017-06-02 16:55:10.455327: step 8340, loss = 0.93 (1526.9 examples/sec; 0.084 sec/batch)
2017-06-02 16:55:11.316860: step 8350, loss = 0.81 (1485.7 examples/sec; 0.086 sec/batch)
2017-06-02 16:55:12.195186: step 8360, loss = 0.89 (1457.3 examples/sec; 0.088 sec/batch)
2017-06-02 16:55:13.007662: step 8370, loss = 0.85 (1575.4 examples/sec; 0.081 sec/batch)
2017-06-02 16:55:13.889362: step 8380, loss = 1.07 (1451.7 examples/sec; 0.088 sec/batch)
2017-06-02 16:55:14.786230: step 8390, loss = 0.87 (1427.2 examples/sec; 0.090 sec/batch)
2017-06-02 16:55:15.729440: step 8400, loss = 1.08 (1357.1 examples/sec; 0.094 sec/batch)
2017-06-02 16:55:16.479576: step 8410, loss = 0.90 (1706.3 examples/sec; 0.075 sec/batch)
2017-06-02 16:55:17.337627: step 8420, loss = 0.93 (1491.8 examples/sec; 0.086 sec/batch)
2017-06-02 16:55:18.203007: step 8430, loss = 0.96 (1479.1 examples/sec; 0.087 sec/batch)
2017-06-02 16:55:19.080271: step 8440, loss = 0.86 (1459.1 examples/sec; 0.088 sec/batch)
2017-06-02 16:55:19.983845: step 8450, loss = 0.80 (1416.6 examples/sec; 0.090 sec/batch)
2017-06-02 16:55:20.847937: step 8460, loss = 0.89 (1481.3 examples/sec; 0.086 sec/batch)
2017-06-02 16:55:21.683120: step 8470, loss = 0.93 (1532.6 examples/sec; 0.084 sec/batch)
2017-06-02 16:55:22.558201: step 8480, loss = 0.99 (1462.7 examples/sec; 0.088 sec/batch)
2017-06-02 16:55:23.432268: step 8490, loss = 0.86 (1464.4 examples/sec; 0.087 sec/batch)
2017-06-02 16:55:24.369379: step 8500, loss = 0.90 (1365.9 examples/sec; 0.094 sec/batch)
2017-06-02 16:55:25.141670: step 8510, loss = 0.91 (1657.4 examples/sec; 0.077 sec/batch)
2017-06-02 16:55:26.003076: step 8520, loss = 0.84 (1485.9 examples/sec; 0.086 sec/batch)
2017-06-02 16:55:26.845391: step 8530, loss = 0.99 (1519.6 examples/sec; 0.084 sec/batch)
2017-06-02 16:55:27.727453: step 8540, loss = 1.12 (1451.1 examples/sec; 0.088 sec/batch)
2017-06-02 16:55:28.601754: step 8550, loss = 1.07 (1464.1 examples/sec; 0.087 sec/batch)
2017-06-02 16:55:29.436512: step 8560, loss = 0.67 (1533.3 examples/sec; 0.083 sec/batch)
2017-06-02 16:55:30.324347: step 8570, loss = 0.85 (1441.7 examples/sec; 0.089 sec/batch)
2017-06-02 16:55:31.213879: step 8580, loss = 1.11 (1439.0 examples/sec; 0.089 sec/batch)
2017-06-02 16:55:32.086783: step 8590, loss = 0.85 (1466.4 examples/sec; 0.087 sec/batch)
2017-06-02 16:55:33.064714: step 8600, loss = 0.97 (1308.9 examples/sec; 0.098 sec/batch)
2017-06-02 16:55:33.805894: step 8610, loss = 0.80 (1727.0 examples/sec; 0.074 sec/batch)
2017-06-02 16:55:34.672682: step 8620, loss = 0.86 (1476.7 examples/sec; 0.087 sec/batch)
2017-06-02 16:55:35.513704: step 8630, loss = 1.20 (1522.0 examples/sec; 0.084 sec/batch)
2017-06-02 16:55:36.392321: step 8640, loss = 0.85 (1456.8 examples/sec; 0.088 sec/batch)
2017-06-02 16:55:37.257285: step 8650, loss = 0.90 (1479.8 examples/sec; 0.086 sec/batch)
2017-06-02 16:55:38.074852: step 8660, loss = 0.90 (1565.6 examples/sec; 0.082 sec/batch)
2017-06-02 16:55:38.954800: step 8670, loss = 0.86 (1454.6 examples/sec; 0.088 sec/batch)
2017-06-02 16:55:39.851261: step 8680, loss = 0.94 (1427.8 examples/sec; 0.090 sec/batch)
2017-06-02 16:55:40.675635: step 8690, loss = 0.78 (1552.7 examples/sec; 0.082 sec/batch)
2017-06-02 16:55:41.680405: step 8700, loss = 1.02 (1273.9 examples/sec; 0.100 sec/batch)
2017-06-02 16:55:42.471298: step 8710, loss = 0.87 (1618.4 examples/sec; 0.079 sec/batch)
2017-06-02 16:55:43.327678: step 8720, loss = 1.00 (1494.7 examples/sec; 0.086 sec/batch)
2017-06-02 16:55:44.151542: step 8730, loss = 0.66 (1553.6 examples/sec; 0.082 sec/batch)
2017-06-02 16:55:45.052764: step 8740, loss = 1.00 (1420.3 examples/sec; 0.090 sec/batch)
2017-06-02 16:55:45.920555: step 8750, loss = 0.82 (1475.0 examples/sec; 0.087 sec/batch)
2017-06-02 16:55:46.743448: step 8760, loss = 1.01 (1555.5 examples/sec; 0.082 sec/batch)
2017-06-02 16:55:47.626252: step 8770, loss = 0.96 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 16:55:48.497706: step 8780, loss = 0.82 (1468.8 examples/sec; 0.087 sec/batch)
2017-06-02 16:55:49.341528: step 8790, loss = 0.88 (1516.9 examples/sec; 0.084 sec/batch)
2017-06-02 16:55:50.317090: step 8800, loss = 0.92 (1312.1 examples/sec; 0.098 sec/batch)
2017-06-02 16:55:51.072774: step 8810, loss = 1.04 (1693.8 examples/sec; 0.076 sec/batch)
2017-06-02 16:55:51.912625: step 8820, loss = 1.15 (1524.1 examples/sec; 0.084 sec/batch)
2017-06-02 16:55:52.787932: step 8830, loss = 0.90 (1462.3 examples/sec; 0.088 sec/batch)
2017-06-02 16:55:53.677399: step 8840, loss = 1.03 (1439.1 examples/sec; 0.089 sec/batch)
2017-06-02 16:55:54.549584: step 8850, loss = 0.75 (1467.6 examples/sec; 0.087 sec/batch)
2017-06-02 16:55:55.419512: step 8860, loss = 0.83 (1471.4 examples/sec; 0.087 sec/batch)
2017-06-02 16:55:56.343804: step 8870, loss = 0.73 (1384.8 examples/sec; 0.092 sec/batch)
2017-06-02 16:55:57.211729: step 8880, loss = 0.83 (1474.8 examples/sec; 0.087 sec/batch)
2017-06-02 16:55:58.040969: step 8890, loss = 0.93 (1543.6 examples/sec; 0.083 sec/batch)
2017-06-02 16:55:59.016940: step 8900, loss = 0.84 (1311.5 examples/sec; 0.098 sec/batch)
2017-06-02 16:55:59.786219: step 8910, loss = 0.72 (1663.9 examples/sec; 0.077 sec/batch)
2017-06-02 16:56:00.617947: step 8920, loss = 0.99 (1539.0 examples/sec; 0.083 sec/batch)
2017-06-02 16:56:01.482371: step 8930, loss = 0.78 (1480.7 examples/sec; 0.086 sec/batch)
2017-06-02 16:56:02.327544: step 8940, loss = 1.00 (1514.5 examples/sec; 0.085 sec/batch)
2017-06-02 16:56:03.139865: step 8950, loss = 0.84 (1575.7 examples/sec; 0.081 sec/batch)
2017-06-02 16:56:04.011551: step 8960, loss = 0.89 (1468.4 examples/sec; 0.087 sec/batch)
2017-06-02 16:56:04.902413: step 8970, loss = 1.11 (1436.8 examples/sec; 0.089 sec/batch)
2017-06-02 16:56:05.726922: step 8980, loss = 0.88 (1552.4 examples/sec; 0.082 sec/batch)
2017-06-02 16:56:06.599275: step 8990, loss = 0.85 (1467.3 examples/sec; 0.087 sec/batch)
2017-06-02 16:56:07.557209: step 9000, loss = 0.86 (1336.2 examples/sec; 0.096 sec/batch)
2017-06-02 16:56:08.313478: step 9010, loss = 1.15 (1692.5 examples/sec; 0.076 sec/batch)
2017-06-02 16:56:09.127681: step 9020, loss = 0.84 (1572.1 examples/sec; 0.081 sec/batch)
2017-06-02 16:56:09.988488: step 9030, loss = 0.90 (1487.0 examples/sec; 0.086 sec/batch)
2017-06-02 16:56:10.862396: step 9040, loss = 0.74 (1464.7 examples/sec; 0.087 sec/batch)
2017-06-02 16:56:11.701389: step 9050, loss = 0.89 (1525.6 examples/sec; 0.084 sec/batch)
2017-06-02 16:56:12.584209: step 9060, loss = 0.90 (1449.9 examples/sec; 0.088 sec/batch)
2017-06-02 16:56:13.484344: step 9070, loss = 0.90 (1422.0 examples/sec; 0.090 sec/batch)
2017-06-02 16:56:14.315323: step 9080, loss = 0.81 (1540.4 examples/sec; 0.083 sec/batch)
2017-06-02 16:56:15.177910: step 9090, loss = 1.02 (1483.9 examples/sec; 0.086 sec/batch)
2017-06-02 16:56:16.169331: step 9100, loss = 0.81 (1291.1 examples/sec; 0.099 sec/batch)
2017-06-02 16:56:16.951847: step 9110, loss = 0.87 (1635.7 examples/sec; 0.078 sec/batch)
2017-06-02 16:56:17.774915: step 9120, loss = 0.82 (1555.2 examples/sec; 0.082 sec/batch)
2017-06-02 16:56:18.673812: step 9130, loss = 0.88 (1424.0 examples/sec; 0.090 sec/batch)
2017-06-02 16:56:19.577863: step 9140, loss = 0.91 (1415.9 examples/sec; 0.090 sec/batch)
2017-06-02 16:56:20.406182: step 9150, loss = 1.01 (1545.3 examples/sec; 0.083 sec/batch)
2017-06-02 16:56:21.292084: step 9160, loss = 0.93 (1444.9 examples/sec; 0.089 sec/batch)
2017-06-02 16:56:22.137951: step 9170, loss = 0.76 (1513.3 examples/sec; 0.085 sec/batch)
2017-06-02 16:56:22.958816: step 9180, loss = 0.89 (1559.3 examples/sec; 0.082 sec/batch)
2017-06-02 16:56:23.822534: step 9190, loss = 0.84 (1482.0 examples/sec; 0.086 sec/batch)
2017-06-02 16:56:24.810842: step 9200, loss = 0.77 (1295.1 examples/sec; 0.099 sec/batch)
2017-06-02 16:56:25.519885: step 9210, loss = 0.86 (1805.3 examples/sec; 0.071 sec/batch)
2017-06-02 16:56:26.356578: step 9220, loss = 0.90 (1529.9 examples/sec; 0.084 sec/batch)
2017-06-02 16:56:27.206598: step 9230, loss = 1.00 (1505.8 examples/sec; 0.085 sec/batch)
2017-06-02 16:56:28.019296: step 9240, loss = 1.07 (1575.0 examples/sec; 0.081 sec/batch)
2017-06-02 16:56:28.908696: step 9250, loss = 1.15 (1439.2 examples/sec; 0.089 sec/batch)
2017-06-02 16:56:29.787807: step 9260, loss = 0.88 (1456.0 examples/sec; 0.088 sec/batch)
2017-06-02 16:56:30.676060: step 9270, loss = 0.81 (1441.0 examples/sec; 0.089 sec/batch)
2017-06-02 16:56:31.518415: step 9280, loss = 0.89 (1519.5 examples/sec; 0.084 sec/batch)
2017-06-02 16:56:32.428938: step 9290, loss = 0.94 (1405.8 examples/sec; 0.091 sec/batch)
2017-06-02 16:56:33.391782: step 9300, loss = 0.87 (1329.4 examples/sec; 0.096 sec/batch)
2017-06-02 16:56:34.109174: step 9310, loss = 0.87 (1784.3 examples/sec; 0.072 sec/batch)
2017-06-02 16:56:34.991950: step 9320, loss = 0.86 (1450.0 examples/sec; 0.088 sec/batch)
2017-06-02 16:56:35.865728: step 9330, loss = 0.90 (1464.9 examples/sec; 0.087 sec/batch)
2017-06-02 16:56:36.736494: step 9340, loss = 0.78 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 16:56:37.566957: step 9350, loss = 0.92 (1541.3 examples/sec; 0.083 sec/batch)
2017-06-02 16:56:38.445561: step 9360, loss = 0.92 (1456.9 examples/sec; 0.088 sec/batch)
2017-06-02 16:56:39.311856: step 9370, loss = 0.96 (1477.5 examples/sec; 0.087 sec/batch)
2017-06-02 16:56:40.157471: step 9380, loss = 0.89 (1513.7 examples/sec; 0.085 sec/batch)
2017-06-02 16:56:41.022382: step 9390, loss = 0.95 (1479.9 examples/sec; 0.086 sec/batch)
2017-06-02 16:56:41.991549: step 9400, loss = 0.86 (1320.7 examples/sec; 0.097 sec/batch)
2017-06-02 16:56:42.714980: step 9410, loss = 0.89 (1769.3 examples/sec; 0.072 sec/batch)
2017-06-02 16:56:43.574924: step 9420, loss = 0.87 (1488.5 examples/sec; 0.086 sec/batch)
2017-06-02 16:56:44.451119: step 9430, loss = 0.97 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 16:56:45.285249: step 9440, loss = 0.77 (1534.5 examples/sec; 0.083 sec/batch)
2017-06-02 16:56:46.125979: step 9450, loss = 0.85 (1522.5 examples/sec; 0.084 sec/batch)
2017-06-02 16:56:46.995427: step 9460, loss = 0.83 (1472.2 examples/sec; 0.087 sec/batch)
2017-06-02 16:56:47.844068: step 9470, loss = 0.76 (1508.3 examples/sec; 0.085 sec/batch)
2017-06-02 16:56:48.649777: step 9480, loss = 0.81 (1588.7 examples/sec; 0.081 sec/batch)
2017-06-02 16:56:49.529016: step 9490, loss = 0.91 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 16:56:50.501815: step 9500, loss = 0.89 (1315.8 examples/sec; 0.097 sec/batch)
2017-06-02 16:56:51.256179: step 9510, loss = 0.93 (1696.8 examples/sec; 0.075 sec/batch)
2017-06-02 16:56:52.144674: step 9520, loss = 0.89 (1440.6 examples/sec; 0.089 sec/batch)
2017-06-02 16:56:53.022938: step 9530, loss = 0.69 (1457.4 examples/sec; 0.088 sec/batch)
2017-06-02 16:56:53.884028: step 9540, loss = 0.95 (1486.5 examples/sec; 0.086 sec/batch)
2017-06-02 16:56:54.719370: step 9550, loss = 1.17 (1532.3 examples/sec; 0.084 sec/batch)
2017-06-02 16:56:55.612198: step 9560, loss = 0.94 (1433.7 examples/sec; 0.089 sec/batch)
2017-06-02 16:56:56.513565: step 9570, loss = 0.98 (1420.1 examples/sec; 0.090 sec/batch)
2017-06-02 16:56:57.335233: step 9580, loss = 0.74 (1557.8 examples/sec; 0.082 sec/batch)
2017-06-02 16:56:58.244861: step 9590, loss = 0.87 (1407.2 examples/sec; 0.091 sec/batch)
2017-06-02 16:56:59.234258: step 9600, loss = 0.85 (1293.7 examples/sec; 0.099 sec/batch)
2017-06-02 16:56:59.974860: step 9610, loss = 0.82 (1728.3 examples/sec; 0.074 sec/batch)
2017-06-02 16:57:00.868628: step 9620, loss = 1.06 (1432.1 examples/sec; 0.089 sec/batch)
2017-06-02 16:57:01.735490: step 9630, loss = 0.84 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 16:57:02.574828: step 9640, loss = 0.85 (1525.0 examples/sec; 0.084 sec/batch)
2017-06-02 16:57:03.460265: step 9650, loss = 0.81 (1445.6 examples/sec; 0.089 sec/batch)
2017-06-02 16:57:04.341116: step 9660, loss = 0.95 (1453.1 examples/sec; 0.088 sec/batch)
2017-06-02 16:57:05.204405: step 9670, loss = 0.96 (1482.7 examples/sec; 0.086 sec/batch)
2017-06-02 16:57:06.060707: step 9680, loss = 0.83 (1494.8 examples/sec; 0.086 sec/batch)
2017-06-02 16:57:06.935833: step 9690, loss = 1.03 (1462.6 examples/sec; 0.088 sec/batch)
2017-06-02 16:57:07.902604: step 9700, loss = 0.95 (1324.0 examples/sec; 0.097 sec/batch)
2017-06-02 16:57:08.658957: step 9710, loss = 0.94 (1692.3 examples/sec; 0.076 sec/batch)
2017-06-02 16:57:09.529771: step 9720, loss = 0.97 (1469.9 examples/sec; 0.087 sec/batch)
2017-06-02 16:57:10.401303: step 9730, loss = 0.94 (1468.7 examples/sec; 0.087 sec/batch)
2017-06-02 16:57:11.225626: step 9740, loss = 0.90 (1552.8 examples/sec; 0.082 sec/batch)
2017-06-02 16:57:12.087817: step 9750, loss = 0.88 (1484.6 examples/sec; 0.086 sec/batch)
2017-06-02 16:57:12.935274: step 9760, loss = 0.94 (1510.4 examples/sec; 0.085 sec/batch)
2017-06-02 16:57:13.806003: step 9770, loss = 0.84 (1470.0 examples/sec; 0.087 sec/batch)
2017-06-02 16:57:14.660963: step 9780, loss = 1.00 (1497.1 examples/sec; 0.085 sec/batch)
2017-06-02 16:57:15.527829: step 9790, loss = 1.06 (1476.6 examples/sec; 0.087 sec/batch)
2017-06-02 16:57:16.536250: step 9800, loss = 0.84 (1269.5 examples/sec; 0.101 sec/batch)
2017-06-02 16:57:17.262620: step 9810, loss = 1.01 (1761.9 examples/sec; 0.073 sec/batch)
2017-06-02 16:57:18.140045: step 9820, loss = 0.92 (1458.8 examples/sec; 0.088 sec/batch)
2017-06-02 16:57:19.035143: step 9830, loss = 1.05 (1430.0 examples/sec; 0.090 sec/batch)
2017-06-02 16:57:19.909293: step 9840, loss = 0.71 (1464.3 examples/sec; 0.087 sec/batch)
2017-06-02 16:57:20.746922: step 9850, loss = 0.90 (1528.1 examples/sec; 0.084 sec/batch)
2017-06-02 16:57:21.614194: step 9860, loss = 1.07 (1475.9 examples/sec; 0.087 sec/batch)
2017-06-02 16:57:22.469611: step 9870, loss = 0.74 (1496.4 examples/sec; 0.086 sec/batch)
2017-06-02 16:57:23.298574: step 9880, loss = 0.93 (1544.1 examples/sec; 0.083 sec/batch)
2017-06-02 16:57:24.145840: step 9890, loss = 0.91 (1510.7 examples/sec; 0.085 sec/batch)
2017-06-02 16:57:25.106705: step 9900, loss = 0.76 (1332.1 examples/sec; 0.096 sec/batch)
2017-06-02 16:57:25.843521: step 9910, loss = 0.89 (1737.2 examples/sec; 0.074 sec/batch)
2017-06-02 16:57:26.695931: step 9920, loss = 0.92 (1501.6 examples/sec; 0.085 sec/batch)
2017-06-02 16:57:27.558696: step 9930, loss = 0.83 (1483.6 examples/sec; 0.086 sec/batch)
2017-06-02 16:57:28.434885: step 9940, loss = 0.95 (1460.9 examples/sec; 0.088 sec/batch)
2017-06-02 16:57:29.280014: step 9950, loss = 0.96 (1514.5 examples/sec; 0.085 sec/batch)
2017-06-02 16:57:30.167473: step 9960, loss = 0.86 (1442.3 examples/sec; 0.089 sec/batch)
2017-06-02 16:57:31.046709: step 9970, loss = 0.90 (1455.8 examples/sec; 0.088 sec/batch)
2017-06-02 16:57:31.876815: step 9980, loss = 0.87 (1542.0 examples/sec; 0.083 sec/batch)
2017-06-02 16:57:32.783475: step 9990, loss = 0.87 (1411.8 examples/sec; 0.091 sec/batch)
